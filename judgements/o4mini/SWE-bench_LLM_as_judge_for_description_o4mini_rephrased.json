[
    {
        "astropy__astropy-13033": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue in astropy/timeseries/core.py\u2019s _check_required_columns is clearly described: when a non-time required column is removed, the ValueError misleadingly references the time column. The steps to reproduce, expected vs. actual behavior, and target function are all given. The fix aligns with formatting the message correctly, and the test file astropy/timeseries/tests/test_sampled.py shows exactly what to assert.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a straightforward fix: modify a few lines in _check_required_columns in core.py to update the error message formatting and add a small test in test_sampled.py. An experienced engineer could implement and verify this in well under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues. The description and provided patches are sufficient and the test scenario is self-contained.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-13977": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the problem in Quantity.__array_ufunc__, shows a minimal working example, the expected behavior (return NotImplemented to trigger __radd__), and the current ValueError. It names the method (__array_ufunc__), points out the inconsistent behavior with numpy docs, and even volunteers to open a PR. An experienced developer can work from this text to implement the needed change without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding numpy's ufunc dispatch protocol, studying Astropy's converters_and_unit, modifying __array_ufunc__ to catch TypeError/ValueError and return NotImplemented appropriately, and updating tests to cover various duck types. There are multi-line edits and new test classes (~60 lines), so it would likely take 1\u20134 hours for an experienced engineer to familiarize and implement correctly.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\", \"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14096": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints exactly where the failure occurs: in astropy/coordinates/sky_coordinate.py within __getattr__, it mistakenly raises AttributeError for the property name instead of delegating to __getattribute__, and it even provides a minimal reproduction and expected vs actual behavior. The test file patch clearly shows the regression scenario under test_sky_coord.py.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue requires understanding Python attribute lookup (__getattr__ vs __getattribute__) and making a small change in one method, plus adding a short regression test. An experienced engineer could locate the code and implement it in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14182": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the bug: passing header_rows into the RST writer raises a TypeError because the __init__ signature does not accept that parameter. It specifies the desired behavior (two header rows rendered in RST), points at the exact location (astropy/io/ascii/rst.py, __init__, write, and read methods), and includes sample commands and expected output. The gold patch and tests show precisely what changes are needed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding the FixedWidth and RST writer classes, extending the __init__ to accept header_rows, adjusting write and read to offset by the number of header rows, and adding corresponding tests. It spans two files and involves modifying ~20\u201330 lines. For an experienced engineer, this is straightforward but requires a few hours to become familiar, implement, and test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14309": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description explicitly identifies the failing function (is_fits in astropy/io/fits/connect.py), the exact error (IndexError due to args[0] on empty tuple), and the context (detect_format for FITS). It clearly states the intended behavior (recognize ECSV by extension and avoid IndexError) and provides steps to reproduce, so an engineer can pinpoint and implement the small patch without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves a localized change in one function (is_fits) to adjust the return flow and a single added test. Locating the function, understanding the extension check, and writing the boolean-return change plus a simple regression test would take an experienced engineer less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14365": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the file and function to modify (astropy/io/ascii/qdp.py, _line_type regex and NO value parsing), provides a minimal reproducible example and expected behavior, and even supplies the exact lines to change. It specifies the requirement to ignore case for commands and handle 'NO' in any case, leaving no ambiguity about what constitutes a correct solution.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This fix involves only two small edits: adding the re.IGNORECASE flag to the regex compile call and replacing a direct string comparison with a case-insensitive check (v.upper() == 'NO'). An experienced engineer could locate and implement this change in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14508": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the context (FITS header card formatting), describes the problematic behavior in detail (excessively long float string causing comment truncation), provides step-by-step reproduction steps, and specifies the expected correct behavior. All necessary information for an engineer to implement a fix is present.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding the FITS card length constraints, locating and updating the float formatting logic, and adding corresponding tests. It involves modifying a small helper function and test file but requires familiarity with domain conventions and careful handling of string formatting, which would take an experienced engineer a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14995": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the bug in NDDataRef mask propagation when combining masked and unmasked operands using handle_mask=np.bitwise_or. It pinpoints the file (astropy/nddata/mixins/ndarithmetic.py), the method (_arithmetic_mask), the erroneous branch (elif operand is None vs operand.mask is None), and provides concrete reproduction steps, version info, expected vs actual behavior. This gives sufficient detail to implement the correct conditional logic.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer familiar with the codebase would need to locate the mask\u2010arithmetic code (ndarithmetic.py), understand the propagation logic, modify two lines for the proper mask check, and add a test. This is a small change requiring minor code and test edits, taking under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The reproduction steps, scope, and test requirements are fully contained in the description.\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-7336": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes the bug: the decorator tries to call `.to` on a None return from a constructor annotated -> None. It specifies the exact condition and expected behavior (skip unit conversion on None). The reproducer, workaround, and possible fix guidance give enough context to implement a targeted change.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer would need to locate the decorator code, adjust a simple conditional to exclude None return annotations, and add or modify a test. This is a focused one-line change plus test updates, achievable in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-7606": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the context (unit equality check), the unexpected behavior (TypeError when comparing with None), and the expected behavior (return False). It specifies the code paths (__eq__ methods), desired outcome, and includes reproduction steps, so an engineer can implement a targeted fix without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires modifying two small __eq__ methods to catch exceptions and return NotImplemented instead of raising, plus adding a few tests. Understanding the equality semantics and NotImplemented return is straightforward and implementation is limited in scope.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-7671": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes where the TypeError occurs (in minversion in astropy/utils/introspection.py), explains that LooseVersion fails when comparing numeric components to string labels (e.g. '1.14.3' vs '1.14dev'), and even notes that pkg_resources.parse_version worked correctly. The desired outcome is to prevent the TypeError by normalizing or stripping non-numeric suffixes before passing to LooseVersion. There is no ambiguity: an engineer knows to locate the minversion function, add logic to sanitize the input version string (e.g. via regex), and update tests accordingly.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires modifying a single function (minversion) by inserting a small regex-based transformation and adding one test case. An experienced engineer could understand the bug and implement a regex or alternative patch within 15\u201360 minutes, including writing and running the updated test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-10554": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description gives step-by-step reproduction instructions for Django querysets: how two filtered querysets are unioned, how ordering is applied and then cleared, and the resulting SQL error (\u201cORDER BY position not in select list\u201d). It references specific functions (get_order_by in compiler.py) and expected vs. actual behavior, making it clear what change is needed to fix the bug.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s SQL compiler internals, modifying get_order_by to handle ordering terms that aren\u2019t aliased, adding a new helper in query.py, and writing appropriate tests. This involves editing multiple files and understanding the select/ordering logic, which would likely take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the sample is self-contained and suitable for benchmarking coding ability.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11087": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides clear reproduction steps, environment details, the specific SQL queries causing the error, and outlines exactly what behavior is undesired: Django loading an unnecessary text_log_error.line field during cascade delete. It names file paths, line numbers, and distinguishes the root cause (unicode decoding) from the broader performance optimization (only select required fields). Therefore it is straightforward to understand the task and what a successful solution entails.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this issue requires understanding Django\\u0019s internal deletion logic in django/db/models/deletion.py, adding helper methods, altering query construction to defer unneeded fields, and writing or updating comprehensive tests. This involves nontrivial edits across multiple methods and comprehension of signal handling, which would take an experienced engineer approximately 1\\u00134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"While the sample is suitable for benchmarking coding ability, it assumes familiarity with Django ORM deletion internals and signal connection logic. This specialized knowledge might advantage candidates with Django experience over those with general programming skills, potentially confounding assessment of pure coding ability.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11265": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the exact behavior: exclude on a FilteredRelation annotation raises a FieldError. It names the failing function (split_exclude in django/db/models/sql/query.py) and shows a failing test (tests/filtered_relation/tests.py). The desired fix is to propagate _filtered_relations into the subquery and adjust the join logic in trim_start. With this information, an experienced engineer can locate the relevant code paths, reproduce the error, and implement the two small patch additions.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves adding a single line to propagate _filtered_relations in split_exclude and adjusting a conditional in trim_start within django/db/models/sql/query.py. An engineer can understand the ORM query building flow from the existing tests and apply the two-line patch in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes exactly what is going wrong in Django\u2019s SQL generation when both OR and AND appear, gives a minimal reproduction (a model with two fields and a CheckConstraint), shows the malformed CREATE TABLE and rename failure, and states the expected behavior (unqualified field names in both clauses). An engineer can locate the relevant code in django/db/models/sql/query.py and tests in tests/migrations/test_operations.py and tests/queries/test_query.py to produce a fix without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s internal Query._add_q recursion, how SimpleCol wrappers are applied, and propagating a flag through several method calls to adjust SQL generation. While the change itself is localized to adding a simple_col parameter (about 10\u201315 lines) in query.py and writing corresponding tests, diagnosing the precise spot and testing across SQLite/Oracle takes a couple of hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\", \"q2_5_confidence\":4}"
    },
    {
        "django__django-11532": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies the failing test (tests/mail/tests.py:368), explains that a non-ASCII hostname encoded with ISO-8859-1 causes a UnicodeEncodeError in message header generation (django/core/mail/message.py:260), and even suggests the high-level fix (convert the domain to punycode before encoding). It points to specific files and lines, includes a reproduction step, and the desired behavior is unambiguous (Message-ID header must use punycode for non-ASCII domains).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer must understand email header encoding, locate all points where domain names are used (in message generation, utils.get_fqdn, validators, html), create a punycode helper, update imports and calls, write tests, and ensure no regressions. Although each change is small, it spans multiple modules, so it would likely require 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue description and test clearly define the expected behavior and scope.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the faulty function (_check_list_display_item in django/contrib/admin/checks.py), enumerates the specific logical branches that are incorrect after a recent commit, provides a detailed reproduction scenario (PositionField descriptor raising AttributeError on class access), and supplies the expected behavior matrix. The gold patch diff pinpoints exactly where to replace the hasattr(obj.model, item) check with a try/except around get_field and fallback to getattr, plus handling ManyToManyField errors. This is sufficient for an engineer to implement and verify the fix without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s admin check framework, reading through the attached behavior table, adjusting control flow in one ~40-line function, adding a new test case, and verifying all existing branch behaviors. An experienced Django engineer could familiarize themselves and implement the patch in a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns. The sample is self-contained, the reproduction is clear, and the tests are provided.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11734": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines a reproducible failure when using exclude() or a negated Q expression with OuterRef in Django queries, specifies the expected behavior, and pinpoints the runtime ValueError. It names the test file and steps to reproduce, so an engineer knows exactly what to fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding Django ORM internals, locating and modifying methods across multiple files (get_prep_value, get_prep_lookup, split_exclude), and adding targeted test cases. An experienced engineer would need a few hours to research, implement, and validate the patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns; the sample is self-contained and suitable for benchmarking, though it requires Django-specific expertise.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11740": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes initial models (App1 with UUID fields and App2 with UUID PK), the change made (converted the plain UUID field to a ForeignKey to App2 with SET_NULL), and the unexpected behavior (generated migration lacks a dependency entry on testapp2). It specifies the Django version (2.2), database (PostgreSQL), and reproduces the failure in tests. The relevant file for the fix is django/db/migrations/autodetector.py in generate_altered_fields where dependencies should be added when a ForeignKey is altered.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires an engineer to dive into Django\u2019s migration autodetector internals (autodetector.py), understand how AlterField operations are generated, add logic to collect dependencies for ForeignKey changes, and adjust the test suite accordingly. This breadth of impact across core migration code and writing new tests suggests a few hours of work.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11815": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the problem: Django migrations use the Enum member\u2019s translated value rather than its constant name, breaking migrations when translations change. It includes reproduction steps (defining an enum with lazy translations, adding a CharField default), explains the failure mode (ValueError on reapplying migrations in another locale), and explicitly states expected behavior (serialize default as Enum['NAME']). The provided serializer file (django/db/migrations/serializer.py) and MigrationWriter tests give all needed context for implementation without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires modifying the EnumSerializer.serialize method in migrations/serializer.py (a small code change of ~6 lines) to emit the enum name instead of its value, and updating corresponding tests in tests/migrations/test_writer.py. Understanding the serializer_factory pattern and test utilities may take some thought, but the change is localized and straightforward, fitting within a 15\u201360 minute task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is suitable as-is for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11964": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the model definition, reproduction steps, expected vs actual behavior, and root cause. It points out that the default __str__ of the Choices enum returns the enum member name (\u201cMyChoice.FIRST_CHOICE\u201d) rather than its underlying value (\u201cfirst\u201d), and explicitly states that casting to str should yield the enum\u2019s value property. All necessary context is present without external links, making it straightforward to implement an override of __str__ in django/db/models/enums.py.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires adding a __str__ method in the Choices base class and updating a small test suite. The change is localized to one file (django/db/models/enums.py) and a handful of lines in tests, and an experienced engineer can identify the correct location and verify behavior within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12155": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints the exact failure in trim_docstring (django/contrib/admindocs/utils.py): including the first line with zero indentation causes indent calculation to be zero. It reproduces the error steps in the Django admin, cites the problematic code snippet, and proposes skipping the first line.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is localized\u2014a one-line change to skip the first docstring line in indent calculation and switch to cleandoc, applied in two places and updating one test. An experienced engineer can make, verify, and test this fix within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified; the sample is self-contained and clearly suitable for evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly describes that custom template tags with keyword-only arguments (with or without defaults) raise an unexpected keyword argument error instead of the correct error or handling. It specifies the user-facing symptom (TemplateSyntaxError), the context (simple tags and inclusion tags), and the versions affected. However, it leaves open where in the codebase to apply the change (e.g., parse_bits in django/template/library.py) and the precise mechanism to distinguish between positional, kwonly, and varkw parameters. An engineer must infer the fix location and how to integrate keyword-only defaults support into existing parsing logic.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a localized change affecting a single parsing function (parse_bits) in the template library. An experienced developer can locate the parsing routine, adjust the conditional to include kwonly parameters, and update or add a few tests in under an hour. The test suite and PR patch illustrate exactly what behavior is expected, making the implementation straightforward.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample provides a specific user bug, corresponding PR diff, and test updates. It is self-contained and suitable for use in a benchmarking scenario without external dependencies.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12325": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the broken behavior in Django\u2019s multi-table inheritance when two OneToOneFields target the same parent model. It explains that ordering of the explicit parent_link marker is forcing a spurious error and outlines exactly how the model definitions trigger the improper configuration exception. As an experienced engineer with access to the base.py and options.py implementation, one can pinpoint where the filtering logic for OneToOneField should be adjusted to honor the parent_link attribute rather than field declaration order.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s model class construction in base.py and its invalid model checks in options.py, modifying two small code blocks, and adding a few targeted tests. An experienced engineer could locate the relevant loops, change the isinstance condition and remove the redundant exception, then verify with added tests within a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12663": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides detailed context: it lists the Django models, the exact subquery annotation, the use of SimpleLazyObject, and the failure mode (TypeError on int()). It clearly states what is broken and how to reproduce it via tests. However, it does not point to a specific function or file to patch, so an engineer must interpret where to unwrap the lazy object in the ORM core. This requires some familiarity with Django\u2019s query.py internals to locate the correct hook for value conversion.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s ORM internals, locating the lookup preparation flow in query.py, and adding logic to unwrap SimpleLazyObject instances. The change itself is small (a few lines) but involves research, testing against existing tests, and adding a new test. An experienced engineer would likely need 1\u20134 hours to familiarize with the code paths, implement the conditional unwrap, and validate behavior.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Although there are no blocking issues for using this sample, it relies on deep knowledge of Django\u2019s internal query construction and functional testing patterns. Engineers unfamiliar with Django internals may struggle to find the correct extension points, which could affect the benchmark\u2019s general applicability.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12708": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly lays out the scenario: a Django model with both unique_together and index_together on the same fields, the exact reproduction steps (defining the model, removing index_together, running makemigrations/migrate), the observed ValueError due to indistinguishable constraints, and the expected behavior (delete only the index constraint without affecting the unique constraint, plus avoid index recreation when moving declarations). It even suggests two high-level goals. This information is sufficient for an experienced engineer to locate the alter_index_together method in django/db/backends/base/schema.py, apply a targeted patch adding {'unique': False} to the delete call, and write appropriate tests in tests/migrations to verify index and unique constraint behavior.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"To implement the fix, an engineer must familiarize themselves with the Django migrations framework, locate the alter_index_together method, understand how constraints are introspected and deleted, modify the call to _delete_composed_index, and then extend the existing test suite to assert unique vs index presence. Although the change is localized, it spans multiple files and requires understanding Django internals and test patterns, which would likely take 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional blockers are apparent. The issue is self-contained and does not depend on external context or ambiguous requirements. It cleanly maps to modifying a single method and updating tests. The test suite already covers migrations, and the patch integrates directly, so this sample is appropriate for benchmarking coding ability.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12754": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states the conflict: moving the \u2018title\u2019 field from the base model Readable into the subclass Book causes a FieldError on migrate due to operation ordering. It gives a reproduction recipe (initial models, makemigrations, migrate error), the expected behavior, and the Django exception. All necessary details (class names, field names, migration operations) are provided.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s migration autodetector internals, locating the generate_created_models method, and adding logic to reorder RemoveField/CreateModel operations. An experienced engineer would need time to explore ModelState, migration dependency resolution, and write tests. This is a moderate change spanning multiple files and concepts, taking 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12858": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints the exact failure in django/db/models/base.py within the _check_ordering method: the default ordering lookup test rejects lookups (supply__product__parent__isnull) because get_transform returns None. It clearly states that allowing lookup-based ordering should restore prior behavior. The model relationship chain is spelled out and the examples of primary keys in both orders confirm the intended semantics. From this, an engineer can locate the checks.Error in base.py, extend the if-condition to include get_lookup(part), and add the corresponding test in tests/invalid_models_tests/test_models.py, as shown in the PR. There are no missing details about what file or function to change or what the expected outcome is.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced Django contributor only needs to find the _check_ordering logic in django/db/models/base.py, change a single conditional to include get_lookup(part), and add a small unit test in tests/invalid_models_tests/test_models.py. Familiarity with Django\u2019s system checks and lookup/transform APIs allows one to implement and validate the fix within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13012": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes the problem in django/db/models/expressions.py: ExpressionWrapper incorrectly includes constant expressions in GROUP BY. It provides a minimal reproduction (wrapping Value(3) vs annotating it directly), the error thrown by PostgreSQL, and the desirable behavior (omit literals). The expected fix is obvious: override get_group_by_cols on ExpressionWrapper to delegate to the inner expression so that constants produce an empty list. The filename, class (ExpressionWrapper), and relevant methods (get_group_by_cols, as_sql) are all mentioned, making for a well-specified task.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer familiar with Django\u2019s ORM expressions would spend some time locating where GROUP BY columns are gathered (in ExpressionWrapper), then add a small override (~3\u20135 lines) to call through to the wrapped expression, and finally write two simple tests. This is a focused change touching one file plus tests and should take under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is self-contained, reproducible, and suitable as a benchmark task.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13028": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides the relevant model definitions, reproduction steps for the filter operation, the error raised, and a minimal example of how renaming the 'filterable' field works around the issue. It clearly states the expected behavior (filtering by metadata_type should return results) versus the actual outcome. However, it does not explicitly point to the ORM check implementation location, requiring the engineer to locate the check_filterable method in the Django codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires modifying a single method in django/db/models/sql/query.py to adjust the existing filterable check and adding a simple test case. An experienced Django engineer could locate the relevant code, implement the conditional, and update tests in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13112": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue provides clear reproduction steps (Django 3.1b1, mixed-case app name \u2018DJ_RegLogin\u2019 in INSTALLED_APPS, model definitions in model.py, settings.py and apps.py), the exact runtime error with context, and points to the file and method needing change (django/db/models/fields/related.py deconstruct()). A developer can understand that the bug arises from lowercasing the app label in ForeignKey deconstruction and can implement the patch accordingly.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires reading the Django migrations code (the deconstruct() method in related.py), understanding how lazy model references are generated, and modifying a small code path to preserve mixed-case app labels. Writing accompanying tests also demands familiarity with Django\u2019s test suite. Overall, an experienced engineer could implement and validate this in a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the sample is self-contained and reproducible.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly describes the problem\u2014subtracting DateTimeField values raises a FieldError because the mixed types (datetime and duration) require an explicit output_field. It states the expected behavior: temporal subtraction should work without wrapping in an ExpressionWrapper. However, it does not point to the exact module or class to modify (e.g., CombinedExpression.resolve_expression) and leaves open how to integrate the fix into Django\u2019s expression system. An engineer can sensibly infer the task but must explore the codebase to locate and extend the right methods.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced Django engineer familiar with the ORM expression API will need to read the existing CombinedExpression (and its as_sql/resolve_expression methods), understand the handling of DurationExpression and TemporalSubtraction, and then implement type inference logic in resolve_expression plus minor changes in compile. The change spans multiple code paths and requires writing and verifying new logic across two methods, plus running and possibly adjusting tests. This is substantial but fits within a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the affected code path (TemplateView.get_context_data and _wrap_url_kwargs in django/views/generic/base.py), explains the change in behavior from Django 3.0 to 3.1 (kwargs wrapped in SimpleLazyObject causing SQLite binding errors), shows how converting to a plain string resolves the lookup, and even mentions the relevant test failure. All necessary details are stated to implement a fix without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires reading the generic base view implementation, understanding Django\u2019s lazy evaluation utilities, replacing SimpleLazyObject with lazy(), updating the context wrapping logic, and adjusting or adding a concise test. This is a multi-step but localized change in one module and its tests, taking an experienced engineer 1\u20134 hours to complete and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13406": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides the Toy model definition, minimal reproduction steps, expected vs. actual behavior, and the AttributeError observed. It clearly states that after pickling and restoring a query with values()/annotate(), the iterable class is not reset to ValuesIterable, leading to model instances instead of dicts. This information is sufficient to implement a fix in the Query.query setter.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is a small conditional in the Query.query setter to detect values_select and set _iterable_class to ValuesIterable, plus adding two simple tests. An engineer familiar with Django internals could implement and validate this within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13449": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies the file (django/db/models/expressions.py), class (Window), and method (SQL generation for LAG expressions). It describes the exact error (\u201cnear \u2018OVER\u2019: syntax error\u201d), the cause (CAST only wraps the LAG call for DecimalField), and reproduces the problematic SQL. The expected behavior and workaround (output_field=FloatField) are given, so an engineer can directly implement a patch.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s Expression and Window SQL compilation, adding a SQLite-specific override (as_sqlite) in expressions.py, and adjusting output_field handling. It involves reading ~100\u2013200 lines of ORM code and writing ~15 lines of patch plus tests, which would take 1\u20134 hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"N/A\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13568": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue statement clearly identifies the exact behavior to change in Django\u2019s system checks: when USERNAME_FIELD is enforced via a UniqueConstraint in Meta.constraints, it should bypass the \\\"must be unique\\\" error. It points to the relevant check in django/contrib/auth/checks.py and describes the desired logic extension, so an experienced engineer can locate the function, add the constraint scan, and adapt existing tests accordingly.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves editing a single method in checks.py to add a simple constraint check and extending a test file with two new small test cases. An engineer familiar with Django internals could implement and verify this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13807": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the failure point in Django\u2019s SQLite backend: missing quoting of SQL keywords in PRAGMA statements. It names the file (django/db/backends/sqlite3/base.py), the function (check_constraints), and the exact lines (around 327 and 333) where PRAGMA foreign_key_check and PRAGMA foreign_key_list interpolate table names without using quote_name. It specifies the root cause and quotes the problematic code and desired behavior, making it clear that a successful solution must wrap table names with self.ops.quote_name in those SQL statements.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a localized fix in a single backend file, requiring only wrapping three string interpolations in calls to self.ops.quote_name. An engineer familiar with Django internals and the SQLite backend could identify the utility method and apply the change within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13810": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints the code location in django/core/handlers/base.py within the load_middleware method (around line 58), explains how the handler variable is overwritten incorrectly when MiddlewareNotUsed is raised, and outlines the difference between expected and actual behavior. It clearly states that the handler should only be updated in the else branch and not when a middleware skips itself. With these details, an engineer can identify exactly where and how to apply the patch.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires reading and understanding Django\u2019s middleware loading logic (~50\u2013100 lines of code), modifying two lines in load_middleware, and adding tests to cover both synchronous and asynchronous middleware paths. An experienced engineer familiar with Django internals would likely need 1\u20134 hours to implement and verify the change.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This issue is highly specific to Django\u2019s ASGI middleware internals and requires deep familiarity with adapt_method_mode and middleware chaining behavior. It may not be appropriate for a generic coding benchmark focused on broader skills.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14017": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text concisely describes a TypeError when combining Q and Exists in reversed order, points out the missing __rand__ support, and even hints at updating the Q._combine method. It references specific behavior of the & operator, and one would know to inspect django/db/models/query_utils.py in the Q class. The reproduction steps, error message context, and desired commutativity make it clear what change is required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An engineer needs to locate the Q class in django/db/models/query_utils.py, understand the _combine method\u2019s isinstance check, add a clause to accept conditional (Exists) expressions or implement __rand__, and update tests. This small, focused change plus test additions should take under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14140": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"In the deconstruct method in django/db/models/query_utils.py there is a conditional that treats a single child specially: it attempts to unpack child[0] and child[1] into kwargs, which fails if the child is not a tuple but an object like Exists. The issue clearly describes the inconsistent behavior between single-condition (kwargs) and multi-condition (args) cases and the TypeError when deconstructing Q(Exists(...)). It specifies the expected unified behavior (always use args) and even provides a patch outline. This reproduction and expected result are precise, pointing to specific lines in query_utils.py and the nature of the TypeError, so no further clarification is needed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Q.deconstruct\u2019s implementation in django/db/models/query_utils.py, rewriting the args/kwargs assignment block (removing the special-case conditional and consolidating args and kwargs logic), and updating multiple existing tests while adding new ones to cover boolean expressions inside Q. An engineer must read and modify a core utility method and adjust test files across at least two directories. This is a moderate, self-contained change expected to take 1\u20134 hours once familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14238": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly pinpoints the problem in django/db/models/fields/__init__.py within the AutoFieldMeta.__subclasscheck__ method, stating that custom subclasses of BigAutoField and SmallAutoField are wrongly rejected. It explains expected versus actual behavior, names the exact method to modify, and even implies the solution approach (switch from direct membership to issubclass). The accompanying patch shows the one-line change and precise test adjustments, leaving no ambiguity about what must be done.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, localized change: replacing a membership check with issubclass() in one method and extending existing tests with a few subclass assertions. An engineer familiar with Django\u2019s metaclass patterns can implement and test this fix in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14351": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes a discrepancy between Q(agent__property_groups__in) and Q(agent__property_groups__id__in), explains how Django\u2019s SQL compiler is including all default columns in the subquery (lines showing U0.\\\"id\\\", U0.\\\"created\\\", etc.), and pinpoints the failure in get_default_columns. It provides example code and SQL output and even a suggested hack in django/db/models/sql/query.py, so an engineer could directly locate the functions to modify and know exactly what behavior to change.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this bug requires understanding Django\u2019s internal SQL compiler, navigating lookup processing in django/db/models/lookups.py, and extending get_group_by_cols to conditionally clear and reset select fields on a Subquery. An experienced engineer would need a few hours to read the ORM internals, write the new method, and update tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14493": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies the root cause (UnboundLocalError due to an uninitialized `substitutions` variable when `max_post_process_passes` is zero), provides reproduction steps (custom storage subclass, setting, collectstatic run), and even links to the exact lines (L246\u2013L257 in storage.py). It specifies what a correct solution entails (initialize `substitutions` before use) and the desired behavior once fixed. There is no ambiguity in the change required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, localized fix\u2014a single-line initialization added before a loop. An experienced engineer can understand the bug by reading the error and code, identify the missing initialization, and implement and test the patch in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The sample is self-contained, the reproduction steps are clear, and the accompanying tests fully verify the fix. It makes for a focused benchmark problem: locate an uninitialized variable, add its initialization, and confirm via existing test harness.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14580": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines how to reproduce the bug in Django 3.2.4 by defining custom fields, abstract base models, and mixins. It specifies the expected behavior (valid imports in generated migrations) versus the actual NameError. It even hints at the faulty module (django/db/migrations/writer) so an experienced engineer can locate the code in django/db/migrations/serializer.py and add the missing import for django.db.models. This is sufficient to attempt a solution without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue requires locating the migration serializer logic in django/db/migrations/serializer.py, updating the special_cases list to include ['from django.db import models'], and adding a corresponding test in tests/migrations/test_writer.py. These changes are confined to one source file and its test, so after a brief familiarization with the migrations writer system, an experienced engineer could implement, test, and validate the fix in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14672": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue pinpoints a missing make_hashable call in the identity property of ManyToManyRel in django/db/models/fields/reverse_related.py, shows the specific method (identity), the offending attribute (through_fields), minimal repro steps, and expected solution. It\u2019s clear what file and line to edit and how to validate the fix.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"Fixing this requires adding a single make_hashable call in one method (identity of ManyToManyRel) and updating/adding a few tests. The change is localized and straightforward, taking under 15 minutes for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14787": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies a specific bug: method_decorator uses functools.partial which strips wrapper metadata such as __name__ and __module__. It provides a concise reproduction (a custom logging decorator applied via method_decorator), the expected behavior (the method should log and return normally), and the actual error (AttributeError due to missing __name__). The relevant code location is explicit in django/utils/decorators.py at the bound_method assignment. The test patch shows exactly how to validate the fix. There's no ambiguity about what needs to be changed or what success looks like.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is localized to a small section of code (two lines in django/utils/decorators.py). An engineer familiar with Python decorators and functools.wraps could understand the issue and implement the wrap in well under an hour. The test patch is straightforward and existing patterns in the codebase guide the solution.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15104": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the failure point in django/db/migrations/autodetector.py, in the only_relation_agnostic_fields method, where `del deconstruction[2]['to']` raises a KeyError. It explains how a custom ForeignKey field\u2019s deconstruct method drops the 'to' key, causing the error, and even suggests replacing `del` with `pop('to', None)`. This pinpoints both the file and the exact line to change and the expected behavior, leaving minimal ambiguity for implementation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer, once familiar with Django\u2019s migration autodetector, can locate the one\u2010line change and implement `pop('to', None)` in under an hour. Writing or adapting the accompanying test in tests/migrations/test_autodetector.py is straightforward. The overall scope is a small patch requiring light thought and verification.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15128": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the bug context, how to reproduce it (including Django versions and model relationships), and pinpoints the root cause in Query.combine and change_aliases alias mapping. It even proposes a concrete approach (bumping the RHS alias prefix and ensuring disjoint change_map) and references specific methods and lines (e.g., Query.combine, Query.change_aliases). This makes it unambiguous what change is required for a successful fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires deep familiarity with Django\\u0019s ORM QuerySet internals, understanding alias generation and join logic, and safely modifying Query.combine, change_aliases, and adding bump_prefix. It\\u0019s not trivial but should be solvable in a few hours by an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues; the reproduction and scope are well-contained.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15277": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the exact code location (CharField.__init__ in django/db/models/fields/__init__.py) and the root cause (unconditional addition of MaxLengthValidator when self.max_length is None). It includes reproduction steps, expected outcome, runtime error, performance numbers, and even a precedent (BinaryField.__init__). The required change is explicitly described (wrap the validator append in an `if self.max_length is not None`). All necessary context and files are provided for a direct fix.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a small, localized change\u2014adding a two-line conditional and updating tests. An experienced engineer can locate CharField.__init__, implement the `if` guard, and run existing tests in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15280": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is described with explicit model definitions for User and Profile, the exact prefetch behavior and SQL queries are provided. The reproduction steps clearly outline that accessing user.profile.user.kind triggers an unexpected extra database query due to deferred fields inheritance across nested prefetches. The expected behavior is documented along with printed deferred fields and test case descriptions. This gives enough context to locate and modify related_descriptors.get_prefetch_queryset and to write a targeted test confirming no additional queries.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this bug requires understanding Django\u2019s ORM deferred field mechanism and the internals of prefetch_related. The engineer must navigate core code in related_descriptors.get_prefetch_queryset, reason about when to bypass cache overrides, implement a simple guard, and add a test in tests/prefetch_related/tests.py. While the patch is small, diagnosing the root cause in a mature codebase and verifying no regressions should take around 1\u20134 hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue is quite domain-specific to Django ORM internals and assumes familiarity with deferred fields and prefetch behavior. Engineers unfamiliar with Django core may need time to locate the relevant code. However, the issue is self-contained, with clear reproduction steps, SQL logs, and a precise test expectation. The provided test patch ensures automated validation, making it well-suited for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15315": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the precise method to change (__hash__ in django/db/models/fields/__init__.py), explains the problem (hash values change when a Field is attached to a model), shows steps to reproduce, and even references the ticket where the bug was introduced. It specifies the expected behavior (hash should remain constant) and hints at reverting a prior change. This gives an engineer all necessary information to implement and validate a fix without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires locating a single method (__hash__) in one file, simplifying its implementation, and adding a small test case. An experienced Django coder can understand the context, write the change, and run tests in well under an hour, likely in 15\u201330 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues identified; the sample is self-contained and only requires familiarity with Django field internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15375": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that using Sum(..., default=0) after annotate leads to an empty SELECT list in the generated SQL and thus a syntax error. It specifies the context (Django ORM, PostgreSQL/SQLite), reproduces the commands (Book.objects.annotate(idx=F('id')).aggregate(x=Sum('id', default=0))), and shows that the workaround using Coalesce(Sum('id'), 0) succeeds. The root cause\u2014incorrect default handling in resolve_expression\u2014can be deduced, and the change needed is explicit: wrap the aggregate in Coalesce, preserve the is_summary flag, and adjust resolve_expression in django/db/models/aggregates.py. The expected outcome (default value applied correctly) is unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires modifying a single method in django/db/models/aggregates.py and adding a couple of tests. The developer can follow the existing Coalesce workaround as a guide, introducing only a few lines to wrap the aggregate and set is_summary. An experienced Django engineer familiar with the ORM internals could implement and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns. The issue maps directly to a single-file fix and straightforward test additions.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15380": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the context (Django migration autodetector), the exact steps to reproduce (renaming a model and field in one step via makemigrations), the observed error (unexpected key lookup error due to missing original model in memory) and the expected outcome (migrations should be generated). This level of detail pinpoints the failing function generate_renamed_fields in autodetector.py and makes it straightforward to identify and implement the one-line fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue requires understanding the existing rename mapping logic in generate_renamed_fields, locating the single incorrect lookup, and applying a small patch plus a test case. An experienced engineer familiar with Django\u2019s migration internals could implement and validate this change in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15503": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly explains that JSONField.has_key, has_keys, and has_any_keys lookups are misinterpreting numeric string keys as array indices on certain backends like SQLite. It provides concrete versions, a reproduction recipe including creating records with string and numeric keys, expected versus actual query counts, and notes cross-database behavior differences. This gives enough context to locate and extend compile_json_path logic in django/db/models/fields/json.py to treat numeric keys as object keys rather than array indices.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s JSONField lookup infrastructure, specifically KeyTransform, compile_json_path, and the backend lookup classes for SQLite, Oracle, and Postgres. It involves adding a helper method to handle final key compilation, refactoring HasKeyLookup and related classes, and updating tests. Implementing and validating across multiple database backends would likely take a couple of hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified; the sample is well-scoped and the existing tests cover the behavior once the lookup logic is corrected.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15525": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text provides a complete reproduction: it describes the one-to-many Book\u2013Author models with natural_key methods, shows the JSON fixture, and details the loaddata failure on a secondary database. It specifies the exact error (DoesNotExist raised because natural_key calls Author lookup before loading into the non-default DB), and outlines expected behavior (that natural_key should work unchanged across databases). The model definitions, manager methods, and command-line invocation are all included, leaving minimal ambiguity about what must be fixed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s serialization internals (base.py's build_instance), recognizing that Model(**data).natural_key() doesn\u2019t know which DB to use, then modifying state and adding obj._state.db=db. Additionally, one must add a new fixture and write a test in the Django test suite. This is more than trivial but under 4 hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15695": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text explicitly outlines the problem: reapplying RenameIndex on an unnamed unique_together index restores the original name, then fails forward because the new name already exists. It provides reproduction steps, test file references, and expected behavior without requiring external context, making the requirement clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves adding a simple early-exit condition when old and new index names match and extending an existing test. An engineer familiar with Django migrations can locate the single method, apply a few-line change, and update the test within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns; the description, code, and tests provide sufficient detail for independent implementation and verification.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15732": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that when removing a unique_together constraint on a field which also has its own unique index (primary key or unique=True), Django\u2019s migration framework incorrectly finds two unique indexes and fails. It specifies the affected functions (_delete_composed_index, alter_unique_together), shows the reproduction steps, the runtime error, and the expected behavior (drop only the extra constraint). There\u2019s enough detail to locate the constraint handling logic in django/db/backends/base/schema.py and implement the additional filtering by primary_key=False.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced developer would need to familiarize themselves with Django\u2019s migration schema code, locate _delete_composed_index and alter_unique_together, understand constraint_kwargs and the multiple-constraint feature flag, then implement the conditional filtering logic and write tests. This spans editing multiple related methods and adding test cases, requiring several hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15814": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue describes in detail the proxy model setup (CustomModel, ProxyCustomModel, AnotherModel), the exact Django ORM calls (select_related(\\\"custom\\\") followed by only(\\\"custom__name\\\")) that trigger the error, and points to the specific file and line in django/db/models/sql/query.py (around line 745) where cur_model._meta is used. It also clearly states the expected behavior (loading only the related name field) and the observed failure (missing primary key), and even suggests a concrete change (using cur_model._meta.concrete_model._meta). This is sufficient to understand both the problem and the desired fix without ambiguity.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django ORM internals, specifically how deferred_to_data handles proxy models, reading django/db/models/sql/query.py, and verifying that adding cur_model = cur_model._meta.concrete_model works. Writing and running the test in tests/proxy_models/tests.py also takes some time. Overall this is a moderate change of a handful of lines but requires ~1-4 hours of familiarization and testing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue text, models, failing behavior, suggested fix, and tests are all clearly provided. The sample is self-contained and suitable for a coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15930": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description gives clear reproduction steps (building a Django query with Case and ~Q(pk__in=[])), specifies the SQL error caused by an empty WHEN clause, and points to the exact location to patch (django/db/models/expressions.py in the as_sql method). It even explains the sentinel value behavior and the expected semantics. An engineer familiar with Django\u2019s expression compilation can immediately identify where to insert the empty-condition check.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves editing a single method (as_sql in django/db/models/expressions.py) by adding a small conditional branch for condition_sql == \\\"\\\" and writing one new test in tests/expressions_case/tests.py. An experienced Django contributor could understand the code, implement the change, and update tests within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15957": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly identifies that slicing a queryset in Prefetch leads to an assertion error and that the desired behavior is to limit related objects (e.g., posts per category) via slicing without triggering filters-on-sliced-queryset errors. It names the affected method (get_prefetch_queryset in django/db/models/fields/related_descriptors.py) and gives a concrete use case and expected outcome, though it does not prescribe the exact ORM expressions needed to implement window functions.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing this fix requires diving into Django's ORM internals, understanding how QuerySet slicing works, and using Window expressions (RowNumber) to filter per partition. It involves modifying get_prefetch_queryset, adding helper functions, clearing limits, and writing comprehensive tests across M2M and FK cases, which would take an experienced engineer roughly 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15973": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the scenario with three Django apps (fonte, fonte_variavel, variavel), specifies model definitions, the many-to-many relationship using a through model, the exact AttributeError and conditions under which it occurs, and even a workaround. The reproduction steps, error message, and expected behavior are all present, providing enough information to craft a targeted fix without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django's migrations autodetector internals, locating the right code path in django/db/migrations/autodetector.py, modifying the handling of field.remote_field.through vs the model reference, and adding comprehensive tests. An engineer needs a few hours to navigate this mature codebase and validate the patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16032": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines reproduction steps: filtering Book records, annotating and aliasing fields, expected four publisher names (Apress, Sams, Prentice Hall, Morgan Kaufmann), and the operational error caused by a nested subquery returning too many columns. It references QuerySet.alias(), annotate(), clear_select_clause(), add_fields(), set_values() in django/db/models/fields/related_lookups.py and modifications in django/db/models/sql/query.py. This provides a precise understanding of what change is needed without external context.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires understanding Django ORM internals, reading and modifying two core modules (related_lookups.py and sql/query.py), adding a new property, adjusting method implementations, and updating tests to cover alias behavior. An experienced engineer would need 1\u20134 hours to fully grasp and implement the multi-file changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16136": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the problem behavior (server crash on GET due to awaiting HttpResponseNotAllowed), gives reproduction steps (Django 4.1.1, async post-only view), and specifies the expected behavior (return 405 instead of 500). It identifies the relevant function (http_method_not_allowed in django/views/generic/base.py) so an engineer can implement and test a fix without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves modifying a single method to conditionally wrap the response in a coroutine when view_is_async is true, and adding a small test case. An experienced engineer familiar with Django internals could implement and validate this change within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16429": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the problem: calling django.utils.timesince.timesince() with USE_TZ=True and a date more than one month ago causes a TypeError due to mixing naive and aware datetimes. It points to the pivot creation in django/utils/timesince.py (around lines 93\u2013100) missing tzinfo on the new datetime, and even suggests adding tzinfo=d.tzinfo. The accompanying test patch shows modifying tests/utils_tests/test_timesince.py by subclassing with @override_settings(USE_TZ=True) and verifying the fix. An engineer can definitively locate the code in timesince(), apply the one-line change, and validate with the updated tests, so the requirements and success criteria are unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, focused change\u2014a single-line addition of tzinfo in django/utils/timesince.py plus adapting an existing test class in tests/utils_tests/test_timesince.py. An engineer familiar with Django's timezone machinery can find the pivot datetime creation, add d.tzinfo, and run the tests in well under an hour. It involves minimal code and a straightforward test update.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues detected. The sample cleanly isolates a one-line fix and a test extension, making it suitable for a coding benchmark without external dependencies or ambiguity.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16454": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that Django\u2019s custom CommandParser subclass does not propagate its error formatting behavior to subcommands created via add_subparsers(), causing raw tracebacks on missing arguments. It specifies the location (django/core/management/base.py), explains the failure scenario, and outlines the high-level solution of overriding add_subparsers() to copy parser_class and error formatting settings to subparsers.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Implementing this fix involves adding a few lines to override add_subparsers() in BaseCommand, using functools.partial to carry over the called_from_command_line flag. It requires understanding of argparse and Django\u2019s management command internals but is a localized change likely to take under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16485": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the crash in django/template/defaultfilters.py within the floatformat() function when p=0 and m is falsy. It specifies expected behavior (return \\\"0\\\" for \\\"0.00\\\" and Decimal(\\\"0.00\\\")). It describes the runtime error (ValueError due to invalid precision) and the context around m (the regex match indicator) and p (precision), making it straightforward to infer that the conditional should accept p<=0 rather than p<0. The inputs \\\"0.00\\\" and Decimal(\\\"0.00\\\") and the section of code around lines 168\u2013170 are well-defined, and the required change is narrowly scoped.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves a one-line change in defaultfilters.py (adjusting the conditional from p<0 to p<=0) and adding two test assertions. An engineer familiarizing with the floatformat implementation and test structure could implement and verify this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16569": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides clear details on the failure scenario in django/forms/formsets.py, specifically that add_fields() raises a TypeError when index is None under certain flag combinations. It identifies the exact code location (line 493), explains reproduction steps, outlines the conditional logic error, and proposes the precise guard to add. Given these specifics, an engineer can locate the code, understand the root cause, and implement the correct fix without further clarification.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"An experienced engineer can reproduce the issue, locate the two-line condition in add_fields, add the index is not None check, and update one assertion in the test file. This change is trivial and can be completed in under 15 minutes once familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues were identified; the sample is ideal for benchmarking coding ability because it presents a small, targeted bug fix with clear reproduction instructions and a succinct expected patch. The description and test addition are concise, and the context is self-contained, making it straightforward for evaluation purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16667": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the exact failure point in SelectDateWidget.value_from_datadict where datetime.date(int(y), int(m), int(d)) raises OverflowError when a component exceeds platform limits. It provides reproduction steps, example inputs, expected behavior (return a validation error rather than crash), and points directly to the code block to modify. There is no ambiguity about what needs to be done.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized to adding a try/except around the date construction in one method and returning a sentinel, plus adding a few test cases. An experienced engineer could understand the widget code and implement this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16950": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text provides concrete model definitions (UUIDModel, Thing, SubThing), the admin.py setup (SubThingInline, ThingAdmin), and a step-by-step reproduction of the runtime integrity error. It clearly states that inline form saving nullifies the parent UUID, causing a database constraint failure. An experienced Django engineer can locate the add_fields method in django/forms/models.py (around the to_field.has_default() check), understand the formset logic, and propose a change to prevent clearing the default PK in this context. Thus, all necessary details (code locations, failure scenario, desired behavior) are present.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the fix is a small conditional change (~6 lines) in the add_fields method of django/forms/models.py, one must understand Django\u2019s inline formset internals, locate the correct spot in a large codebase, decide the right conditional (distinguishing PK vs alternate key and form.data), update the logic, and add corresponding tests. This will likely take 1\u20134 hours for an experienced engineer to research, implement, and verify.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-20859": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the bug summary, provides reproducible code steps, the actual vs expected outcomes, and even pinpoints the specific lines in lib/matplotlib/legend.py that need adjustment. It specifies that changing a type check from Figure to FigureBase resolves the error and includes Matplotlib version details and environment. An engineer can locate SubFigure\u2019s inheritance of FigureBase, update the isinstance check, and validate via the provided test, with no ambiguity about requirements.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves understanding the Figure/FigureBase hierarchy in Matplotlib, making a small edit (two isinstance checks) in legend.py, and adding a short test. It is localized to one file plus a test, requiring minimal investigation and about 15\u201360 minutes for a familiar engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-22719": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies two precise code locations in lib/matplotlib/category.py: the convert() function at lines 58\u201365 where a deprecation warning is emitted for empty numeric input, and the update() method at lines 230\u2013237 mapping string values that can be parsed as numbers. It specifies the behavior on empty sequences and the expected outcome (no deprecation warning when passing empty data), and even provides a minimal reproduction script. The associated test change in lib/matplotlib/tests/test_category.py shows exactly where to insert a smoke test. Together, these details give a developer full context on what to modify and how to validate the fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves adding two simple `if data.size` conditions around existing branches in category.py and writing a short smoke test. Locating the two code blocks and verifying behavior with pytest would take an experienced engineer well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"While the patch targets the x-axis unit conversion warnings, similar handling might be needed for the y-axis or other unit converters, so additional tests could be added for symmetry. Also, developers should consider whether vectorized or sparse array inputs need similar guards.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-23299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes the unintended side effect of get_backend() clearing figures created under rc_context. It provides precise reproduction steps, explicit actual vs expected outcomes, and specifies the relevant functions (Gcf.figs, rc_context) and rcParams handling. All necessary context (versions, backend) is included, making a correct patch derivable without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves a targeted change in the rc_context implementation to preserve the backend setting. It requires a quick review of rc_context, modifying two lines to exclude 'backend' from the reset copy, and adding a small test. An experienced engineer could complete this within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified. The sample is straightforward, self-contained, and suitable for benchmark use.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-23476": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes a reproducible bug: after pickling and unpickling on an M1 Mac, the figure dpi value doubles repeatedly due to device pixel ratio not being reset. It states the observed behavior, includes code reproduction steps, and specifies the expected constant dpi. There is no ambiguity about what needs fixing or how to verify it.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires finding the __getstate__ implementation in the Figure class, inserting a small dpi reset using the stored original dpi, and adding a concise test case. An experienced engineer could locate the method, apply the three-line patch, and write the test in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\", \"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-24026": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints the root cause: stackplot is resetting the Axes cycler using set_prop_cycle on alias strings (e.g., \\\"C0\\\",\\\"C1\\\") which the validator rejects, triggering a ValueError. It names the file (lib/matplotlib/stackplot.py), shows the problematic call to axes.set_prop_cycle(color=colors), and provides clear reproduction steps and expected vs. error behavior. The desired change\u2014avoid mutating the axes cycler and cycle colors locally\u2014is unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a focused modification in a single function: import itertools, replace set_prop_cycle usage with a local colors iterator, and adjust one existing test in test_axes.py. An experienced engineer familiar with Matplotlib internals could implement and validate this fix within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24149": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue provides a clear summary, reproduction steps, actual vs expected outcomes, and pointers to the specific function in lib/matplotlib/axes/_axes.py around line 2182 where input validation fails. It describes exactly that ax.bar with all-NaN input should catch the StopIteration and fall back to the first element. The necessary file (_axes.py) and method (_convert_dx) and relevant cbook functions are identified, and even tests to verify the fix are included, making it straightforward to craft a correct PR.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this bug involves adding two StopIteration handlers in _convert_dx (around lines 2182 and 2191) and adding a small test in test_axes.py. Locating the failing call and writing the fallback is a simple edit requiring modest familiarity with the codebase, likely taking under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-24970": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly explains that applying a uint8 NumPy array to a Matplotlib colormap triggers future-deprecation warnings. It specifies reproduction steps, actual vs expected outputs, environment details (OS, Python, Matplotlib versions), and points directly to the color handling code (__call__ in lib/matplotlib/colors.py) where deprecated casting occurs, making it straightforward to identify and implement the required change.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Choosing level 2 because the fix involves locating the __call__ implementation in lib/matplotlib/colors.py, understanding NumPy dtype conversions and error-state context managers, adjusting a few lines to wrap the integer cast correctly, and adding a parametrized test in test_colors.py. While this spans multiple lines and requires working knowledge of the codebase and NumPy behavior, an experienced engineer should complete and validate it within a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is self-contained, uses standard NumPy and Matplotlib, and the provided test patch cleanly verifies expected behavior across different data types. There are no external dependencies or environment-specific quirks that would affect its use in a benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-25311": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the scenario (draggable legend in a Matplotlib figure), the expected behavior (pickle.dumps should succeed), and the actual failure (TypeError due to FigureCanvasQTAgg being unpicklable). An engineer can reproduce the bug from the reproduction steps, identify the DraggableLegend class in lib/matplotlib/offsetbox.py (where self.canvas is stored), and understand that removing or converting that unpicklable attribute (e.g. making it a property) will resolve the error. There is no ambiguity about what needs to be fixed or how the test should verify the fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires locating the DraggableLegend implementation, understanding Python pickling semantics, modifying lib/matplotlib/offsetbox.py to make canvas a property, and updating test_pickle.py to assert absence of GUI canvas references via pickletools. For an experienced engineer familiarizing with the Matplotlib codebase and Python\u2019s pickle protocol, this is a multi-step change across source and tests, reasonably taking 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-25332": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear reproduction recipe, explicitly naming the function (`align_labels()`), the expected outcome (successful pickling), and the actual error (a TypeError due to an unpicklable internal reference type). It includes the context (Matplotlib version, operating system) and step-by-step code instructions. An experienced engineer can locate the relevant code in `matplotlib/cbook.py` (the `Grouper` used by label alignment) and know to implement pickle support via `__getstate__`/`__setstate__` based on this description.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding how Matplotlib\u2019s label alignment stores weak references in the `Grouper` class, how Python\u2019s pickle protocol works, locating the class in `lib/matplotlib/cbook.py`, and writing symmetric `__getstate__` and `__setstate__` methods. It also involves updating tests in `test_pickle.py`. This is more than a trivial tweak but should take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; minor missing metadata (Python version, backend) do not impede solving the core problem.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-25479": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes how registering a colormap under a different name fails because the colormap object retains its original name. It provides minimal example code showing registration and imshow usage, specifies expected vs. actual behavior, and names the exact functions and classes (register in cm.py, name property in Colormap). This makes it straightforward to implement the required change (update the cmap.name on registration) and write tests accordingly.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Resolving this requires a small patch\u2014adding a name update in register() and tweaking equality comparison\u2014across two source files and adding a concise test. An experienced engineer could understand the codebase and implement the fix within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-26291": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description identifies that within `mpl_toolkits/axes_grid1/inset_locator.py`, the `__call__` method receives a None renderer and triggers an attribute error. It clearly states what happens (missing renderer), what is expected (empty inset axes like the demo), and where the bug occurs. All necessary context, including environment, backend, and version, is provided, making it straightforward to implement a None-check and fallback to `ax.figure._get_renderer()` at the indicated code location.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves adding just two lines inside the existing `__call__` method in `inset_locator.py` to handle a None renderer. An experienced engineer can locate the method, recognize the need to call `ax.figure._get_renderer()`, and implement the change. Writing a corresponding test using the existing test framework (e.g., `test_inset_axes_tight`) is similarly simple. The entire process, including running tests, can be completed in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None\",\"q2_5_confidence\":4}"
    },
    {
        "psf__requests-1724": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the error scenario: using a Unicode method name (u'POST') with requests.request triggers a UnicodeDecodeError due to mixing Unicode header values with raw multipart body bytes. It specifies where in the code (sessions.py:313) the problematic line is (req.method = method.upper()) and the underlying cause (default ASCII codec cannot decode non-ASCII byte). It is straightforward to infer that the fix is to coerce the method argument to a native string type before uppercasing. No essential details are missing, making the requirements for a successful PR unambiguous.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"The fix involves adding a simple type conversion (builtin_str(method)) in sessions.py before calling upper(), affecting only a few lines of code. An experienced engineer could locate the relevant file, apply the change, and run the existing test suite within minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The test harness and provided test patch align with the change and would validate the solution in the existing test suite.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-5414": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes the problem: a UnicodeError is raised for URLs with an empty label (e.g. http://.example.com) rather than the intended InvalidURL exception. It provides expected vs actual behavior, reproduction steps, and references the relevant file (requests/models.py) and function (prepare_url). The test case to add is shown in tests/test_requests.py, making requirements unambiguous.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a trivial two-line patch in requests/models.py to include \u2018.\u2019 in the host.startswith check and an added test in tests/test_requests.py. An experienced engineer could implement, test, and review within 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3151": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text provides a clear MCVE, expected behavior, and error details. It specifies that combine_by_coords should ignore non-varying coordinates and not raise a ValueError on identical non-monotonic \u2018y\u2019 labels. It references the specific function combine_by_coords in xarray/core/combine.py and describes where the monotonic check occurs, making requirements unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing the issue requires a small, localized change in combine_by_coords: iterate over concat_dims instead of all dims when checking monotonic indexes. An engineer would spend under an hour understanding the existing code, making a 6-line patch in combine.py, and adding a corresponding test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The patch is self-contained, with straightforward modifications and accompanying tests. It does not introduce architectural changes or require external context beyond the codebase provided.\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-3677": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description precisely explains the incorrect behavior of the ds.merge method when given a DataArray, contrasts it with the global merge function, and pinpoints the root cause (expecting a mapping with items()). It clearly states the expected outcome and even provides a minimal repro (variables \u201ca\u201d and \u201cb\u201d), so one can directly locate xarray/core/dataset.py, modify the merge() method, and write a corresponding test without extra clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is a one-line change in the merge method of core/dataset.py (adding a to_dataset() conversion) plus a small test in test_merge.py. An experienced engineer can locate the merge implementation, apply the change, and add the test in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified; the sample is self-contained and suitable for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-4094": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that calling Dataset.to_stacked_array and then to_unstacked_dataset on single-dimensional variables fails due to a merge conflict on the newly created 'y' dimension. A minimal reproducible example is provided, along with the exact expected round-trip output. The faulty function is identified as to_unstacked_dataset in xarray/core/dataarray.py. The required change (adding drop=True to the sel call) is unambiguous and validated by the provided test patch.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves a one-line change in xarray/core/dataarray.py to add drop=True to the sel method and adding a small regression test. An experienced engineer can locate the function, implement the change, and verify with existing tests in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues. The sample includes an MCVE, the precise location of the faulty code, and test cases. It is self-contained and suitable for the proposed benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-4695": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the exact failure scenario: naming a DataArray dimension \\\"method\\\" leads to a ValueError when using .loc, explains that the dimension name is mistaken for a fill-method argument, provides reproducible steps with code and expected behavior, and includes environment/version details. This is sufficient to implement a fix without additional clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires a small code change in DataArray.__getitem__ (one-line adjustment to how sel() is called) and updating/adding a focused unit test. An experienced engineer can locate the relevant method, understand the parameter collision, apply the patch, and verify with tests within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major concerns. The issue is self-contained, well-specified, and the provided test patch ensures validation of the fix.\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-6599": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue provides a clear problem statement, an MVCE showing exactly how to reproduce the bug with timedelta64 coordinates, expected vs actual behavior, and environment details. It\u2019s self-contained and specifies that xr.polyval should treat timedeltas like datetimes, so an engineer can immediately understand what change is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating the numeric conversion utility in xarray (a single function), adding a branch to cast timedeltas to float, and adding one test case. An experienced engineer can make and validate the change within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues; the sample is suitable for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-6721": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified because the user clearly states that accessing the chunks property triggers fetching of all data from a remote Zarr store, whereas the desired behavior is reading only the encoding metadata. It specifies that the implementation should inspect the encoding on DataArray objects rather than calling data.chunks, guiding the engineer to change get_chunksizes accordingly.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The necessary change is a small one-line update in core/common.py to use v._data instead of v.data and a simple test addition. Understanding of xarray internals may be needed, but the code change itself is straightforward and could be done in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pylint-dev__pylint-6386": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that the single-letter -v verbose flag is incorrectly requiring an argument, whereas the long --verbose option behaves as expected. It describes reproduction steps, expected behavior, Pylint version, and the help message context. This provides sufficient information to locate and modify the argument parsing logic in Pylint without ambiguity.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Pylint\u2019s argument parsing implementation, updating several related modules to treat -v like --verbose, and adding a test. It spans multiple files but remains localized to argument handling, which should take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is well-defined and ready for benchmarking.\",\"q2_5_confidence\":4}"
    },
    {
        "pylint-dev__pylint-6903": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states the conditions under which pylint crashes (Kubernetes Pod with --jobs=0), identifies the root cause (avail_cpu calculation yielding 0), references the exact function (_query_cpu in pylint/lint/run.py) and lines where the math happens, and describes the expected behavior and a suggested fallback. No additional assumptions are needed to implement a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a localized change in a single function (_query_cpu) and a corresponding test. It involves adding a simple conditional to handle zero CPU cases and writing a small mock-based test. An experienced engineer can understand, implement, and validate this in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major blockers. The sample is self-contained and only relies on mocking file reads for cgroup values, which is straightforward in a test environment.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-8898": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that the bad-names-rgxs option (in pylint/config/argument.py) uses a CSV transformer that naively splits on commas, mangling valid regex patterns containing commas (e.g. quantifiers like {1,3}). It provides reproduction steps, observed vs expected behavior, Pylint version, and environment details, allowing a developer to locate and modify _regexp_csv_transformer and implement a brace-aware splitting logic without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"A skilled engineer can update the CSV transformer in argument.py, add a helper in utils/utils.py, adjust imports in utils/__init__.py, and extend tests in test_config.py. The change spans a few small files and amounts to a moderate parsing function and test adjustments, taking roughly 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10051": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the conflict between caplog.get_records() and caplog.clear(), provides a step-by-step reproduction, the expected vs actual behavior, and environment details. It specifies exactly what behavior needs to change: after clear(), get_records() must be emptied and reflect new records. It even points to the relevant methods (caplog.clear(), caplog.get_records(), caplog.records in src/_pytest/logging.py).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, localized API consistency fix requiring only a few lines of code in src/_pytest/logging.py (adding a clear() method to the handler and updating caplog.clear to call it) and corresponding test adjustments. An experienced engineer could understand and implement this in under an hour after reviewing the caplog implementation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues. The sample is self-contained, has clear reproduction steps, a defined expected outcome, and minimal external dependencies. It cleanly maps to a small code change and test update, making it ideal for benchmarking coding ability in isolation.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-10081": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly explains the unexpected behavior (tearDown running on class-level skipped tests when using --pdb), provides a minimal reproducing example, shows both expected and actual behavior, lists environment details, and identifies the exact function (`runtest` in `_pytest/unittest.py`) to modify. This is sufficient for an engineer to begin implementing a fix without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding pytest\u2019s unittest plugin internals, locating the `runtest` method in `src/_pytest/unittest.py`, adjusting skip logic to account for class-level skips, writing or extending tests to cover the new behavior, and ensuring no regressions. For an experienced engineer, exploring the codebase, designing the change and tests, and validating the fix would likely take 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-5262": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly explains that the EncodedFile wrapper in pytest advertises a binary mode (\\\"rb+\\\"), leading youtube-dl to write bytes to a stream that only accepts text, resulting in a TypeError. It includes detailed reproduction steps, expected behavior, environment details, and points directly to the relevant class (_pytest.capture.EncodedFile). An engineer can implement the provided patch unambiguously from this information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is very localized: add a @property to strip the 'b' from buffer.mode and include one extra test case. This is a straightforward edit in a single file plus an additional small test, which an experienced engineer can complete in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5631": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that patch decorator\u2019s new parameter being a numpy array leads to a ValueError because the membership test 'p.new in sentinels' returns an array of booleans. The user notes the pytest versions, the specific function being called during collection, and the faulty line in src/_pytest/compat.py in num_mock_patch_args. The root cause (ambiguous truth value of array) and desired behavior (use identity check against sentinel objects) are explicitly stated. This gives enough information about where to change code and what logic to apply for a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Determining that the sentinel membership test needs to be replaced with an identity check, locating num_mock_patch_args in compat.py, and modifying the test logic is a small, localized change. Writing a couple lines of code and a regression test for numpy-like types should be completed in under an hour, given familiarity with pytest\u2019s code structure.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-5787": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that when tests with chained exceptions are run under pytest-xdist, only the final exception is reported rather than the full exception chain. It describes exactly how to reproduce the problem (two test functions raising explicitly chained and implicitly chained ValueErrors) and what the desired behavior is (include all chained exceptions in the serialized report). A developer can locate the relevant code in src/_pytest/reports.py (functions _to_json and _from_json) and knows to modify serialization to capture ExceptionChainRepr.chain and deserialization to reconstruct it. No additional clarification is needed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer would need to understand how pytest serializes exceptions, locate the JSON serialization routines in reports.py, and extend them to handle the chain attribute. They must also update the deserialization logic and add or adapt tests. While the changes span multiple helper functions and require reading existing patterns (~100\u2013200 LOC), this is a focused feature enhancement and should take a few hours (1\u20134) once familiar with the codebase.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"Although the issue is well specified and the test harness is provided, this sample is very large and highly specific to pytest internals. The patches touch many lines of code across core serialization routines and extensive test updates. As a benchmark problem, it may be too involved and niche, requiring deep knowledge of pytest\u2019s repr classes, JSON serialization, and terminal writer abstractions, rather than general algorithmic or API design skills. This could make fair evaluation difficult under time constraints.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-5809": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the problematic parameter in src/_pytest/pastebin.py (lines 68-73) where \\\"lexer\\\" is set to \\\"python3\\\". It describes the HTTP 400 error from bpaste.net, explains that using \\\"text\\\" fixes it, and includes a code example. The desired change (switching lexer to \\\"text\\\") is unambiguous, and tests in testing/test_pastebin.py explicitly verify the new behavior.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"Fixing this issue requires a single-line edit in src/_pytest/pastebin.py to change the lexer value from \\\"python3\\\" to \\\"text\\\" and updating one assertion in testing/test_pastebin.py. No deep code exploration or design work is needed, so this should take under 15 minutes for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is self-contained, has clear instructions, and includes tests, making it suitable for benchmarking.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5840": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that pytest lowercases folder paths in 5.1.2, preventing conftest.py discovery. It specifies the unexpected behavior (path normalization), the expected behavior (preserve original casing), and the failure mode (module-not-found during test collection). This is sufficient to guide an engineer to locate the path handling in config/_pytest/config and pathlib and to implement a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer could locate and update the few path-handling methods (removing unique_path, using Path.resolve()) and adjust tests in under an hour. The required code changes span two files and are conceptually straightforward.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-6197": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes the regression introduced in pytest v5.2.3 where any __init__.py under the working directory is unintentionally imported at collection time. It contrasts behavior between pytest 5.2.2 (correctly only collecting test files) and pytest 5.2.3 (importing __init__.py and aborting). The reproduction steps using tox are concrete, describe the expected versus observed behavior, and pinpoint the file (__init__.py) and pipeline (pytest collection) where changes are needed. An experienced engineer can understand the high\u2010level requirement\u2014prevent pytest from importing arbitrary __init__.py files during test collection\u2014without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the change surface is small (modifying the python collector and adding a couple of test cases), it requires understanding pytest\u2019s collection internals (PyobjMixin, FSCollector, Module collector, object mounting logic) and crafting new tests with testdir fixtures. An engineer unfamiliar with pytest internals would need time to locate the right hook points and verify behavior, so it\u2019s a moderate task requiring a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-6202": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description precisely identifies the bug in src/_pytest/python.py getmodpath (around lines 274\u2013292) where s.replace('.[', '[') corrupts parameter names. It includes clear reproduction steps with pytest parametrize on the string '..[', expected vs actual output, and links to reports.py (lines 129\u2013149) and nodes.py (432\u2013441) to trace call hierarchy. This gives a developer all necessary context to implement the one-line fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix entails a small code change\u2014removing a single replace call in getmodpath\u2014and updating or adding a few assertions in an existing test file. An experienced engineer can locate the code, make the one-line patch, and adjust the tests within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns; the issue is self-contained, reproducible with minimal setup, and suitable for a coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7205": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue pinpoints a single function (_show_fixture_action in src/_pytest/setuponly.py) where bytes parameters are formatted via str(), causing BytesWarning under -bb. It explicitly suggests using saferepr, and the gold patch shows importing saferepr and replacing tw.write(\\\"[{}]\\\".format(...)) with saferepr(fixturedef.cached_param). This is precise and leaves no ambiguity about what code to change.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating the _show_fixture_action function, importing saferepr from _pytest._io.saferepr, and replacing one line of formatting. Writing the corresponding test is also straightforward. An experienced engineer could complete this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7236": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a minimal reproducible example, detailed environment info, and clearly states the unexpected behavior (tearDown running on skipped tests with --pdb). It specifies the expected behavior (skip without teardown) and notes the regression between pytest versions, making the requirements for a correct solution unambiguous and actionable.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires understanding pytest\u2019s unittest integration internals, locating where tearDown is conditionally executed, and adding skip detection logic. Writing and testing the helper function and updating fixture behavior across multiple code paths is non-trivial and would likely take 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7324": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"While the issue clearly describes the crash symptom (interpreter aborts on compile of literal False in debug build) and the desired outcome (return a code object or controlled exception rather than a crash), it does not point to the specific module or function in the pytest codebase where AST expressions are built. An engineer must locate the mark\u2010expression parser, infer that special constants like True, False, None must be prefixed or handled specially, and decide how to patch both the AST builder and its associated matcher. The scope and solution approach are clear, but implementation details require navigating the code.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer will need to find and understand the mark expression parser in pytest (src/_pytest/mark/expression.py), add logic to prefix special constants (True, False, None), adjust the matcher to strip prefixes, and update tests. This touches multiple code paths and tests and likely takes a couple of hours, especially if unfamiliar with pytest\u2019s internal AST handling.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and the tests provided in the PR sufficiently verify the fix. The change is localized and won\u2019t introduce broader integration problems.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7490": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description for dynamically adding an xfail marker in pytest is precise and complete. It clearly states versions of pytest involved (5.x vs 6.0.0rc1), the exact behavior difference, and how to reproduce the problem with a minimal test function using the request fixture and request.node.add_marker. It specifies expected and actual outcomes when running pytest with \u201c-rsx\u201d, and includes OS and pytest version information. Given this, an engineer can confidently know which functions to inspect (evaluate_xfail_marks, pytest_runtest_setup, pytest_runtest_call in src/_pytest/skipping.py) and how to write a test to verify the fix (in testing/test_skipping.py). No ambiguity remains about the defect or the success criteria.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this issue requires familiarity with pytest\u2019s hook implementations and the internal item._store mechanics for tracking xfail/skip state. One must locate and modify pytest_runtest_setup and pytest_runtest_call in skipping.py, ensure correct ordering of setting xfail flags, and add the final check after yield. Writing new tests in testing/test_skipping.py also requires understanding Testdir and assertion helpers. An experienced engineer would need a few hours to understand the hooks, reason about edge cases for dynamic markers, implement the patch across ~20 lines, and validate with the provided tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7521": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the environment (pytest 6.0.0rc1 on Fedora with Python 3.8/3.9), the steps to reproduce (setting COLUMNS and LINES, creating ProgressIndicatorPercent, capturing output via capfd.readouterr()), the expected behavior (preserve '\\\\r' at end) versus observed behavior (converted to '\\\\n'), and even provides a minimal repro using print(end='\\\\r'). This gives a precise testing context and a specific failure point in src/_pytest/capture.py.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires understanding Python\u2019s open file newline handling and locating the capture implementation in pytest\u2019s capture.py. Once the correct file and API are identified, adding newline=\\\"\\\" is a small API change and adding a parametrized test is straightforward, which should take an experienced engineer under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues identified.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-10297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that RidgeClassifierCV lacks support for the store_cv_values parameter despite documentation indicating it should work. It provides explicit reproduction steps, expected vs. actual behavior, and version details. The necessary change is unambiguous: add a boolean store_cv_values argument to RidgeClassifierCV.__init__, propagate it to the base class, update docstrings, and implement corresponding tests.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires locating the RidgeClassifierCV class in ridge.py, adding a new parameter and docstring entries, updating the super call, and adding/testing new test cases. These are straightforward edits that an experienced engineer can implement in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-10908": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the current behavior of get_feature_names raising NotFittedError even when a vocabulary is provided, and specifies that get_feature_names should work immediately if vocabulary parameter is supplied. It references specific methods (_validate_vocabulary, transform) and expected behavior, leaving no ambiguity about the fix.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This requires a small change in one method (adding a pre-check in get_feature_names) and updating/adding a few tests. An experienced engineer familiar with the codebase could implement and test this in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns; the sample is straightforward and self-contained.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-12585": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the failure mode: calling sklearn.clone on an estimator whose parameter is an estimator class results in a TypeError because get_params is invoked on a class without a self. It specifies exact file (base.py), line, and suggests the precise condition change. It outlines steps to reproduce and expected vs. actual behavior, allowing an engineer to implement and test the fix without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves a small change to the clone function in sklearn/base.py\u2014adding an isinstance(estimator, type) check\u2014and writing one new simple test. An experienced engineer familiar with the codebase could implement and verify the change within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns; the sample is self-contained and suitable for benchmarking.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13135": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states that KBinsDiscretizer with strategy='kmeans' can produce unsorted bin edges causing np.digitize to raise an error. It provides a minimal reproducible example (values [0,0.5,2,3,9,10] with five bins and ordinal encoding), explicit steps to reproduce the failure, expected vs actual behavior, and the relevant file path (_discretization.py). It is evident that sorting the cluster centers before computing bin_edges will resolve the error, making the requirements unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is a small, localized change in sklearn/preprocessing/_discretization.py: after computing centers from KMeans, insert centers.sort(). Adding this one line and a corresponding test case is straightforward but requires understanding of np.digitize\u2019s requirement for sorted bin edges and the existing test structure. An experienced engineer could implement and verify this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13142": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description for GaussianMixture clearly states the discrepancy between fit_predict and predict when n_init>1, provides steps to reproduce with code, shows expected vs actual results, and includes environment versions to isolate the error.  It is obvious that the fix is to ensure a final E-step is run before returning labels, and the PR context confirms exactly where to insert that call in the fit_predict method.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a minimal change: understanding the logic in fit_predict, adding a single e_step call, and updating two small test files. An experienced engineer could implement and validate this in well under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13328": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the problem (HuberRegressor.fit fails on boolean X due to unary minus on bool in _huber_loss_and_gradient), the expected behavior (booleans should be implicitly converted to floats like in LinearRegression), and even points to the relevant code paths (sklearn/linear_model/huber.py fit method, use of check_X_y without dtype coercion). It provides reproduction steps, error context, and desired change scope. The provided patch shows exactly where to add dtype=[np.float64, np.float32].\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, well-scoped change: adding a dtype argument to an existing check_X_y call in sklearn/linear_model/huber.py and extending the test suite with a new test_huber_bool in test_huber.py. An experienced engineer could locate the bug, verify the fix, and run tests in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13779": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes the failure: when an estimator is set to None, the VotingClassifier fit loop still calls methods on None, causing an AttributeError. It specifies reproduction steps on the iris dataset, the expected behavior (skip None estimators or handle missing ones properly), and where in the code (the sample_weight handling loop in fit) a check is missing. This is sufficient for an engineer to locate the fit method in sklearn/ensemble/voting.py, add the conditional guard (if step is None: continue), and add corresponding tests in test_voting.py to ensure that setting an estimator to None and fitting with sample weights no longer crashes.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Locating the sample_weight loop in fit and adding a simple None check is straightforward. Understanding the expected behavior and writing the two-line patch plus a parametrized pytest is a small change requiring minimal time (15min\u20131h).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14053": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes the error scenario, the function involved, steps to reproduce, and the expected vs actual behavior, so an engineer has sufficient information to implement a fix without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves a small change to handle undefined features in the existing list comprehension and adding corresponding tests, a task likely taking under an hour for an experienced developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14087": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the context (LogisticRegressionCV with refit=False), the exact parameters used (solver='saga' or 'liblinear', tol=1e-2, five folds), the expected behavior (complete CV without error and return selected regularization parameters), and the actual error (IndexError during coefficient path averaging due to wrong array dimensions). It identifies precisely where in sklearn/linear_model/logistic.py the failure occurs ('best_indices', 'coefs_paths', 'l1_ratios_') and what the correct behavior should be, so no further clarification is needed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires reading the fit method in sklearn/linear_model/logistic.py, understanding how coefs_paths and best_indices interact, deciding to guard the l1_ratio logic when penalty!='elasticnet', and adding a small conditional plus adjusting multi_class handling. Including updating or adding tests adds some overhead. An experienced engineer familiar with the codebase could implement and test this in a couple of hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blocking issues. The sample is self-contained, dependencies and steps to reproduce are fully described, and the provided test patch covers the new behavior.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14629": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes an AttributeError when using cross_val_predict(method='predict_proba') with MultiOutputClassifier in sklearn/model_selection/_validation.py lines 857\u2013866. The reporter pinpoints that MultiOutputClassifier lacks a classes_ attribute and suggests populating self.classes_ from the estimators in its fit() method (e.g. mo_clf.estimators_[i].classes_). Reproduction steps and expected vs. actual behavior are well-defined, so an engineer can jump straight to editing sklearn/multioutput.py and writing tests.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires understanding the MultiOutputClassifier class (in sklearn/multioutput.py), adding two lines in fit() to set self.classes_, and adding a parametrized test in test_multioutput.py. The change spans a single file plus tests and can be implemented and reviewed within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14710": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes that early stopping\u2019s internal scorer receives y_true as integer codes while predictions remain as original string labels, causing a type mismatch error. The reproduction steps specify how to generate data with string labels, instantiate HistGradientBoostingClassifier with early stopping, and observe the exception. The potential resolution diff pinpoints exactly where to insert code in sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py, referencing methods _check_early_stopping_scorer, train_score_, and validation_score_. There is no ambiguity about what the problem is or how success is measured (no exceptions and passing tests).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires understanding the early-stopping scorer implementation in gradient_boosting.py, locating the _check_early_stopping_scorer method, and adding a small conditional conversion using classes_. The required change spans only a few lines in one file, plus corresponding test adjustments in test_gradient_boosting.py. An experienced engineer familiar with sklearn utilities and classification encodings could implement and test this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14894": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear explanation of the failure in _sparse_fit when no support vectors exist, including a minimal reproducible example (explicit sparse 4\u00d74 CSR matrix, target values, model parameters), the exact error (ZeroDivisionError), and the expected behavior (dual_coef_ set to an empty CSR matrix). The location is identified (sklearn/svm/base.py::_sparse_fit), and the goal\u2014guard against an empty support_vectors_ case\u2014is unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized to a single function, requiring the addition of a simple if-check for n_SV==0 and returning an empty sparse matrix. Understanding the existing CSR construction and writing the branch takes minimal investigation. Adding a test is straightforward. An experienced engineer could complete this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified; the sample is self-contained and suitable for benchmark use.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-25747": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes a pandas transform output problem in FeatureUnion: when pandas output mode is enabled, the framework incorrectly tries to reattach the original hourly index to the aggregated daily DataFrame, causing a mismatch. The root cause and desired behavior (not reassigning index if data is already a DataFrame) are unambiguous, and the code location (_wrap_in_pandas_container in sklearn/utils/_set_output.py) is identified.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing the issue involves a small change in _wrap_in_pandas_container to skip resetting the index when the input is a DataFrame, plus adding a few test cases. An engineer familiar with scikit-learn\u2019s output wrapper can implement and test this within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-25931": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the problematic behavior (a spurious feature-name warning when fitting IsolationForest with non-default contamination), outlines why it\u2019s unexpected, and even pinpoints the likely root cause (calling predict during fit). It provides explicit reproduction steps, expected vs. actual results, and version details. An engineer can locate the relevant code in ensemble/_iforest.py (around fit and score_samples), understand what to change, and validate the fix with the provided test. There is no ambiguity about what constitutes a successful solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires understanding the estimator\u2019s internals in _iforest.py, identifying that score_samples triggers validation removing feature names, and introducing a private _score_samples method plus updating fit and score_samples calls. It involves editing two files (one for implementation and one test), adding a helper method and adjusting existing logic. Familiarity with scikit-learn\u2019s validation patterns and warning mechanisms is needed. Overall, this is a nontrivial change that should take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-25973": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the problem (SequentialFeatureSelector failing to accept an iterable cv), provides reproducible steps (synthetic data, LeaveOneGroupOut splits), expected vs actual behavior, and relevant context (scikit-learn version, code snippet). It is sufficiently detailed to implement a fix without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires a small update: importing and using check_cv in fit, adjusting method signatures, and updating a test. This is a targeted change across one module and its test, achievable within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-10323": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states how the literalinclude directive with prepend or append loses its leading whitespace, shifting code examples left. It provides reproduction steps in a Sphinx document, the expected indented XML snippet, and the observed misalignment. The problem domain is specific to filters applied in sphinx/directives/code.py, and the tests added in tests/test_directive_code.py illustrate the failing behavior. From this information, an experienced developer can identify where to adjust the order of filters (inserting dedent_filter before prepend_filter) to preserve indentation. There is no ambiguity about the expected fix or the location in codebase, and the tests define acceptance criteria.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Understanding the filter pipeline in sphinx/directives/code.py and recognizing the incorrect ordering of dedent, prepend, and append filters requires reading about one function and updating a small list. Writing the test change is also straightforward. This is a focused change, taking less than an hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-10673": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies that Sphinx\u2019s toctree directive skips generated index documents (genindex, modindex, search) with warnings because they are not in env.found_docs. It specifies the desired behavior: allow these auto-generated docs to be referenced without errors in a toctree. The code location to modify is unambiguous (sphinx/directives/other.py parse_content, sphinx/environment/adapters/toctree.py resolve, and sphinx/environment/collectors/toctree.py). The test expectations and sample directive usage give precise success criteria.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding Sphinx\u2019s internal toctree handling: the environment.domains['std'].initial_data labels, merging found_docs with generated docs, and adjusting logic across three modules (directives/other.py, adapters/toctree.py, collectors/toctree.py). An engineer must familiarize with these APIs and write coordinated changes plus tests. This is more than a trivial tweak but fits within 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional blockers. The issue is straight-forward for someone familiar with Sphinx\u2019s extension points and testing setup.\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-7440": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies that Sphinx treats glossary terms case-insensitively and lowercases all term text when building indices, causing \u201cMySQL\u201d and \u201cmysql\u201d to collide. The reporter wants MySQL and mysql treated distinctly. They point to the file sphinx/domains/std.py and the glossary.rst source, describe reproduction steps, and state the expected behavior unambiguously. The necessary change is to remove the lowercase conversion in note_object and disable lowercase in XRefRole, which is straightforward to implement.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing the issue involves changing two lines in sphinx/domains/std.py: removing the .lower() call in std.note_object and dropping lowercase=True in XRefRole. An engineer familiar with the codebase can locate these calls, apply the edits, and run existing tests in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7462": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the failure in Sphinx\u2019s Python domain unparse logic when handling ast.Tuple nodes with no elements: the existing code unconditionally calls pop() on an empty result, causing IndexError. It specifies the reproduction steps, environment, and expected outcome, making it straightforward to implement a branch for empty tuples in both unparse functions.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves adding simple conditional checks for empty tuple nodes in two unparse implementations and updating tests. Understanding the AST unparse logic and editing two small code blocks plus tests is straightforward and can be completed in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7985": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue summary and reproduction steps clearly articulate the missing behavior: Sphinx\u2019s linkcheck builder should treat non-http URIs as local file references and verify their existence under the source directory. The reproduction gives instructions to generate a project, include a valid and an invalid reference, and invoke make linkcheck to observe the current failing behavior. The expected result simply states that local links should also be checked. This provides sufficient information on what code to modify \u2013 in linkcheck.py, add a conditional branch for local URIs and use os.path.exists to mark working or broken \u2013 and how to validate it using tests.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Because the required change is limited to extending a conditional in a single builder module and updating existing tests, an experienced engineer can familiarize themselves with the linkcheck builder, implement the new branch for local file URIs, and adjust tests within a couple of hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-8269": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that linkcheck reports an anchor-not-found error even when the HTTP request returned a status error (404/500). It specifies steps to reproduce, expected vs actual output, and identifies the specific behavior in builders/linkcheck.py where raise_for_status() should occur before anchor checking.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change confined to one function in sphinx/builders/linkcheck.py (adding response.raise_for_status()), plus adding a focused unit test. An experienced engineer could locate the relevant code and adjust it within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major concerns. The description and tests make it straightforward to integrate into a benchmark workflow.\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-8551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue provides a clear description of the bug, reproduction steps, and the expected behavior. It identifies specifically how :type: and :rtype: annotations resolve names differently than explicit xrefs and where the ambiguity arises. The context (module/class names) is fully specified, so an engineer can directly locate and modify the relevant code in sphinx/domains/python.py and sphinx/util/docfields.py.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves adding two simple attribute assignments and passing an existing env parameter in two small code locations. The changes are less than ten lines across two files, and tests only need minor adjustments. An engineer familiar with Sphinx internals could implement and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-9711": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that the `verify_needs_extensions` function in sphinx/extension.py uses string comparison on `extension.version` leading to incorrect ordering (e.g., '0.6' vs. '0.10'). It provides reproduction steps on the mplcursors project, specifies the file (`sphinx/extension.py`) and function (`verify_needs_extensions`) to modify, references the attribute `extension.version`, and states the expected behavior (using version-aware comparison). This is sufficient to implement and test the fix without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change confined to one function (adding `packaging.version.Version` parsing and a fallback) and updating a test file. An experienced engineer can identify the appropriate library, implement the comparison logic, and write the tests within 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-13372": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints the exact failing behavior (UnboundLocalError when Mul args are reversed), identifies the missing precision initialization branches, and even suggests the precise patch (adding else: raise NotImplementedError). There\u2019s no ambiguity about what code to change or what correct behavior to achieve.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"The fix is a small, localized change: adding two else clauses in evalf and updating a single test. An experienced engineer familiar with the codebase could understand, implement, and test this patch in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-13480": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the SymPy functions and the specific expression \u2018coth(log(tan(x)))\u2019 to be evaluated, the substitution of integer values like 2, 3, etc., and the NameError caused by referencing an undefined variable \u2018cotm\u2019 in sympy/functions/elementary/hyperbolic.py around line 587. The expected behavior (returning a numerical result) and the failure mode (NameError) are plainly described, so an engineer can locate the bug and correct the conditional from \u2018if cotm is S.ComplexInfinity\u2019 to \u2018if cothm is S.ComplexInfinity\u2019.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"The patch is a single-line fix in hyperbolic.py plus two additional assertions in test_hyperbolic.py. Locating the typo in the conditional is straightforward for someone familiar with the hyperbolic function implementation, and writing the simple test cases is trivial. Overall, an experienced developer can make these fixes well under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13877": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes how to reproduce the problem (building an n\u00d7n symbolic matrix, computing its determinant for successive n, and observing NaN or failures at n=5,6), states the expected behavior (meaningful symbolic results instead of NaN), and hints at the root cause (use of the Bareiss algorithm only valid for integer matrices and invalid NaN comparisons). The provided test patch shows exactly where new tests should go (sympy/matrices/tests/test_matrices.py) and what assertions to add. The code patch shows precisely which functions to modify (_eval_det_bareiss in sympy/matrices/matrices.py and random_complex_number in sympy/utilities/randtest.py), including filenames, function names (_is_zero_after_expand_mul, bareiss, random_complex_number), and the lines to insert or change. There is no ambiguity about the inputs, the expected outputs, or the scope of changes required.\" ,\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An engineer must first understand SymPy\u2019s matrix determinant implementation, locate the Bareiss algorithm in sympy/matrices/matrices.py, and grasp how iszerofunc is used for pivot selection. They then create a helper (_is_zero_after_expand_mul), adjust the pivot logic, modify random_complex_number to accept a tolerance parameter, and update tests in sympy/matrices/tests/test_matrices.py. These changes span multiple files but are localized. Familiarization plus coding and testing would take around 1-4 hours for an experienced developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The test harness and change scope are clear, and no hidden dependencies or unclear requirements remain.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-16792": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear minimal reproduction (wrapping a constant expression with an unused MatrixSymbol argument), shows the expected versus observed behavior, and pinpoints that the generated C function signature mistakenly declares the array argument as a plain double instead of a pointer. It identifies that the error only occurs when the expression does not depend on the array argument and even suggests the general area of code (codegen) where the fix should be made. This level of detail makes it straightforward to write and test a patch.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires understanding the sympy.utilities.codegen module, in particular how input arguments and their metadata (dimensions) are handled when generating C code. The developer must navigate the Routine builder logic, add a helper for dimensions, modify argument handling in two places, and add a corresponding test case. This involves editing about 20 lines across codegen.py and test_codegen.py, making it more than a trivial fix but manageable within a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17139": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states how to reproduce the error (simplify(cos(x)**I)), explains why it fails (comparing complex I to zero is invalid), and implicitly what the correct behavior should be (skip the negative exponent check when the exponent is not real). The reproduction steps, faulty behavior, and desired outcome are all specified, enabling an engineer to implement a targeted fix without ambiguity.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a small, localized change: adding a simple conditional check for non-real exponents in one function and updating a few tests. An experienced developer familiar with the codebase could implement and test this fix in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17318": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly states how to reproduce the IndexError, describes the expected behavior (either simplify or return unchanged), and pinpoints the failure in the internal surd\u2010splitting routine. It specifies that non\u2010denestable expressions must be returned intact, providing a clear \u201cwhat\u201d to implement and guiding where to apply the fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An engineer familiar with the codebase must locate the surd-splitting logic in sqrtdenest.py, understand the tuple lookup that can fail, and add a rational & positivity check before unpacking. The patch touches two functions and requires writing a new test, which entails reading ~100 lines of math code and ensuring no regressions\u2014this spans a couple of hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17630": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that in block multiplication a zero block is converted into a plain Zero, causing an AttributeError when querying its cols. It even shows the snippet\\n\\n    >>> type(b._blockmul(b).blocks[0, 1])\\n    <class 'sympy.core.numbers.Zero'>\\n\\nThe required change is to adjust the block multiplication logic, likely in sympy/matrices/expressions/matexpr.py (in the _postprocessor or _blockmul routine), to return ZeroMatrix rather than Zero for zero blocks. This pinpoints the file and behavior to modify, making a meaningful attempt at a solution possible without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires understanding Sympy\u2019s BlockMatrix implementation, specifically the internal _blockmul method and the _postprocessor in sympy/matrices/expressions/matexpr.py. An engineer would need to locate where results from block multiplication are collapsed, ensure zero blocks remain ZeroMatrix, implement a small conditional or conversion, then update or add tests in tests/test_blockmatrix.py and tests/test_matadd.py to validate the behavior. This moderate change across a few files and validating with the test suite would likely take an experienced engineer one to four hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"While the issue itself is straightforward\u2014preserving ZeroMatrix blocks during multiplication\u2014an engineer must be comfortable navigating Sympy\u2019s expression tree and matrix expression internals. Writing suitable test cases also requires understanding of block_collapse, MatAdd, and how _postprocessor works. There are no further obstacles or missing information; this is a well-contained sample.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17655": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that multiplying a Point by a float works when using __mul__ (p4 * 2.0) but raises a GeometryError when the scalar is on the left (2.0 * p4). It includes a minimal reproduction, the observed failure, and the expected behavior (\u201cboth lines give the same result\u201d). There is no ambiguity about what needs to be implemented: add an __rmul__ method delegating to __mul__.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a trivial fix: implement __rmul__ in the Point class (a few lines) and add a couple of test assertions. An experienced engineer familiar with Python\u2019s data model and the codebase could complete this in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-18211": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that calling solveset on the equation n*cos(n) - 3*sin(n) = 0 raises NotImplementedError and that instead a ConditionSet representing the solution over the Reals is expected. An engineer can locate the relevant _eval_as_set method in sympy/core/relational.py, wrap the call to solve_univariate_inequality in a try/except for NotImplementedError, import ConditionSet, and return it. The input example and desired output are explicitly given.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, localized change: modify the _eval_as_set method in one file to add a try/except, import ConditionSet, and adjust return behavior, plus add two simple tests. An engineer familiar with the codebase could implement and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-20428": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text clearly describes the bug in clear_denoms and shows how bad_poly.rep contains a leading zero. It explains the erroneous behavior of Poly.is_zero vs as_expr() and its downstream impact on gcd and content routines, but it doesn\u2019t directly tell you which method to patch or how to invoke is_zero in the ExpressionDomain.__bool__ method. You must locate the EX __bool__ implementation in sympy/polys/domains/expressiondomain.py and change ex != 0 to ex.is_zero, and add a new test case, so while the what is clear, the how requires inspecting domain and DMP internals, indicating some blanks remain.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced SymPy contributor would spend time tracing the call stack for clear_denoms, inspecting DMP representations, and discovering that EX.__bool__ is implemented via ex != 0 instead of ex.is_zero. Locating the right file (expressiondomain.py), updating __bool__, and writing a test takes more than an hour but under four hours for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-20438": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The author clearly shows how to reproduce the wrong subset test: a = {1,2}, b = ProductSet(a,a), c = FiniteSet((1,1),(1,2),(2,1),(2,2)). They note that b.is_subset(c) returns None, simplify(Eq(b,c)) raises an AttributeError, and FiniteSet(b) prints the symbolic power instead of the expected pairs. From these symptoms it is straightforward to infer that dispatch for ProductSet\u2286FiniteSet is missing and that the simplify path needs to guard against non-Expr types before calling equals. The tests in test_sets.py give concrete assertions to aim for.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding SymPy\u2019s multiple dispatch system, locating the subset handlers in sets/handlers/issubset.py, adding a case for ProductSet vs FiniteSet, then adjusting relational._eval_simplify to import Expr and guard equals calls, plus updating the comparison dispatch in comparison.py. The patch touches three modules and test files, so unpacking the abstractions and verifying behavior would take a few hours.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This issue description actually bundles three related but distinct problems: missing subset dispatch, simplify(equality) crash, and repr of FiniteSet(ProductSet). That breadth may distract a solver or require additional clarification about scope for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-20590": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states the problem (Symbol instances lost __dict__ due to missing __slots__), demonstrates behavior differences between versions 1.6.2 and 1.7, and even hints at the mixin class in sympy/core/_print_helpers.py where __slots__ was dropped. The desired outcome (adding empty __slots__ to restore attribute storage) is unambiguous.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"Recovering __dict__ support requires a trivial one-line change: add __slots__ = () to the Printable mixin in sympy/core/_print_helpers.py. An experienced engineer can locate and apply this change and rerun tests in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-21379": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description includes a minimal reproducible example (MWE) with clear reproduction steps: how to clear the cache, declare symbols, build the specific expression involving Piecewise, sinh, exp, and a division, and then apply `subs({1:1.0})` to observe a `PolynomialError`. It also specifies the context (the hyperbolic functions, real symbol assumptions, and caching behavior) and lists precise conditions under which the error occurs or vanishes. This level of detail makes it clear what the problem is and what the fix must address.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing the issue involves locating the `eval` method for `Mod` in `sympy/core/mod.py`, adding an import for `PolynomialError`, and wrapping the existing `gcd` and `gcd_terms` calls in a try/except block to catch `PolynomialError` and set `G = S.One`. A matching test case then needs to be added in `sympy/core/tests/test_arit.py`. This is a targeted change in two small sections of code and one test file, requiring some familiarity with Sympy\u2019s internals but solvable within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-22714": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the context (SymPy\u2019s evaluate(False) mode), the unexpected error message (\u201cImaginary coordinates are not permitted.\u201d), and the exact reproduction steps using sp.S with Point2D. It specifies the expected behavior (producing a Point2D with real integer coordinates) versus the observed behavior (raising an error only when evaluation is off). All necessary information to pinpoint the conditional check in geometry/point.py and devise the small patch is provided without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue requires locating the imaginary\u2010coordinate check in geometry/point.py, understanding the im(a) property and evaluation contexts, and changing one Boolean condition plus adding a simple test. An experienced engineer could implement and verify this patch in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23824": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the function (kahane_simplify), explains the incorrect behavior, and provides concrete minimal examples of expected vs actual results (\u03b3^\u03bc\u03b3_\u03bc\u03b3^\u03c1\u03b3^\u03c3 vs \u03b3^\u03c1\u03b3^\u03c3\u03b3^\u03bc\u03b3_\u03bc). It pinpoints the root cause (a reversed insertion loop) and the precise location in code, leaving no ambiguity about what needs to be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a localized one-line bug in the gamma_matrices.py file. An experienced engineer, after reading the simple examples and inspecting the insertion loop, would take 15\u201360 minutes to understand the reversal error, implement the fix, and verify tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-23950": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The GitHub issue explicitly identifies that the as_set method on the Contains class in sympy/sets/contains.py is unimplemented, raising NotImplementedError rather than returning the contained set. It explains the problem context: membership tests on real numbers fallback incorrectly, causing attribute errors due to missing as_relational. It references specific code paths and file names, allowing an engineer to locate the Contains class, understand the expected behavior (return the second argument of Contains), and implement the missing functionality.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a trivial single-line change to implement Contains.as_set by returning the argument representing the set. It requires minimal familiarity with the codebase, modifying one method and adding or updating a few test assertions. An experienced engineer can make this fix in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24066": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the specific function (_collect_factor_and_dimension) and the precise failure mode when handling an exponential of a ratio, explaining that exponentials require dimensionless arguments. It shows how to reproduce the error, states the expected behavior, and implies where to inject the dimensionless check, making the requirements unambiguous for a knowledgeable engineer.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires understanding the unit system in sympy, locating and modifying the code path for Function handling in _collect_factor_and_dimension, adding logic to treat dimensionless exponents correctly, and updating tests. This moderate investigation and code update across multiple lines and test files would likely take 1\u20134 hours for an experienced engineer familiarizing themselves with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the sample is self-contained and suitable for benchmarking.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-24443": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the broken behavior in sympy/combinatorics/homomorphisms.py, specifically the internal _image() function, and even gives line references (L336-L337) in combinatorics/homomorphisms.py. It describes the reproduction steps (building a DihedralGroup of order 6, mapping generators to themselves) and pinpoints the cause: the in gens test fails when r[i] is an inverted generator. The expected identity homomorphism and the exact failure (ValueError) are described, so an experienced engineer knows exactly where to look and what to change.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding the representation of group elements (r.array_form, symbols, mapping between symbolic and actual generators), rewriting ~30 lines of the _image() function, updating tests, and ensuring compatibility with both PermutationGroup and FpGroup code paths. An experienced engineer would likely need 1\u20134 hours to familiarize with the combinatorics module, implement the simplified algorithm, and validate with new tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    }
]