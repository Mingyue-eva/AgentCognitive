[
    {
        "astropy__astropy-13033": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that removing a required column leads to a misleading exception. It provides reproduction steps and the expected behavior, specifying that the error should inform users about missing required columns. The bug is localized in the _check_required_columns method. From this information, an engineer can deduce that updating the ValueError formatting (including handling lists of required columns) is necessary. All context required for a fix is given, including how remove_column triggers this check.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change involving adding a helper for string formatting and updating one method and its tests. An experienced engineer could implement and verify it within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Although the issue is straightforward, one minor consideration is ensuring compatibility across different usages of _required_columns in other parts of the codebase. While the change only affects the message formatting in _check_required_columns, developers should verify that no other methods depend on the exact exception string matching. Furthermore, additional tests around pluralization and edge cases (empty required_columns list or single element) ensure robustness.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-13977": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the method (__array_ufunc__ in astropy/units/quantity.py) that should be modified to return NotImplemented instead of raising ValueError for incompatible unit operations. It provides minimal reproducible code examples showing the current failure modes and the desired behavior (allow __radd__ to be called), references the numpy subclassing docs, and lays out concrete test scenarios. An experienced engineer can directly locate the function, understand the expected change, and implement it without additional clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding numpy ufunc subclass protocols, the existing converters_and_unit logic in __array_ufunc__, catching and distinguishing TypeError/ValueError cases, and updating both the core quantity code and its test suite. Although nontrivial, it is a focused change in a single file plus tests and can be completed within a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14096": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly demonstrates the problem using a minimal reproducible example: subclassing SkyCoord with a custom property that accesses a non-existent attribute. It states that the raised AttributeError references the property name rather than the missing attribute. The snippet highlights __getattr__ in sky_coordinate.py; it is obvious that the remedy is to delegate to __getattribute__ to let Python report the correct attribute. No further clarification is needed.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"I chose a difficulty of 0 because the fix involves a small change to just a few lines in the __getattr__ method in sky_coordinate.py, replacing the custom AttributeError raise with a call to __getattribute__ to produce the proper exception, plus adding a simple regression test. For someone familiar with Python\u2019s attribute lookup, this is a trivial change requiring less than 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14182": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides concrete Python REPL examples showing existing behavior and desired output, names the relevant class (RST in io/ascii/rst.py), specifies parameters like header_rows and outlines expected table formatting. The request is precise and actionable without ambiguity, including both read and write behavior.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This change requires understanding of the RST writer internals in astropy/io/ascii/rst.py, modifying constructor, write and read methods, and updating test files. It spans multiple small edits (~30 lines) and test additions, likely taking 1\u20134 hours to implement and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14309": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly identifies the function at fault (astropy.io.fits.connect.is_fits) and reproduces the failure with a minimal snippet (identify_format(\\\"write\\\", Table, \\\"bububu.ecsv\\\", None, [], {})). It also references the upstream commit that introduced the change and describes the old vs. new behavior (the function returned None for non-FITS extensions but now skips that branch and invokes isinstance). An experienced engineer can locate the conditional in connect.py, see the endswith block, and modify it to return the boolean evaluation directly. While one must inspect the code to implement the patch, there is a sensible interpretation and no major ambiguity about what needs to be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a localized change: modifying a 2-line conditional in connect.py and adding a small regression test. An engineer familiar with the repository can find the is_fits function quickly, adjust the return logic, and validate with the provided test snippet in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14365": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states that ascii.qdp currently enforces uppercase QDP commands and provides a minimal reproducible example showing how a lowercase command line (\u201cread serr 1 2\u201d) crashes. It specifies that commands should be accepted case-insensitively. From this description, it is straightforward to locate the parsing logic in astropy/io/ascii/qdp.py (the _line_type regex and NO value handling) and implement a solution without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer familiar with the codebase could locate the two small changes needed (adding re.IGNORECASE to the command regex and making the NO literal comparison case-insensitive) and update a couple of lines plus a few tests. This is a small change requiring a bit of thought but doable in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The issue is self-contained and suitable for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14508": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified: it describes the exact behavior to fix (unnecessarily expanded float string causing comment truncation), provides reproduction steps including code examples and a test FITS file, pinpoints the problematic function (`io.fits.Card._format_float`), and outlines a clear expected change (use Python\u2019s `str(value)` first if it fits within 20 characters). There is no ambiguity about what constitutes a correct solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a localized change in a single helper function (`_format_float`) and adding a small regression test. An experienced engineer familiar with the codebase could understand the problem, implement the change, and validate it within 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\", \"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14995": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly defines the context (v5.3 mask propagation), reproduces the error with code examples, specifies expected behavior, and points to the relevant function (_arithmetic_mask). It unambiguously states that when one operand has no mask, the existing mask should be propagated, making the required change straightforward.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An engineer must locate the mask propagation logic in nddata/mixins/ndarithmetic.py, understand the existing condition, change a single branch to test operand.mask instead of operand, and add a few test cases. This involves reading ~50 lines of code and writing a small patch, which should take 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-7336": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that adding a return annotation of None causes the decorator to attempt unit conversion on None, shows a minimal repro and suggests skipping the check when return_annotation is None. It is unambiguous what change is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires a small conditional change in the decorator and adding a simple test case. An experienced engineer could implement and validate it within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7606": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that comparing an UnrecognizedUnit instance to None should return False instead of raising a TypeError. It identifies the class (UnrecognizedUnit) and the operation (__eq__ with None) that need to be adjusted. Though it does not specify exactly how to handle other unmatched types, the obvious interpretation is to catch the exception in Unit.__eq__ and UnrecognizedUnit.__eq__ and return False (or NotImplemented) for None, making the desired behavior unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves locating the __eq__ methods in astropy/units/core.py, adding exception handling around Unit(other, parse_strict='silent'), and returning NotImplemented instead of False or allowing fallback. This is a small change across two methods and updating a few tests, achievable within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-7671": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the problem in the astropy.utils.minversion function: comparisons against version strings containing non-numeric suffixes like 'dev' fail due to a known LooseVersion bug. It provides concrete interactive examples showing the incorrect behavior with LooseVersion and the correct behavior with pkg_resources.parse_version. The issue text references the specific function (minversion) and module (astropy/utils/introspection.py) where the change is needed, and describes exactly what outcome is expected. This level of detail makes it straightforward to determine what code needs to be modified and what tests should pass.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer would need to understand the minversion implementation, research the LooseVersion limitations, craft a suitable regex based on PEP440, integrate it into the existing function, and update or add tests. While the change itself is localized and small, it requires careful thought about version parsing and regex correctness, and likely 1\u20134 hours to implement and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-10554": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a clear reproduction of the bug with example code showing how a unioned queryset ordering breaks when re-evaluated. It includes relevant methods and sample output, but doesn\u2019t explicitly state the expected result, leaving it implicit that ordering should be preserved. While an experienced Django developer can infer what to fix, newcomers might need to guess the exact desired behavior.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s SQL compiler and query building internals, modifying methods across two files, adding a helper to manage select columns, and ensuring tests cover various ordering cases. An experienced engineer familiarizing themselves with the codebase would likely need between one and four hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues for benchmarking. However, this task relies heavily on in-depth knowledge of Django\u2019s ORM internals, so candidates without that background may struggle even if they can code effectively.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11087": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the context (upgrading Django 1.11 from Python 2.7 to 3.6), the precise failure (UnicodeDecodeError during .delete() of Jobs), and reproduces it with debug\u2010level SQL logs. It specifies the problematic table/field (text_log_error.line), identifies two root causes, and states the desired behavior (avoid fetching unnecessary fields during cascade delete). It even points to the exact file and lines where the change should occur (deletion.py), making it straightforward to locate, implement, and test a solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix requires understanding Django\u2019s deletion machinery, modifying multiple methods in deletion.py (can_fast_delete, collect), selectively deferring fields, and adding new tests to verify generated SQL. An experienced engineer would need time to explore the ORM internals, refactor code, and ensure full test coverage\u2014likely 1\u20134 hours of focused work.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is self\u2010contained, with clear reproduction steps, code pointers, and test cases, making it suitable for benchmarking.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11265": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description includes a concrete failing example in tests (changing .filter() to .exclude()), the error (FieldError on annotation name), and even points to the likely faulty function (split_exclude in django/db/models/sql/query.py). It names files, functions, and shows the expected vs. actual behavior. This gives a clear specification of what needs to be fixed (preserve _filtered_relations on the subquery and refine join trimming logic).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer will need to familiarize themselves with Django\u2019s Query object internals (split_exclude, alias_map, LOUTER joins, filtered_relation flags), update two methods across repository files, and run existing tests. This is more than a trivial one-line change but fits within a few hours of work.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that Django's CheckConstraint SQL generation erroneously includes fully qualified column names when combining OR and AND. It provides model definitions, migration SQL, and the desired SQL without prefixes. The root cause (use of Col vs SimpleCol) is identified, and the relevant file (django/db/models/sql/query.py) and function (_add_q in Query) are referenced. This detail is sufficient for implementing and testing a correct solution without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized to a one-line change in django/db/models/sql/query.py to pass the simple_col flag, plus adding targeted tests. An experienced engineer familiar with Django\u2019s Q/SQL generation can understand and implement this within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues; the sample is clear and self-contained.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11532": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly articulates the failure context: non-ASCII hostname triggers Message-ID header encoding errors under iso-8859-1. It points to specific files (django/core/mail/message.py and utils), lines of code, and provides reproduction steps and a targeted test stub. The suggestion to convert the domain to punycode makes the desired behavior explicit, leaving little room for ambiguity when implementing the fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing this fix involves adding a punycode utility, adjusting imports and calls across multiple modules (encoding, mail.message, mail.utils, validators, html), and updating the test suite. While it spans several files, the change is straightforward and follows a common pattern, making it achievable in a few hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major concerns; the issue is self-contained and tests fully capture the expected behavior.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the bug in _check_list_display_item, specifies how the change in Django 2.2.1 broke behavior for descriptor-only fields like PositionField, and details the expected logic with specific code paths, examples, commit references, and tests. A developer can directly derive what to change and how to write a patch.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding Django\u2019s admin check framework, the interplay of getattr, hasattr, _meta.get_field, and descriptors, and updating both the check function and tests. This is non-trivial but localized to one file and a test, so would likely take a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11734": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly demonstrates failing tests when using exclude()/~Q() with OuterRef, provides code snippets showing filter() works and exclude()/~Q() crashes, and indicates desired behavior (exclude should not crash). The failure context is unambiguous and points to ORM methods handling subqueries, so a developer can locate and fix the bug.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s ORM internals (expressions, lookups, Query.split_exclude), working across multiple files (fields/__init__.py, related_lookups.py, sql/query.py), and updating handling of OuterRef in exclude/negation. An experienced engineer should need 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the sample is ready for benchmark use.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11740": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the context (two Django apps with UUID fields), shows the exact model code before and after changing a UUIDField to a ForeignKey, and pinpoints the observed problem: the autogenerated migration is missing a dependency on App2 despite introducing a FK. It is unambiguous what\u2019s wrong (missing dependencies in the migration) and what a correct solution entails (detect foreign-key dependencies and add them to the migration).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires digging into Django\u2019s migration autodetector internals, understanding generate_altered_fields, calling _get_dependencies_for_foreign_key, and updating both the code and test suite. This is more than a trivial one-file change but still localized to a specific subsystem, taking an experienced contributor 1\u20134 hours to implement and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Though well-specified, this issue is quite Django-specific and relies heavily on familiarity with its migration framework. This may limit its applicability as a general benchmarking task for coding ability outside of Django.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11815": "{\n  \"q1_1_is_well_specified\": 0,\n  \"q1_2_explanation\": \"The issue description clearly shows the problematic behavior, provides example code and migration output, and specifies the desired change: use the enum member name rather than its translated value in generated migrations. The files and lines to update are implied (EnumSerializer in django/db/migrations/serializer.py and relevant migration writer tests), so an engineer can start implementing without further clarification.\",\n  \"q2_1_difficulty\": 2,\n  \"q2_2_explanation\": \"An engineer will need to locate and understand the EnumSerializer in django/db/migrations/serializer.py, adjust the serialization logic to emit the enum name lookup, update associated tests in tests/migrations/test_writer.py to assert the new format, and verify no regressions. This involves touching multiple files and understanding Django\u2019s migration serialization internals, which should take between 1\u20134 hours.\",\n  \"q2_3_other_issues\": 0,\n  \"q2_4_other_notes\": \"\",\n  \"q2_5_confidence\": 4\n}"
    },
    {
        "django__django-11964": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies that Choices enum values return enum.Enum instances rather than native strings or ints, provides a minimal reproducible example in django/db/models/enums.py, and shows failing tests in tests/model_enums/tests.py. The expected behavior is unambiguous: casting field values to str should yield the underlying string or integer. The precise change is to override __str__ on Choices to return str(self.value).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix only requires adding a __str__ method to the Choices base class (editing ~6 lines) and a handful of test assertions. An experienced engineer familiarizing with the enums module and test suite could implement and validate the change within 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12155": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue pinpoints exactly where the bug occurs in trim_docstring (in django/contrib/admindocs/utils.py) and explains why the first-line indentation of zero breaks the existing logic. It even provides the precise code change (skip lines[0] by slicing lines[1:]) and shows how to update parse_docstring to use inspect.cleandoc. The reproducing scenario is clear, the intended behavior defined, and the patch straightforward to implement and verify via tests.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires modifying two small functions and updating corresponding tests. An engineer familiar with Django and Python docstring handling can understand the bug, identify inspect.cleandoc as a solution, implement and verify it within about 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional blockers identified: the issue is self-contained to admindocs utilities and tests. All context needed is in the description and code snippets. The solution leverages existing Python library functions and follows Django\u2019s conventions. \",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description pinpoints the problem with simple_tag and inclusion_tag keyword-only defaults by providing concrete code examples of both valid and invalid usage (in library.py context). It shows two scenarios: a tag with a default-only keyword argument and supplying a keyword twice. Although it does not name the exact Django file or function (parse_bits in django/template/library.py), the examples clearly highlight where to look and what behavior to change. An experienced engineer can infer the correct parsing logic and implement the fix based on these examples.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding Django\u2019s template tag parsing in django/template/library.py (specifically parse_bits), updating the condition to check the \u2018kwonly\u2019 list instead of the outdated \u2018unhandled_kwargs\u2019, and adding tests in the existing test suite. This is a focused change spanning one core function and related tests, likely taking an experienced developer 1\u20134 hours to complete, including review and validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12325": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is rather terse and assumes familiarity with Django\u2019s multi-table inheritance internals. It shows two class definitions with reordered OneToOneField declarations and states that field order should not matter due to explicit parent_link markers. While a developer experienced with Django internals can infer that the resolution involves filtering OneToOneField instances by the parent_link attribute, readers lacking this background would struggle. The expected fix\u2014adjusting metaclass field collection logic\u2014must be deduced rather than explicitly specified, creating some ambiguity in how to implement the solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"I selected difficulty level 2 (1\u20134 hours) because resolving this issue requires understanding the Django model metaclass machinery, identifying where OneToOneField instances are collected, and modifying both base.py and options.py to filter by the parent_link attribute. The task also involves updating multiple test files to validate the new behavior. An experienced engineer familiar with Django internals would need a few hours to navigate the codebase, propose the patch, and ensure all tests pass.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"One potential concern is that this sample is highly specific to Django\u2019s internal model implementation, making it less suitable for a general coding ability benchmark. It requires deep domain expertise in Django ORM metaclasses, field resolution, and test infrastructure. Candidates unfamiliar with these details would likely be unable to progress, reducing the assessment\u2019s fairness.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12663": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text provides precise model definitions (A, B, C), a clear failing test case invoking SimpleLazyObject in nested subqueries, and pinpoints the regression in django/db/models/sql/query.py output_field lookup. This context is ample to guide reproducing and fixing the bug without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is confined to a single method (output_field in query.py) and adding a test, which is straightforward. While it requires understanding Django ORM internals, the small scope keeps it within an hour for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12708": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the context (a Django migration crash when deleting index_together while unique_together exists) and provides reproducible steps. It specifies the expected behavior (removing an index_together should not affect unique constraints) and hints at the specific code location (alter_index_together in django/db/backends/base/schema.py). The two key points are identified: coherent handling of index vs unique constraints and avoiding unintended index re-creation. An engineer can infer where to implement the fix and what tests are needed from this information.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires understanding Django\u2019s migration internals, locating and modifying the alter_index_together method, adjusting index deletion metadata, and updating or adding migration tests. This involves reading several files, working with the schema editor API, and writing new test cases\u2014an effort that typically takes 1\u20134 hours for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and suitable for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12754": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a minimal reproducible example with original and modified model code, clearly states the failure in migration ordering (CreateModel before RemoveField) and cites the expected behavior (reverse operation order). It references specific files (autodetector.py) and migration operations, making the task of updating the migration autodetector and writing a corresponding test straightforward without ambiguity.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\\u0019s migration autodetector internals, state management for models, and writing a unit test for migrations. An experienced engineer would need to explore and modify autodetector.py and tests, amounting to a few hours of work.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12858": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that E015 is incorrectly raised for lookups (e.g., isnull) in ordering, shows the failing examples, and identifies where to update the check logic in django/db/models/base.py in _check_ordering. The root cause and desired behavior are obvious.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires adding a simple additional condition in _check_ordering and updating tests. The change is isolated to adding get_lookup support and a test case, which an experienced engineer can implement and verify in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13012": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that wrapping a constant Value with ExpressionWrapper leads to an invalid GROUP BY clause in Postgres. It shows the execQuery function, the generated SQL, and contrasts with the correct behavior when not using ExpressionWrapper. It pinpoints the file django/db/models/expressions.py and even provides the precise method to change (adding get_group_by_cols). This is sufficient to implement and test a fix without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves adding a simple forwarding method in ExpressionWrapper and writing two unit tests. An engineer familiar with Django\u2019s ORM and expression system could implement and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The sample is self-contained, the test patch verifies behavior, and no external context is needed.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13028": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies a collision between a BooleanField named \u201cfilterable\u201d on a model and Django\u2019s internal check_filterable logic in django/db/models/sql/query.py. It shows the failing filter call, the two model definitions (ProductMetaDataType with filterable and ProductMetaData), and the exact error scenario when filtering on metadata_type. The user even points out that renaming the field to filterable_test avoids the error, implying the root cause. Together with proposed code and test patches, there is sufficient detail (file path, function name, test changes) to implement and verify a fix without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the code change itself is small\u2014a conditional wrap around the existing filterable check\u2014the engineer must understand Django\u2019s Query class and Expression API. They need to locate check_filterable in query.py, reason why non-Expression objects like model instances trigger NotSupportedError, craft the hasattr(expression, 'resolve_expression') guard, and add tests in tests/queries/tests.py. This requires reading core ORM code and writing test modifications, which realistically takes 1\u20134 hours for someone familiar with Django internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues: the description includes model definitions, the failing filter invocation, the workaround, and the desired fix. The provided code patch and test diff cover both code and verification. All necessary context is in the text. This sample is appropriate for a benchmark use case.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13112": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The description clearly states that makemigrations crashes when using a mixed-case app name and provides the relevant model definitions, INSTALLED_APPS entry, and AppConfig class. However, it does not include the actual error message or traceback, so the precise failure mode must be inferred. Despite this missing detail, there is enough context (version change from Django 3.0 to 3.1b1, sample models, and settings) to understand the problem and determine the necessary code adjustments to preserve the mixed-case app_label in the migration deconstruction logic.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced Django engineer could identify the deconstruct method in related.py, add a split on the dotted string, and lowercase only the model name in under an hour. The change is localized to under 10 lines of code and the test adjustments follow the existing pattern.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue title and minimal code snippet make it clear that Django\u2019s ORM should support subtracting two F expressions on DateTimeField (and other temporal fields) without requiring an explicit ExpressionWrapper with an output_field. However, the issue does not provide error messages or the specific failure behavior, so the engineer must infer that the ORM currently raises an error or fails silently and that the request is to introduce TemporalSubtraction and DurationExpression logic automatically. While this is a reasonable interpretation for someone familiar with Django internals, some context is implicit rather than explicitly documented.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires exploring django/db/models/expressions.py, understanding how as_sql, resolve_expression, and compile handle field types, and adding conditional logic for native duration support, TemporalSubtraction, and DurationExpression. The patch touches multiple methods (~40\u201350 lines of code) and test updates. An engineer unfamiliar with this part of the ORM would need to spend time reading docs and code, but an experienced Django contributor could complete it in a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers beyond the domain knowledge requirement: the issue assumes familiarity with Django\u2019s Expression API, test framework, and temporal field handling. Tests cover multiple edge cases (None values, subqueries), so careful review is needed to ensure comprehensive coverage.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes that TemplateView.get_context_data wraps URL kwargs in SimpleLazyObject causing issues when using filters like get_object_or_404. It provides minimal working and failing examples referencing the OfferView class and urls.py, and shows the expected behavior when casting to string. The gold patch pinpoints the exact changes in django/views/generic/base.py (replacing SimpleLazyObject with lazy) and additions in tests/generic_views/test_base.py. This makes it straightforward to locate the relevant code and implement the fix without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced Django engineer can locate django/views/generic/base.py, adjust the import and wrap logic, and update a test in tests/generic_views/test_base.py in under an hour. The change touches a small number of lines and is conceptually simple.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional blockers; the sample is self-contained, has clear examples and tests, and is suitable for use in a coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13406": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified: it provides a clear, minimal reproducible example including model definition, database setup, the exact code that crashes, observed vs expected behavior, and relevant import paths. The reporter references official Django docs on pickling querysets and explicitly states how objects are incorrectly reconstructed as model instances instead of dicts. This level of detail makes it straightforward to implement and test a fix in the query setter.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is small and localized: an experienced developer can locate the QuerySet.query setter, add a two-line conditional to switch the iterable class when values_select is present, then extend existing tests. The change touches one method and adds a couple of test cases, which is well within a 15-minute to 1-hour window.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13449": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly defines the model fields, shows the generated SQL with the misplaced CAST(), and explains that DecimalField lag functions on SQLite fail. It references django/db/models/expressions.py\u2019s Window class and provides a minimal reproduction and expected behavior. This is sufficient to implement and test a fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding Django\u2019s expression compilation, extending the Window class with SQLite-specific casting logic, adding a mixin, and writing corresponding tests. Implementing ~20 lines of code and tests would take an experienced engineer a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues noted.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13568": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that the existing system check in django/contrib/auth/checks.py only verifies if the username field has unique=True, but does not consider unique constraints declared via Meta.constraints. It states the desired behavior: to extend the check to inspect Model._meta.constraints for a UniqueConstraint on USERNAME_FIELD. It references the class Meta, constraints, UniqueConstraint, and the E003 check, making it specific enough to implement the change without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Implementing the change involves modifying a small block in checks.py to add a constraint lookup and updating or adding two tests. With familiarity in Django\u2019s model _meta API and test framework, an experienced engineer can accomplish this in 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13807": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that when using loaddata on SQLite, table names that are SQL reserved words (like order) are not properly quoted, causing PRAGMA foreign_key_check(table) to fail. It specifies the file (django/db/backends/sqlite3/base.py), function (check_constraints), and exact lines (around 327 and 333) where unquoted table_name is used. Reproduction steps are given, and the expected solution (wrap table names and column names with self.ops.quote_name) is implied. A developer can directly locate and apply the fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small localized change in one file (base.py) to wrap existing SQL statements with quote_name. It requires understanding Django\u2019s quoting API and updating two or three lines, plus verifying existing tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the issue is self-contained and the tests fully cover the fix.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13810": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines the problem, showing specific context (ASGI, django-debug-toolbar, custom middleware) and reproducing steps (broken SESSION_FILE_PATH, accessing /admin). It references the exact file and line (django/core/handlers/base.py:L58) where the behavior occurs and explains how MiddlewareNotUsed poisons the handler chain. The desired semantics (sync-only middleware should skip without poisoning ASGI) are clearly stated, making it straightforward to devise a patch.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer familiar with Django internals would need to locate the load_middleware method, understand the sync/async adaptation logic, and apply a small refactor (renaming the adapted handler variable and moving assignment into the else block). This requires some thought but is a focused change of under 15 lines.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"N/A\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14017": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue provides a minimal reproducible example showing that Exists(...) & Q() works while Q() & Exists(...) raises a TypeError. It clearly states the expected behavior (commutativity of & and | between Q and Exists) and even hints that a missing __rand__ implementation is the root cause. A developer can locate the combine logic in django/db/models/query_utils.py, identify the _combine method, and implement the necessary check for conditional expressions. The context is sufficient to write a fix and add corresponding tests without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating the Q._combine implementation in django/db/models/query_utils.py, extending its type check to accept conditional expressions, and adding a small set of tests. An experienced engineer could understand the pattern and draft a patch in under an hour, as it\u2019s a minor extension of existing logic with minimal code changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14140": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly explains how Q.deconstruct currently treats single-child Q objects differently by populating kwargs instead of args and shows concrete examples with Q(x=1) versus Q(Exists(...)). It identifies the file and method (django/db/models/query_utils.py:deconstruct) and states the desired behavior change (remove the special-case branch). The examples, including REPL snippets and proposed patch, remove any ambiguity about what constitutes a correct solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires diving into Django\u2019s QueryUtils implementation for Q.deconstruct, removing the conditional branch for single-child non-Q children, ensuring connector and negation flags are still handled, and updating several existing tests while adding new ones to validate the new behavior. While it\u2019s localized to one method and related test modules, someone unfamiliar would need 1\u20134 hours to understand the code paths, verify backward compatibility concerns, and run the test suite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14238": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The description clearly identifies the symptom (DEFAULT_AUTO_FIELD subclass check failure), the relevant setting and metaclass (AutoFieldMeta.__subclasscheck__), and suggests extending support to subclasses of BigAutoField and SmallAutoField. It names the file and method to patch, though it omits an explicit error trace and full reproduction steps. Nonetheless, an experienced engineer can infer the necessary change.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, targeted change: updating one method from membership check to issubclass and adding a few lines in tests. No large refactoring is needed; 15\u201360 minutes suffices to locate the metaclass, implement the one-line fix, and update tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Overall, the sample is straightforward, self-contained, and focused on a single-line core change plus test adjustments. It makes an excellent candidate for a coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14351": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue precisely describes a difference in how Django\u2019s QuerySet Q lookups generate SQL for __in vs. __id__in, shows the broken SQL subselect including all columns (in django/db/models/sql/query.py), the specific function (get_default_columns) where the bug manifests, and a clear desired behavior. It references file paths (django/db/models/sql/query.py, django/db/models/lookups.py), code snippets, and test failures, making it straightforward to implement a PR.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires diving into Django ORM internals, identifying the correct hook in lookups (adding get_group_by_cols in django/db/models/lookups.py), understanding select vs. group_by clauses, writing the change, and adding a targeted test. This would take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional blockers: the issue text and test patch are self-contained and runnable against Django\u2019s test suite, with clear version context (2.2.5 vs 3.2) and a skipUnlessDBFeature decorator.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14493": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly reproduces the error by subclassing ManifestStaticFilesStorage with max_post_process_passes=0, points to the specific code block (lines 246-257) where the variable `substitutions` is only set inside the loop, and states the intended behavior. It is straightforward to see that initializing `substitutions = False` before the loop fixes the crash.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"An experienced engineer can locate the error in post_process, add a one-line initialization for `substitutions`, and update or add a simple test within 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected; the reproduction steps and required patch are concise and isolated.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14580": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly shows that the generated migration file references models.Model without importing 'models'. It provides the problematic migration snippet with bases=(app.models.MyMixin, models.Model) and states that makemigrations omitted the import. The user can deduce that the bug lies in django/db/migrations/serializer.py (or writer), where TypeSerializer\u2019s special_cases should include 'from django.db import models' for models.Model. All necessary context is present and there is a straightforward interpretatation of the required change.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating TypeSerializer in django/db/migrations/serializer.py, adding 'from django.db import models' to the special_cases tuple for models.Model, and adding a small test in tests/migrations/test_writer.py. It\u2019s a narrow change spanning only a few lines, so an experienced engineer could complete and validate it within 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues were identified. The test harness provided in the PR covers the change, and there are no external dependencies or side effects.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14672": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the missing make_hashable call on the through_fields attribute in ManyToManyRel.identity, explains why a tuple element containing a list fails hashing, references the specific class (ForeignObjectRel/ManyToManyRel) and file (reverse_related.py), provides minimal repro model code, and even states the precise patch needed. An engineer can confidently locate the identity() method, apply make_hashable to self.through_fields, and verify the fix with existing or new tests, without ambiguity.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"An experienced engineer familiar with Django\u2019s ORM internals can locate the identity method in reverse_related.py in under a minute, insert a one-line make_hashable wrapper around through_fields, run the existing test suite, and confirm the fix. The change is trivial and well-contained.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14787": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"This issue clearly states that when using method_decorator on a method, the underlying function is wrapped with functools.partial and loses attributes like __name__ and __module__. It provides a concrete example with a logger decorator, shows the buggy behavior (partial object without function metadata), and names the function to fix (method_decorator in django/utils/decorators.py). It is clear that the goal is to preserve wrapper assignments, i.e., to apply functools.wraps to the partial, and the expected outcome is shown in both the gold patch and the test. An experienced engineer can locate method_decorator in the codebase, see where partial is used, and apply wraps to that partial. The requirements are unambiguous and succinct.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix consists of a one-line change in django/utils/decorators.py to wrap the partial object with functools.wraps, along with adding a small test to verify __name__ and __module__ are preserved. An engineer familiar with functools and Django utilities can implement this in under an hour once the relevant code is located.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15104": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear, self-contained reproduction including the exact error scenario in django/db/migrations/autodetector.py. It names the CustomFKField class, shows how deconstruct removes `to`, and pinpoints the `del deconstruction[2]['to']` line to replace with `pop('to', None)`. The repro test `ReproTestCase.test_reprodution` and location within MigrationAutodetector make it unambiguous what change is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves a one-line change in `django/db/migrations/autodetector.py` (replacing `del deconstruction[2]['to']` with `deconstruction[2].pop('to', None)`) and adding a small test in `tests/migrations/test_autodetector.py`. An experienced Django engineer can implement and validate this patch within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15128": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly outlines how to reproduce the AssertionError in Django\u2019s Query.combine and change_aliases methods: it provides minimal model definitions, a snippet that triggers the failure on OR-ing two QuerySets, and a detailed account of how alias_map and change_map collide. It specifies where in the code (Query.combine, Query.join, Query.change_aliases, Query.table_alias) the fix should be applied and even suggests a high-level bump_prefix approach. An engineer familiar with the codebase can implement the patch and add tests without needing further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"I estimate a 1-4 hour effort because an engineer must understand Django\u2019s query aliasing system, locate the alias collision logic in query.py, design and implement the bump_prefix helper, integrate the prefix bumping into combine and change_aliases, and write corresponding tests. The changes are substantial but localized to a few methods and test files.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and suitable for benchmarking coding ability.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15277": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the performance hotspot in Value._resolve_output_field when instantiating CharField without checking max_length. It points to django/db/models/fields/__init__.py in CharField.__init__, shows the existing unconditional validators.append(MaxLengthValidator(self.max_length)) and proposes guarding it with if self.max_length is not None, following the precedent in BinaryField.__init__. That single-file, single-method change is fully specified, leaving no ambiguity about what code to write.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"An experienced engineer can locate CharField.__init__ in django/db/models/fields/__init__.py, add a simple if guard around the existing validators.append call, and run the existing test suite within 15 minutes. The change is one conditional around a single line of code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the description, patch, and test cases are self-contained and run without side effects.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15280": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly defines the models (User and Profile), shows an explicit test case illustrating the unexpected behavior, and details the specific functions and file paths involved (e.g., django/db/models/fields/related_descriptors.py, get_prefetch_queryset). It explains what queries are executed, what deferred fields are reported, and the failed assertion when accessing user.profile.user.kind. With this information alone, an experienced engineer can reproduce the failure and know that they need to modify the prefetch logic to respect cached fields.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s prefetch_related internals, locating the get_prefetch_queryset implementation in related_descriptors.py, and recognizing that the missing is_cached check causes incorrect inheritance of deferred fields. While the patch itself is small (adding an if not self.field.is_cached check), reasoning about deferred field behavior and writing the appropriate tests can take a few hours, including test validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional blockers. The sample is self-contained and relies only on Django models/tests. It does require familiarity with Django\u2019s ORM internals and deferred field semantics, but there are no external dependencies or ambiguous requirements.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15315": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that Field.__hash__ in django/db/models/fields/__init__.py changes when a field is assigned to a model, names the offending PR (#31750), and explicitly calls to revert that change so the hash depends only on creation_counter. It is clear what code to modify and what behavior is expected.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a single\u2010line change in the __hash__ method of django/db/models/fields/__init__.py (reverting the PR #31750 change) plus an added test. An experienced engineer can locate and apply this fix in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes. The scope and requirements are unambiguous.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15375": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly demonstrates the failure scenario with concrete examples using Django\u2019s ORM shell, identifies that the new default parameter on the Sum aggregate is the source of the crash, and provides a known workaround using Coalesce. It references the specific API (Sum(..., default=0)) and shows expected vs. failing commands. A developer can locate the related resolve_expression implementation in django/db/models/aggregates.py, understand how default is wrapped, and implement the necessary change without ambiguity.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer would need to read the resolve_expression method in aggregates.py, trace how the default argument is handled, and realize that the aggregate\u2019s is_summary flag must be propagated onto the Coalesce wrapper. This requires understanding of Django\u2019s expression resolution and summary behavior but only a small code change (three lines) and addition of tests, which should take on the order of one to four hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15380": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description states that the autodetector \u201ccrashes when renaming a model and field in a single step\u201d but provides no traceback, no file or function context, and no details about the error location. An engineer would have to guess where in django/db/migrations/autodetector.py the failure occurs (e.g. within generate_renamed_fields or deep_deconstruct) and what state lookup is wrong. Without a stack trace or clearer reproduction output it\u2019s unclear exactly what needs to be changed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the fix itself is a one-line change in generate_renamed_fields, an engineer must first locate the correct function in django/db/migrations/autodetector.py, understand how renamed_models mapping and state lookups work, set up a repro case, and write corresponding tests. This exploratory work plus implementation and testing would likely take 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues: the test infrastructure is provided and the patch is straightforward once the failure point is identified.\",\"q2_5_confidence\":3}"
    },
    {
        "django__django-15503": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states that JSONField lookups using has_key, has_keys, and has_any_keys fail to find numeric keys on SQLite (but not PostgreSQL). It provides reproduction steps, model definition, exact test code, expected versus actual behavior, and versions. The problem scope is well-bounded to how numeric JSON keys are compiled in lookup paths across backends.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires understanding Django\u2019s JSONField lookup internals, modifying compile_json_path logic to distinguish numeric keys from array indices, and integrating changes across SQLite, MySQL, and Oracle backends. It spans several small code changes and adding tests, likely taking an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15525": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description includes the full model definitions, manager methods, natural key definitions, the fixture data, and the exact command used (including --database other). While it does not show the exact exception message, it clearly describes that loaddata fails only on the non-default database when using natural keys with foreign key dependencies. An experienced engineer can infer the root cause and a sensible fix (ensuring the correct DB state is used for natural key resolution).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\\u0019s serialization and natural key lookup internals, locating the build_instance logic, modifying state to route queries to the correct database, and adding corresponding tests. This is a multi-file edit and non-trivial but should be solvable within a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15695": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints a specific failure in RenameIndex() when handling an unnamed unique_together index moving backward and then forward. It references the exact test file (tests/migrations/test_operations.py) and the failing code paths in django/db/migrations/operations/models.py (database_backwards and database_forwards). It clearly states expected behavior (restoring the auto-generated name and no-op on reapply) and shows a concise reproduction snippet. An engineer can locate RenameIndex.deconstruct, database_forwards, and get_index_by_name in models.py and implement the missing early return, so the requirements are explicit and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires reading one method in django/db/migrations/operations/models.py (RenameIndex.database_forwards), understanding the backward/forward logic, and adding a simple conditional to noop when old and new names match. Additionally, updating an existing test in tests/migrations/test_operations.py. An experienced engineer could complete this change in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15732": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is localized in django/db/backends/base/schema.py within the alter_unique_together and _delete_composed_index methods, which currently assume only a single unique constraint per field. The reporter is unable to drop a unique_together constraint on the primary key or a unique=True field because the logic picks up both the primary key constraint and the unique_together constraint and fails when more than one is found. The goal is to modify _delete_composed_index to detect and exclude the primary key constraint (or any unique=True constraint) and target only the unique_together constraint by name, as shown by the proposed _unique_constraint_name helper. The corresponding tests in tests/migrations/test_operations.py illustrate exactly what behavior is expected (removal of the unique_together constraint without affecting the PK or unique field constraint). Given these details\u2014file paths, method names, expected constraint names, and test cases\u2014the task is clearly specified.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s migration schema internals, specifically how alter_unique_together and _delete_composed_index work in django/db/backends/base/schema.py. One must implement logic to filter out the primary key constraint, introduce a helper to generate unique_together names, adjust SQL generation, and then write or update tests in tests/migrations/test_operations.py. This entails modifying multiple methods across two files and validating with new test cases, which should take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15814": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the failure scenario: using select_related() and only() on a proxy model triggers an AttributeError because opts is taken from the proxy _meta rather than the concrete model. The models are defined, the Django version is given, the exact file (django/db/models/sql/query.py) and line snippet are shown, and even a workaround change is provided. From this information, an experienced engineer can reproduce the bug, locate the relevant code, and understand exactly what change is required to fix the issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Once the root cause is identified\u2014using the proxy model\u2019s _meta instead of its concrete_model._meta\u2014the fix is a single-line change in query.py. Writing a small test case to cover the proxy scenario is straightforward. Understanding Django\u2019s _meta options may take a few minutes, but overall this can be implemented and validated within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15930": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear code snippet showing the use of Case() with ~Q(pk__in=[]), states that it generates a syntax error, and explains the expected behavior of annotating all rows True. However, it omits the exact exception message, stack trace, and Django version context, so an engineer must infer how the internal SQL compilation is producing an empty condition. Despite these missing details, there is a sensible interpretation of what change is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires a small change in the Django ORM compiler: detecting when a compiled condition SQL string is empty and substituting a TRUE predicate, plus adding a brief test. For an experienced engineer familiar with the expressions module, locating the as_sql method and adding ~5 lines of code and a test should take under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained, the snippet clearly demonstrates the failure and expected behavior, and tests can validate the fix in isolation.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15957": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that using Prefetch with a sliced queryset (e.g., Post.objects.all()[:3]) does not constrain the prefetch results, and the user wants only a fixed number of related objects per parent. The purpose is described (efficiently display example items) and the exact failure mode (Django ignores QuerySet limits when prefetching) is obvious. It is clear that the solution must detect sliced querysets (query.is_sliced), extract low/high marks, and apply a filter\u2014likely via SQL window functions\u2014to respect the slice per instance.\",\"q2_1_difficulty\":3,\"q2_2_explanation\":\"Fixing this requires deep familiarity with Django\u2019s QuerySet internals, understanding how prefetch_related and Prefetch use get_prefetch_queryset, and how slicing is represented (query.is_sliced, low_mark/high_mark). One must research how to apply per-partition limits (using Window and RowNumber), clear existing limits, and write tests covering M2M and reverse relations. The patch spans ~100 lines across descriptors and tests, so an experienced engineer would need several hours to fully prototype, validate, and test it.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15973": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The description names the three Django apps (fonte, variavel, fonte_variavel) and their models (FonteModel, VariavelModel, FonteVariavelModel), shows the M2M field definition using through in core/fonte/models.py, and includes the generated migration snippet. It states reproducible behavior (works when the through model lives in the same module, fails otherwise). This provides enough detail to know that autodetector.py needs to resolve field.remote_field.through instead of the default remote model, and the accompanying test diff in tests/migrations/test_autodetector.py explicitly shows the added test case.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires a small change in django/db/migrations/autodetector.py (one line) and adding a test in test_autodetector.py. An experienced Django maintainer familiar with the migration autodetector would spend under an hour locating the correct resolve_relation call and writing the minimal test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and has both code and test diffs, so it maps cleanly to the benchmark setup.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16032": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description includes a concrete test in tests/annotations/tests.py that reproduces the bug and specifies the expected output for Publisher.objects.filter(book__in=...). It clearly states the problem occurs when using __in after annotate() and alias(), but does not spell out which methods need adjustment. The engineer must inspect django/db/models/fields/related_lookups.py (get_prep_lookup) and django/db/models/sql/query.py to infer changes to clear_select_clause, add_fields vs set_values, and the has_select_fields flag as shown in the gold patch.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django ORM internals across two modules, modifying get_prep_lookup in related_lookups.py and the Query class in sql/query.py, and validating with existing tests. An experienced engineer could complete this within 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Although the issue is self-contained, it demands familiarity with Django\u2019s internal ORM code, particularly Query.get_prep_lookup and the Query class in the SQL layer. Additionally, setting up and running the Django test suite for this specific module may incur extra setup overhead that could affect time estimates but is not a blocker.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16136": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the failure (GET hit on an async-only view), provides reproduction steps, minimal code (Demo View, URL conf), environment details (Django 4.1.1, Python 3.10.6), and the exception context. It specifies that HttpResponseNotAllowed is not awaitable under async handling, making it obvious that the fix must wrap or convert this response in a coroutine when view_is_async is true.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Allocating ~15\u201360 minutes: locate http_method_not_allowed in django/views/generic/base.py, add an async wrapper around HttpResponseNotAllowed, update tests to assert coroutine/response. The change spans a single file and related tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16429": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that when USE_TZ=True and the computed interval is greater than or equal to one month, calling timesince(d) results in a TypeError due to a missing tzinfo argument in the pivot datetime construction. It provides a specific failing test (test_long_interval_with_tz) in tests/utils_tests/test_timesince.py, shows the expected output (\u201c1\\\\xa0month\u201d), and even points to the code region in django/utils/timesince.py (lines 93-100) where datetime.datetime is invoked without tzinfo. The suggested fix (adding tzinfo=d.tzinfo) is unambiguous and directly addresses the failure, so a developer can implement it without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, well-contained change: one line in django/utils/timesince.py to add tzinfo, plus minor test updates. An experienced engineer can understand the timezone context, locate the code, and apply the fix within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16454": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies that add_subparsers in django/core/management/base.py doesn\u2019t propagate the custom parser_class and called_from_command_line flag into its subparsers, causing missing arguments on subcommands to raise raw stack traces. The description references the add_arguments method in BaseCommand, the use of CommandParser (a subclass of argparse.ArgumentParser), and shows both the failing behavior and the desired behavior. This points directly to modifying the add_subparsers method in CommandParser to copy parser_class and related keyword args into created subparsers, making the scope of the change explicit and self-contained.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires a focused change in django/core/management/base.py: override add_subparsers to wrap parser_class with functools.partial so that subparsers inherit the called_from_command_line attribute. The patch is limited to ~10 lines plus adding two test cases in tests/user_commands. An experienced engineer could locate the relevant class, implement the override, run existing tests, and adapt new tests in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16485": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description pinpoints that floatformat crashes when given inputs \\\"0.00\\\" (as string) or Decimal('0.00') with p=0, so you know exactly which function and parameters to test. It lacks the specific error message or explicit expected output, but the behavior can be sensibly inferred (returning \\\"0\\\").\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, localized bug fix: adjusting a single conditional in defaultfilters.py and adding two test assertions. An experienced engineer familiar with the codebase could implement and verify within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16569": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the bug in django/forms/formsets.py: add_fields() fails when index is None under specific can_delete settings. It provides a minimal reproduction example, points to the exact file and line (line 493), and even suggests the precise conditional fix. The goals are unambiguous and the steps to reproduce demonstrate the problem, making it straightforward to implement and verify a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves modifying a single conditional in add_fields() (3 lines) and adding one assertion in an existing test. An experienced engineer familiar with Django internals and the test suite could make and validate these changes in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is self-contained, reproducible, and the test coverage ensures correctness once the small change is applied.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16667": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the crashing function (SelectDateWidget.value_from_datadict in django/forms/widgets.py) and reproduces the error with specific GET parameters that cause an OverflowError in datetime.date(int(y), int(m), int(d)). It specifies where to catch the exception and what return value to provide. The provided code context and PR patch illustrate exactly what needs to be changed, so no ambiguity remains about the required solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can locate the widget\u2019s value_from_datadict method, wrap the single date-construction line in a try/except, and update two test files to assert the new error handling. This is a small, localized change and should take well under an hour once familiar with the widget implementation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16950": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly lays out the relevant models (UUIDModel, Thing, SubThing) and admin configuration, provides concrete reproduction steps (create a Thing, add a SubThing inline, observe that the UUIDField id becomes null), and specifies the expected behavior (the UUID default should be preserved). The reporter even identifies the approximate location of the logical bug in Django\u2019s inline formset implementation. An engineer familiar with Django can locate {\\u001bdjango/forms/models.py\\u001d}, the add_fields() method, and implement a conditional around to_field.has_default() to avoid nulling default values. There is no ambiguity about what the user is seeing or what a correct solution looks like.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires understanding Django\u2019s inline formset lifecycle, navigating to django/forms/models.py, analyzing the add_fields method, and crafting a conditional patch to avoid clearing default PKs in the specific inline scenario. Writing and validating a couple of targeted tests also takes some time. An experienced engineer could complete this in roughly 1\\u001c4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-20859": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the bug (calling subfig.legend() raises an error), provides a minimal reproducible code snippet, and states the expected outcome (\u201cproduce a legend on a SubFigure\u201d). It even identifies the exact file (lib/matplotlib/legend.py) and line range (around L433\u2013L442) where a type check should change from Figure to FigureBase. No further clarification is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires finding the legend initialization code, updating two isinstance checks in lib/matplotlib/legend.py to use FigureBase instead of Figure, and adding a small test in test_legend.py. This is a focused, small change that an experienced engineer could complete within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-22719": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes a DeprecationWarning triggered by passing empty data through string \\\"unit converters\\\" in Matplotlib. It includes a minimal reproducible snippet (ax.xaxis.update_units and ax.plot([],[])), identifies the specific code in lib/matplotlib/category.py (convert and update methods) where empty-data handling is too broad, and shows the desired behavior. The expected outcome and deprecation rationale are stated, and the test to add is provided. This is precise enough to guide a developer to implement and verify the two conditional checks and add a smoke test.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Locating the two spots in category.py where unit conversion warns on empty data and adding a size check is straightforward. Writing the small patch and accompanying test requires understanding of existing ConversionInterface and update logic but is a focused, <1h fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear bug summary, a minimal reproducible code snippet, and explicit actual versus expected behavior. It identifies the relevant module (lib/matplotlib/__init__.py), function (rc_context) and data structure (rcParams, Gcf.figs) and outlines the specific circumstance under which get_backend() clears figures. Environment details (OS, Python/Matplotlib versions, backend) are given, ensuring a developer can reproduce and address the bug without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Resolving the issue involves a small, well-scoped patch: excluding the 'backend' key from the original rcParams copy in rc_context (two-line change) and adding a new test in test_rcparams.py. Locating the rc_context implementation and understanding rcParams behavior is straightforward. An experienced engineer could complete this work, including writing and running the test, within one hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and has a clear test harness for validation.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23476": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very detailed and self-contained: it includes a minimal, runnable reproduction script demonstrating DPI doubling on M1 Mac, prints out environment details (Matplotlib version, backend, OS, Python version), and shows both actual and expected output sequences. This leaves no ambiguity about the bug or the success criteria for a PR fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Resolving this issue requires a small, focused change (three lines in figure.py) to restore the original DPI during pickling state, plus adding a test case. An experienced engineer can locate the serialization logic quickly and implement the fix within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The sample is platform-specific (M1 Mac), but the code change and corresponding test are straightforward, self-contained, and do not introduce external dependencies. The behavior can be reliably tested with pickle on any environment.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-24026": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly explains the problem: stackplot currently raises a ValueError when attempting to use color aliases (e.g., 'C0', 'C1') and it unintentionally consumes or resets the Axes color cycler. The reproduction code snippet demonstrates the failure, and the desired behavior is exactly analogous to ax.plot or Rectangle facecolor, i.e., accept color aliases and preserve the cycler. This is sufficient for an engineer to locate the stackplot implementation, understand where colors and cycler are handled, and implement a fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer would need to familiarize themselves with the stackplot implementation, understand how Matplotlib\u2019s color cycler and fill_between work, and integrate itertools.cycle to manage provided colors without resetting the cycler. This involves editing around 15\u201320 lines, writing or updating tests, and ensuring no regression elsewhere\u2014likely a 1\u20134 hour task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified. The reproduction is self-contained, and the required changes are limited in scope.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-24149": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly and concisely states the failure of ax.bar when given only NaN values, provides minimal reproducible code, actual versus expected outcomes, and version context. The root cause (x position being NaN) is highlighted, making it straightforward to identify where to apply a fix without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can locate the _convert_dx function in axes/_axes.py, understand that cbook._safe_first_finite can raise StopIteration when all elements are NaN, and add a fallback to cbook.safe_first_element. Adding the corresponding test case using the existing check_figures_equal decorator is also straightforward. The change is small and self-contained, fitting within a 15\u201360 minute window.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-24970": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text gives a clear reproduction snippet and expected outcome, allowing me to reproduce the deprecation warnings under NumPy 1.24. However, it omits the exact warning messages and doesn\u2019t pinpoint the file/function causing them. An engineer must search the codebase (colors.py colormap __call__) to locate and suppress the invalid cast warnings. The high-level goal (no warnings) is clear, but details like which warnings to suppress and where require exploration.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires identifying the warning source in the colormap __call__ implementation, wrapping the right operations in np.errstate, adjusting the cast logic, and adding tests. This involves reading the code, reproducing the issue, modifying a core utility, and verifying with existing tests\u2014a solid few hours of work.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-25311": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is precise and self-contained. It includes minimal code to reproduce the bug, clearly highlights the failure when pickling a figure with a draggable legend, and specifies the expected behavior. The root cause is apparent: a direct canvas attribute in the DraggableLegend class prevents pickling. The desired change\u2014replacing the canvas attribute with a property\u2014is straightforward. The accompanying test patch further illustrates how picklability should be verified, removing any ambiguity about the success criteria.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves a small, targeted change in the offsetbox implementation (removing one attribute and adding a property) and updating a test file. An experienced engineer familiar with the codebase could identify and implement the solution in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-25332": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue reproducer clearly shows that calling fig.align_labels() breaks pickle.dumps(fig) and specifies that pickling should succeed. The reproduction code, expected outcome, and context (weakrefs in Grouper for label grouping) are sufficient to pinpoint that align_labels introduces unpicklable state, so implementing __getstate__/__setstate__ to serialize/deserialize the weakref mapping is the logical resolution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires understanding Python pickling and weakref behavior in the cbook.Grouper class, then adding two methods (__getstate__/__setstate__) in lib/matplotlib/cbook.py and updating a test file. It\u2019s a focused change to one module and a test, so an experienced engineer could complete it in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-25479": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly demonstrates the mismatch between the colormap\u2019s internal `name` attribute and the registered name, showing exactly which functions (`cm.register_cmap`, `plt.set_cmap`, `LinearSegmentedColormap.from_list`) and modules (`lib/matplotlib/cm.py`, `lib/matplotlib/colors.py`, `lib/matplotlib/tests/test_colors.py`) are involved. Example code and expected behavior are provided, so an engineer can pinpoint where to update the registry logic and adjust the equality comparison, and write a test to assert the `name` is correctly updated.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer must read the colormap registration code in `cm.py`, understand how the copy and name handling works, also adjust the equality logic in `colors.py`, and add a new test in `test_colors.py`. This involves navigating multiple modules and adding a small code patch and test, which would take several hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues; the sample is self-contained, with clear input/output examples and a straightforward patch.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-26291": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text clearly shows reproduction code using inset_axes(ax, width, height) and states the expected result (an empty inset box in the top-right) by referencing the Matplotlib demo. However, they omitted the actual outcome details (no error trace or screenshot), leaving ambiguity about whether an exception was thrown or nothing was drawn. Despite that gap, the expected behavior and the location to inspect (mpl_toolkits/axes_grid1/inset_locator.py) are clear enough for a developer to attempt a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating the __call__ method in mpl_toolkits/axes_grid1/inset_locator.py, recognizing that renderer can be None, and adding a simple default assignment (two lines). The accompanying test addition is also minimal. An experienced engineer could identify and implement this within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the sample is straightforward and self-contained.\",\"q2_5_confidence\":4}"
    },
    {
        "psf__requests-1724": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints exactly where and why a UnicodeDecodeError occurs when a unicode method name is passed in Python 2.7.2. It provides minimal reproduction code, shows that sessions.py at the line `req.method = method.upper()` is the culprit, and explains the desired behavior of allowing unicode or native strings for the HTTP method. This is sufficient for an engineer to propose adding a cast using `builtin_str(method)` before calling `upper()` without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"I assessed this as a 15-minute to 1-hour fix because the problem is confined to a single line in sessions.py where the method string is uppercased. An experienced engineer can quickly locate the offending code via the traceback, recognize the need to convert unicode to a native str using the existing `builtin_str` helper, implement the change, and run the provided test suite to validate the fix. The test patch is already included, so no additional design or multi-file rewrites are required.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-5414": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints a specific UnicodeError thrown by requests.get(\\\"http://.example.com\\\") in the prepare_url method within requests/models.py (around line 401). It explicitly states that UnicodeError should be converted to InvalidURL (matching behavior referenced in the code at the given link). The reproduction steps, expected exception (InvalidURL: URL has an invalid label.), and test snippet unambiguously define the required change: modify the conditional in prepare_url to catch labels starting with '.' (in addition to '*') and update tests accordingly. All necessary details\u2014including file paths, function names, line numbers, and desired behavior\u2014are provided, making the task well-specified for an engineer with no external context.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This fix involves a single small change in requests/models.py (altering an existing elif to include host.startswith((u'*', u'.'))) and adding two test cases in tests/test_requests.py. An experienced engineer who locates the prepare_url function and understands the existing exception handling can implement and validate this patch in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-3151": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description includes a clear minimal reproducible example, shows input datasets, the erroneous behavior (ValueError on non-monotonic but identical coordinates), and states the expected behavior (combine_by_coords should ignore identical non-varying coords). It references the relevant function (combine_by_coords in xarray/core/combine.py) and documentation requirements for coordinate handling. The provided MCVE, expected output, and test patch give all necessary context to implement a fix without external clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires understanding the combine_by_coords implementation and identifying that the monotonicity check loops over all dims rather than only concatenation dims. It involves a small code change in one function and adding a test case, which should take an experienced engineer under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-3677": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly demonstrates the erroneous behavior of ds.merge(da) versus the working top-level merge(), provides minimal reproducible code, and states the expected result. It specifies exactly where in xarray/core/dataset.py the change is needed (handling DataArray by converting it to a Dataset) and illustrates the test that should pass. There is no significant ambiguity about what the solution must accomplish.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires a single conditional conversion inside the existing merge() method (in xarray/core/dataset.py) and adding one test case. Locating the merge implementation and writing a one-line change plus a small test would take an experienced engineer roughly 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues. The sample is straightforward and self-contained for a coding ability benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-4094": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description includes a minimal reproducible example showing how to create a DataArray and Dataset, stack and unstack it, and documents that unstacking fails for single-dimension variables. It clearly states the expected roundtrip behavior and points to the relevant methods (to_stacked_array and to_unstacked_dataset in xarray/core/dataarray.py). This is sufficient for an engineer to understand what change is needed: adjust the selection logic in to_unstacked_dataset to handle the single-dimension case by using drop=True.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing the issue requires locating the to_unstacked_dataset method, understanding the sel().squeeze() call, and adding the drop=True parameter. The change is a one-line edit and adding a small regression test. An experienced engineer should be able to perform and test this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The test patch already covers the regression and the fix is isolated. This sample is well-supported by the provided code and test.\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-4695": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a minimal reproducible example showing DataArray constructions, highlights exactly which call fails (.loc with a dim named 'method'), and states the desired behavior (dimension names should be irrelevant). It identifies the file and method (__getitem__ in xarray/core/dataarray.py) where the indexing logic lives. The inputs, expected behavior, and error context are clear, enabling a developer to pinpoint the change.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing the bug requires understanding that using **key passes dimension names as keyword arguments to sel, which conflicts when a dimension name matches a parameter name. The one-line change to pass key as a mapping rather than kwargs is straightforward once located. Writing a test to cover the collision is also trivial. Overall this is a small, localized change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected; this sample is suitable for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6599": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and self-contained. It explains the mismatch in results between stable and main branches, provides a focused minimal reproducible example (including code, data, and expected vs. actual outputs), and demonstrates the erroneous behavior with datetime and timedelta coordinates. All necessary context\u2014function names (xr.polyval), data types (timedelta64[ns]), and environment\u2014is provided, so there is no ambiguity in what a correct solution should accomplish.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Locating the numeric conversion logic in xarray/core/computation.py and adding a small branch for timedelta64 is straightforward. The developer needs to recognize dtype.kind 'm', call .astype(float), and update a few lines plus add one test case. This is a targeted change requiring moderate familiarity with the codebase but little additional research.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-6721": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states the discrepancy between the observed behavior (ds.chunks triggering a full load) and the expected behavior (inspecting encoding only), provides a concise code snippet illustrating the problem on a remote zarr store, and lists the relevant environment. From this description, an engineer can locate the implementation of the chunks property in the xarray codebase and know exactly what to change without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating the get_chunksizes implementation (in xarray/core/common.py), identifying that it uses v.data rather than the internal v._data, making a one-line change, and writing or updating a regression test. This is a small change that would take an experienced engineer under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pylint-dev__pylint-6386": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that the short \u201c-v\u201d flag in Pylint\u2019s argument parser is incorrectly defined to expect an argument, whereas the long \u201c--verbose\u201d flag does not. It specifies expected behavior, shows usage examples, and includes a precise test case in tests/config/test_config.py. The modules to change (config/argument.py, arguments_manager.py, utils.py, base_options.py) are indicated by the gold patch, making the required work unambiguous and self-contained.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing the bug requires touching a small number of modules\u2014config/argument.py, arguments_manager.py, utils.py, base_options.py\u2014and adding one test. Each change is straightforward (adding a metavar, special-casing -v, adjusting prefix logic). Locating the relevant code and implementing these brief edits would take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pylint-dev__pylint-6903": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints a crash when pylint\u2019s new function _query_cpu (in pylint/lint/run.py at lines ~34,55,60) returns zero cores under Kubernetes cgroup settings. It supplies exact file paths (/sys/fs/cgroup/cpu/*), sample contents (-1,100000,2), the failing command line (--jobs=0), and expected behavior (never return 0, suggest fallback to 1). With this information, an engineer can locate the relevant code and implement the proposed or equivalent patch.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue requires reading ~20 lines of code in pylint/lint/run.py, adding a simple conditional to set avail_cpu=1 when it\u2019s zero, and writing a small pytest that mocks open/Path. An experienced Python engineer familiarizing themselves with the function, cgroup logic, and test framework should complete it in 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pylint-dev__pylint-8898": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that the bad-names-rgxs option in pylint/config/argument.py currently uses a CSV splitter that naively splits on commas, mangling quantifiers like {1,3}. It specifies expected behavior (support any valid regex, provide escaping). The patch shows updating argument.py to call a new utils function _check_regexp_csv, adding that function in pylint/utils/utils.py (with logic to ignore commas inside braces), updating __init__.py and tests in tests/config/test_config.py. All required context, filenames, functions and behavior are specified.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer would need to read the CSV transformer in argument.py, devise logic to avoid splitting commas inside regex quantifiers, implement and test _check_regexp_csv in utils/utils.py, wire it up in __init__.py and argument.py, and write parametrized pytest cases. This touches multiple files and requires careful string parsing, estimated 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-10051": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies that caplog.get_records() and caplog.clear() diverge because clear() replaces caplog.records rather than clearing the shared list. It even cites the relevant lines in src/_pytest/logging.py (line 699 for initialization and line 345 for reset), and provides a minimal reproducer in test(caplog) showing exactly when get_records stops updating. From this one can determine that the fix must change clear() to clear the existing list instead of reassigning records. All functions (get_records, clear, handler.reset) and file paths are explicit, so a PR can be drafted without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted change affecting only the clear/reset logic in logging.py and one test file. An experienced engineer can locate caplog.clear(), implement a new clear() method on the handler, adjust the call in messages(), and extend the test in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10081": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the unexpected behavior when running pytest with --pdb: tearDown is invoked on skipped TestCase classes. It provides a minimal repro in test_repro_skip_class.py, environment details (pytest, Python versions), and expected behavior (tearDown should not run). This is sufficient to implement and validate a fix without additional clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves a small change in src/_pytest/unittest.py (runtest) to extend the skip check to the TestCase parent, plus adding a new parametrized test in testing/test_unittest.py. An experienced engineer should locate the relevant code and write this patch within one hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-5262": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text succinctly identifies that _pytest.capture.EncodedFile.mode reports the underlying stream\u2019s mode (\\\"rb+\\\") including the 'b' flag, which causes youtube-dl to treat the stream as binary and raise on write(bytes). It provides environment details (pip list, pytest version, OS), a minimal reproducible example (test.py invoking youtube_dl.YoutubeDL().extract_info), and clearly states that mode should omit 'b'. The required fix\u2014adding a @property mode to strip 'b' from buffer.mode\u2014is unambiguous.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"The change is trivial: add a @property mode that returns buffer.mode.replace(\\\"b\\\",\\\"\\\"), plus one small test. Locating the capture.py file and writing these <10 lines of code can be done in under 15 minutes by an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is direct, self-contained, and suitable for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5631": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description identifies precisely where the failure occurs (in num_mock_patch_args), what causes it (p.new is a numpy array, so `p.new in sentinels` returns array of booleans), and hints at the correct approach (use identity comparison instead of equality). It references the relevant code location in compat.py and links the problematic commit, giving enough context to propose and implement a fix without external information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer familiar with pytest internals and the compat module could locate the faulty equality check, replace it with identity comparisons, and add appropriate tests within an hour. The change is localized to one function and involves updating a handful of lines.\", \"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-5787": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that when using pytest-xdist to run tests, chained exceptions are not serialized and displayed in the JSON output, with examples of tests failing for both explicit and implicit chaining. The expected behavior (include the full exception chain) is unambiguous and the failure context is shown in test names, file paths (src/_pytest/reports.py), and method names (_to_json, _from_json). The tests in test_reports.py and test_code.py illustrate exactly what change is needed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing this fix requires understanding pytest\u2019s internal report serialization logic, editing multiple methods (_to_json, _from_json) in reports.py, adding helper functions, importing ExceptionChainRepr, and updating a large suite of tests to use a new fixture. This spans several files and about 300 lines of diff, so it is a non-trivial task that would take an experienced engineer 1\u20134 hours to implement and test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-5809": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly pinpoints the faulty parameter 'lexer=python3' in create_new_paste within src/_pytest/pastebin.py and shows that changing it to 'text' resolves HTTP errors. It references specific lines (L68-73) and provides an example using urllib.request.urlopen with a sample data file, as well as links to the code and the test failure. The desired patch is trivial and unambiguous: update the params dict in src/_pytest/pastebin.py and the expected lexer in testing/test_pastebin.py.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This fix is trivial and can be completed in under 15 minutes. It involves a one-line change to the params dict in src/_pytest/pastebin.py and a corresponding update in the test file to assert 'lexer=text'. No complex logic or deep research is required.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5840": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue text describes an ImportError loading conftest on Windows after upgrading pytest due to folder name casing, but doesn\u2019t point to specific code locations or functions to change. It doesn\u2019t explain the unique_path logic or how pytest currently uses lowercase paths vs. real-case paths. The scope (which file or class to modify) and detailed requirements for a solution (use Path.resolve, remove unique_path, update dict keys) must be inferred by reading the code and discussion, leaving room for ambiguity.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding pytest\u2019s plugin manager and path resolution internals, locating and modifying unique_path logic, updating two modules (_pytest/config and _pytest/pathlib), and adding Windows-specific tests. This involves reading existing code, writing cross-platform path handling, and validating behavior, which takes a few hours to implement and test.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This sample is platform-specific (Windows case-insensitive file systems) and may be hard to run or validate on Linux/macOS CI environments without additional setup or mocking. It also assumes familiarity with pytest\u2019s conftest import internals and py.path vs pathlib differences, which could be a steep learning curve for some candidates.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-6197": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes a regression in pytest 5.2.3 where any __init__.py under the current directory is imported (causing failures), provides a minimal reproduction (tox config, commands, and logs showing pytest-5.2.2 passing vs pytest-5.2.3 error), and states expected behavior (not collecting __init__.py). It pinpoints the problem location in collection logic, so an experienced engineer can understand what needs fixing.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires digging into pytest\u2019s collector implementation (classes PyobjMixin, Module.collect, FSCollector), understanding how obj mounting and __init__.py handling works, and applying changes across methods and test files. While a clear reproduction exists, it involves reading ~50\u2013100 lines of complex framework code, writing a small patch, and adding tests\u2014likely taking 1\u20134 hours for an experienced engineer.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This sample is quite domain-specific: it requires familiarity with pytest\u2019s internal collection architecture (nodes.File, PyCollector, FSCollector, marker unpacking). Engineers unfamiliar with pytest internals may struggle, making it less suitable for a generic coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-6202": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a minimal reproducible example showing pytest parametrization with a string containing \\\"[\\\" and how the test headline is incorrectly displayed due to a replace call. It references the exact file (src/_pytest/python.py) and the problematic lines (s = \\\".\\\".join(parts); return s.replace(\\\".[\\\",\\\"[\\\")), and suggests the precise change to remove that replacement. The author also describes adding a new parametrized test in testing/test_collection.py to verify the correct behavior and links directly to the code. This makes it clear what change is needed and how to validate it.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Locating the bug requires reading the issue text, following the provided links to pytest\u2019s implementation in src/_pytest/python.py, identifying the single-line replace call, and removing it. Then updating or adding a small test case in testing/test_collection.py. This is a straightforward one-line code change plus test adjustments that an experienced engineer could complete within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7205": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies a BytesWarning triggered when running pytest with --setup-show on a test parametrized by a bytes object. It names the relevant code path (_pytest/setuponly.py, function _show_fixture_action) and even suggests using saferepr instead of the implicit str() conversion. There is a straightforward mapping from the description to the needed change: import saferepr and call saferepr on fixturedef.cached_param. The intent of the fix and the expected behavior are unambiguous from the issue alone.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An engineer familiar with pytest internals could locate the _show_fixture_action function in src/_pytest/setuponly.py and observe the tw.write call that formats cached_param. Adding an import and swapping str formatting for saferepr is a small localized change. Updating tests to expect quoted bytes output is similarly trivial. The entire edit spans a few lines in two files, likely requiring 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7236": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a minimal reproducible example with both code and commands, clearly shows expected behavior when running pytest normally and with --pdb, includes version details, and explicitly states the mismatch in behavior. There is no ambiguity about what needs to change: skipped tests should not call tearDown even with --pdb.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the fix is localized to a few spots, it requires understanding pytest\u2019s unittest integration, writing a new helper function, and updating multiple code paths where teardown and skip logic intersect. An engineer familiar with pytest internals would likely need 1\u20134 hours to implement and verify the change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample includes both the bug description and test patch, making it straightforward to integrate into a benchmark without extra clarification.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7324": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The description only states that Expression.compile(\\\"False\\\") crashes on Python debug builds but gives no detail on expected behavior or context of Expression.compile, leaving ambiguity about the root cause and the precise fix. An engineer would need to explore codebase internals and the related bpo-40870 discussion to infer the solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing involves a small localized change in the expression parser (adding an identifier prefix and adjusting matcher logic) plus updating one test, which should take a skilled engineer under an hour once code structure is understood.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7490": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue includes a minimal reproduction snippet, clearly describes the change in behavior between pytest versions, and defines the expected behavior (dynamic xfail should ignore failures). An engineer can directly see what needs fixing without ambiguous requirements.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding pytest\u2019s skip/xfail hook implementations, modifying behavior in multiple hook points, and adding corresponding tests. An experienced engineer would need a few hours to familiarize with the skipping.py internals and write the patch and tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and suitable for benchmark use.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7521": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly reproduces the bug using a minimal test in testing/test_capture.py, shows expected behavior (capfd.readouterr should preserve carriage returns) versus actual behavior under pytest 6.0.0rc1. The reproducer points directly to capfd.readouterr in src/_pytest/capture.py, the use of TemporaryFile with default newline conversion, and makes it obvious that adding newline=\\\"\\\" to the TextIOWrapper constructor will fix the issue. There is no ambiguity about the required change or the desired outcome.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can locate the TextIOWrapper instantiation in src/_pytest/capture.py, understand Python\u2019s newline translation semantics, and apply a one-line change to add newline=\\\"\\\". Writing or adjusting the corresponding tests in testing/test_capture.py would also take minimal time, well under one hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues noted. The sample is self-contained, has a concise reproducer, clear expected behavior, and leverages existing test harness, making it ideal for evaluating coding ability.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-10297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies that RidgeClassifierCV does not implement the store_cv_values flag even though its documentation suggests it should. It provides a minimal reproducible example invoking RidgeClassifierCV(alphas,normalize,store_cv_values=True). It states the expected behavior (cv_values_ attribute after fit) and references the documentation section that claims support. Therefore, an engineer can immediately locate sklearn/linear_model/ridge.py, spot the missing parameter and attribute, and know exactly what to implement.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Implementing this fix involves copying the existing RidgeCV store_cv_values pattern into RidgeClassifierCV: updating the class docstring, adding a constructor parameter, passing it to the base class, and adjusting tests to cover cv_values_ shapes. It affects a few dozen lines across one module and one test file and requires understanding of the CV machinery, so it should take 15\u201360 minutes for an experienced developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10908": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly pinpoints the method (CountVectorizer.get_feature_names in sklearn/feature_extraction/text.py) that raises a NotFittedError when a vocabulary parameter is provided but not yet fitted. It describes the current behavior (calling get_feature_names without training fails) versus the expected behavior (automatic vocabulary validation via _validate_vocabulary if vocabulary was set at init). The diff shows exactly where to insert a call to _validate_vocabulary and adds tests in test_text.py to verify custom vocabulary handling. There is no ambiguity about what code to modify or how the tests should behave.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This fix is a minor one: adding two lines to an existing method in a single file and updating one test file. For someone familiar with the code structure, locating get_feature_names and _validate_vocabulary and applying the patch would take under 15 minutes. The changes are straightforward and self-contained.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues. The sample is self-contained and clearly specifies the change scope. It cleanly illustrates how to handle a provided vocabulary without prior fitting, and the test patch corrects a typo and adds sufficient assertions. This makes it ideal for benchmarking without hidden complexities.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-12585": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the clone function failure when estimator parameters are types, points to sklearn/base.py clone logic, provides repro steps, expected vs actual, and even suggests a one-line conditional change in base.py, making the required fix and test straightforward.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires a single additional condition in clone (base.py) and a small test case update, which an experienced engineer can implement and verify within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13135": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear summary stating that KBinsDiscretizer with strategy='kmeans' produces unsorted bin_edges, breaking np.digitize, includes minimal code to reproduce the error, expected versus actual behavior, and even points directly to the relevant function in preprocessing/_discretization.py where centers are computed. This is sufficient to understand what needs to be done\u2014a sort on cluster centers\u2014and where to apply the fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Implementing the fix requires familiarizing with the discretization code path, identifying the center sorting oversight, and adding a single call to centers.sort() before computing bin_edges. Adjusting the test to cover a 5-bin case is straightforward. An experienced engineer can complete this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13142": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states the problem (fit_predict and predict disagree when n_init>1), provides reproduction code, expected vs actual behavior, and context within GaussianMixture\u2019s fit_predict. There is no ambiguity about what fix is required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can locate the missing final e-step in fit_predict, add it back, and update the two small test files. This is a localized change requiring less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blockers; the sample is self-contained and suitable for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13328": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear and self-contained. It provides a reproducible code snippet showing HuberRegressor.fit(X, y) succeeding, HuberRegressor.fit(X_bool, y) raising a TypeError, and HuberRegressor.fit(X_bool_as_float, y) passing. It explicitly states the expected behavior\u2014that boolean arrays should be converted to float in HuberRegressor.fit, analogous to LinearRegression. It names the relevant function (HuberRegressor.fit), refers to check_X_y, and specifies input types. The description includes environment and version details but the core requirement is unambiguous: add dtype=[np.float64, np.float32] (or include bool-to-float conversion) in the fit method.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"The solution is a minimal one-line change in sklearn/linear_model/huber.py, adding dtype support to the check_X_y call, plus a simple new test in test_huber.py. An experienced engineer familiar with the codebase can locate the fit method, modify the signature, and add the bool test in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13779": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes a failure in VotingClassifier.fit when sample_weight is provided and an estimator is set to None. It points to the sample_weight loop in sklearn/ensemble/voting.py, stating that None estimators aren\u2019t skipped. A solution naturally is to add a \u2018if step is None: continue\u2019 check in the loop. The test patch also shows where to add coverage. No ambiguity remains.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires adding a simple conditional inside the existing loop in voting.py to skip None estimators when checking sample_weight support. Locating the code and inserting two lines is straightforward and can be done in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues: the scenario is self-contained, dependencies are clear, and tests demonstrate the fix. It\u2019s suitable for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14053": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that export_text does not handle the single-feature case, provides reproducible code using DecisionTreeClassifier on a one-dimensional feature array, and shows the unexpected output. It is obvious that _tree.TREE_UNDEFINED entries need special handling when mapping feature indices to names in export_text, so a PR can be written unambiguously against export.py.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this is a small change to export_text: alter the list comprehension to handle TREE_UNDEFINED indices and add tests. Familiarity with tree_.feature and basic Python comprehension is enough and tests are already provided, so it should take under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14087": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that an IndexError is thrown when refit=False in LogisticRegressionCV. It includes a minimal reproducible example, expected behavior (\u201cNo error is thrown\u201d), version details, and the context (cross-validation without refitting). An experienced engineer can understand what to fix (avoid the IndexError by guarding code paths when refit=False) and where (in sklearn/linear_model/logistic.py).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding the indexing logic in fit() of LogisticRegressionCV, identifying the wrong condition on self.multi_class and missing branch for penalties other than elasticnet, updating both implementation and tests. It spans two code locations and involves nontrivial array indexing, which typically takes 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blocking issues. The reproducible example and tests in the PR provide full guidance, and the fix scope is well-contained.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14629": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the bug when using cross_val_predict(method='predict_proba') with MultiOutputClassifier, pinpoints the problematic code in model_selection/_validation.py, and provides minimal reproducible example code and expected behavior. It specifies exactly what should be changed (using mo_clf.estimators_[i].classes_ instead of estimator.classes_) and even invites a patch, so an engineer can immediately understand what needs fixing without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires understanding the MultiOutputClassifier internals, adding a small fit override to set classes_, and updating tests. It touches one core file and a test file with under ~40 lines of patch, so an experienced engineer can implement and validate it in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14710": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the root cause: during early stopping, y_true is passed as integer codes while y_pred remains as original string classes. It provides a focused reproducible example with code to reproduce the error, expected behavior (no exception), and even a candidate diff targeting gradient_boosting.py. All necessary context (file names, methods, class names, parameters) is present, enabling a developer to implement and validate the fix without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An engineer needs to locate the _check_early_stopping_scorer method in sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py, understand how labels are encoded under early stopping, and insert a small snippet mapping integer-encoded y back to self.classes_. The change spans a few lines in one file and adding a test, which can be achieved in 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected; the sample is straightforward and self-contained.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14894": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue targets a specific function (_sparse_fit in sklearn/svm/base.py) and clearly describes the failure mode when support_vectors_ is empty, leading to a division by zero when computing dual_coef_indptr. It provides reproduction steps, expected behavior (self.dual_coef_ = sp.csr_matrix([])), and actual error. The location in code, the inputs, and the correct fix behavior are all unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires adding a simple conditional to check n_SV (number of support vectors) before constructing the sparse matrix and updating tests. An experienced engineer familiar with NumPy and SciPy sparse structures can implement and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the sample is straightforward and suitable for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-25747": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"While the steps and code to reproduce the bug are provided, the description omits the exact error message and lacks clarity on the internal expected behavior under \u201cpandas\u201d transform_output. An experienced engineer can infer the problem\u2014FeatureUnion wrapping pandas Series instead of a DataFrame\u2014but the absence of the traceback and detailed context means one must make assumptions about the underlying API behavior, so the spec is mostly clear but not fully self-contained.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating and understanding the _wrap_in_pandas_container helper in the set_output module, then conditionally skipping index assignment when data_to_wrap is already a DataFrame, and updating a handful of tests. It is a small, focused change on one function plus test adjustments and would take 15\u201360 minutes for an engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-25931": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies that IsolationForest.fit(X) with a non-default contamination parameter invokes predict under the hood (see _iforest.py around line 337), causing feature names to be dropped and a warning from input validation. It provides a minimal reproducible example, expected vs actual behavior, and points to the precise code path (_validate_data in fit and score_samples). The regression test adds test_iforest_preserve_feature_names in tests/test_iforest.py, making the goal unambiguous: call a private _score_samples without validation to preserve feature names.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires reading the fit method in ensemble/_iforest.py, tracing how contamination != 'auto' leads to score_samples(X) (with validation that strips feature names), and deciding to bypass validation by implementing a private _score_samples. The change is localized (~10 lines), plus adding a pytest test in test_iforest.py. An experienced engineer could implement and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-25973": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the bug in SequentialFeatureSelector: passing an iterable of splits to the cv argument fails, even though the docs claim it should be supported. The report includes reproduction code, the exact error context (cv parameter), library versions, and expected behavior without errors. There is no ambiguity about what to change: add a call to check_cv in fit(), adjust method signatures to accept the validated splits iterator, and then pass the resulting cv generator into cross_val_score. All relevant filenames (_sequential.py, test_sequential.py) and methods (fit, _get_best_new_feature_score, cross_val_score, check_cv) are implicitly identified by the reproduction snippet, making the required solution clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized to a single module, _sequential.py, and involves importing check_cv, invoking it on self.cv, updating internal calls to pass the normalized cv, and adding a small non-regression test. An engineer familiar with the API and test suite could navigate the codebase and write the patch, plus validate with existing tests, in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-10323": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the problem with Sphinx\u2019s literalinclude directive when using the prepend option: code examples lose their leading whitespace, causing indentation to be incorrect. It provides reproduction steps including the index.rst snippet, the pom.xml content, and actual versus expected XML output. The user states the expected behavior and even suggests a workaround with dedent. With this information one can determine exactly which filters in sphinx/directives/code.py need reordering to preserve indentation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves locating the filter pipeline in sphinx/directives/code.py and adjusting the order of the dedent, prepend, and append filters. The change is localized to one file and accompanied by a pytest in tests/test_directive_code.py. An experienced engineer familiar with the codebase can understand the filter logic, apply the minimal reorder, and validate via tests in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-10673": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the error (\u201ctoctree contains reference to nonexisting document 'genindex', 'modindex', 'search'\u201d), shows how users reproduce it in their .rst files, and specifies the desired behavior (allow these toctree entries without errors). It includes code snippets demonstrating the current and expected directives, making it unambiguous what needs to change in Sphinx\u2019s toctree handling.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Sphinx\u2019s internals (domains, env.found_docs, label handling) and updating multiple modules (directives/other.py, adapters/toctree.py, collectors/toctree.py). It involves merging document name sets, filtering generated docs, and adjusting test cases\u2014likely taking an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified. The sample provides all necessary context, code, and tests for a benchmark; it is self-contained.\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-7440": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that glossary terms with different letter case (e.g. \u201cmysql\u201d vs \u201cMySQL\u201d) are treated as duplicates due to lowercase normalization. It indicates where the bug occurs (glossary.rst and Sphinx build) and expected behavior (case-sensitive terms), but does not point directly to the exact lines in code to change. A developer must locate the lowercase conversion in dom/std.py, so there is a small gap to fill, but the overall requirement and outcome are clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, focused change in the Sphinx standard domain implementation: remove the lowercase conversion in note_object and disable lowercase on XRefRole, then update two test assertions. An experienced engineer could locate and implement this in under an hour once familiar with domain code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7462": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is clearly described: when using an empty tuple annotation (Tuple[()]) Sphinx\u2019s unparse() in both domains/python.py and pycode/ast.py treats ast.Tuple by always popping the trailing punctuation, leading to pop() on an empty list. The error \u2018IndexError: pop from empty list\u2019 pinpoints exactly where the logic fails. The reproduction steps and environment details are complete, and the expected behavior (\u2018()\u2019 for empty tuple annotations) is unambiguous. An engineer can locate the unparse(ast.Tuple) branches in both files and add a simple conditional for node.elts empty. The provided test changes further clarify the desired output for Tuple[()].\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires editing two small unparse implementations (~10-15 lines each) to add an `if node.elts` conditional and handle the empty case. An experienced engineer familiarizing with AST handling and Sphinx internals could implement and test this within 15\u201360 minutes. The logic is straightforward and doesn\u2019t necessitate deep research.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7985": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that the linkcheck builder currently skips non-HTTP(S) URIs and that users want it to verify local file references instead. It gives a reproduction recipe and expected outcome, so it\u2019s possible to infer a sensible file\u2010existence check approach. Some low\u2010level details (e.g. how to detect file paths vs. unsupported schemes) must be designed by the implementer.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing this fix involves locating the linkcheck builder code, adding URI\u2010scheme detection, inserting os.path.exists checks against the source directory, and adjusting tests. It\u2019s more involved than a trivial one\u2010line change but remains a focused task of under a few hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-8269": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text precisely describes that when linkcheck_anchors=True, the linkcheck builder always reports \u201cAnchor not found\u201d even if the HTTP request returns an error (e.g., 404 or 500). It includes reproduction steps, actual vs expected output, and environment details. The change scope is clear: locate check_uri() in sphinx/builders/linkcheck.py and insert a status check before anchor validation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires adding a single line (response.raise_for_status()) in the existing check_uri() function and writing a corresponding test in tests/test_build_linkcheck.py. An experienced engineer familiar with the codebase should find the location quickly and implement the change within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly explains the behavior discrepancy between implicit :type:/ :rtype: directives and explicit xref roles, provides a minimal reproducer including code blocks, the expected resolution, and environment details (Sphinx versions). It specifies exactly what names resolve incorrectly and how they should resolve.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing requires understanding Sphinx\\u0000s PythonDomain implementation and docfields logic, modifying make_xref and transform methods to propagate env.ref_context, and writing appropriate tests. This spans multiple files and requires familiarity with the codebase, estimating a few hours to implement and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified.\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-9711": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that the `verify_needs_extensions` function in `sphinx/extension.py` is performing string-based version comparisons, causing versions like '0.10' to be misordered relative to '0.6'. It provides a concrete reproduction with commands, the expected behavior, and the impacted function name. Together with context on where to change (using `packaging.version.Version` and handling `InvalidVersion`), it leaves little ambiguity about the goal and approach.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing the bug requires locating the single `verify_needs_extensions` function, adding imports from `packaging.version`, updating comparison logic, and writing or updating tests. An experienced engineer familiar with Python packaging and semantic version comparison can implement and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-13372": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"Although the report identifies the UnboundLocalError in evalf and even suggests adding `else: raise NotImplementedError` in the reprec/imprec clauses, it does not explicitly name the file or line numbers. An engineer must locate `evalf` in `sympy/core/evalf.py` and identify the two `elif` branches. Despite this minor gap, the user\u2019s suggestion provides a clear, actionable fix that is easy to interpret once the relevant function is found.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change is localized to `sympy/core/evalf.py`, requiring only two additional `else: raise NotImplementedError` blocks in the existing logic and one new test assertion. An engineer familiar with the repo can make and verify this small amendment in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues or blockers; the scope is narrowly defined and aligns with existing patterns in the codebase.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-13480": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue gives a minimal but reproducible example showing that .subs on coth(log(tan(x))) with integer x values raises an error. While the root cause and expected exact output (beyond \u201cno error\u201d and a consistent symbolic result) aren\u2019t spelled out, there is a clear demonstration of failure on specific inputs, making it sensible to attempt a fix by inspecting the relevant eval method in sympy/functions/elementary/hyperbolic.py.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized\u2014a single conditional in the coth.eval method needs a small correction (changing cotm to cothm), plus adding two lines of tests. An engineer familiar with SymPy\u2019s function evaluation machinery could locate and implement this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-13877": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear minimal reproduction showing \u201cdet(Matrix(\u2026))\u201d yielding nan for n=5 while earlier cases give 0 or -a. It names the suspected culprit (the Bareiss algorithm only valid for integer matrices) and points to the det method in sympy/matrices/matrices.py. However, it does not spell out exactly how to adapt the algorithm\u2019s zero-test or pivot logic for symbolic entries, leaving the implementation approach to the engineer\u2019s interpretation.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires reading the Bareiss determinant implementation in sympy/matrices/matrices.py, understanding the zero-detection and pivot selection logic, learning about SymPy\u2019s iszerofunc and expand_mul utilities, then adding a custom zero test and wiring it into _find_reasonable_pivot. It also touches two files and adds tests. For someone familiar with SymPy internals, this is a moderate (1\u20134 hour) task.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"The provided code patch also includes unrelated changes to sympy/utilities/randtest.py (adding a tolerance parameter to random_complex_number) which are not referenced in the issue description and are not covered by the accompanying test changes. This inconsistency could confuse candidates: they must decide whether to replicate these tangential edits even though no tests will fail if they are omitted.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-16792": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a minimal reproducible example, clearly states the expected versus actual behavior, pinpoints the incorrect C function signature, and explains under what conditions the bug occurs. It specifies exactly what needs to change (array arguments should generate pointers, not scalars) and even outlines the intended fix location in codegen.py.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding SymPy\u2019s code generation infrastructure, locating where InputArgument metadata is constructed, adding a helper for array dimensions, and adjusting two code paths. It spans multiple related functions, so it would likely take an experienced engineer 1\u20134 hours to navigate the codebase and implement the patch correctly.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17139": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue title and minimal repro indicate that simplify(cos(x)**I) performs an invalid comparison on a complex exponent. While the expected behavior (leave the power unchanged) isn\u2019t explicitly stated in the description, it is a sensible inference. The engineer must add a guard in fu.py to skip simplification when the exponent is not real. The required change is clear enough to attempt a fix without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a localized change: adding a check (if not rv.exp.is_real: return rv) in fu.py and updating two tests. An experienced Sympy developer could understand the few lines in the simplify pipeline and implement the fix within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Although the description could explicitly state the expected return value, the test suite clarifies the requirements. No additional blockers or environment issues are present that would prevent using this issue as a benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17318": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that sqrtdenest is raising an IndexError for a specific complex expression. It specifies the desired behavior (return the expression unchanged if it cannot be denested), provides both the failing input and expected output, and references the function in sympy/simplify/sqrtdenest.py as well as the relevant test module. This is sufficient information for an experienced engineer to implement and verify the fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue involves understanding the existing sqrtdenest algorithm for matching and splitting surds, locating the conditional that only checks for Rational squares, and extending it to also check for positivity. The change itself is small (adding a positivity check in two lines of code) and requires adding a few test assertions. An experienced engineer could complete this in 1\u20134 hours after familiarizing themselves with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue description, code context, and test changes are self-contained and do not rely on external commentary. The sample is suitable for use in a coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17630": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly demonstrates the incorrect behavior when multiplying a BlockMatrix containing ZeroMatrix blocks: block_collapse(b*b) works, but block_collapse(b*b*b) raises an exception because zeros become Zero instead of ZeroMatrix. It identifies the failing method (_blockmul in matexpr.py) and outlines the intended behavior, making the required fix unambiguous.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires diving into Sympy\u2019s BlockMatrix implementation, understanding matexpr.py\u2019s postprocessor flow, ensuring ZeroMatrix propagation, and adding tests. Familiarization with the codebase and writing a small patch and tests fits within a 1-4 hour window.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and suitable for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17655": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly shows that multiplication commutes when writing point1 + point2 * factor but fails when factor * point2 is used. It points directly to missing __rmul__ in sympy/geometry/point.py. The expected behavior is stated unambiguously and the diff demonstrates a one\u2010method addition. No further clarification is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Adding an __rmul__ method in sympy/geometry/point.py and updating tests is a small, localized change. It follows a standard Python operator\u2010overload pattern and requires minimal code edits and testing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-18211": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies that calling Eq(...).as_set() triggers a NotImplementedError in solve_univariate_inequality and that instead a ConditionSet should be returned. It specifies the exact input invocation, the expected ConditionSet output, and points to the relevant methods (_eval_as_set in relational.py and tests in test_relational.py). All function and class names are given, so it is straightforward to implement a try/except around solve_univariate_inequality.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves modifying a single method (_eval_as_set in sympy/core/relational.py) by adding an import and a small try/except block around solve_univariate_inequality, plus a few lines in an existing test file. An experienced engineer could implement, test, and verify this change within 15 to 60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns: the change is localized, well-tested, and does not introduce compatibility issues. It fits directly into the existing testing framework without external dependencies.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20428": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text includes reproducible REPL examples showing that clear_denoms() produces a Poly with rep DMP([EX(0)], EX, None) instead of an empty rep, causing bad_poly.is_zero to misreport. It names the relevant functions (clear_denoms, Poly.is_zero, as_expr, terms_gcd, primitive) and files (sympy/polys/domains/expressiondomain.py and sympy/polys/tests/test_polytools.py). It also explains the desired behavior (leading zeros should be stripped so rep becomes DMP([], EX, None)). This is sufficient for a developer to locate and fix the logic for stripping leading zero coefficients in clear_denoms.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"I chose level 2 because fixing the bug requires an experienced engineer to understand Sympy\u2019s internal polynomial DMP structure, trace clear_denoms implementation in polys/domains/expressiondomain.py, add logic to strip leading zero coefficients, and validate with tests\u2014tasks that typically take 1\u20134 hours.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This issue is extremely domain-specific to Sympy\u2019s polynomial internals and DMP representation, demanding advanced symbolic algebra knowledge rather than general coding skills. Candidates unfamiliar with these abstractions may struggle, making this sample a poor fit for a generic coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-20438": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description reproduces a minimal example with FiniteSet and ProductSet, shows that b.is_subset(c) gives no Boolean while c.is_subset(b) returns True. It\u2019s clear that the intended behavior is symmetry: b.is_subset(c) should return True. The snippet and context point directly to fixing is_subset handler for ProductSet vs. FiniteSet. The expected outcome is unambiguous.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Sympy\u2019s multipledispatch and set handler modules, adding a dispatch for ProductSet vs. FiniteSet in issubset.py and small guards in relational simplification. It touches multiple files and requires familiarity with the codebase, so ~1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the sample is straightforward to include in a benchmark as it has a clear reproduction, expected behavior, and associated tests.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-20590": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that Symbol instances unexpectedly gained a __dict__ in version 1.7 due to a missing __slots__ in a parent class. The desired fix (adding __slots__=() to the Printable mixin) is explicit, and the test patch shows exactly how to validate no __dict__ exists and that assignment raises AttributeError.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a localized, small change: adding a few lines (__slots__=()) to the Printable class in sympy/core/_print_helpers.py and adding corresponding tests. An experienced developer familiar with Python\u2019s slots mechanism and the codebase could implement and test this within 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-21379": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text provides a minimal reproducible example (MWE) showing the exact sequence of operations in sympy (imports, symbol definitions, cache clearing, expression construction, and subs calls) that triggers the unexpected PolynomialError. It clearly states the conditions (using sinh/tanh, real symbols, with and without division by z or exp) under which the error occurs, and what the expected behavior is (subs should succeed without error). The target for the fix is pinpointed to the gcd extraction in sympy/core/mod.py, and the provided test patch shows exactly how to verify the fix. This level of detail makes the problem statement unambiguous and actionable.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An engineer familiar with the sympy codebase would need to locate the implementation of Mod.eval in sympy/core/mod.py, understand the role of gcd in simplification and its interaction with PolynomialError, and then add a try/except block around the gcd extraction. They would also write or adapt the provided test case in sympy/core/tests/test_arit.py. Investigating the root cause, updating code, and verifying tests would take on the order of 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-22714": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that using `with evaluate(False)` on `Point2D(Integer(1),Integer(2))` raises a ValueError \u201cImaginary coordinates are not permitted.\u201d The code snippet and expected behavior make it obvious that the check in `sympy/geometry/point.py` __new__ method is too strict under evaluation suppression. It is straightforward to locate and adjust the conditional that tests `im(a)` and then add a minimal test case in `sympy/geometry/tests/test_point.py`.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves changing a single boolean condition in the `Point2D.__new__` method to use `im(a).is_zero is False` and adding a small test. Once the relevant method is found, implementing and verifying the patch is straightforward and can be done in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The test suite is already in place and no external clarification is needed.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-23824": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the bug in kahane_simplify() in sympy/physics/hep/gamma_matrices.py: leading free gamma matrices are removed then reinserted in reverse order. It shows concrete examples in test_kahane_leading_gamma_matrix_bug(), expected vs actual outputs, and pinpoints the backward insertion loop involving free_pos and resulting_indices. This provides sufficient context to locate and fix the one-line bug without external clarifications.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating kahane_simplify in sympy/physics/hep/gamma_matrices.py, understanding the free_pos/removal logic, and updating the insertion loop. The patch is a small refactor of a few lines and adding two test assertions. An experienced engineer familiar with the codebase could implement and verify this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-23950": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that Contains.as_set() erroneously returns a Contains object rather than a Set instance. It references the method in sympy/sets/contains.py and shows the interactive example \u201cContains(x, Reals).as_set() \u2192 Contains(x, Reals)\u201d to illustrate the bug. The description explains why this is incorrect (missing as_relational and boolean behavior) and even points to a failing use case in Piecewise. Together with the provided test diff showing desired assertions for as_set, it is immediately clear that the implementation should simply return self.args[1]. This leaves virtually no ambiguity about the requirements for a successful fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced SymPy contributor could locate the Contains class implementation in sympy/sets/contains.py and see that as_set is currently unimplemented (raises NotImplementedError). Understanding the AST and argument structure of BooleanFunction in SymPy should take only a few minutes, and writing a one\u2010line change (returning self.args[1]) plus adding or updating three simple test assertions is trivial. Overall this small method override and corresponding test adjustments are straight\u2010forward and would likely require under an hour, assuming basic familiarity with the codebase and SymPy\u2019s typical patterns.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns. The sample is self\u2010contained and easily testable with the provided test diff.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-24066": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that SI._collect_factor_and_dimension fails to detect that an exponent is dimensionless when using exp(expr), provides minimal reproducible code and expected behavior. It names the specific function in sympy/physics/units/unitsystem.py, shows the call sequence and asserts expected dimensionless result.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating _collect_factor_and_dimension in unitsystem.py, understanding its existing branches, and adding a small special-case for dimensionless exponents, as shown in the provided 5-line patch. An experienced engineer can implement and test this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-24443": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The report pinpoints a specific function (_image) in sympy/combinatorics/homomorphisms.py (lines 336\u2013337), shows a minimal REPL example using DihedralGroup where an inverted generator fails the `in gens` check, and suggests a simplification. An experienced engineer can locate the file, understand the array_form and generator mapping, and craft a fix without extra context.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires inspecting how sympy represents permutations (array_form, generator symbols), understanding the existing loop logic, and rewriting ~30 lines to simplify and correct inverted generators. For someone familiar with sympy\u2019s combinatorics internals, developing and testing the new loop takes on the order of 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and ready for benchmarking.\",\"q2_5_confidence\":4}"
    }
]