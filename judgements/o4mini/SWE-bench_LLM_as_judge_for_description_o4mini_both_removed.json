[
    {
        "astropy__astropy-13033": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly identifies the location of the problem in BaseTimeSeries._check_required_columns (astropy/timeseries/core.py) and describes that the exception message when removing a required column is misleading. It specifies the behavior change (\u201cexception that informs the users required columns are missing\u201d) but does not prescribe exact wording, formatting, or handling of single vs. multiple columns. Thus an engineer can sensibly implement a more informative error, but must interpret details like quoting and pluralization.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix touches a single method (_check_required_columns in core.py) plus its corresponding test (test_sampled.py) and involves adding a small helper and adjusting the error string. An experienced engineer familiarizing with the codebase could make and validate these changes within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-13977": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies the specific method (`Quantity.__array_ufunc__`) in Astropy where behavior needs changing, describes the desired new return value (`NotImplemented`) when units are incompatible, references the numpy protocol for returning `NotImplemented` to trigger reflected operations, and even suggests where to hook in (catching TypeError/ValueError and returning `NotImplemented`). An experienced engineer familiar with ufunc overrides and Astropy\u2019s `converters_and_unit` can design a patch based solely on this information.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this involves understanding NumPy\u2019s `__array_ufunc__` protocol, Astropy\u2019s unit conversion logic (`converters_and_unit`), catching the right exceptions, returning `NotImplemented` in the correct cases, and updating existing tests and writing new ones. Editing the core `quantity.py` logic, and extending two test modules with detailed `pytest.raises` and `NotImplemented` checks, is a substantive effort likely to take 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14096": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the context (subclassing SkyCoord with a custom property), the observed failure (AttributeError message naming the property rather than the missing underlying attribute), and the expected behavior (error should refer to the missing attribute). This gives a concrete target (adjust __getattr__ to delegate to __getattribute__) and contains no ambiguous requirements.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires understanding Python\u2019s attribute lookup semantics and modifying a small section of code (__getattr__ in sky_coordinate.py) with a 2-line change plus adding a regression test. An experienced engineer could implement and verify this within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14182": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly specifies that the RestructuredText output format should accept a `header_rows` argument (much like the fixed_width writer) to emit multiple header rows (names, units, etc.). It points to the `ascii.rst` writer in `astropy/io/ascii/rst.py` where one would modify the `__init__`, `write`, and `read` methods of the `RST` class to pass through and honor the new `header_rows` list. The provided test patch in `astropy/io/ascii/tests/test_rst.py` shows exactly how calling `.write(..., header_rows=[...])` should shape the output and how `.read(..., header_rows=[...])` should interpret the input. There is no ambiguity about what\u2019s expected or where the change goes.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing this feature requires familiarization with the existing `RST` writer and header/data pipeline in `astropy/io/ascii/rst.py`, adding and propagating the `header_rows` argument in the class constructor, updating the `write` and `read` logic, and then writing corresponding tests. It touches multiple methods and understanding of the table I/O flow, so an experienced engineer would likely spend 1\u20134 hours to design, code, and validate the solution.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14309": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The description pinpoints a regression: cron tests in HENDRICS started failing due to a change in identify_format (io.registry). It explains that file paths without FITS extensions previously returned None (i.e.\\nFalsey behavior), but now fall through to an isinstance check, altering behavior. However, it does not explicitly state that filepaths lacking valid extensions should return False. The engineer must inspect the identify_format or is_fits implementation, locate the code block handling filepath.lower().endswith(...), and reintroduce an unconditional return of that test before the isinstance fallback. This requires mapping \u201cidentify_format\u201d to the is_fits function in astropy/io/fits/connect.py, and inferring the intended semantics for non-FITS extensions.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix touches just a few lines in one function plus adding a small regression test. An experienced engineer can locate the is_fits implementation, adjust the conditional to return the endswith() result, and add a test in under an hour, after a brief familiarization with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14365": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text explicitly identifies that ascii.qdp\u2019s internal regex _line_type_re in astropy/io/ascii/qdp.py (around line 68) is compiled without case insensitivity and that the literal comparison v == \\\"NO\\\" (around line 306) fails on lowercase input. It clearly states the desired behavior: treat commands and the \u201cNO\u201d mask case\u2010insensitively. The tests shown add re.IGNORECASE and change v == \\\"NO\\\" to v.upper() == \\\"NO\\\", and they even illustrate how to modify test_qdp.py to parameterize lowercase input. This provides a complete, unambiguous mapping from description to code change.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer could locate the regex compilation at _line_type in qdp.py and the NO literal check in one pass, then update re.compile to include IGNORECASE and change v == \\\"NO\\\" to v.upper() == \\\"NO\\\". Adding a small test variant is straightforward. Total time including review and test run: well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14508": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints the problematic function `_format_float` at lines 1298\u20131302 in `astropy/io/fits/card.py`, explains the incorrect behavior (overly long float strings causing comment truncation), and provides clear guidance on the desired behavior including when to use Python\u2019s default `str()` representation versus manual formatting. It even enumerates examples and lists the versions tested, enabling a developer to implement and verify the fix without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves modifying a single helper function `_format_float` to add a fallback to `str(value)`, adjusting string length logic, and adding a handful of test assertions. An experienced engineer can understand the existing logic and implement and validate the change within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-14995": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that mask propagation in NDDataRef._arithmetic_mask fails when one operand lacks a mask, describes expected behavior (copy existing mask) and references the v5.2 behavior. The fix is localized to adjusting the conditional in nddata/mixins/ndarithmetic.py (_arithmetic_mask) and adding corresponding tests in test_ndarithmetic.py. No ambiguity remains about what a correct solution entails.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires understanding a small portion of the arithmetic masking logic in nddata/mixins/ndarithmetic.py (~20 lines), updating one conditional from `elif operand is None` to `elif operand.mask is None`, and adding tests. An experienced engineer could implement and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other concerns; the sample is self-contained and suitable for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-7336": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies a defect in the units.quantity_input decorator: when a function or constructor is annotated to return None, the existing guard against empty annotations fails to recognize None and attempts to call .to() on it. The desired behavior is that None should be treated like inspect.Signature.empty, skipping the unit conversion path. This pinpoints wrapper() in astropy/units/decorators.py and the single conditional that needs to include None in the skip set.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small targeted change: updating the conditional in wrapper() to treat None as an empty annotation and adding or updating a few unit tests. An experienced engineer can locate the decorator logic, adapt the if-statement, and extend existing tests in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-7606": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that comparing an UnrecognizedUnit instance to None should not raise a TypeError but instead return False (i.e. __eq__ should yield \u201cnot equal\u201d when other is None). The existing __eq__ implementations in astropy/units/core.py show how recognized units already handle non\u2010unit inputs by catching exceptions and returning NotImplemented, which leads to False when compared. An engineer can locate the UnrecognizedUnit.__eq__ method around line 1710 in core.py and insert a try/except similar to the earlier __eq__, so this is well enough specified to implement without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves editing one small method in core.py (UnrecognizedUnit.__eq__), adding a try/except block to catch ValueError, UnitsError, and TypeError, then returning NotImplemented before comparing names. Tests need a few assertions. An experienced engineer familiar with the codebase would need less than an hour to locate the method, apply the pattern from the other __eq__, and validate with the new tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "astropy__astropy-7671": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description \u2018minversion failures\u2019 is very brief and refers only to a PR and a bug in LooseVersion, without showing any failing inputs or outputs, error messages, or describing which version strings cause the failure. It doesn\u2019t specify what minversion should do versus what it currently does, nor detail the desired behavior or edge cases. The developer must infer the solution approach (e.g., strip non-numeric suffix or reintroduce parse_version) and must consult external bug reports. This leaves significant ambiguity about the requirements.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves modifying a single function (`minversion`), adding a few lines of regex filtering, and updating a small set of tests. An experienced engineer familiar with the codebase and Python version parsing could locate the function, implement the change, and validate it against tests within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-10554": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description provides only a high-level symptom (\u201cUnion queryset with ordering breaks on ordering with derived querysets\u201d) and a vague hint about evaluating the queryset instead of creating a new one. It does not specify where the bug occurs in the SQL compiler, what error is thrown, or how the ordering logic should be fixed. An engineer would have to explore the Django ORM internals to understand which methods to modify and what behavior to implement.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s SQL compiler and query construction internals, identifying where to inject select columns for ordering in a UNION queryset, adding a new method, modifying two core files, and writing corresponding tests. An experienced engineer would likely need 1\u20134 hours to research the code paths, implement the changes, and verify the tests.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"Beyond the fix itself, this issue demands deep familiarity with Django\u2019s ORM, SQL compiler behavior, and the union/queryset code paths. Setting up the proper test environment and crafting edge-case tests for ordering across combined querysets adds complexity that may be too specialized for a general coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11087": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text clearly identifies a problem in Django\u2019s delete behavior where unnecessary fields are fetched during cascade deletion. It references specific code areas in django/db/models/deletion.py (methods like can_fast_delete and collect) and describes expected behavior (skip text_log_error.line and only select referenced fields). However, it doesn\u2019t prescribe the exact implementation steps, leaving the engineer to infer how to use QuerySet.only, check for signal listeners, and compute referenced_fields. This requires domain knowledge of Django internals but has a sensible interpretation.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires exploring Django model deletion internals, tracing through can_fast_delete and collect methods, understanding signal listeners, QuerySets, and writing code to modify deletion.py and add new tests. The patch spans over multiple methods and adds helper functions, plus comprehensive test updates. An experienced engineer would need 1\u20134 hours to familiarize, implement, and validate the solution.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"The issue description mentions two distinct problems\u2014the mysqlclient-python UnicodeDecodeError under Python 3 and the unnecessary field fetching\u2014but the provided solution and tests only address the deletion optimization. Participants might be confused whether both issues must be fixed. Furthermore, implementing the Unicode aspect would involve working with external driver settings, which isn\u2019t covered in the tests and increases complexity.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11265": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that using exclude() on a queryset with an annotated FilteredRelation triggers a FieldError on the annotation name. It even pinpoints the split_exclude function as faulty and explains that a new Query is created without copying _filtered_relations. An engineer with access to the codebase can locate django/db/models/sql/query.py, see split_exclude, and write a fix without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the actual patch is small (adding query._filtered_relations assignment and adjusting an if condition), it requires understanding Django\u2019s internal Query class, FilteredRelation mechanics, alias_map entries, and join types. Familiarizing with these parts and writing two small code changes plus a new test would take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11299": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly identifies the problem: Django\u2019s CheckConstraint SQL uses fully qualified field names in mixed OR and AND clauses, causing malformed schemas on SQLite/Oracle. It names the classes involved (Col vs SimpleCol), highlights where logic diverges, and specifies what needs to change (unify clause handling to use SimpleCol in OR branches). While the engineer must locate the build_query/_add_q implementation in query.py and adapt its parameters, there is a straightforward interpretation of the required change and test expectations.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer would need to understand Django\u2019s SQL query builder internals, locate and modify the _add_q method signature and its call sites, adjust the recursion to pass the simple_col flag, and add corresponding tests. This involves reading multiple files and writing tests but does not require a complete redesign, fitting a 1-4 hour estimate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues identified.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11532": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes the failure: non-ASCII hostnames crash Message-ID header encoding under iso-8859-1. It explicitly names the files and functions to change (django/core/mail/utils or django/core/mail/message), and even suggests converting the domain to punycode before use. The desired behavior and fix strategy (apply IDNA punycode to domain) are unambiguous.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the change is straightforward (add a punycode helper and apply it in several locations), it spans multiple modules (message.py, utils.py, validators.py, html.py) and requires updating tests. An engineer would need to familiarize themselves with Django\u2019s mail code and write/adjust tests, which takes on the order of 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the root cause\u2014an early hasattr check in _check_list_display_item in django/contrib/admin/checks.py that misfires for PositionField descriptors. It specifies the correct logic flow, which functions to touch (hasattr, get_field, getattr), and the exact error types (E108 vs E109). It references class names (ModelAdmin, PositionField) and the file path, so an engineer can implement the fix unambiguously.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires understanding Django\u2019s ModelAdmin validation, inspecting django/contrib/admin/checks.py, and correctly handling exceptions and descriptor behavior. One must reproduce the test failure, write or adapt tests, and ensure the change respects existing patterns. An experienced engineer will need 1\u20134 hours to navigate the codebase, implement the exception logic, and validate via tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11734": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"While the description clearly states the problem\u2014exclude()/~Q() with OuterRef in an Exists annotation crashes compared to filter()\u2014it omits specific error messages, stack traces, or references to functions like split_exclude, get_prep_lookup, and get_prep_value. Therefore, an engineer must explore the ORM internals to locate the failing code paths before crafting a targeted fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this issue requires diving into Django\u2019s SQL query construction internals\u2014understanding how expressions are prepared (get_prep_value), how lookups process values (get_prep_lookup), and how split_exclude handles filters. The fix spans three modules and needs corresponding tests, which likely takes 1-4 hours depending on familiarity with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11740": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states that after changing a UUIDField to a ForeignKey, the generated migration lacks a dependency on the referenced app. It specifies the exact symptom (\u201cdid not include any dependency on testapp2, despite the FK\u201d) and the desired behavior (\u201cmigration should declare a dependency on testapp2\u201d). An engineer can locate the django/db/migrations/autodetector.py file and the generate_altered_fields method as the point of change, and write a targeted fix. The use of Django 2.2 and PostgreSQL is noted but not critical to the logic, and no further ambiguity remains.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s migration autodetector logic, specifically editing generate_altered_fields in django/db/migrations/autodetector.py (around lines 912\u2013972) to collect and pass foreign key dependencies, and then adding a test in tests/migrations/test_autodetector.py. For an experienced Django engineer familiarizing themselves with ~100 lines of code and writing a small test, this is a 1\u20134 hour task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns. The issue is self-contained, touches only migration generation and existing tests, and does not depend on external services or undocumented behavior. Suitable for benchmarking coding ability.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11815": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the bug in django/db/migrations/serializer.py within the EnumSerializer.serialize method, explaining that using the translated Enum.value in generated migrations can break on language changes. It specifies the desired behavior (referencing Enum by name, e.g. Status['GOOD']) and even provides example snippets in both the description and associated tests in tests/migrations/test_writer.py. An experienced engineer can immediately locate the EnumSerializer class, understand that the serialize() return value must change to use self.value.name with bracket notation, and update the tests accordingly without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires reading and understanding the Django migration serialization code, modifying one method in serializer.py to change how Enum members are represented, then adjusting multiple test assertions in test_writer.py to match the new bracket notation. While straightforward, it involves familiarity with Django\u2019s migration internals, ensuring imports are correct, running and updating many tests, which is likely to take a couple of hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11964": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description explicitly identifies that Django\u2019s TextChoices/IntegerChoices fields return enum members when cast to strings, rather than their .value attribute. It even references the Choices class in django/db/models/enums.py and provides code examples showing the unexpected output. The desired behavior\u2014overriding __str__ to return self.value\u2014is clearly stated, along with a test demonstrating success. This leaves no ambiguity about what needs to be implemented.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change involves adding a __str__ method to the Choices class in django/db/models/enums.py and adding a straightforward test to tests/model_enums/tests.py. Locating the class and writing the override plus test is a small, focused update likely taking under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12155": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints the faulty calculation in trim_docstring and explicitly shows the one-line change needed to skip the first line (using lines[1:]). The code snippet, file path (utils.py), and context are provided, making the requirement for a successful solution completely unambiguous.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"Fixing this issue involves a single-line adjustment in trim_docstring and corresponding test updates. An engineer familiar with Python and the codebase could locate the file, apply the change, run existing tests, and add the one new test in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional concerns. The change is self-contained within django/contrib/admindocs/utils.py and related tests, with clear guidance and minimal dependencies on other modules, making it an ideal benchmark example.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly describes that simple and inclusion tags with keyword-only arguments that have defaults fail when extra variables are supplied or the default argument is passed twice, identifying the exact behavior to correct in the template library. It names the relevant feature (keyword-only defaults) and highlights two distinct error cases, giving an engineer a concrete target for fixing the parsing logic.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires a single-line change in the parsing condition of the template library and adding a few test cases. For a developer familiar with Django\u2019s tag parsing, it can be understood and implemented within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The fix touches one core function and extends existing tests without needing additional refactoring or external dependencies. No further issues detected.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12325": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text indicates that ordering of OneToOneFields matters and that the existing parent_link marker should make ordering irrelevant. It names the relevant feature (parent_link) and where behavior deviates, but it does not explicitly state which lines to change or the exact code path to alter. An experienced engineer can sensibly infer that the OneToOneField filtering logic in django/db/models/base.py needs to check for parent_link, and that the options validation should be updated, but some investigation is required.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this bug requires understanding Django's model inheritance internals, locating the filtering logic in base.py, adjusting the OneToOneField check, removing the improper exception in options.py, and adding or updating multiple tests. While the change is localized, it spans multiple files and requires writing new test cases and ensuring compatibility, which would take an experienced engineer approximately 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the sample is self-contained and tests cover the intended behavior after the change.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12663": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is very brief and refers to a regression when combining SimpleLazyObject with nested subquery annotations, but does not include a concrete reproduction snippet, expected Vs actual behavior, or stack traces. It simply states \u201cas demonstrated below\u201d without providing the demonstration in the text. An engineer must inspect test code and ORM internals to infer what\u2019s wrong and what the fix should be, making the description vague and open to interpretation.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s ORM internals \u2013 specifically how Query.select entries store annotation fields, handling SimpleLazyObject, and adjusting the output_field property. The change spans both core query code and tests. An experienced engineer would need to read existing tests, trace the regression, and write a small but nontrivial patch, which would likely take 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12708": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly identifies a failure in Django\u2019s migration framework when deleting index_together on fields that are also part of a unique_together constraint. It pinpoints the exact method (alter_index_together in django/db/backends/base/schema.py) and describes two specific behaviors: deleting an existing composite index independently of unique_together and preventing index re-creation when moving its declaration. An experienced engineer can interpret these requirements to modify the flags passed to _delete_composed_index and adjust the SQL generation logic accordingly.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue involves examining the alter_index_together implementation, understanding how Django handles index and unique constraints at the database abstraction layer, and updating both the schema logic and test suite. It requires editing multiple files (the core schema.py and several migration tests), writing new test cases, and validating against different database backends, which would likely take an experienced engineer around 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12754": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that when creating a model subclass and moving a field in the same migration, the migration order leads to a name clash and must be reversed. It specifies that the auto-detector should insert a RemoveField dependency before CreateModel. While the high-level requirement is clear, the implementer must locate and modify django/db/migrations/autodetector.py, specifically generate_created_models, to detect removed base fields and adjust dependencies. The test expectations (ordering of operations) are also provided, so the developer can verify correctness.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An engineer must familiarize themselves with Django's migration autodetector internals, locate the correct hook in autodetector.py, implement logic to detect removed base fields and adjust dependency ordering, then add corresponding tests. This requires understanding of migration state models and test scaffolding, a moderate amount of complexity suitable for 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-12858": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly identifies that E015 is raised when ordering uses lookups instead of transforms and provides a concrete example chaining Stock.supply \u2192 Supply.product \u2192 Product.parent. It implies the fix must allow lookups in ordering checks, but leaves which code method to adjust implicit. An engineer must infer updating the _check_ordering routine to use get_lookup in addition to get_transform.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Locating _check_ordering in django/db/models/base.py, understanding the distinction between transforms and lookups, and updating its conditional to include get_lookup requires reading core ORM validation logic and writing a matching test. With familiarity, this is a moderate task taking a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected; the example and tests are self-contained and runnable.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13012": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that ExpressionWrapper constant expressions are incorrectly placed in the GROUP BY clause in Postgres when wrapped, whereas unwrapped constants are omitted. It explains the context (constant vs non-wrapped expression behavior) and the expected behavior (constants should be omitted), making it clear what change is needed (override get_group_by_cols to delegate to the inner expression).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, targeted change: add or override the get_group_by_cols method in ExpressionWrapper to delegate to the inner expression, and write a couple of tests. An experienced engineer familiar with Django ORM internals could implement and verify this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13028": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that a BooleanField named \u201cfilterable\u201d raises NotSupportedError when used in filters and that renaming the field works around it. It identifies the cause (the filterable attribute on expressions) and the desired fix (to handle only Expression instances). This is sufficient to understand what needs to be changed in the ORM\u2019s check_filterable method.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can locate the check_filterable method in django/db/models/sql/query.py, add a conditional guard for Expression instances, and update tests. This is a small, focused change requiring familiarity with ORM internals, taking roughly 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13112": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that makemigrations crashes because of a mismatch between the lowercased app label used in the ForeignKey reference (\u2018dj_reglogin.category\u2019) and the mixed-case app installation name (\u2018DJ_RegLogin\u2019). It identifies exactly where the problem occurs (in migrations deconstruction of ForeignKey relationships) and what the correct behavior should be (retain/apply the correct case for the app label portion when splitting and lowercasing). An experienced engineer can, with this description alone and access to the codebase, locate the deconstruct method in django/db/models/fields/related.py and implement the conditional split-and-lower-case logic shown in the patch.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s migration system (specifically how ForeignKey .deconstruct() works), locating the correct method in related.py, and writing a small conditional branch plus accompanying tests. While not trivial boilerplate, it\u2019s a focused change touching one method and one test file, so an experienced engineer would likely complete it within a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text clearly states the goal (native support for temporal subtraction without ExpressionWrapper) and describes the error context (FieldError on mixed types), but doesn\u2019t specify exact file locations or class names. An engineer must explore Django\u2019s expression codebase (e.g., expressions.py) to locate and modify the appropriate methods, so some investigation is needed before coding.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding Django ORM\u2019s expression framework, identifying and extending both as_sql and resolve_expression logic to handle temporal subtraction, and updating existing tests. It involves editing multiple code sections and writing tests, which will take an experienced engineer about 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13297": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that TemplateView.get_context_data() now returns SimpleLazyObject instances for URL kwargs in Django 3.1 and that these must be converted to strings so get_object_or_404 works. It identifies the exact location (get_context_data) and the symptom (filtering crash), but does not prescribe the exact API call (lazy vs. str), leaving the implementer to inspect the code and choose the appropriate wrapper. This is sufficient to guide a developer but requires them to fill in details of how to apply string conversion in a lazy context.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this involves a small patch in the generic/base.py file (changing an import, removing a decorator, wrapping the closure with lazy) and minor adjustments in one test file. An experienced Django engineer familiar with SimpleLazyObject and lazy would complete this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13406": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states that pickling queryset.query and re-running a query that uses values()/annotate causes model instances to be returned instead of the expected list of dicts. It specifies the incorrect behavior, the expected behavior, and the relevant Django APIs (values(), annotate(), QuerySet.query.setter). This gives a developer enough information to locate the code in django/db/models/query.py and implement a targeted fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the fix itself is a small change (two lines added to set _iterable_class based on the values_select flag), an engineer must understand Django's QuerySet internals, find the right extension point in query.setter, and update tests. This exploration and validation typically takes 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13449": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly states that on Django 3.0.7 with SQLite, annotating a Window expression using Lag() on a DecimalField produces a syntax error because CAST() is only applied to the LAG() call rather than the full OVER clause, and that setting output_field=FloatField() works around it. It identifies the specific class (Window), the function (Lag), the database backend (SQLite), and the problematic field type (DecimalField). It is clear that the fix must involve adjusting how the SQL is generated for DecimalField in SQLite, so minimal assumptions are needed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding Django\u2019s Expression API, the Window class in django/db/models/expressions.py, how output_field influences SQL generation, and where to hook into SQLite-specific rendering (as_sqlite). The gold patch edits one class and adds ~15 lines for a specialized override, plus corresponding tests. A proficient engineer could research and implement this in roughly 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional blockers: the tests provided in tests/expressions_window/tests.py clearly demonstrate how to validate the fix for DecimalField with Lag(), and the codebase already contains an SQLiteNumericMixin to follow. All dependencies and context are present in the main issue text and code layout, so this sample is suitable for benchmarking.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13568": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that Django's auth system check E003 incorrectly flags non-unique USERNAME_FIELD fields when a UniqueConstraint is used instead of unique=True. It specifies that Model._meta.total_unique_constraints should be inspected for a constraint matching the USERNAME_FIELD. It even hints at iterating over constraints and comparing fields tuples. The code example shows exactly where to modify the logic in django/contrib/auth/checks.py. The tests outline the expected behavior changes. Hence, the requirements are clear and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Modifying a single check function and writing two new test cases is straightforward for someone familiar with Django internals; it requires reading metadata APIs and writing basic constraint logic, which should take under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13807": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints the exact file (django/db/backends/sqlite3/base.py) and function (check_constraints), references specific line numbers (around line 327 and 333), and highlights the root cause (\u201cmissing back ticks around %s in PRAGMA statements\u201d). It shows the relevant code excerpts for both PRAGMA foreign_key_check and PRAGMA foreign_key_list calls. An experienced engineer can clearly see that quoting of table names is required (e.g., using self.ops.quote_name) and understand where to apply the change. There is no ambiguity about the desired behavior or where the fix should go.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires only minor edits to two SQL statement constructions in base.py, wrapping table names (and primary/foreign key column names) with the existing quote_name utility. An engineer familiar with Django\u2019s backend and PRAGMA statements can locate the check_constraints method, apply the quote_name calls, and validate using the existing tests in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-13810": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text pinpoints a specific bug in the load_middleware method in django/core/handlers/base.py: when MiddlewareNotUsed is raised, adapt_method_mode still overwrites the handler variable, poisoning the async middleware chain. It explains that synchronous-only middleware must explicitly support async to avoid this, and that the documentation misleads by suggesting async_capable=False is sufficient. From this description, an engineer can locate adapt_method_mode, adjust the order of assignment, and update tests. The filenames and exception class are referenced, making the requirements clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires reading Django\u2019s middleware loading logic, locating the adapt_method_mode call, and refactoring around the MiddlewareNotUsed exception. The patch is small (~10 lines in base.py) plus additions to a test file. Understanding ASGI vs WSGI handler adaptation and writing the async test takes some thought but remains localized. An experienced Django engineer would likely need 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14017": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies a specific, reproducible error: using the & or | operators on Q-Exists operand pairs is not commutative, causing a TypeError in one operand order but not the reverse. It explicitly states the expected behavior (commutativity) and the suspected root cause (missing __rand__ definition). An engineer familiar with Django\u2019s Query expressions can locate the operator overload implementations in django/db/models/query_utils.py and write a patch to add or adjust __rand__ logic. The description provides enough context and direction without external references, making the requirements for a successful solution unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue requires editing the Q._combine method (and/or adding a corresponding __rand__ method) in a single file and updating a small number of test cases to verify commutativity. An engineer who knows the codebase can read the operator overload logic and implement the change within 15\u201360 minutes, then run and adjust tests accordingly.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major concerns; the sample is straightforward to integrate into the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14140": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints the bug in the Q.deconstruct() implementation (in django/db/models/query_utils.py) by explaining that single-child Q objects are treated as kwargs when the child isn\u2019t a subscriptable tuple. It even references the exact function and the conditions under which the behavior should change. The desired change\u2014to remove the single-child special case so that all children are in args, leaving only connector and negated flags in kwargs\u2014is unambiguous. An engineer can locate the deconstruct() method, see the existing len(self.children)==1 branch, and know to remove or simplify it, then adjust tests accordingly.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"While the change itself is localized to a single method, it requires understanding the internal Q object deconstruction, Django\u2019s query construction, and updating multiple test files to reflect new args/kwargs behavior. An experienced engineer would need time to familiarize with Q internals and write comprehensive tests, making this a several-hours task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14238": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description explicitly names the method and class to change (AutoFieldMeta.__subclasscheck__), explains the root cause (subclass check excludes subclasses of BigAutoField/SmallAutoField), and even states the desired fix (allow issubclass(subclass, self._subclasses)). An engineer can locate the __subclasscheck__ implementation in django/db/models/fields/__init__.py and apply the change without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires a small, localized change (two lines in the __subclasscheck__ method) and corresponding updates to existing tests, which should take an experienced engineer between 15 minutes and one hour to implement and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14351": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly describes the error (subquery returning multiple columns) and pinpoints the difference in behavior between using agent__property_groups__in and agent__property_groups__id__in. An engineer can search for get_default_columns in django/db/models/lookups.py, observe how default fields are added, and sensibly decide to restrict the select clause to just the primary key when combining subqueries. The required outcome (select only pk in the subquery) is unambiguous, though the exact method names (e.g., clear_select_clause, add_fields) must be discovered in the codebase.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"A developer needs to understand Django\u2019s ORM internals, locate the lookup processing logic, and add a small helper method (~10\u201315 lines) plus a corresponding test. This would involve reading around the get_default_columns implementation, writing get_group_by_cols, and verifying behavior with a test\u2014likely a 1\u20134 hour task for someone familiar with Django.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14493": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the source file (django/contrib/staticfiles/storage.py) and function (post_process) where an uninitialized variable (substitutions) causes a crash when max_post_process_passes is set to 0. The user explains why substitutions is only assigned inside the loop and why setting max_post_process_passes to 0 triggers the bug. This is sufficient to pinpoint the one-line fix (initialize substitutions before the loop) without additional context or ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Once an engineer locates the post_process method and reproduces the crash, it is trivial to see that substitutions is not defined before the loop. Adding a single initialization line and updating a related test requires minimal code changes and could be done in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues; the sample is suitable for the benchmark as it tests understanding of Python control flow and variable initialization and involves a simple one-line patch plus a small test case.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14580": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that Django\u2019s migration writer omits the import of django.db.models when serializing models.Model. It specifies the expected and actual behavior, mentions the relevant module (django/db/migrations/serializer.py), and points to TypeSerializer.serialize. There is no ambiguity in what change is required: add an import statement for django.db.models in the migration serializer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can locate the migration serializer in django/db/migrations/serializer.py, understand the serialization logic, and add the missing import. Writing a small test and confirming behavior is straightforward. The change is localized and self-contained, requiring minimal familiarity with Django internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is self-contained and provides both the issue description and a focused patch. It involves a simple code addition and a single test change, with no external dependencies or complex setup. It is suitable for a coding benchmark as it tests understanding of import handling and migration serialization in Django.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14672": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that in ManyToManyRel.identity in django/db/models/fields/reverse_related.py, the through_fields attribute can be a list and is missing a call to make_hashable. The description pinpoints the file, method, and line, and even provides the exact change and corresponding tests, making the required solution unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer should be able to locate the identity method in reverse_related.py, insert a single make_hashable call, and update or run existing tests. This minimal, focused change with clear guidance typically takes between 15 minutes and one hour once familiarized with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; the sample is straightforward with clear code location, patch, and tests.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14787": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text explicitly identifies that method_decorator\u2019s inner bound_method is constructed via partial(method.__get__(...)) and therefore lacks standard function metadata like __name__ and __module__. It clearly states the need to preserve wrapper assignments. The provided gold patch and corresponding test demonstrate exactly how to apply functools.wraps to the partial, leaving no ambiguity about the required change or its intended effect.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is localized to a single function and consists of wrapping an existing partial object with functools.wraps. It requires knowledge of Python decorators and functools but is straightforward to implement, test, and validate. An engineer familiar with the codebase can complete it within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is self-contained, with clear input and output requirements, an existing test suite to verify the change, and minimal code modification. It is ideal for benchmarking decorator-related skills.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15104": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints exactly which file and method to patch (django/db/migrations/autodetector.py, only_relation_agnostic_fields) and what change to make (replace del deconstruction[2]['to'] with pop). The symptom, cause, and one-line fix are described clearly, leaving little ambiguity about the expected solution.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a straightforward one-line change in a single method, requiring minimal code navigation and no redesign. An experienced engineer can locate the deep_deconstruct call, apply the pop() fix, and run tests in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15128": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the source file (django/db/models/sql/query.py) and the functions affected (Query.combine, Query.change_aliases, Query.bump_prefix). It explains the alias collision scenario in `__or__` QuerySet combination, the resulting AssertionError, and outlines a specific strategy (bumping alias prefixes and adjusting join logic) and documenting non\u2010commutativity. All necessary details are present to implement and test the fix without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding Django\u2019s SQL query alias mechanics, modifying multiple methods in query.py, adjusting alias prefix logic, and adding tests. An experienced engineer familiar with Django internals could complete this in a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15277": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies the exact code location and behavior to change: in django/db/models/fields/__init__.py within CharField.__init__, the line appending MaxLengthValidator should be guarded by `if self.max_length is not None`, mirroring BinaryField.__init__. It provides the specific code snippet to modify and shows the context of Value._resolve_output_field returning a CharField with unnecessary validators. The test patch in tests/expressions/tests.py defines a new test method, `test_output_field_does_not_create_broken_validators`, which will fail until the conditional change is applied, making it unambiguous what a correct solution entails.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This requires editing a single constructor in CharField, copying a small conditional check from BinaryField.__init__. No deep algorithmic changes or multi-file coordination are needed and existing tests immediately validate the fix, making it a trivial sub-15 minute change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15280": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly describes the Django model definitions (User, Profile), the exact queryset operations (only on email, nested prefetch_related back to User limited to only kind), and the unexpected behavior (kind still triggers a query despite being deferred). It even points to checking get_deferred_fields and suggests the wrong inheritance of deferred fields. The precise file (related_descriptors.py) and method (get_prefetch_queryset) are identified, and examples of both code usage and expected test assertions are provided, leaving no ambiguity about what the correct solution must do.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is confined to a single method in related_descriptors.py, adding an is_cached guard around two lines of attribute setting. Writing and running one new test as shown is also straightforward. An experienced engineer familiar with Django\u2019s prefetch logic could identify and implement this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and suitable for a benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15315": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the class and method to change (Field.__hash__ in django/db/models/fields/__init__.py). It explains that the hash output currently depends on mutable model attributes, causing it to change when a field is assigned to a model. It states that reverting the change from PR #31750 will restore immutability. The provided patch shows exactly which lines to remove and how to update tests in tests/model_fields/tests.py. There is no ambiguity about what behavior is expected.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, localized change: one method in a single file and a small addition to the test suite. An engineer familiarizing themselves with the Django model field code and its test layout should be able to implement and verify the fix within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues. The sample is self-contained, has a clear failure case, and includes a focused test patch. It does not rely on external context beyond the provided description.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15375": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that using the new `default` argument on the `aggregate()` call after an `annotate()` leads to a crash, whereas using the long\u2010form Coalesce expression works. It identifies the context (Django ORM, PostgreSQL and SQLite), the version (4.0.1), and the specific user workflow (annotate \u2192 aggregate(default=\u2026) \u2192 crash). An engineer familiar with Django can locate `django/db/models/aggregates.py`, inspect `resolve_expression`, and infer that the default value isn\u2019t being wrapped by Coalesce correctly and missing the `is_summary` flag. While it doesn\u2019t show the exact stack trace or error message, there is a clear high\u2010level description of the unexpected behavior and a sensible interpretation of the required fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves modifying a single method (`resolve_expression` in `django/db/models/aggregates.py`) to wrap the column and default value in `Coalesce` (4 added lines) and adding two small tests. An experienced Django engineer could trace the bug, recognize the Coalesce pattern, and implement the change within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15380": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue statement clearly identifies that the migration autodetector fails when a model and a field are both renamed in the same change, indicating where to look (django/db/migrations/autodetector.py generate_renamed_fields) and what behavior is expected (handle both renames without crashing). However, it omits details like the exact error message or stack trace, so one would need to infer the precise failure mode from the code and tests. Overall, there is enough context to make a sensible plan.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s migration autodetector internals, locating the generate_renamed_fields() logic, and adjusting the model lookup for renamed models. It is a small change (one line) but involves reading multiple files, updating tests, and verifying behavior, which would likely take an experienced engineer a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is a good benchmark for testing understanding of a real-world framework internals (Django migrations). It does require candidates to be familiar with Django or to invest time reading its codebase, which might disadvantage those without prior Django experience.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15503": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies a specific bug in Django\u2019s JSONField lookup logic when numeric keys are used on non-PostgreSQL backends (SQLite, MySQL, Oracle). It names the affected lookup methods (has_key, has_keys, has_any_keys), the file path (django/db/models/fields/json.py), and describes the failure mode (numeric keys treated as array indices rather than object properties). It also specifies environment versions and backend behavior differences. This gives a developer enough context to locate the code to patch and understand what behavior needs to change (treat numeric keys as JSON object keys rather than array indices for the final lookup element).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s JSON lookup compilation, modifying compile_json_path behavior for the final key, updating multiple lookup classes, and adding targeted tests. A developer must familiarize themselves with Django\u2019s ORM lookup internals, the compile_json_path utility, and write a small subclass and test cases. Overall this is a moderate task likely to take 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15525": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text presents the Author and Book model definitions with their natural_key and get_by_natural_key implementations, including natural_key.dependencies on fixtures_regress.Person. It explains that calling loaddata against the default database works, but loading the same fixture on a secondary database raises an exception. A developer can reproduce the problem by running `manage.py loaddata nk_with_foreign_key.json --database=other`, locate the `build_instance` function in django/core/serializers/base.py, and understand that the instance DB state isn\u2019t set, making the requirements and expected behavior clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires familiarizing with Django\u2019s serialization internals (the build_instance function), tracing how natural_key lookups use obj._state.db, and applying a 3-line patch plus adding tests in two test modules. An experienced engineer could read the code, debug the DB routing issue, implement and verify the change within 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15695": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description pinpoints a precise failure in the RenameIndex operation in Django migrations. It states that when an unnamed index (auto-generated for unique_together) is moved backward and then forward again, the operation crashes because the old and new names are identical. It clearly identifies the RenameIndex class and the database_forwards step, and the expected behavior of a no-op when names match is unambiguous. With full codebase access, an experienced engineer can locate django/db/migrations/operations/models.py and implement the guard exactly as described.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves a small change in a single method (adding a conditional early return in database_forwards) plus a brief test addition. Locating the RenameIndex implementation and understanding Django\u2019s auto-generated index names may take some context gathering, but the patch itself is trivial. Overall this would take about 15 to 60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns identified. The sample is self-contained, focuses on a small, well-specified bug, and is appropriate for benchmarking a candidate\u2019s ability to read existing code, reason about edge cases, and write minimal tests.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15732": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that a Django migration fails when removing a unique_together constraint on a primary key field because the code expects exactly one unique constraint but finds two (the primary key and the unique_together). It specifies the model context, the error case, and that PostgreSQL is used, making it unambiguous what behavior to change in the migration code.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this requires understanding Django\\u0012s schema_editor and migration internals, locating and modifying the _delete_composed_index and related methods, adding a new helper to build constraint names, and updating tests to cover the behavior under different DB features. An experienced engineer familiar with the codebase would likely need 1-4 hours to trace constraint discovery logic, implement the change, and verify with tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is self-contained given the codebase and provides both code and test diffs to validate the fix.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15814": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the failing behavior when using QuerySet.only() after select_related() on proxy models, names the exact file (django/db/models/sql/query.py) and line number (around line 745), shows the relevant code snippet, and even suggests a minimal change (using cur_model._meta.concrete_model._meta). With full context on the codebase, an engineer can implement this small change and verify using the provided tests. There is no ambiguity about what is causing the crash or how to resolve it.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer familiar with Django ORM would need to locate the query building code, understand proxy models vs concrete_model, insert a single line, and update/add a test. This requires reading core framework code and running tests, which would take approximately 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15930": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly states that using Case() with ~Q(pk__in=[]) leads to a crash, noting that ~Q(pk__in=[]) is a sentinel value sometimes returned by application code. An experienced engineer can infer that the failure occurs in the Case.as_sql method in django/db/models/expressions.py when condition_sql is empty, and that the fix involves adding a check to compile Value(True) for an empty predicate. While the error stack trace isn\u2019t provided, the context is sufficient to locate the faulty code path and implement a guard in a sensible way.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer familiar with Django\u2019s ORM would spend roughly 15\u201360 minutes to locate the Case.as_sql method, understand how condition_sql is generated for ~Q(pk__in=[]), implement the empty-condition guard, and write or adjust a test. The change is localized to a small block of code and requires minimal refactoring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15957": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that Django\u2019s Prefetch() does not support sliced QuerySets and that users expect to limit prefetch to a slice (e.g., .all()[1:3]). The code paths in django/db/models/fields/related_descriptors.py (specifically in ForwardManyToOneDescriptor.get_prefetch_queryset and the m2m reverse branch) currently ignore queryset.query.is_sliced. It is immediately obvious that we need to detect slicing, extract low_mark/high_mark, construct a Window(RowNumber()) over the existing order_by, clear the limits, and apply GreaterThan/LessThanOrEqual filters. The expected API change is well-scoped: update get_prefetch_queryset and introduce a _filter_prefetch_queryset helper. No missing details prevent a meaningful implementation.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing this fix requires a moderate understanding of Django ORM internals: QuerySet.query metadata, window functions, and how Prefetch interacts with QuerySets in both forward and reverse relations. One must import Window, RowNumber, related lookups, clear existing limits, and ensure behavior across databases. Writing and validating tests further adds time. An experienced engineer could complete this in 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues to report. The issue is clear, the challenge is appropriately scoped for the benchmark, and there are no hidden dependencies or ambiguous requirements that would impede an engineer working solely from the provided description.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-15973": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly explains the apps involved, their M2M relationship using a through model, and the differing behaviors depending on where the through model is defined. It identifies the exact error message and context (migration), and the desired outcome (migrations should work when the through model is defined in its own app). The gold patch and tests show precisely what change is needed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the code change is a single-line replacement in the migration autodetector, it requires understanding Django's migration internals and how through relationships are resolved, as well as updating and adding detailed tests. An experienced engineer would need about 1\u20134 hours to familiarize and implement the fix correctly.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16032": "{\"q1_1_is_well_specified\":3,\"q1_2_explanation\":\"Only the issue title and a very brief phrase are provided without any concrete code context, examples, or reproduction steps. There is no detail on what \u201cselected fields\u201d refers to, how alias() is currently implemented, or what exactly must change in the ORM internals. Without further information about the relevant code paths, models, or existing behavior, it is almost impossible to determine the intended semantics or success criteria for a fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this issue requires diving into Django\u2019s internal ORM modules (related_lookups.py and query.py), understanding how annotations, aliasing, and select clauses interact, replacing calls, and adding a new flag. It also involves updating or creating tests to validate the behavior. An experienced engineer familiar with the codebase would likely need between one and four hours for analysis, implementation, and validation.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This issue is highly specific to Django\u2019s internal QuerySet implementation and annotation machinery, demanding deep framework knowledge. Candidates unfamiliar with Django internals may struggle, so it may not fairly assess general coding ability.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16136": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text clearly states that an async View subclass with only an async post method will throw an exception on GET because HttpResponseNotAllowed is not awaitable. It implies modifying http_method_not_allowed in the generic base to handle async views. While it does not explicitly reference method names or internal flags, an experienced Django developer would identify that http_method_not_allowed must detect async views and wrap the response in a coroutine.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating the http_method_not_allowed implementation, adding a simple conditional around view_is_async, and wrapping the HttpResponseNotAllowed return in a coroutine when needed. It touches only one function and a few tests, a task feasible in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected. The sample is self-contained, tests are provided, and the scope is limited. The issue and patch focus solely on one function and its tests, making the sample suitable for benchmarking.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16429": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue explicitly names the file (django/utils/timesince.py) and function (timesince), describes the error context (USE_TZ=True, >1 month interval), pinpoints the root cause (mismatch between offset-naive and offset-aware datetimes), and suggests the precise change (adding tzinfo=d.tzinfo to the pivot datetime construction). This is clear and actionable.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This fix involves a single-line change in timesince(), updating the pivot construction with tzinfo, plus a small addition to the tests. An experienced engineer could implement and verify this in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is straightforward, self-contained, and suitable for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16454": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description specifies the relevant class and method (CommandParser.add_subparsers), explains exactly which arguments aren\u2019t propagated to subparsers and why, and states the desired behavior change. An engineer can locate this method in base.py and implement a copy of the relevant arguments, so the specification is complete and unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Implementing this change involves adding a few lines in CommandParser.add_subparsers to wrap the parser_class via functools.partial, plus adjusting or adding tests. Understanding existing patterns and argparse behavior, then writing and verifying tests, is straightforward and would take under an hour for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16485": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue text only states \u201cfloatformat() crashes on '0.00'\u201d without any context. It doesn\u2019t specify what kind of crash occurs, which input types trigger it, or what the expected behavior should be. There are no stack traces, code snippets, or details about where in the codebase the issue arises. An engineer must infer the function\u2019s location, input handling (e.g. str vs Decimal), and the desired output, making it too vague to solve unambiguously.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Once the bug is understood, fixing it requires locating the floatformat filter implementation, adjusting a single conditional from `p < 0` to `p <= 0`, and adding two simple assertions to the test suite. For an experienced engineer familiar with the codebase, this is a small change that can be implemented and verified in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16569": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that in django/forms/formsets.py add_fields(), when index is None and can_delete is True while can_delete_extra is False, the comparison \u2018index < initial_form_count\u2019 causes a TypeError. It specifies the scenario (empty_form), shows the problematic line, and even provides the exact patch to guard against None, making the task entirely unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this involves a single localized change in add_fields() to add an `index is not None` check, plus adding one assertion to the existing test suite. An experienced engineer can locate the method, apply the patch, and update the test within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16667": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue pinpoints a clear failure in django/forms/widgets.py within SelectDateWidget.value_from_datadict: passing raw year, month, and day into datetime.date without overflow checks. The expected fix (catching OverflowError and returning a sentinel like \\\"0-0-0\\\") is directly implied. The PR metadata even shows the precise file and function to edit, as well as test files (tests/forms_tests/field_tests/test_datefield.py and tests/forms_tests/widget_tests/test_selectdatewidget.py) where assertions should be added. There is no ambiguity about what to implement or where to place the code.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, self\u2010contained change: wrap the existing datetime.date call in a try/except OverflowError block in django/forms/widgets.py and add a few assertions in the corresponding test modules. An engineer familiar with the codebase could implement and verify this in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected. The behavior and test modifications are straightforward, and the change does not introduce complex edge cases beyond the OverflowError handling.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-16950": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly describes that when creating a parent model (with a UUIDField default) and an inline child in the Django admin, the parent\u2019s UUID default isn\u2019t applied and ends up null. It specifies relevant models, Django version, and observed behavior. However, it does not point to the exact file or method in Django\u2019s codebase where the default logic should be fixed, so the engineer must locate the inline form handling code themselves. The requirements for a successful solution (ensuring the UUID default is applied in the inline formset) are nonetheless clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Django\u2019s inline formset implementation in django/forms/models.py, locating the add_fields method, and adjusting the default-handling logic for foreign keys with defaults. The change itself is small (tweaking an if condition) and adding a couple of tests, but requires a few hours to research the code path, verify related edge cases, and write tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-20859": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text explicitly states that adding a legend to a SubFigure fails because legend.py only checks for Figure, not FigureBase. It references the exact file (lib/matplotlib/legend.py), line 437, and suggests replacing isinstance(parent, Figure) with isinstance(parent, FigureBase) and updating the error message. This gives a precise code location and clear instructions for a patch, so no additional clarification is needed.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"An experienced engineer could locate the relevant type check in legend.py, change two isinstance checks, update the exception message, and add a simple test in under 15 minutes. The change is minimal, affecting only a handful of lines.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-22719": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly identifies two code paths in lib/matplotlib/category.py\u2014convert(value, unit, axis) and Axis.update(self, data)\u2014that emit a MatplotlibDeprecationWarning on empty inputs. It states that empty data should not trigger the warning and points to the generic catch in unit conversion. An engineer can sensibly interpret that the fix is to guard the warning emission behind a check on values.size or data.size, matching the edge case of an empty array. The provided test patch also shows exactly where and how the behavior should change. Although one must dive into the code to locate the correct conditional, there is no ambiguity about the requirement: suppress the deprecation warning for empty data.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a localized change in two small functions: add a guard on values.size in category.convert and data.size in Axis.update, then add a smoke test. Locating the spots requires reading two functions, but the fix itself is straightforward and limited to a few lines. With familiarity of the converter code, an experienced engineer can implement and test this within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-23299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue pinpoints a specific interaction between matplotlib.get_backend and the context manager rc_context: calling get_backend triggers a code path that resets the figure registry Gcf.figs, unexpectedly removing figures that were created within an rc_context. It clearly states which functions are involved (__init__.py rc_context, get_backend, Gcf.destroy_fig) and what behavior is expected (backend param not reset and figures preserved), making it straightforward to implement the minimal patch provided by the PR.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer would need to locate the rc_context implementation, understand that rcParams.copy() should exclude the 'backend' key, apply a two-line change plus add a targeted test. This involves moderate code navigation and understanding of context managers, but the fix itself is small and can be written and verified within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is self-contained, the test harness clearly checks the behavior, and the minimal changes target a well-defined part of the codebase. This makes it a good benchmark for assessing the ability to navigate large codebases and write focused patches.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-23476": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that unpickling a figure causes its DPI to double on M1 Macs, and identifies the symptom, environment and versions. However, it does not specify the precise code location or internal attributes (__getstate__, _original_dpi) to modify. An engineer must browse the serialization code in lib/matplotlib/figure.py, identify the getstate hook, and implement the reset logic. The overall requirement\u2014to prevent DPI scaling on unpickle\u2014is well-defined, but the \u2018how\u2019 is left to exploration of internal APIs.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this bug requires understanding how Matplotlib pickles figures, locating the __getstate__ method within lib/matplotlib/figure.py, and learning that the figure stores both dpi and _original_dpi. The engineer must then implement code to reset the dpi to the original before pickling, and add a test for pickle.loads and pickle.dumps. This involves modifying one function, adding three lines, and writing a small test; overall it\u2019s more than a trivial tweak, but can be completed in a couple of hours once familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-24026": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that stackplot should not alter the Axes color cycler and that supplying a \u2018CN\u2019 color alias currently raises a ValueError. It describes the desired behavior (supporting the CN alias and preserving the cycler) and gives a concrete failure mode. The inputs (plot types, CN aliases) and the expected output (no error, consistent color cycling) are sufficiently described to guide an implementation without external context, though precise internal references (e.g. function names) must be inferred from familiarity with the codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue requires locating the color-handling logic in stackplot.py, replacing a direct set_prop_cycle call with a color cycling iterator, importing itertools, and updating two or three spots where get_next_color was used. Adding a small test case in test_axes.py is straightforward. An experienced engineer could implement, test, and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-24149": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that ax.bar raises an exception when all data values are NaN under Matplotlib 3.6.1, and notes that this breaks Seaborn\u2019s histogram behavior. It references a related release note but does not include sample code or point directly to the failing function. A developer must interpret that safe\u2010first\u2010finite routines should handle StopIteration and fallback gracefully. This provides enough context to guide an implementation after exploring the internal _convert_dx and cbook functions, but leaves specifics to be discovered.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires locating the _convert_dx helper in axes/_axes.py, understanding the existing safe_first_finite logic, adding StopIteration handlers in two try/except blocks, and writing or extending a test case. An engineer familiar with the codebase can implement and validate this in a couple of hours\u2014including writing the new test and ensuring consistency with style guidelines\u2014so it falls in the 1\u20134 hour range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-24970": "{\"q1_1_is_well_specified\":3,\"q1_2_explanation\":\"The issue description simply states \u201cStarting NumPy 1.24 I observe several deprecation warnings\u201d with no warning text or stack traces. There is no mention of which functions, modules or lines are triggering the warnings, nor guidance on the expected behavior after the fix. Without at least example warning messages or affected code locations, it is almost impossible to determine what needs to be changed.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"Assuming the specific warnings were identified, the fix shown in the patch involves moving a small block of code and adding a single cast inside a controlled np.errstate context. It touches one function in colors.py and a handful of lines in a test file. An experienced engineer could understand the context and implement this in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-25311": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description merely states that pickling a figure with a draggable legend or annotation fails, without providing error messages, stack traces, or code examples. It gives version and OS information but no context about where in the code the failure occurs or what the exception is. A developer must infer the root cause (canvas reference not being picklable) and the required fix from external knowledge rather than the issue text itself, leaving room for ambiguity and guesswork.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Assuming the problem is clarified (i.e., that the figure.canvas attribute must be converted to a property to remain picklable), the fix requires editing the offsetbox module to remove the direct canvas assignment, adding a property, and updating associated tests. This involves understanding Matplotlib\u2019s offsetbox internals, the pickle protocol, and its test suite. It is more than a trivial change but does not require extensive multi-file rewrites, so ~1\u20134 hours is reasonable.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":3}"
    },
    {
        "matplotlib__matplotlib-25332": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The GitHub issue only states that pickling fails after calling align_labels() but provides no sample code, error message, or context\u2014no traceback, no details on how align_labels is implemented in lib/matplotlib/cbook.py in the Grouper class. Without guidance on where the failure occurs or what exception is raised, an engineer must explore the codebase to locate the __getstate__/__setstate__ methods, infer how to serialize weakref._mapping, and anticipate align_labels\u2019 behavior. This makes the requirements vague with room for ambiguity.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding the internal Grouper implementation in lib/matplotlib/cbook.py, particularly how _mapping uses weakref.ref, and writing custom __getstate__ and __setstate__ methods to convert between weak and strong references. Additionally, the engineer must update test_pickle.py to cover aligned labels and ensure pickling works. This involves editing multiple files, writing around 20\u201330 lines of code and tests, and verifying pickling across subplots, which would take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-25479": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text describes exactly the problem: registering a colormap under one name while its internal .name attribute differs causes pyplot lookups to fail, since pyplot uses cmap.name rather than the registry key. It points out when and where the lookup breaks (in pyplot functions like imshow or set_cmap) and suggests that updating the .name attribute during registration would resolve it. While implementation details (exact function names or module paths) are not spelled out, an experienced engineer can infer that modifying the register() method in cm.py to reset cmap.name and adjusting equality behavior would satisfy the requirement.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer familiar with the colormap registry in matplotlib could locate the register() implementation in lib/matplotlib/cm.py and implement the name assignment in under an hour. Writing a small unit test for the new behavior and updating the equality check in colors.py are straightforward tasks, requiring modest code edits across two files.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "matplotlib__matplotlib-26291": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description makes clear that inset_axes fails when following the first example on the Matplotlib website and specifies the expected behavior\u2014an empty inset box in the top right of the subplot. However, it omits the actual error traceback, the user\u2019s code snippet, and direct references to the function or file causing the failure (axes_grid1/inset_locator.py). An engineer must infer or locate the missing context, but the high-level requirement and outcome remain sensible.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this bug involves exploring the axes_grid1 inset_locator implementation, identifying that renderer is sometimes None, and modifying the __call__ method to guard against that. It also requires writing or updating a test to cover bbox_inches=\\\"tight\\\". An experienced engineer would need to understand the code flow, apply a small patch, and verify tests, which should take roughly 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "psf__requests-1724": "{\n  \"q1_1_is_well_specified\": 0,\n  \"q1_2_explanation\": \"The issue description clearly identifies that using a Unicode HTTP method leads to Unicode headers via the .upper() call in sessions.py, and points to adding a cast to a native string (builtin_str) before uppercase. With access to the codebase it is straightforward to locate sessions.py, identify the problematic line, import builtin_str from compat, wrap the method argument, and update the test suite accordingly. The desired behavior and change are unambiguous and well-scoped.\",\n  \"q2_1_difficulty\": 1,\n  \"q2_2_explanation\": \"This fix requires adding one import and wrapping the method parameter with builtin_str, modifying a single function in sessions.py, and adding a small test case. An experienced engineer can trace the origin of the bug, locate the compat module, apply the change, and run the tests comfortably within an hour.\",\n  \"q2_3_other_issues\": 0,\n  \"q2_4_other_notes\": \"\",\n  \"q2_5_confidence\": 4\n}"
    },
    {
        "psf__requests-5414": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that requests.models.prepare_url is currently raising a UnicodeError for URLs with a leading dot in the host, but should instead raise InvalidURL. It provides the exact example URL (\u2018http://.example.com\u2019) and references the existing exception path in models.py around line 401. The expected behavior and location of the change are unambiguous, making it trivial to implement the small patch and verify with added tests.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a simple two-line change in a single function (modify an elif condition to include a dot prefix) plus adding two test cases. An experienced engineer familiar with the codebase could identify the fix, implement it, and run the existing tests in well under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3151": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies that combine_by_coords currently enforces monotonicity on all dimensions found in the concatenated dataset, including dimensions whose coordinates are identical across inputs. It states that identical coordinate dimensions should be ignored in monotonicity checks, and points to the ValueError thrown when non-monotonic identical coords occur. The intended behavior, pulling from documentation, is to ignore non-coordinate dimensions and constant coords for monotonic tests, so a developer can definitively locate the loop in xarray/core/combine.py that must use the concat_dims list rather than concatenated.dims.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue requires modifying a short loop in combine_by_coords to iterate over the existing concat_dims list instead of concatenated.dims, and updating or adding a test case. An engineer familiar with the codebase and its testing conventions can locate the relevant section in combine.py and write the new test in test_combine.py in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-3677": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that using ds.merge(DataArray) fails while the top-level merge() works. The requirement is to support DataArray inputs by converting to Dataset under the hood. It points to the merge method in xarray/core/dataset.py and contrasts behavior with the free function. This gives a concrete \u201cwhat\u201d and hints at \u201chow\u201d (type check and conversion).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, localized change: add a type check and conversion in the merge method (around one line) plus a corresponding test. An experienced engineer familiarizing with the module should implement and test this within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-4094": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly describes that to_unstacked_dataset fails when variables have only a single dimension, indicating that a drop of the stacking dimension is required. While it does not include an explicit error message or stack trace, the intent is obvious: implement proper handling (squeeze/drop logic) for single-dim variables. The code example in core/dataarray.py shows where sel is called without drop=True, and the regression test in test_dataset.py illustrates the expected behavior. This yields a sensible interpretation of the required fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is a one-liner change in to_unstacked_dataset (adding drop=True to the sel call) plus a small regression test. An experienced engineer familiar with xarray would locate the sel call, understand the API flag, and implement and test the change within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-4695": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is brief but clearly states that naming a dimension \\\"method\\\" triggers an error when calling .loc, implying a collision with an internal parameter. While it doesn\u2019t show code or stack traces, it sensibly leads the engineer to inspect xarray.core.dataarray.DataArray.__getitem__, see that .loc delegates to sel(**key), and realize that using sel(key) instead of sel(**key) avoids keyword conflicts.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue requires a focused change in a single method (DataArray.__getitem__) and adding a small test case. Once familiar with the sel vs. loc implementation and the keyword collision, an experienced engineer could implement and verify the patch within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-6599": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue title and high\u2010level description identify that timedelta64 coords in polyval produce wrong results, but no example input, actual wrong output, or expected result is provided. A developer must explore xarray\u2019s computation.py, locate the to_floatable helper, and infer how timedeltas should be handled. The lack of repro code and explicit expected behavior leaves room for interpretation about the precise fix and where to add the branch, making the specification somewhat vague.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires reading the existing to_floatable implementation in xarray/core/computation.py, recognizing the missing dtype.kind 'm' case, adding an elif branch, and then writing a small pytest param. This is a modest one\u2010file change plus one test tweak, doable in 15\u201360 minutes by an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns. Apart from the previously noted ambiguity in the issue description (lack of repro and expected outputs), the sample is suitable: the required change is localized, and the provided test patch fully captures the necessary behavior. It presents a realistic but small bugfix task that aligns well with the benchmark setup.\",\"q2_5_confidence\":4}"
    },
    {
        "pydata__xarray-6721": "{\"q1_1_is_well_specified\":3,\"q1_2_explanation\":\"The issue description omits any explanation of the bug behavior beyond the environment context. There is no description of how accessing chunks triggers a full array load, no reference to functions, classes, or methods involved, and no examples illustrating the failure mode. Without this core information, it is almost impossible to infer what exactly needs to change or how the solution should behave.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"Given a clear specification, the fix consists of a one-line change in get_chunksizes (switching v.data to v._data) and adding a simple regression test. An experienced engineer familiar with the codebase could implement and verify this in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pylint-dev__pylint-6386": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that the short flag \u201c-v\u201d for the verbose option is currently defined as expecting an argument (e.g., metavar or arg requirement) while the long form \u201c--verbose\u201d does not. It explains how the short form help message misleadingly shows a value, and the expected behavior is parity with the long form. From this description alone, an engineer can locate the option definitions in pylint/config/utils.py (the mapping of flags to handlers and kwargs), arguments_manager.py (_add_parser_option calls), and base_options.py (option metadata) to remove the argument requirement and add the \u201c-v\u201d alias without external context.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding Pylint\u2019s internal argument definition and parser registration, touching multiple small modules: config/utils.py (option conversion), arguments_manager.py (_add_parser_option), and base_options.py (option metadata). Adding a new flag alias and updating tests is straightforward but involves reading existing abstractions and adding a parameter mapping (metavar), so roughly a few hours of familiarization and coding.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pylint-dev__pylint-6903": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that the function pylint.lint.run._query_cpu() returns 0 when calculating available CPUs in a Kubernetes Pod with --jobs=0, causing a crash in multiprocessing. It specifies exactly where to apply the fix (in run.py within _query_cpu) by appending an `or 1` fallback. The test patch shows updates in tests/test_pylint_runners.py to mock cpu.shares and cpu.cfs_quota_us, demonstrating how to validate the change.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, localized change: modify one function in pylint/lint/run.py to handle the zero-CPU case, and add a corresponding unit test. An experienced engineer could understand the code and implement the fallback within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and suitable for benchmarking.\",\"q2_5_confidence\":4}"
    },
    {
        "pylint-dev__pylint-8898": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states that the \u201cbad-names-rgx\u201d configuration is being split on commas, causing regular expressions with commas (e.g., quantifier syntax like `{1,3}`) to be mangled. It specifies the relevant option and expected behavior (a list of regex strings rather than pieces split at every comma). An engineer can locate the CSV transformer in pylint/config/argument.py and implement a split function that skips commas inside brace-delimited quantifier expressions. The problem statement is succinct but contains enough context to define a clear solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this bug requires modifying the CSV transformer logic in config/argument.py, adding a helper in utils to split on commas only when not inside braces, updating imports and calls, and writing a small suite of parametrized pytest cases. While it spans multiple files, the algorithm is straightforward; an experienced engineer should complete and test these changes within 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-10051": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states that calling caplog.clear() replaces the internal records list rather than clearing its contents, causing caplog.get_records() to reference an outdated list and not capture new log records. It specifies the exact methods involved (caplog.get_records(), caplog.clear(), caplog.records) and the expected behavior (clear existing records and continue capturing new ones). There is sufficient context to implement a handler.clear() that mutates the existing records list and update caplog.clear() to call this new method.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a small, localized change affecting only a few lines in the logging capture handler: one new clear() method and updating caplog.clear() to call it. An experienced engineer familiar with pytest internals could identify and implement this fix in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-10081": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description precisely identifies a pytest bug: when using the --pdb option, unittest.TestCase.tearDown is still called for tests skipped\\nat the class level. It explicitly references the analogous #7215 function-level case and asks for identical behavior when the skip decorator is applied to a class rather than a method. A reader can immediately locate the relevant runtest() implementation in src/_pytest/unittest.py and know to extend the existing _is_skipped(obj) check to also inspect the parent class object. All necessary context (what is happening, under what conditions, and where to modify code) is conveyed without relying on external links or discussion.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can understand the bug in under 15 minutes by reading the summary and locating the runtest() method. Identifying the missing skip check on the class parent and writing the three-line patch plus a small test addition requires a bit of thought and validation, but is straightforward and would likely take 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5262": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the root cause\u2014the EncodedFile class\u2019s mode property incorrectly includes the \u2018b\u2019 character\u2014and explains that removing \u2018b\u2019 from the advertised mode is required to prevent write() from raising an exception. The context of youtube-dl checking out.mode makes the desired behavior unambiguous.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"Implementing the fix requires only adding a property in one class to strip out the \u2018b\u2019 character and adding a small test. The change is localized, minimal, and straightforward to implement and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5631": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states that using pytest 3.6.0 with @patch new set to a NumPy array triggers a ValueError during collection because `p.new in sentinels` returns an array of booleans rather than a single boolean. It points directly to the function `num_mock_patch_args` in `src/_pytest/compat.py` and describes the defective membership check, making it obvious that the fix is to use identity checks (`is`) against mock sentinels. No external context or clarification is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can locate the `num_mock_patch_args` function, understand the membership-vs-identity issue, and implement the fix in under an hour. The change is localized to one function, involves replacing the `in` check with identity comparisons against known sentinel objects, and adding a small test. This is a straightforward patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and suitable for benchmarking.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-5787": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description only states that exception serialization should include chained exceptions and notes that with xdist only the last exception is shown. It does not reference any specific file, function, or code location to modify, nor does it describe the expected behavior in terms of data structures or output format. The engineer would need to infer which serialization routines to update, what \\\"xorxdist\\\" hooks to patch, and how chained exceptions should be represented. This leaves substantial ambiguity about what exactly must be changed and where.\",\"q2_1_difficulty\":3,\"q2_2_explanation\":\"Fixing this requires deep understanding of pytest\u2019s reporting internals, the xdist plugin, and how exception chaining is represented and serialized. The solution spans over 300 lines of code across serialization and deserialization routines, addition of new helper functions, changes to several classes, and extensive updates to test code. An engineer would need to explore the codebase, design a new JSON schema for chained exceptions, implement two-way conversion, update tests and fixtures, and verify correctness with existing tests.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This sample has additional challenges: it depends on internal pytest APIs and xdist extension points, so an engineer unfamiliar with these internals may struggle. The patch is very large and impacts many areas, making it difficult to review or adapt. The benchmark setting may not provide sufficient context or test harness details for a candidate to solve this without extensive exploration of pytest\u2019s codebase.\",\"q2_5_confidence\":3}"
    },
    {
        "pytest-dev__pytest-5809": "{\n  \"q1_1_is_well_specified\": 0,\n  \"q1_2_explanation\": \"The issue description pinpoints the exact code location in src/_pytest/pastebin.py (lines 68-73) where the 'lexer' parameter is set to 'python3'. It clearly explains that this leads to HTTP Error 400 for certain contents and recommends changing 'lexer' to 'text' because pytest console output is arbitrary text. The accompanying gold patch shows the single-line change in the params dict and corresponding update in testing/test_pastebin.py, leaving no ambiguity about the required modification.\",\n  \"q2_1_difficulty\": 0,\n  \"q2_2_explanation\": \"This is a trivial one-line change in a small helper function plus a corresponding update in an existing test. An experienced engineer can locate the params dict, adjust the 'lexer' value from 'python3' to 'text', and update the test assertion in under 15 minutes.\",\n  \"q2_3_other_issues\": 0,\n  \"q2_4_other_notes\": \"\",\n  \"q2_5_confidence\": 5\n}"
    },
    {
        "pytest-dev__pytest-5840": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is extremely brief and lacks concrete details about where and how the path is being converted to lowercase, what functions or modules are affected, and how conftest is being imported. It simply states an ImportError due to folder casing issues after upgrading to 5.1.2. Without references to code locations (e.g., unique_path or config internals) or error messages, there is significant ambiguity about what needs to be changed, even though the general problem (case conversion on Windows causing import failures) can be inferred.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding pytest\u2019s plugin loading internals (_importconftest, _getconftestmodules), the difference between py.path.local and pathlib.Path, and Windows case-insensitivity. It involves edits in multiple files (config/__init__.py, pathlib.py, and updating tests) and careful handling of realpath vs resolve. An experienced engineer familiar with pathlib and pytest internals would need 1\u20134 hours to research, develop, and validate the patch.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This sample is OS-specific (Windows case-insensitivity) and relies on behavior that may not be reproducible on Unix-based systems. Engineers working on Linux might struggle to observe the failure or verify the fix without a Windows environment or special configuration. This environment dependency could hinder its use in a uniform benchmarking setup.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-6197": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly states that pytest 5.2.3 is importing every __init__.py file under the current working directory, including those in packages only meant for Windows, leading to import errors on Linux. The symptom (\u201cpytest tries to import any __init__.py file\u201d) and context (we have a Windows\u2013only package) are explicit, and the desired behavior (skip these files during collection) is understandable. There are some implementation details left unspecified (exact hook or function to modify), but an engineer familiar with the pytest collector code can sensibly infer where to apply the fix and write tests to confirm the change.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires understanding the pytest collection internals, locating the PyobjMixin and Module.collect implementations, adjusting property handling to defer mounting and patching the logic to skip __init__.py files, plus adding test cases. This spans editing ~20\u201330 lines of core code and updating multiple test files, likely taking an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue text, code patch, and tests are self-contained. The sample is suitable for a benchmark since it involves nontrivial code navigation, modification, and validation.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-6202": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text pinpoints the exact undesired behavior (\\\"return s.replace('.[', '[')\\\" in getmodpath), identifies the file (src/_pytest/python.py) and the offending lines (around L274\u2013292 and L294\u2013308, specifically line 306), and proposes the desired change (remove the replace call). It also provides a test case diff in testing/test_collection.py that asserts the corrected module path formatting. There is no ambiguity about what needs to be done: update getmodpath to return \\\"\\\".join(parts) and adjust tests to expect the new behavior.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer only needs to locate the getmodpath implementation, remove the replace call (one line change), update or add a couple of assertions in the existing test file, and verify with the pytest suite. This is a small, self-contained change that should take under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"N/A\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7205": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states the problem (pytest uses str() on bytes parameters in setup output, triggering BytesWarning) and suggests the remedy (use saferepr for a safe representation). It references the specific behavior to change, and any experienced engineer can locate the `_show_fixture_action` function in `src/_pytest/setuponly.py` to import and apply `saferepr`, making the requirements unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves a few straightforward edits: import `saferepr`, replace one `tw.write` call to wrap the bytes parameter in `saferepr`, and add a small test case. An engineer familiar with the codebase could implement and verify this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7236": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue title and description clearly identify a regression in pytest\u2019s unittest plugin: when running tests with --pdb, tearDown still executes on skipped TestCase methods even though PHPUnit 5.4.1 did skip teardown. The user specifies the version change (5.4.2 vs 5.4.1) and expected behavior. Although the description does not point to specific code locations, an experienced engineer can interpret that the skip flag (__unittest_skip__) should be checked before calling teardown in the --pdb path in src/_pytest/unittest.py.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires locating the code in pytest\u2019s unittest plugin where teardown is deferred under --pdb, adding a guard to skip teardown for skipped tests. It involves a small change (adding an _is_skipped check) across a few lines, writing one helper, and updating tests. An experienced engineer could implement this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7324": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue text consists only of a brief title (\u201cPytest crashes the interpreter on debug build for 3.8+\u201d) and a link to an external bpo issue, without any reproduction steps, stack traces, or detailed context. It does not describe which code paths trigger the crash, what error messages appear, or where in the pytest codebase the failure occurs. An engineer cannot infer from this alone how the mark expression parser or AST conversion is involved, so there is significant ambiguity about the root cause and required fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the description is ambiguous, once clarified it requires understanding pytest\u2019s custom expression parser in src/_pytest/mark/expression.py, recognizing that Python keywords like True/False/None conflict with identifier rules, devising a prefix-based workaround, and updating both the parser and test suite. This involves reading and modifying ~20\u201330 lines across multiple functions, working with the AST module, and adding test cases, which would likely take an experienced engineer 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues beyond the initial ambiguity of the description. Once the scope is clear, the implementation and tests are straightforward.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7490": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the regression in pytest 6: dynamically added xfail markers via request.node.add_marker(mark) no longer suppress failures as expected. It specifies the user\u2019s expected behavior under pytest 5.x and the observed behavior under pytest 6.0.0rc0 and later on macOS. There is no ambiguity about what needs to be fixed: restore the previous behavior so that dynamic xfail markers cause failures to be reported as expected xfails. The description pinpoints the exact API (request.node.add_marker) and the desired outcome, making it straightforward for an experienced engineer to locate the relevant hook implementations and implement the fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves understanding pytest\u2019s hook implementations in skipping.py, locating where xfail marks are evaluated in pytest_runtest_setup and pytest_runtest_call, and adjusting the conditions to restore dynamic xfail behavior. The patch is small (around 20 lines) and accompanied by two new tests. An experienced engineer familiar with pytest internals could implement and verify this change in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-7521": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that capfd.readouterr() is converting carriage returns \\\"\\\\r\\\" to newlines \\\"\\\\n\\\" and that this behavior was not documented. It identifies the exact file (src/_pytest/capture.py) where newline handling must be adjusted and provides a failing test in testing/test_capture.py. The desired behavior and context are unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires understanding Python's TextIOWrapper newline parameter and pytest capture internals in src/_pytest/capture.py, adding a single argument and extending a parametrized test. The change spans two small diffs and should take under an hour for someone familiar.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-10297": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly points to the RidgeClassifierCV class in sklearn/linear_model/ridge.py lacking support for the store_cv_values flag even though the documentation mentions it. It is sensible to interpret that we need to mirror the implementation in RidgeCV: add the store_cv_values parameter in __init__, update the docstring for cv_values_, and adjust the constructor call to the base class. The expected tests should mirror those in test_ridge.py for RidgeCV, verifying the cv_values_ shape. All filenames (ridge.py, test_ridge.py) and class names are given, making the fix straightforward.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires a small set of changes: modify the RidgeClassifierCV __init__ signature to include store_cv_values, update its docstring for cv_values_, pass the flag to the superclass, and add analogous tests in test_ridge.py. A developer can follow the existing RidgeCV implementation in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues noted; the sample is suitable for evaluation.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-10908": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the specific behavior in CountVectorizer.get_feature_names that needs to change: it should not raise NotFittedError when a vocabulary is provided at initialization. It references the exact methods (_validate_vocabulary and _check_vocabulary) and describes the expected workflow. An experienced engineer can locate these methods in text.py, understand when vocabulary_ is set, and implement the minimal change.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves adding a small conditional in a single method to call _validate_vocabulary if vocabulary_ is missing, plus minor test updates. An engineer familiar with the codebase could implement and test this within 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-12585": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the function (`clone` in `sklearn/base.py`), the failing behavior when parameters are estimator classes rather than instances, and suggests a specific conditional change (`elif not hasattr(estimator, 'get_params') or isinstance(estimator, type)`). The context (wrappers for sklearn estimators) and version are given. It specifies exactly what needs to change and how to verify via tests, so an experienced engineer can immediately locate the code and implement the fix without additional information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires a single-line change in `clone` plus adding a short test case in `test_base.py`. An engineer familiar with the codebase could make and validate this change in under an hour, likely in 15\u201330 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13135": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly indicates that in preprocessing/_discretization.py within the fit method of KBinsDiscretizer, the KMeans cluster centers may not be returned in sorted order, causing bin_edges computed by averaging adjacent centers to be unsorted and breaking numpy.digitize. One can follow the file and functions in sklearn/preprocessing/_discretization.py around lines 167-178 to locate where centers is defined and add a centers.sort() call.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is a straightforward one-line addition of centers.sort() in _discretization.py and adding a few lines to the existing test to cover the 5-bin case. An experienced engineer familiar with NumPy and scikit-learn\u2019s discretization logic would spend 15\u201360 minutes locating the code path and modifying the tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues. The code change is minimal and well-isolated, but one should be mindful that KMeans cluster center order is inherently non-deterministic unless init or random_state are set, so adding explicit sorting is appropriate. Tests should seed the random generator to ensure consistency across runs.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13142": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description concisely identifies that fit_predict and predict disagree when n_init>1, pinpoints the functions involved (GaussianMixture.fit_predict vs. .predict) and the missing final E\u2010step, and names the unit test that currently does not catch the error. It clearly defines the expected behavior\u2014that fit_predict should be consistent with predict\u2014and guides the implementer to where the change is needed. This provides enough information to craft a correct solution without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is minimal: insert a final call to _e_step in fit_predict and add a small unit test. The patch affects only two files and a handful of lines. An experienced engineer can locate the method implementation, understand the EM algorithm\u2019s E\u2010step, and write the code and test within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13328": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that HuberRegressor.fit should accept boolean X by converting bool arrays to float internally, just like LinearRegression. The failure is a TypeError on boolean arrays. The solution is to modify the check_X_y call in sklearn/linear_model/huber.py to include dtype=[np.float64, np.float32], and add a test in sklearn/linear_model/tests/test_huber.py (test_huber_bool) to ensure bool data no longer crashes.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves a small code change\u2014adding the dtype argument to an existing check_X_y call in huber.py\u2014and adding a minimal test in test_huber.py. An experienced engineer familiar with the codebase could implement and verify this in about 15\u201330 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Overall, this issue is self-contained and low risk. There are no external dependencies or side effects beyond the targeted change, making it suitable for a straightforward coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-13779": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description succinctly explains that a Voting estimator\u2019s fit method fails when sample_weight is passed and an estimator is None. It clearly states that a None check should be added to skip None estimators in sklearn/ensemble/voting.py before handling sample weights. This information is sufficient to implement a targeted conditional check and add corresponding tests.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This fix only requires adding a simple 'if step is None: continue' before existing sample_weight logic in voting.py and updating or writing a small test, which an experienced engineer could complete in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14053": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly names the function (export_text in sklearn/tree/export.py), the error (IndexError when tree has a single feature), and the expected behavior (export_text should handle one-feature trees without error). It\u2019s unambiguous which list comprehension needs updating and what the end behavior should be.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this involves locating the small list comprehension in export_text, adding a simple conditional to skip undefined feature indices, and writing a corresponding test case. An experienced engineer could implement and verify this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14087": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue text states that an IndexError occurs when using LogisticRegressionCV with refit=False, and it lists the environment, but it does not show the actual error message or stack trace, nor does it pinpoint where in sklearn/linear_model/logistic.py the failure happens. Key variables like self.multi_class vs multi_class and self.penalty are not mentioned, and there are no reproduction steps or specific code snippets triggering the error. An engineer would have to search across multiple methods and test files (e.g., logistic.py and test_logistic.py) to locate the root cause, making the requirements vague and open to interpretation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this bug requires inspecting the fit method in sklearn/linear_model/logistic.py to understand how best_indices and l1_ratio_ are handled with refit=False, adding a conditional for penalty!='elasticnet', and updating parametrized tests in test_logistic.py. This is a focused change spanning about 10\u201315 lines across two files and should take an experienced engineer under an hour to implement and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14629": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies a bug in cross_val_predict(method='predict_proba') when used with MultiOutputClassifier. It points to the exact lines in validation.py (lines 857\u2013866) where estimator.classes_ is accessed incorrectly, and explains that for multioutput you need mo_clf.estimators_[i].classes_. The user proposes a precise patch location in sklearn/multioutput.py and provides a test to validate classes_. There is no ambiguity about what the change should do or where to implement it.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can understand the bug by reading the cross_val_predict code and the MultiOutputClassifier class, then add a few lines in fit() to set self.classes_ correctly. Writing the small patch and corresponding test is straightforward and takes under an hour once familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14710": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies a specific bug in BaseHistGradientBoosting (gradient_boosting.py) when using string labels and early stopping: the scorer receives integer y_true but string y_pred, causing a type mismatch. The report points to the exact function (_check_early_stopping_scorer), shows sample code, and proposes a minimal patch (using classes_ to re-encode y_true). The file paths, class names, and lines to change are explicit, making the requirements unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves understanding how early stopping scoring works in HistGradientBoostingClassifier, locating two spots in a single file, and adding conditional encoding using classes_. The change is small but requires familiarity with the scorer interface and test suite. An experienced engineer could implement and validate this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified. The problem is localized, tests are provided, and there are no external dependencies or unclear requirements.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-14894": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that in sklearn/svm/base.py in the function _sparse_fit, when support_vectors_ is empty (n_SV=0), the computation of dual_coef_indptr with a step of size/n_class causes a ZeroDivisionError. It identifies the file (sklearn/svm/base.py), the function (_sparse_fit), and the error (ZeroDivisionError) under a specific condition (empty support_vectors_), making the required guard trivial to interpret.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires adding a simple if-check around the existing CSR matrix construction in the same file and adding a minimal regression test. It\u2019s a localized change (<15 lines) and straightforward to implement once the error condition is understood.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-25747": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"While the issue title and description clearly state that FeatureUnion fails when using a pandas transform output with aggregation, the exact error and root cause (index being reset) are not shown. However, a reasonable interpretation is that the helper function _wrap_in_pandas_container should preserve existing DataFrame index rather than override it. With this assumption one can infer what to change to satisfy the user\u2019s intent.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer could locate the _wrap_in_pandas_container helper, understand that it resets the index for DataFrames, and adjust a two-line conditional check. They would then add or update tests to verify index preservation. This scope fits comfortably into a 15\u201360 minute task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues prevent using this sample in the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-25931": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes when and why the warning is raised: fitting an IsolationForest with a DataFrame and non-default contamination triggers an unintended predict() call on training data, stripping feature names. It points to the exact code path in _iforest.py and explains the root cause. This provides enough information for an engineer to locate the logic in fit(), understand that score_samples() invokes validation and drops feature names, and to implement a fix calling a private method to preserve names.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer can locate the warning source in fit(), recognize that input validation in score_samples() drops feature names, and switch to a private scoring method. This involves editing a few lines and adding a helper method plus a test, all achievable within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "scikit-learn__scikit-learn-25973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly states that SequentialFeatureSelector.cv should accept an iterable of splits (e.g. the output of cross-validator.split) but currently fails. It specifies the expected behavior and the misuse scenario. While it does not include the exact error message or code snippet demonstrating the failure, an experienced engineer can infer that the implementation must call check_cv on the cv parameter and propagate the iterable into cross_val_score. The gap in explicit error details constitutes a small blank, but the requirement is nonetheless unambiguous: ensure cv accepts split iterators.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires locating the _sequential.py implementation of fit and _get_best_new_feature_score, importing and invoking check_cv, updating method signatures and calls, and adding or extending a unit test. This touches multiple lines across implementation and tests but follows established patterns already used elsewhere in the codebase. An experienced engineer could complete this within a few hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"None.\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-10323": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that when using the literalinclude directive with prepend or append options, the leading whitespace is removed due to the ordering of filters. It specifies the affected file (sphinx/directives/code.py) and the exact behavior that prepend should preserve indentation. From this description, it is clear that the fix involves reordering the dedent_filter relative to prepend_filter and append_filter in the filter list so that dedent does not strip whitespace from prepended lines.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change touches one function and requires understanding the filter pipeline in Sphinx. Reordering a list entry and adding a filter call is straightforward once you locate the filter list in sphinx/directives/code.py; writing or updating tests is minimal. Overall, this should take an experienced engineer well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-10673": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that Sphinx\u2019s toctree rejects references to generated documents (genindex, modindex, search) because they aren\u2019t in env.found_docs. The user shows the failing directive, desired directive, and provides concrete tests in tests/roots/test-toctree-index/* and test_environment_toctree.py. The solution patches are explicit\u2014e.g. in sphinx/directives/other.py\u2019s parse_content add generated_docnames = frozenset(env.domains['std'].initial_data['labels'].keys()) and OR it with found_docs, skip generated entries later. Similar changes in toctree adapter (resolve) and collector. The behavior and patch locations are unambiguous and fully specified.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Applying this fix requires understanding three modules in Sphinx (directives/other.py, environment/adapters/toctree.py, environment/collectors/toctree.py), locating the correct sets, OR-ing in generated_docnames, writing a small test root and assertions. A proficient engineer familiar with Sphinx internals could implement, test, and validate this in ~1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues; the sample is self contained and uses existing Sphinx testing patterns.\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-7440": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue title and description are very minimal. It only states \u201cglossary duplicate term with a different case\u201d and provides a link to the relevant rst file plus environment metadata. It does not illustrate how the duplicate term bug manifests in the rendered output or any error logs, nor does it show sample input and expected behavior. An engineer must explore the glossary domain implementation to infer where terms are normalized to lowercase and how duplicate detection works. This lack of reproduction steps or concrete examples leaves room for ambiguity about what exactly needs to be changed before a proper fix can be attempted.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Once the requirements are clear, the fix touches only two methods in sphinx/domains/std.py to remove lowercase normalization and adjusts a few assertions in the existing test file. This localized change spans fewer than ten lines of code and leverages the existing test framework, so an engineer familiar with the codebase can complete it in 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Although the code change is straightforward, an engineer will need to have Sphinx\u2019s internals knowledge and set up its test harness. This adds a small initial overhead but does not fundamentally block using this issue in the benchmark scenario.\",\"q2_5_confidence\":3}"
    },
    {
        "sphinx-doc__sphinx-7462": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that when Sphinx processes an empty tuple AST node during type annotation parsing, the existing unparse() logic attempts to pop the last element from an empty list, triggering an IndexError. It provides the exact context\u2014Tuple[()] annotation\u2014and lists the functions (sphinx/domains/python.py:unparse and sphinx/pycode/ast.py:unparse) where the change is needed. The failure mode is explicitly described, and a minimal reproduction (empty tuple in annotation) is given. This gives sufficient information to implement a branch to handle node.elts == [].\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Locating the unparse implementations in two modules and adding a simple conditional for empty tuple nodes is straightforward. The fix involves only a few lines in each function, guided by the clear bug description and test expectations. An experienced engineer would need 15\u201360 minutes to verify behavior, add the conditional checks and update tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-7985": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue text only states that linkcheck doesn\u2019t check local/internal links and that this feature would be useful, but provides no detail on what qualifies as a local link, how the check should behave (e.g., treat existing vs missing files), or where in the code to make changes. It\u2019s unclear exactly what behavior is desired without examining external context or tests.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing this fix requires understanding Sphinx\u2019s linkcheck builder logic, adding URI scheme detection, filesystem checks, updating ignore logic, and adjusting tests. It\u2019s more than a trivial tweak but modest in scope, likely taking 1\u20134 hours for an experienced engineer to navigate the codebase, implement, and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues; apart from the vague description, the sample is straightforward for benchmarking once clarified.\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-8269": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that linkcheck always reports an anchor-not-found error when linkcheck_anchors=True, even if the HTTP response status is an error (e.g., 404 or 500). It specifies that the expected behavior is to surface HTTP errors instead of masking them as missing anchors. The precise file and location are indicated: add response.raise_for_status() before the anchor check in sphinx/builders/linkcheck.py in the check_uri() function.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this requires adding one line (response.raise_for_status()) in the check_uri function and creating a small pytest that spins up an HTTP server returning 500 to validate the behavior. This is a straightforward change and test addition, doable in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies the misbehavior of Sphinx\u2019s implicit xrefs via the :type: and :rtype: info fields, contrasts it with explicit xref roles, and states that unqualified names should resolve within the current module and its parents (mod.submod.A). The description names the domain code (python.py) and docfields.py where lookups occur, specifies expected resolution and warnings to remove, and gives precise Sphinx versions tested. This is sufficient to locate relevant functions (make_xref, transform) and implement the patch without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An engineer needs to familiarize themselves with Sphinx\u2019s domain infrastructure, locate the make_xref method in sphinx/domains/python.py and the transform method in sphinx/util/docfields.py, understand how env.ref_context is propagated, and write and test a small patch across two files. While the change is concise, understanding the cross-reference context and validating with the existing test framework would reasonably take 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is self-contained and provides both code and test modifications, making it suitable for the benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "sphinx-doc__sphinx-9711": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes that the current check performs string comparisons on version numbers (e.g., '0.6' > '0.10' stringwise) and explains the desired behavior (use proper version comparison so 0.10.0 is accepted when 0.6 is the minimum). It even names the function (verify_needs_extensions in sphinx/extension.py) where the change is needed and provides concrete examples for expected behavior. There is no ambiguity about what needs to be implemented.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small localized change to one function in sphinx/extension.py. An engineer needs to import packaging.version.Version (and InvalidVersion), wrap the existing comparison in Version() calls with a fallback, and add a few lines of logic. Adding corresponding tests is also straightforward. All of this can be done within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues; the issue is self-contained, testable with the provided tests, and suitable for use in a coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-13372": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description reports an UnboundLocalError in evalf after reordering Mul arguments but omits any repro steps, call context, or stack trace. It merely suggests adding else branches raising NotImplementedError without explaining how the bug arises or where exactly, leaving ambiguity about the root cause and scope of the fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is minimal: two else clauses raising NotImplementedError inside evalf. An experienced engineer can locate the existing reprec/imprec logic and insert these lines in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-13480": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue text simply states that .subs on coth(log(tan(x))) fails for certain integer values, listing some examples. It provides no error message, no description of expected versus actual behavior, and no context of where .subs is invoked, which function or module is affected. A developer would be left guessing whether this is a substitution problem, a parsing issue, or a mathematical simplification bug, and would have to explore the sympy codebase to locate the relevant functions, adding significant ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves locating the typo in the coth evaluation function (one line change) and updating/adding tests. An experienced engineer familiar with sympy\u2019s code structure can identify and correct this in under an hour.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This sample is heavily dependent on domain-specific knowledge of symbolic mathematics and the internal implementation patterns of the sympy library. A candidate without experience in computer algebra systems or familiarity with sympy\u2019s eval dispatch mechanism may struggle to identify the source of the bug and to write an appropriate patch. This specialized context limits the general applicability of the sample for evaluating broad coding ability.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-13877": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue text only poses a question about the validity of the Bareiss algorithm for non-integer matrices and does not describe any concrete failure mode, error messages, or the expected behavior. There is no reproducible example or clear spec of what the algorithm should do in the symbolic case, so an engineer would be guessing the requirements and scope of the fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing a robust fix requires understanding Sympy\u2019s determinant implementation, modifying the pivot\u2010selection logic, adding a new zero\u2010test helper, and updating tests across multiple files. This spans different modules and involves nontrivial mathematics, making it a 1\u20134 hour task for someone familiar with the codebase.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"This issue sample relies heavily on domain knowledge of symbolic algebra and the internal structure of Sympy\u2019s matrix determinant routines. It may not fairly evaluate a candidate\u2019s general coding ability, since success depends on specialized mathematical background and familiarity with Sympy internals.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-16792": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly states that when array arguments are passed to the Cython autowrap code generator but are not used in the final expression, they are treated as scalars rather than pointers. It describes both the observed behavior and the expected behavior (array arguments should always appear as pointers), points to the location in codegen, and even provides a concrete example for reproduction. A developer familiar with the codebase could directly locate the codegen routine, identify where argument metadata is built, and add logic to attach pointer semantics for unused arrays.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this involves understanding the sympy codegen internals, modifying two code paths in the CCodeGen routine, writing a small helper for dimensions, and adding a corresponding test. An experienced engineer would need a couple of hours to trace symbol handling, implement the metadata change, and validate with unit tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues. The issue description, code patch, and test patch together provide a self-contained reproduction and solution path, suitable for benchmarking.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17139": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description consists solely of the title \u201csimplify(cos(x)**I): Invalid comparison of complex I (fu.py)\u201d without any elaboration or examples of the erroneous behavior. There is no description of the failing output or error message, nor a clear statement of the expected behavior. While a developer familiar with Sympy\u2019s fu.py might infer that the exponent check should skip non-real exponents, the lack of context in the main text (no explanation of how cos(x)**I is currently mishandled) leaves room for ambiguity about what precisely to change. It would require the engineer to explore the code and test suite to fully understand the problem.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the code change is small (adding a two-line condition in fu.py), it requires understanding Sympy\u2019s function unification logic and the T/TR transformation functions in simplify/fu.py. An engineer would need a few hours to navigate the repo, locate the correct function, verify that rv.exp.is_real is the right check, and update or add tests accordingly.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is straightforward once the context is understood.\",\"q2_5_confidence\":3}"
    },
    {
        "sympy__sympy-17318": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue text \u201csqrtdenest raises IndexError. If an expression cannot be denested it should be returned unchanged.\u201d is extremely brief and contains no example inputs or concrete reproduction steps. It refers to an IndexError but doesn\u2019t show where or why it occurs, what kind of expressions trigger it, or how the existing code is structured. Without access to code or tests, an engineer cannot be sure whether to wrap calls in try/except, add specific guards, or modify particular functions. This leaves room for multiple interpretations of the required fix and makes it unclear what a successful solution entails.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Once the high-level requirement (\u201creturn the input unchanged on failure instead of throwing an IndexError\u201d) is clear, fixing it involves a small conditional or try/except around the existing denesting logic, plus updating a few lines in one function. An engineer familiar with the codebase could locate the offending code, implement the guard, and add a test in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17630": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description pinpoints that multiplying a BlockMatrix with ZeroMatrix blocks once succeeds but fails on a second multiplication because intermediate zeros are represented as Zero rather than ZeroMatrix. It states the symptom, the context (SymPy version, Python version), and implies the fix requires ensuring zero-block results remain ZeroMatrix. However, it does not specify exactly where in matexpr.py the change should occur or reference the _postprocessor function or block_collapse utility, requiring the engineer to locate the relevant code in sympy/matrices/expressions/matexpr.py.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"An experienced engineer would need to read the block matrix implementation, understand how zero blocks are handled in _postprocessor and block_collapse, and then apply a small two-line change plus add tests. This involves digging into SymPy internals, which should take on the order of 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues; the sample is self-contained and tests cover the fix.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-17655": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue statement clearly indicates that multiplying a Point by a number in reverse order currently fails and that both orders should yield the same result. With full access to the geometry.Point implementation in point.py, an engineer can identify the missing __rmul__ method, implement it by delegating to __mul__, and add tests to verify reverse multiplication, so there is no ambiguity.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"Fixing this requires adding a simple __rmul__ method in point.py and updating test_point.py with a couple of assertions. For an engineer familiar with the codebase, locating the Point class and making these minimal changes should take under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected. The sample is straightforward and self-contained.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-18211": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly explains that when solveset (specifically the _eval_as_set() method in sympy/core/relational.py) encounters an unsolvable equation or inequality and raises NotImplementedError, it should instead return a ConditionSet over the Reals. It even provides an example of the expected output: ConditionSet(n, Eq(n*cos(n) - 3*sin(n), 0), Reals). The description names the relevant method (_eval_as_set), the exception to catch (NotImplementedError), and the replacement behavior (using ConditionSet). This makes it straightforward to locate the code, insert a try/except, import the ConditionSet class, and return it. No further clarification is needed to understand what change is required or where to apply it.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this issue involves modifying a single method in relational.py: importing ConditionSet, wrapping the call to solve_univariate_inequality in a try/except, and returning ConditionSet when NotImplementedError is caught. Adding the corresponding test in test_relational.py is similarly straightforward. An engineer familiar with the codebase could implement and validate this change in under an hour, making this a small, focused update that requires some thought but no extensive research.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-20428": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly identifies the symptoms (clear_denoms producing a Poly printing as zero yet reporting is_zero=False and causing subsequent failures) and pinpoints the root cause (an unstripped leading zero in the DMP coefficient list in the EX domain). It even names the relevant domain (EX) and behavior to correct (strip leading zeros or use f.ex.is_zero). With access to the codebase, an engineer can locate the __bool__ method in sympy/polys/domains/expressiondomain.py or relevant DMP construction, and implement the change. Thus the specification is precise and actionable.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the required code change is small, solving this issue demands understanding of sympy\u2019s internal polynomial domain (EX), DMP representation, and the clear_denoms routine. An engineer must trace the behavior from clear_denoms through Poly construction, find the __bool__ override in expressiondomain.py, and adjust the logic, then run and possibly extend tests. This research and validation would take 1-4 hours for someone experienced.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-20438": "{\"q1_1_is_well_specified\":3,\"q1_2_explanation\":\"The issue description only provides the title \u201cis_subset gives wrong results\u201d and nothing more\u2014no examples, no explanation of what \u2018wrong results\u2019 are, and no context on inputs or expected behavior. It is impossible to know what needs fixing from the text alone.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing the fix requires understanding Sympy\u2019s multiple\u2010dispatch mechanism, locating the appropriate handler for subsets, and adding a small dispatch for ProductSet. The change spans a few related files and includes adding corresponding tests\u2014a developer familiar with the codebase could complete this in 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-20590": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly identifies that Symbol instances unexpectedly have a __dict__ since version 1.7 due to a missing __slots__ declaration in a parent class. By inspecting the file sympy/core/_print_helpers.py and locating the Printable mixin, one can add the line __slots__ = () at the top of that class. The accompanying test file sympy/core/tests/test_basic.py is then updated to include test_immutable verifying that hasattr returns False and setting an attribute raises AttributeError. While the class name and file path must be found by exploring the codebase, the intended fix is unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"An experienced engineer familiar with the codebase can locate the Printable class in sympy/core/_print_helpers.py, add a single __slots__ declaration, and update one test file. This straightforward change and testing would take well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-21379": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies the unexpected PolynomialError when using subs() on expressions involving Piecewise within exp(sinh(...)/z) and related functions. It specifies the Sympy version (1.8.dev), the exact conditions (cosh/tanh vs sinh, division by symbol, outer exp, real symbol assumptions) and the location in code (sympy/core/mod.py in the eval method of Mod). The test patch further clarifies the intended behavior (no exception on expr.subs). Thus, it is straightforward to pinpoint where to add a try/except around gcd(p,q) and adapt the tests accordingly.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"A developer needs to locate the eval method of Mod in sympy/core/mod.py, wrap the gcd computation in a try/except for PolynomialError, adjust imports, and add a few assertions to existing tests. Understanding the failure requires reading the issue description and the test patch, but the code change itself is small and localized, fitting within 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-22714": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states that invoking sp.S on a Point2D object within an evaluate(False) context erroneously raises an \u201cImaginary coordinates are not permitted.\u201d error. It contrasts this with the correct behavior when evaluate(False) is not set or passed directly to sp.S. The scope and expected behavior are unambiguous, indicating precisely where the bug occurs and what needs to be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is a small modification to a single conditional in geometry/point.py, updating how imaginary parts are detected. It also adds a brief test case. An engineer familiar with the codebase could implement and verify this fix in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-23824": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text clearly identifies the function kahane_simplify(), describes precisely how contracted versus uncontracted gamma matrices should behave, and gives concrete counterexamples (both cases with reversed orders). It pinpoints the code section where free matrices are removed and reinserted backwards, naming the loop inversion as the root cause. The expected behavior, location in gamma_matrices.py, and desired fix are all explicitly described, so an engineer can confidently locate and correct the backward insertion loop.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Fixing this bug involves locating the kahane_simplify function in sympy/physics/hep/gamma_matrices.py, understanding the free_pos insertion logic, and replacing the backward loop with a forward concatenation. Writing the fix and adding two simple test assertions takes moderate thought but is straightforward, fitting within a 15\u201360 minute window for an experienced engineer familiar with Python.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Although there are no other blockers, this sample relies on specialized knowledge of gamma matrix algebra and the Sympy physics.hep module. Engineers unfamiliar with this domain might need extra time to interpret mathematical notation and locate the relevant identities, which could affect fairness in a general coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-23950": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly identifies that the method Contains.as_set currently raises NotImplementedError but should return the set argument passed to Contains. The class name (Contains) and method (as_set) are explicitly mentioned, the expected behavior is unambiguous (return self.args[1]), and the provided test patch shows exactly how the output should behave. There is no missing context, and an experienced engineer can implement the one-line fix without guessing any additional requirements.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a trivial fix: replacing a NotImplementedError with a single-line return statement in one method and updating/adding three simple test assertions. An engineer familiar with the codebase can implement and verify this change in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional notes.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24066": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description consists only of a title stating that SI._collect_factor_and_dimension() cannot properly detect that exponent is dimensionless, with no further examples or context. It does not describe how the function works, what inputs or outputs look like, or how the failure manifests. Without inspecting the codebase or tests, it is unclear how to address dimensionless exponents within the existing logic, making this description vague and insufficient for a standalone implementation.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding the internal logic of the _collect_factor_and_dimension function, locating how Function instances are processed, and integrating an is_dimensionless check to correctly treat exponent arguments. One must read and adapt existing code in unitsystem.py and validate behavior using tests. For an experienced engineer familiar with Sympy\u2019s units, this would take around 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-24443": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text identifies the specific function (`_image`) and the failure case (inverted generators causing `in gens` to fail), and points to the exact lines in `homomorphisms.py`. However, it does not describe how the data structures (`r.array_form`, generator symbols) work or how the new simplified algorithm should behave. An engineer must inspect the internal representation of `PermutationGroup` and devise a new mapping strategy, filling in design details not provided in the text.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this requires understanding SymPy\u2019s `PermutationGroup` internals, rewriting about 30\u201340 lines in `_image`, introducing a symbol-to-generator mapping, and updating tests. An engineer familiar with the codebase would need to spend time reading class methods, writing the new loop, and validating behavior, which is moderate complexity taking 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"\",\"q2_5_confidence\":4}"
    }
]