[
    {
        "astropy__astropy-13033": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a precise and reproducible example, including code to generate a TimeSeries object and remove a required column, as well as a clear explanation of the misleading exception. The expected and actual behaviors are laid out in detail, making it obvious that the required fix is to emit a more informative error message when required columns are missing. The relevant context about required columns, the test scenario, and the expected user experience are all present. A knowledgeable engineer would know exactly how to approach implementing the solution and testing its correctness. No significant ambiguities remain.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change requires identifying where the exception is raised in the code when required columns are removed and enhancing the error message for clarity. This is a small but non-trivial change, mostly confined to modifying error message logic and adjusting tests for the new output. It is a targeted improvement, does not require changes to the data model or API, and can be implemented quickly once the codebase location is determined. For someone knowledgeable with the project, this would likely take between 15 minutes and one hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues preventing this sample from being used in a coding benchmark. The instructions, context, and expected behavior are clear. The provided test is self-contained and covers the fixed scenario. The required reasoning is representative of real-world software maintenance.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-13977": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified, although some context or familiarity with numpy and astropy's ufunc and __array_ufunc__ mechanisms is helpful. The problem statement includes example code illustrating the problem (the DuckArray wrapper and how different addition operations behave). The desired behavioral change is clearly specified: namely, that Quantity.__array_ufunc__ should return NotImplemented instead of raising a ValueError when it encounters an operand it can't handle. The rationale is also explained (allowing __radd__ to be called and aligning with numpy's documented convention). While the precise way to identify the correct failure cases for returning NotImplemented requires an understanding of the codebase, the 'what' is clear: change error-raising to NotImplemented where appropriate. This leaves the exact implementation ('how') to the engineer, but supplies enough detail to attempt a solution and write tests. Overall, it's well-specified but not absolutely 100% fully explicit, hence I choose 1 rather than 0.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a clear understanding of Python's ufunc machinery, the interplay between numpy and custom types with __array_ufunc__, and the correct protocol for returning NotImplemented. The changes made touch non-trivial parts of the code (including updating the main __array_ufunc__ logic and test suite). The engineer must carefully catch the correct failure cases, avoid breaking other error handling, and ensure both new and existing behaviors are correct (including writing or updating appropriate tests, especially to confirm right-hand side operations and error reporting still work as expected). The code changes are moderately complex and require thoughtful design/testing, but do not require a large architectural rework or extended research, thus 1-4 hours is an appropriate estimate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major missing aspects in the sample: the problem is clear and solvable from the description, and the codebase is accessible. The issue focuses tightly on the relevant behavioral change, and enough test scenarios are presented in the description to guide implementation and validation. The main risk for the evaluator is a need for background knowledge of ufunc protocol behaviors, but this is inherent to the domain, not a lack of specification.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14096": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified. It shows clearly that a subclass of SkyCoord with a property accessing a nonexistent attribute results in a misleading AttributeError message (for 'prop' instead of 'random_attr'). The example code is concrete, and the undesired/desired behaviors are described. However, it relies on the reader's interpretation of the 'correct' error message, and it could be debated whether this is the right Python behavior for all situations. Overall, a reasonable, specific engineering interpretation is possible based only on the given text.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively small change, primarily a matter of subtle exception handling in Python, specifically related to attribute access. The fix is concise: changing one line in a method to allow the proper propagation of the AttributeError from __getattribute__ rather than raising one directly. An engineer familiar with Python\u2019s attribute resolution would be able to diagnose and fix this within 15 minutes to 1 hour; some debugging may be required to reproduce and test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues remain. The provided information is adequate and the engineering task is clear, with code and tests that are directly relevant and well-contained. The issue is not ambiguous. The only minor caveat is interpretation of what is the 'correct' error to raise, but the test and description are clear.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14182": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite well-specified and provides explicit details of the required functionality and the current problem. It describes the desire for the 'header_rows' argument to be available and respected in 'ascii.rst' output, similar to other fixed-width formats like 'ascii.fixed_width.' The issue includes specific Python code snippets demonstrating both the current behavior (which fails) and the expected behavior. It also references the relevant function (QTable.write) and discusses the symptom (TypeError), so the required change is clear to an experienced engineer familiar with astropy. The solution space is clearly constrained: adding 'header_rows' support for RST-format output, matching what is already done elsewhere, and ensuring tables can be written with the extra requested header rows.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Implementing this change requires familiarity with the Astropy table I/O formats and the way headers are handled in their ASCII output writers. It involves modifying the RST writer class to accept and correctly process the 'header_rows' argument, likely by mirroring logic from the existing fixed-width writer classes (as evidenced by the patch). The difficulty mostly lies in understanding existing code structure and ensuring consistency rather than algorithmic complexity. For an engineer with a bit of context on Astropy's ASCII I/O, this is a straightforward change that would take 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There appear to be no additional major concerns with this issue sample. The requirements are clear, there are specific expected behaviors, concrete examples are given, and the testing requirements can be uniquely tied back to the initial problem statement. Edge cases or downstream dependencies are not indicated in the issue, but that does not negatively affect suitability for a benchmark of coding ability as described. Nothing about this sample would render it inappropriate for such use.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14309": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a specific error scenario (IndexError), describes the root cause (attempting to access args[0] when args is empty), points to the relevant function (is_fits) and explains the behavior change that caused the issue. It gives a clear minimal reproduction example, allowing an engineer to see how the error occurs. However, while the failing scenario is clear, the issue text does not explicitly specify what the expected behavior should be (e.g., should it return False, None, or raise a different error?), nor does it describe what 'fix' would be considered correct for all possible cases. An experienced engineer will sensibly deduce the expected outcome (likely avoiding the IndexError and handling the empty tuple gracefully), but there is a small interpretation gap.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The diff provided shows that the fix is relatively small (changing a few lines in a single function to avoid accessing args[0] when args is empty). The core of the issue\u2014preventing an IndexError by properly checking arguments before use\u2014is a standard defensive programming practice. The engineer must read the function, understand why the empty tuple case falls through, and decide on the correct logic, which should be straightforward and quick for someone familiar with Python. Time required would be 15-60 minutes, including finding the place, writing a fix, and a test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The test and gold patch confirm that the fix is small and the coverage sufficient. The sample is suitable for evaluating coding ability in this context. Only note is that it would be helpful if the issue description clarified what should be returned in the empty-args case (False/None), but context and the test patch make this reasonably deducible.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14365": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear in explaining both the problem and the expectations for the solution. It states that the parser currently requires uppercase QDP commands, but QDP format itself is case-insensitive and many files use lowercase commands. The expected behavior is given (the parser should not crash with lowercase commands and should read the file correctly), and concrete reproduction steps and an example file are provided. The error scenario is described with details, making it easy for an engineer to identify the source of the problem. No ambiguity exists about what needs to be accomplished.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix mainly involves changing how command parsing is handled, likely updating regex or string comparison logic to be case-insensitive for QDP commands. The problem is clearly localized within the parser's command-matching logic, and reproducible test cases are provided. For someone familiar with the (astropy.io.ascii) codebase, this would require reading through the parser's command-detection logic and adjusting the relevant function(s), which would generally take less than an hour. Some additional thought may be required to ensure case-insensitive handling does not break existing behavior, but this is a focused change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The problem is well-specified, has clear test cases, and the fix is unlikely to have broad side effects on unrelated parts of the system. The issue is a good benchmark for coding ability, testing the engineer's reading comprehension and ability to find and change parsing logic appropriately.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14508": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. It clearly identifies the root cause (the string formatting of floats in io.fits.Card._format_float()), provides a concrete example (the representation of '0.009125'), describes both the observed and expected behaviors, and even specifies which line produces the problem. There is a clear 'how to reproduce' section with exact code, as well as precise expectations. The only minor uncertainty acknowledged by the author is the possibility of side-effects elsewhere in the codebase, but the requirements for solving the specific reported behavior are unambiguous. The engineer knows exactly what to change, where, and what the outcome should look like. The issue also provides environmental details for reproducibility. This is a textbook example of a high-quality, actionable bug report.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, precise change that requires a bit of thought but is not very complex. The engineer needs to alter the way float values are formatted as strings to avoid unnecessarily expanding the representation and thereby avoid comment truncation. Understanding the FITS header requirements and the 20-character restriction adds a minor layer of consideration, but everything needed is described in the issue. Only a single function needs to be modified, not the entire codebase. The core logic and impact are clear; it does not require a substantial rewrite or working across multiple files. It is a moderate '15 min - 1 hour' fix for someone familiar with Python string formatting and the code structure.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues are present. The sample is well-scoped for benchmarking coding ability. It requires understanding of string formatting and some domain-specific logic (FITS header formatting), but does not require external information or deep domain expertise. The provided test demonstrates how to validate that the fix works. The task and success criteria are both clear and reproducible. This is an excellent benchmarking case.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14995": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is generally well-specified. It clearly states the problem: when performing arithmetic with mask propagation in NDDataRef (astropy v5.3), if one operand is missing a mask, mask handling fails with a TypeError due to bitwise_or attempting to combine an integer with None. The user provides the expected behavior (mask should just be copied over, as in v5.2), steps to reproduce, and concrete code illustrating both failure and the expected outcome. The only blank is that the user isn't fully sure of the intended propagation logic and references 'whatever was done in v5.2'; however, the example code, context, and desired outcome make the required fix unambiguous for an experienced engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires modifying a conditional logic to handle the special case when one operand has a mask and the other does not, ensuring the mask is copied or handled gracefully instead of failing. It requires some inspection and testing, but is localized and conceptually straightforward\u2014about a 15min-1hr fix for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major concerns. The issue is specific and testable, and the provided code samples and version information make reproducing the bug and verifying the fix straightforward. This sample is suitable for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7336": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It states that the Astropy units.quantity_input decorator raises an exception when used on a constructor (__init__) that includes a return type hint of None (i.e., __init__(...) -> None), because the decorator tries to call 'to()' on None. The issue provides a minimal reproducible code sample, describes the specific error (AttributeError: 'NoneType' object has no attribute 'to'), mentions specific package versions, and offers a suggested direction for the fix (the decorator could explicitly check for None and skip the unit check). All these details would allow an experienced engineer to both reproduce the issue and confidently develop a targeted solution, given full access to the codebase. The affected decorator and function name are both provided, so locating the problem area in the code is straightforward.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue involves understanding how the units.quantity_input decorator operates, especially in how it handles return value annotations, and modifying the decorator to avoid calling 'to()' on None returns (which occurs when __init__ methods are annotated with -> None). The patch shows the change is a simple conditional on the return_annotation, specifically checking for both inspect.Signature.empty and None. It's a small, localized change that requires familiarizing oneself with how the decorator processes return annotations, but does not require deep changes or extensive refactoring. The scope is limited to a single logical fix, and likely takes less than an hour for an experienced contributor familiar with Python introspection and decorators.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major additional issues. The issue clearly states expected and undesired behaviors, provides a minimal test case, and there is no indication of ambiguity. The problem, scope, and required modification are clearly defined, and the context about usage and versions is helpful but does not complicate replicability.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7606": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. It presents a clear code example that currently raises a TypeError when comparing a Unit object with None via ==, where the expected behavior is that the comparison should return False instead. The expected behavior and the reason for the current failure are explicitly described. There is no ambiguity about what needs to be changed: the __eq__ method for Unit and/or UnrecognizedUnit needs to handle None gracefully, and not raise an exception. The minimal code sample illustrates both the undesirable behavior and the expected outcome, so an engineer has all necessary context to make a precise fix. No external context, filenames, or additional clarification is required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small bug fix that requires understanding (or looking up) how Python's __eq__ method works and knowing how to gracefully handle comparison to None (and, perhaps, other non-Unit types). Once the problem is understood, the implementation itself is straightforward and doesn't require any substantial changes to the codebase or significant research. However, it is a little more involved than a trivial fix, since it may entail understanding why TypeError is raised and making sure the comparison logic does not interfere with other equality semantics.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues detected. The issue is clear and the test addition in the provided patch shows it is possible to reliably test for regression. The sample is entirely suitable for evaluating coding ability in bug fixing and defensive method implementations.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7671": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified. It gives a concrete code sample that fails, specifies the error as a TypeError, clearly identifies the root cause in the use of LooseVersion in version comparison, and alludes to its replacement by pkg_resources.parse_version. There is also a relevant example of working and failing cases. However, it does not prescribe *exactly* what the correct behavior should be in all edge cases (e.g., should all dev/rc versions parse as higher or lower in all cases?), nor does it tell the developer what to do about the LooseVersion limitation\u2014should it patch LooseVersion, replace it, or just massage the version string? Nevertheless, a sensible interpretation is to ensure minversion does not throw TypeError with dev/rc suffixes, and to restore behavior similar to parse_version. Therefore, there are some blanks left for the engineer regarding precise solution strategy, but there is a logical problem statement.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required is localized to one function in a single file. The engineer only needs to massage the version string argument to handle non-numeric suffixes in accordance with the way the original behavior worked, or to mimic pkg_resources.parse_version comparison. No substantial rewriting or major cross-file changes are necessary. Some thought is required to come up with an appropriate regex or approach, but it's not a complex re-architecture or multi-hour debugging task. The provided solution is concise and reflects the actual amount of work needed\u2014mostly a fix involving regex matching and one additional import.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major concerns. The issue is concrete, testable, and the described symptoms map directly to a resolvable bug in the code. Test coverage is also clear. There are no external dependencies, security issues, or ambiguities that would undermine using this as an evaluation sample.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-10554": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is somewhat well specified. The description provides a concrete reproduction, including code snippets showing how to construct the problematic queryset using union and ordering, as well as the unexpected error and behavior. It describes the error message (ProgrammingError about ORDER BY position not in select list) and gives some context about why the bug occurs. However, it does not provide in-depth explanation about which part of Django's ORM is at fault or specify exactly what the correct output should be in all cases. Nevertheless, an experienced engineer familiar with Django ORM internals would be able to sensibly interpret the issue and the desired outcome: union querysets, when subsequently ordered or when orderings are cleared, should not produce SQL that refers to columns not in the select list and should maintain correct ORDER BY functionality.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue likely requires understanding the internals of Django's ORM, specifically how querysets, union operations, ordering, and value lists are compiled to SQL. Fixing the issue involves substantial reasoning about SQL query generation, ensuring selected columns match up with ORDER BY clauses in derived querysets after union operations, and updating code in core ORM compiler and query logic. Additionally, new helper methods are required to insert expressions in the select clause. The fix can be applied in under 4 hours by an experienced engineer familiar with the Django codebase, and requires edits to multiple core files, writing additional logic, and adding tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues that would disqualify this sample. The scenario is reasonably specific, and the test verification strategy is valid\u2014by checking that the ORM produces the correct query results and does not raise errors in the described situations.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11087": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified: it clearly describes the context (Django .delete() behavior in cascade deletions), the problem (extraneous fields are selected and cause a UnicodeDecodeError due to mismatched encoding handling between Python 2 and 3), and the ideal improvement (\\\"only required fields\\\" should be selected during deletions). The expected/actual behavior is explicitly described, with referenced code locations. However, there are some blanks for the engineer to fill, such as: which fields are \\\"required\\\" in these situations, the constraints around signal receivers, and how to integrate the solution into Django's ORM/Queryset mechanics. The description gives specific queries but doesn't fully spell out edge cases or the interplay with signals. Still, an experienced engineer would reasonably be able to scope and attempt the improvement from this description.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this issue requires understanding of Django's ORM internals, especially how cascade deletions work, how QuerySet.only() defers fields, and the DB backend's field selection logic. The solution involves changes to model deletion logic (django/db/models/deletion.py) and impacts fast/slow deletion paths and signals, requiring careful reasoning to not break semantics. Tests need to be updated/added to confirm correct low-level SQL and behaviors, taking into account select_related and signals edge cases. While the core code change is not huge, working out the correct, safe, backwards-compatible approach and validating it would likely take 1-4 hours for an experienced engineer new to this part of the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blocking issues not already covered in the assessment. The sample is quite technical and full context (codebase familiarity, Django ORM conventions) is assumed, which matches the intended benchmark for advanced Python engineers. No legal, appropriateness, or reproducibility concerns observed.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11265": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear description of the error encountered when using 'exclude' on a queryset with an annotated FilteredRelation in Django. It includes a concrete example, with a clear before/after diff in a test, the error message raised, and an analysis of where the code seems to fail ('split_exclude' function), as well as why. However, it expects some inference from the reader regarding the correct expected output (the set of objects returned), since not all background about the classes, setup, or dataset is fully specified (e.g., who 'author1' and 'author2' are, although this is more for general context). But overall, there is a sensible, clear interpretation of what the bug is, and what a successful solution should achieve: make exclude work correctly after using such annotation. Thus, it falls between a 0 and 1: not perfectly, rigorously specified, but well enough for a skilled engineer to proceed meaningfully.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding Django's ORM internals, especially how annotations with FilteredRelation and the exclude() operation interact at the query generation level. It involves modifying logic in core query construction code (such as split_exclude), correctly forwarding related information (filtered_relations), and potentially adjusting or defending against unwanted joins/trims. These changes are non-trivial, as they demand deep familiarity with both Django's ORM and SQL query construction to avoid introducing subtle bugs. The solution provided is fairly contained (only a few lines), but arriving at it and confidently verifying correctness requires a substantial investigation and careful study, likely spanning 1-4 hours for a qualified engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues detected. The only minor caveat is that success depends on understanding certain conventions in Django ORM and may require access to a real codebase and tests.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and detailed. It provides a clear context: the problem occurs when using Django's CheckConstraint with a combination of OR and AND clauses. It explains why the issue arises (fully qualified table/field names leaking into constraints), includes a precise code example (model and migration code), and shows the incorrect and correct SQL generated. The expected behavior is explicitly shown by comparing the generated SQL with the ideal version. An experienced engineer familiar with Django migrations and the ORM would have all information needed to reproduce, understand, and fix the bug from this description alone.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding how Django builds SQL for CheckConstraints, the distinction between Col and SimpleCol, and how the SQL rendering can inadvertently produce wrong table-qualified identifiers. The solution is not a simple one-liner and may involve adjusting how query trees translate to SQL. It also requires a decent understanding of Django internals, especially in the ORM and migration layers, making it more substantial than a quick fix but likely achievable in a few focused hours (1-4h) once the underlying code structure is reviewed.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be any major issues preventing this from being used as a benchmark sample. The issue is well-scoped and includes all necessary details for an engineer to attempt a fix. The scenario is somewhat advanced and targets developers with working knowledge of Django ORM and database schema constraints.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11532": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly states the context (handling of non-ASCII domains in email headers in Django), describes how and when the bug occurs (setting the hostname to a unicode value and using a non-unicode email encoding), and provides both steps to recreate the problem and a concrete expected outcome verified by a provided test snippet. It identifies the relevant modules (django.core.mail.message, django.core.mail.utils) and mentions the need to encode the domain using punycode before using it in headers. The description is detailed enough for an engineer to understand what needs to change and which components are affected, making it clear what constitutes a successful solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires touching multiple core areas of Django's email and validation utilities, introducing a utility function, and updating several places where domain names are handled. The solution involves understanding email standards, domain encoding (punycode/IDNA), and ensuring backward compatibility and correct handling in several edge cases. It also requires writing or updating tests. For an engineer familiar with Django, this is more than a quick fix: understanding, refactoring, and ensuring changes propagate safely across multiple files makes this a 1-4 hour task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being used to evaluate coding ability. The issue is described clearly, has a clear impact, an explicit test, and a correctly scoped technical expectation suitable for benchmarking solutions. It is both representative of real-world problems and well-contained for a coding assessment.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is detailed and well-specified. It describes the problem encountered when upgrading Django and how a valid list_display entry for a custom PositionField is now (incorrectly) rejected, causing admin.E108. It clearly references the relevant Django code (the check in django.admin.contrib.checks._check_list_display), the logic error introduced by a particular commit, and even provides the rationale for how the earlier logic worked versus the new one. The expected fix is described, with an informal reference implementation of what the correct logic should look like. For someone familiar with Django, the models, and the admin validation system, the requirements and the cause of the regression are both clear. The test case (list_display with a non-standard field like PositionField) is explicit. Filenames, functions, and context are all provided.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the underlying logic and its context require some understanding of Django's ModelAdmin validation, the actual change needed is limited to a single function. The main work is to re-order and adjust error handling in _check_list_display_item to restore the previous (correct) behavior when dealing with custom model fields accessible only on instances. The code to be altered is well-identified, the problem cause is known, and a pseudocode-like fix is offered. For a Django-proficient engineer, implementing this change and writing/adjusting the related test would take between 15 minutes and an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional blockers are present in the description. The sample is well-structured for benchmarking coding ability, especially for someone familiar with Django's admin checks. The related tests are conceptually straightforward and cover the regression scenario, so this is useable as a benchmark issue.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11734": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well specified: it describes a problem using OuterRef in exclude() and ~Q() expressions with Django queryset annotations and subqueries, leading to a ValueError. The issue includes a reproducible failing test case clearly illustrating the different situations (filter, exclude, ~Q) and explains the observed error. However, the root cause or exact desired fix isn't stated explicitly; the engineer will have to investigate Django's ORM behavior regarding subqueries and OuterRef, but the problematic pattern is sufficiently focused for a knowledgeable Django contributor to act upon. There are some blanks to fill in concerning which internal ORM modules or functions to update, but the requirements are clear enough for meaningful progress.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a solid understanding of Django ORM internals, specifically how subqueries and OuterRef references are resolved in the context of exclude() and inverted Q objects. The solution involves changes across several core ORM modules and fixing the interplay between query construction and expression resolution. The required changes are non-trivial, affect multiple files, and likely require a good deal of context-building and careful adjustment/testing to avoid breaking related functionality. This places it at a 1-4 hour difficulty for an experienced engineer familiar with Django's ORM.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues are apparent: the problem is well-motivated, the relevant code context and the usage scenario are made clear via a test case, and the scope is appropriate for a benchmark coding sample. It does require prior familiarity with advanced Django ORM usage, but that's in line with the intent of the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11740": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear problem scenario with code samples before and after the change: changing a UUIDField to a ForeignKey in Django and observing that the generated migration does not declare the dependency on the related app/model (App2 in testapp2). The reporter also describes the error message encountered (ValueError: Django cannot locate the related model in testapp2) and the expected solution (the migration should create a dependency for App2 when the FK is introduced). However, the user is unsure if this is a Django bug or a misunderstanding, and does not specify exactly where in the code the problem should be fixed or what the migration's dependency field should contain. An experienced engineer can reasonably infer the intent (making migrations handle FK dependencies correctly when changing fields), but some blanks are left to fill, like confirming whether the bug is in migration generation, in documentation, or in another layer. The problem is reasonably well-contained and interpretable, but not as precise as the best examples.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires understanding how Django's migration autodetector handles altered fields, especially the case when a UUIDField is changed to a ForeignKey, and why the dependency on the related model/app is missing. The solution involves editing migration planning logic and understanding the internal representation of dependencies. The corresponding test requirement is also non-trivial, as it must simulate a cross-app field change and verify the generated migration dependencies. This task goes beyond a quick tweak, requiring careful reasoning about internals and touching both core and test code. It is likely to take a few hours for someone not already deeply familiar with Django's migration framework.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample as stated is suitable for evaluating coding ability in the context of framework contributions requiring an understanding of migration/internals. There are no major issues that would prevent its use. If the codebase and test environment are available as in the actual setup, this is a solid benchmark for framework-level problem-solving and testing.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11815": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and clearly explains the problem: when an Enum object is used as a default value for a Django CharField, the migration serializes the value, not the Enum name, which leads to migration failures after translation (since translations change the value, not the name). The provided example demonstrates the faulty code and the expected output after the fix. The solution is clearly to change migration serialization so it uses Enum names (e.g., Status['GOOD']) instead of values. All necessary information is present (file affected: migration serialization, usage context: Django model fields using Enum, concrete example, proposed fix). No substantial open questions or ambiguities remain for an experienced developer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding Django migrations and how Enum values are serialized. The code change itself is relatively isolated (modifying the serialization logic for Enums), and the test updates are straightforward. It does not require a substantial rewrite or touch many files, just some careful adaptation of how serialization is handled. An experienced developer could make and verify this change within 15 mins to 1 hour, assuming some familiarity with Django's migration internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being used for the coding benchmark. The example is clear, concise, and realistic. The problem is well-motivated, and both the problem and expected outcome are detailed for reliable benchmarking.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11964": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear and includes: (1) a concrete code example of both model and enum definitions, (2) specific mention of the current (erroneous) and expected (desired) behavior, (3) actual test code and its result (the string representation of the enum value does not return the raw value, but something else), and (4) an explanation of why this matters in real use cases, such as external API communication. This clarity makes it obvious that the problem exists in the string representation of Choices instances, and that the correct solution would be to change the __str__ method so that str(x) returns x.value for Choices. The required change and its expected effect are both well-articulated. A competent engineer would find it unambiguous what is being asked.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue involves a small change to how a class (Choices) behaves: specifically, modifying its __str__ method to return the underlying value of the enum. The designer must understand Python enums and Django model field choices, but the code change itself is quite minimal, requiring the addition or override of a dunder method. Some brief investigation may be required to verify that this does not break existing uses, but for an experienced engineer this amounts to a focused, single-method update and can be done in less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a well-posed issue that, with the details provided, translates directly into a code-level fix that can be unambiguously tested. There are no substantial ambiguities or issues that would prevent its use as a benchmark for coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12155": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The description provides a concrete example of the problem (docstring where the first line is not empty), pinpoints the culprit code snippet (how indentation is determined in trim_docstring), clearly explains why this fails (first line is unindented), and proposes an explicit solution (skip the first line when determining indentation). This makes the requirements for a successful fix explicit and concrete: adjust the indentation calculation in trim_docstring so that docutils correctly renders docstrings that start with text on the first line. An experienced engineer within the Django codebase would be able to identify the location and nature of the required change without any ambiguity, and the desired outcome is unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a little thought to understand the context and why the current code fails for single-line-starting docstrings, but the problematic code is highlighted and a suggested fix is proposed. The amount of code to change is small (essentially modifying/changing a single line and verifying the result). The fix is straightforward but not totally trivial, as an engineer would want to make sure test coverage is sufficient for both the previous and new behaviors. The difficulty is in understanding the docstring trimming logic and its downstream effects, but the overall impact is limited in scope. For a Django-literate engineer, this would be a 15 min - 1 hour task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this issue from being used in a benchmark for coding ability. The requirements, problem, and solution are clear from the description. The sample exercises understanding of code readability and subtle bugs involving whitespace handling. The existence of relevant tests makes this even better as a benchmark candidate.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified. It provides concrete reproduction steps, including concise code samples that trigger the error, a description of the expected and observed behavior, and clearly identifies the specific error messages and cases where the implementation fails (i.e., keyword-only arguments in custom template tags in Django). The core technical context\u2014working with template tags and how keyword arguments (especially keyword-only arguments with defaults) are handled when passing them in templates\u2014is explained with sufficient detail for an engineer familiar with Django's template system. The scope of the work is clear: ensure template tags properly receive and validate keyword-only arguments with defaults, and report argument errors correctly. However, the issue could potentially be improved by explicitly describing the intended correct behavior more formally or specifying more exhaustive test cases, but for practical purposes it's clear enough for a sensible solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change requires familiarity with Django's template library internals, but only involves a fairly localized change (modifying the argument checking logic in the tag parsing function). The fix is logically simple and isolated, as indicated by the small patch size (~1-2 lines in the core logic), though it requires careful reading to identify the faulty check. Adding corresponding tests to cover more cases raises the effort slightly above 'trivial', but an experienced engineer can realistically complete this in well under an hour once familiarized with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues present. The problem is well-defined, the patch and tests are sensible, and no unusual dependencies or constraints are mentioned. The only possible minor concern is that understanding the bug may require some familiarity with Python's function signatures, template tag registration, and argument parsing details, but these are not out of scope for most Python/Django developers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12325": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it describes an error encountered during Django model inheritance with multiple OneToOneField references, and outlines the difference in behavior depending on the order of field declarations. The error message is given, and the textual example demonstrates both the problematic and working cases. However, the explanation is a bit informal and could be misread: it lacks explicit expected behavior and technical details for newcomers to this part of Django's ORM system. The engineer needs to infer that the challenge is with how Django sets up parent links between models, and that the fix likely involves ensuring the correct OneToOneField is recognized as the parent regardless of order. The core scenario is presented, but some Django ORM background is assumed.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires understanding of Django's model inheritance and meta-classes, specifically how the ORM identifies parent links on model classes with multiple OneToOneField declarations. The solution involves nontrivial reasoning: editing Django's internals to check for parent_link=True instead of relying on field order. This may not take a long time for an engineer familiar with Django internals but does require careful thought, testing, and understanding of the meta-programming involved. The patch touches more than one file and the testing changes are significant, but the diff itself is not enormous. Estimated effort: 1-4 hours for an experienced Django developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues stand out. The sample is suitable for evaluating coding ability in software engineers familiar with Django model internals and meta-classes. The scenario is realistic, and while it may challenge those unfamiliar with this part of the codebase, it is a meaningful and well-constrained software engineering problem.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12663": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well specified. The problem is described clearly with an example, including code samples for models and a testcase that reproduces the problem. The error is explained (TypeError when using SimpleLazyObject in a subquery filter because it cannot be converted to int). What is required for a solution is implicitly clear: make Django's ORM handle a SimpleLazyObject in this subquery context where a User (or similar object) is being compared. However, there are some blanks to fill in:\\n- The desired behavior could be further clarified (e.g., SimpleLazyObject should be unwrapped transparently?) and edge cases explored.\\n- A real engineer would need to ensure generality: Does the fix apply only to User, to all ForeignKeys, to other field types?\\n- The testcase is labeled 'arbitrary' so some interpretation is required about the breadth of the solution.\\nNevertheless, it's clear enough and there is a sensible way to proceed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change is localized (involves understanding the ORM's handling of annotated subquery results and how objects like SimpleLazyObject are compared/converted). Most experienced Django developers could follow the issue, triangulate where evaluation/conversion happens, and patch as needed within an hour. The solution is not trivial, but not extensive\u2014there's some debugging/thinking needed but it doesn't require a rewrite or a research phase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues; test coverage is provided, the scenario is realistic and relevant, and the core required fix is clear enough for benchmarking coding ability in an ORM/Python context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12708": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description includes a clear bug report about Django migrations crashing when trying to delete an index_together if there is a unique_together on the same fields. The reproduction steps are provided (create a model, add both unique_together and index_together to the same fields, delete index_together, and see the migration crash). The error (ValueError due to confusion between unique and index constraints) is described, and the high-level expectation is outlined: that deleting index_together should not confuse or delete the unique_together constraint, and migration logic should distinguish them. However, the issue also lists two possibly separate problems\u2014one about index_together/unique_together interaction, another about refactoring indexes using the new Options.indexes feature. The ask isn't fully precise on the desired behavior in all edge cases (e.g., what should happen in partially overlapping scenarios, preferred migration output), so there is a bit of interpretation left for the implementer. Still, a sensible solution can be derived.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix involves understanding Django's migration and schema editing logic, specifically how composable indexes and unique constraints are distinguished and deleted. Although the code and test patches show the fix is not very large, identifying and safely editing the constraint deletion logic to handle this corner case correctly, as well as writing a meaningful regression test, requires reasoning about Django's ORM, database backends, and migration operations. This would likely take 1-4 hours even for a reasonably experienced Django contributor due to required context and validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"While the bug report mentions a secondary issue regarding refactoring indexes using the new Options.indexes feature, the bulk of the provided patch and tests focuses on the main problem: index_together deletions conflicting with unique_together. As long as the benchmark focuses on this primary interaction, this issue is suitable. The ambiguity over the secondary issue might make the task less ideal, but if scope is communicated to focus on the index_together/unique_together migration bug, it is acceptable. No other major issues identified.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12754": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is generally clear and provides both an example and a summary of the root cause, focusing on migration order when moving a field to a model subclass in Django. It specifies the incorrect migration ordering, the error encountered, and mentions the proper order to resolve the issue. However, it refers to \\\"#21890\\\" (another issue) for context, and omits some details on variations of the scenario or edge cases that might arise in more complex model inheritance structures. The main scenario is nevertheless concrete and actionable, so an experienced developer could reasonably interpret and implement the fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires understanding Django's migrations autodetector, how dependencies are resolved in model state transitions, and the specifics of model inheritance. Diagnosing the underlying problem (conflicting field names through inheritance and migration operation ordering) and ensuring the fix does not affect other migration scenarios requires a good grasp of Django's internals. Implementing the correct dependency ordering is non-trivial and requires care. The required code changes, along with suitable test coverage, likely fall within a 1-4 hour effort for an experienced Django developer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The example and the error context are clear, and the scenario is realistic for evaluating coding ability in a Django context. The issue's reference to another ticket for analogy is a minor drawback but not a blocker, since the main scenario is described in detail. The sample is suitable for a benchmark as is.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12858": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified, describing a SystemCheckError in Django caused by an ordering attribute on a model using a lookup (isnull) across relationships, which currently fails validation despite working at runtime. The description specifies the model field relationships (Stock, Supply, Product, parent) and gives concrete, reproducible code snippets showing the valid usage. However, the core ask\u2014what exactly should be allowed or changed in checks validation\u2014must be inferred based on the discrepancy between the runtime behavior and the system check. The reporter's belief that this was broken by a specific prior change (#29408) adds some context but does not spell out the precise expected behavior, requiring some interpretation by the engineer. The context is strong but not fully exhaustive, hence a score of '1'.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core of the issue involves adjusting Django's model system checks to account for orderings that use lookups as well as transforms. The required fix demands understanding the difference between lookups and transforms on model fields within Django's ORM, locating the system check code for ordering in the codebase, and modifying the logic to recognize valid lookups. The patch is a small logic change, though it requires understanding the context and implications. For an engineer familiar with Django internals, this would require some thought and code review, amounting to roughly 15 minutes to 1 hour of work.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues prevent this from being a suitable benchmark sample. The problem is clear, targeted, and applicable to core model validation logic, making it a solid candidate for evaluating coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13012": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It explains the exact problem, including code samples that produce incorrect SQL due to wrapping a constant expression with ExpressionWrapper. It also contrasts this with the correct behavior when ExpressionWrapper is not used. The essential technical problem is clearly described: wrapping a constant value causes it to appear in the GROUP BY clause, leading to a PostgreSQL error. Based on this, an experienced engineer would know the solution requires ExpressionWrapper to avoid adding constants to GROUP BY. The relevant operations, class names, and expected behaviors are present. No critical context is missing for formulating a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a focused bug in the ORM's SQL generation logic. Once the issue is understood, the main challenge is to ensure ExpressionWrapper's group-by behavior delegates to its underlying expression, matching the unwrapped behavior. The code change is conceptually simple and limited to a single method override. Familiarity with Django's expression API is useful but not deeply required, so it should take on the order of 15-60 minutes for an experienced contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues with using this sample. It is self-contained, specifies the bug and desired behavior, and includes concrete examples. The solution is not trivial but also not extensive. It tests the candidate's understanding of how ORM abstractions connect to generated SQL and will surface relevant misunderstandings.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13028": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a reproducible scenario (the provided model code), a clear statement of the error encountered (NotSupportedError is raised in Django when filtering on a ForeignKey to a model whose field is named 'filterable' and False), and an observed workaround (renaming 'label' to 'filterable_test' resolves the problem). However, there are some blanks remaining: the exact mechanism by which the 'filterable' attribute interacts with Django's filtering system is left to the reader to infer from the code and context, and no specific direction is given as to whether to change documentation or the underlying behavior (\u201cshould be documented or fix\u201d implies a choice). Nonetheless, the minimal reproducible example and error make it reasonably clear what code is involved and the core of the problem.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires tracking down why the presence of a 'filterable' field on a ForeignKey'ed model interferes with the ORM's filtering, then adjusting the place where Django checks the 'filterable' attribute to avoid raising NotSupportedError incorrectly. This requires some familiarity with Django's ORM internals but the actual code change is not complex\u2014it involves a conditional check and a minor test update. An experienced Django engineer could find and fix this within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The only minor ambiguity is whether documentation should be updated instead of changing behavior, but the scenario strongly suggests that changing the ORM behavior is the preferred route. The reproducible example is complete and will work for benchmarking coding ability. No blockers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13112": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the problem: when using a mixed-case app label in INSTALLED_APPS (e.g., 'DJ_RegLogin') but a lowercase reference elsewhere (e.g., 'dj_reglogin'), Django throws a ValueError during migration. The project provides relevant code excerpts (from models.py, settings.py, apps.py) and the error scenario, making it straightforward for an engineer to understand the mismatch issue. The required goal is clear: allow ForeignKey (and similar references) to be robust against letter-case differences in app labels. All necessary context and expected behavior (matching mixed/lowercase labels) is provided for a meaningful and directed solution. The engineer is given enough detail about the components/files involved to make a targeted fix in Django's foreign-key handling.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding Django's internal handling of model references, particularly how ForeignKey handles app and model string references when serializing/deserializing or during migrations. While the fix is not entirely trivial (it involves carefully updating how model labels are processed to normalize or preserve case sensitivity appropriately), it is localized to a small region of the codebase and is conceptually straightforward. An experienced Django developer would be able to identify where string references are processed and realize the need to avoid breaking correct lookups due to casing. Coding and testing the solution should fit within an hour, once the code structure is understood.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues preventing this sample from being used in the benchmark setup. The test case demonstrates that the required fix is testable and that relevant information to reproduce and verify the issue is available. The correspondence between code, error, and desired behavior is strong and supportive for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly outlines a specific code scenario involving Django's ORM (annotating with a DateTimeField subtraction and a Value with DurationField), the triggering of a FieldError, and gives a code example. It states the error occurs because of combining a DateTimeField with a DurationField without an explicit output_field. However, it does not strictly specify the precise way this should be fixed (should Django infer the output_field automatically, or handle the subtraction in a different way?), nor does it explain the desired user experience in detail. There is a sensible interpretation (make subtraction of temporal fields work without always needing ExpressionWrapper and output_field), but an implementer might have to make some reasonable assumptions based on the example given. Thus, it is not perfectly precise (not a 0), but it is much clearer than a vague request (not a 2).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires investigating Django ORM internals and how expressions involving DateTime and Duration fields are resolved, altering logic related to expression handling (specifically subtraction) in a foundational part of Django, and making changes to the query construction machinery. The solution impacts expression resolution, type inference, and likely involves understanding DB backend compatibility. It also affects and requires changes to multiple test cases. This goes beyond a quick fix and requires a good understanding of the ORM internals, so a reasonable estimate is 1\u20134 hours for an experienced Django developer, especially if unfamiliar with this corner of the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The sample is strongly related to expression handling in Django and is a reasonable benchmark for coding skill in the area of ORM internals, provided assessors are comfortable with complex Django code. The original issue and code context tie together well for reproducibility in a benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It provides clear information on what problem occurs: starting in Django 3.1, kwargs passed to TemplateView.get_context_data contain values wrapped in SimpleLazyObject, which causes issues when these values are used as SQL parameters (because SimpleLazyObject isn't automatically converted, and this can break backend serialization). The report includes a concrete example with code, the specific error that arises, a comparison between Django 3.0 vs 3.1, and exactly what change fixes the problem (converting to a string), as well as sufficient context about versions and backend. An experienced developer could meaningfully attempt a solution using only this issue text.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the bug touches a core Django class and requires awareness of how context data and lazy objects interact, the reproduction and required change are pretty clear: ensure that kwargs passed into context data are unwrapped (no SimpleLazyObject remains inappropriately in the context). The fix is local and can be implemented with thought and minor code changes, likely within 15-60 minutes after understanding the problem and verifying that the fix doesn't break compatibility. It does not require rewriting or in-depth changes across many files.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The bug and solution are self-contained and well-described. The test provided in the patch would allow automated checking of solutions; setup for the codebase and test suite is also standard for a Django-based repo. No additional context is required to use this sample to assess coding skill in a benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13406": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well specified. The user provides a clear minimal reproducible example with both a model definition and code to trigger the bug, details about the error seen (AttributeError: 'NoneType' object has no attribute 'attname'), expected vs. actual behavior (should return dicts, but returns model instances in a broken state), and explanation of how pickling and assignment of the query lead to the broken output. The file and object names (models.py, Toy, values(), annotate, query) are all referenced clearly. An experienced engineer would fully understand the required fix: ensure that after unpickling a queryset with values()/values_list(), evaluation returns the correct type (dict/list/tuple), not broken model instances. There should be no ambiguity as to what the desired behavior is.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix, while requiring understanding of Django QuerySet internals, is fairly localized based on the example and error analysis. The underlying problem is that reattaching a pickled query to a new QuerySet does not restore the correct iterable class, leading to objects of the wrong type being returned. The solution is a targeted change: set the iterable class based on the query's values_select on assignment (as seen in the patch). This requires a bit of thought to be correct and might require some light research on how QuerySet/Query internals coordinate, but is not a substantial rewrite or major multi-file change. An experienced engineer should be able to make the fix within an hour, assuming some familiarity with Django's ORM subsystem.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No issues. The description is detailed, includes a minimal example, expected/actual output, and enough context about the internals. This is a very well-formed benchmark sample for coding/developer ability. The minimal reproducible example makes it straightforward for benchmarking.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13449": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite specific and detailed. It clearly explains the context: usage of Django ORM's Window and Lag expressions with a SQLite backend, the error encountered (OperationalError with a syntax error near 'OVER'), the problematic SQL generated (CAST/LAG/OVER clause), and the reason: the way casting is applied causes invalid SQL in SQLite, specifically when Window is used with a DecimalField. The issue also provides a simple model, the query that triggers the bug, the correct behavior (works with FloatField), and a workaround (using output_field=FloatField in Lag). Therefore, the requirements for a fix are explicit: the code should generate SQLite-compatible SQL such that the CAST is in the correct place (outside the OVER clause) when a DecimalField is used. The engineer can derive precise requirements and constraints from the issue alone.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires a good understanding of the ORM's window/aggregation SQL code generation, with a depth sufficient to know how Django generates queries per backend, how output fields are handled, and how to conditionally change behavior for DecimalField vs. others just for SQLite. One must appropriately override the SQL for the Window expression for SQLite, as was achieved in the patch. While the change itself isn't massive, it is nontrivial: it involves subclass/mixin decisions, field introspection, and writing backend-specialized SQL generation code, and ensuring test coverage. Reading and understanding both Django's code and SQLite's requirements for window functions may take up to several hours (particularly for someone unfamiliar with internal ORM details or window function code paths). Thus, an experienced developer could likely complete this in 1-4 hours including testing and validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present. The sample is self-contained, the requirements are clear, and the test cases address the regression scenario well. This serves well as a benchmark challenge for advanced ORM/backend handling and targeted bugfixing.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13568": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it describes a concrete case in Django where defining a user model with a UniqueConstraint (instead of unique=True on the field) for USERNAME_FIELD triggers a system check that expects the field to be unique but does not recognize constraints. It clearly describes what is happening, the undesired behavior (the system check ignoring Meta.constraints), and what would constitute a fix (extend the system check to recognize uniqueness from constraints as well as unique=True). However, there are still a few blanks: The issue does not specify whether all forms of UniqueConstraint, including partial/conditional constraints or compound constraints, should be accepted, and does not say exactly which Django or database versions the expected behavior is for. That said, the main requirement is pretty sensible to infer: recognize when uniqueness is enforced via UniqueConstraint before raising a system check error about missing uniqueness. A good engineer would be able to produce a solution based on this.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a moderate change to the Django system check code: modifying the uniqueness check logic to also respect constraints from Meta.constraints (specifically UniqueConstraint), and updating or adding corresponding tests. While the contribution is non-trivial (touching Django internals), it's fairly focused: locate the check, adjust the logic, and write new or modify existing tests. An experienced engineer familiar with Django codebase would need to understand how constraints are stored/retrieved, but the patch itself is concise, so this is likely a 15-min-to-1-hour issue for someone experienced. Effort is more about precision and understanding Django metadata patterns than scale.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There aren't additional blockers for using this as a benchmark sample. The task is specific and assessable, and the required fix and corresponding test adjustment are clear. Provided the engineer is familiar with Django conventions (or takes time to look them up), this sample works well for evaluating ability to interpret framework-level checks, metadata, and test your changes accordingly.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13807": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It clearly lays out the steps to reproduce, identifies the root cause (failure due to SQL reserved keywords being used as table names, and missing quoting/backticks in the SQL constructed in check_constraints in django/db/backends/sqlite3/base.py), and even points out the relevant lines of code where the problem lies. An experienced engineer would know exactly where to look in the codebase, what needs to be fixed (properly quote or backtick table names in the offending PRAGMA statements), and can infer what the requirements for the fix are. There is little to no ambiguity in what defines a successful solution: PRAGMA statements must handle SQL reserved words used as table/model names without error. The linked stack traces and line numbers remove any substantial blanks.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution is localized to a specific function and involves modifying SQL string construction to ensure proper quoting for table names, which is a well-understood fix for SQL issues involving reserved keywords. The patch shows that only a few lines need alteration. Understanding the underlying bug is straightforward for someone experienced with Django and SQL, even if brief investigation is needed on quoting methods (e.g., self.ops.quote_name). The task is small and focused enough for an experienced engineer to likely complete in 15-60 min, including any hunting for documentation or related usages.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The scenario is realistic and tests the candidate's understanding of SQL, Django ORM, and quoting practices, which are appropriate for a coding benchmark. The level of context provided matches the expectation for a self-contained coding challenge without external clarification. Test modifications and context are also clear.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13810": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description, while somewhat detailed and offering a solid reproduction scenario, is not 100% clear about the desired fix. It describes a problem that occurs when certain middleware patterns are used under ASGI, along with the confusing error behaviors and how removing the middleware changes the observed outcome. The description points out the problematic code, hypothesizes about the root cause, and discusses the desired behavior in terms of Django\u2019s promises about middleware compatibility. However, the report is not written as a crisp engineering task; it is more of an investigation. Still, an experienced engineer could infer that what\u2019s required is to fix the middleware adaptation logic when a MiddlewareNotUsed exception is raised, based on the context clues and the description of what happens versus what should happen. The exact code change required isn\u2019t directly stated, but the intent and context allow for a sensible path forward.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This task requires a solid understanding of Django\u2019s middleware loading logic, ASGI vs WSGI execution, and the role of MiddlewareNotUsed in configuring middleware chains. It involves correctly tracking async adaptation across exception branches in a core Django subsystem. The solution, while concise in code, requires investigation, reading and understanding framework internals, and ensuring no side effects are introduced. Thus, it would likely take 1\u20134 hours for an experienced engineer to carefully understand the bug, trace through the relevant code paths, formulate and test the solution, and verify that the regression doesn't cause collateral issues.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues for benchmark purposes. The description's context and the presence of a clear test case in the final patch mean the scenario is adequately bounded for engineering assessment, albeit not perfectly direct as a ticket.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14017": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified: it clearly describes a problem (TypeError raised when combining Q() & Exists()), provides minimal reproducible examples, and suggests what the correct behavior should be (commutativity of & and | between Q and Exists). The description identifies the likely area of code to modify (missing __rand__ implementation), and a developer familiar with Django ORM and these expressions would have sufficient guidance to attempt a fix. However, it leaves some small blanks for interpretation, such as the precise technical approach to ensure commutativity\u2014e.g., whether to add a method, or widen type checks\u2014so it's not perfectly explicit, but is clear enough for an experienced engineer to act.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem requires understanding the type checks occurring in the __and__ (and likely __rand__/__or__) methods and relaxing them to allow Q and Exists to combine both ways, not just one. The core issue is addressed in a single place with a small logic change (polymorphic type check). The test update is mostly copy-paste and follows from understanding the problem. For an engineer familiar with Django internals and Python dunder methods, this is a modest, focused change that likely takes less than one hour to complete.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major blockers are present. The problem is well-motivated, easy to reproduce, has a clear set of expected behaviors, and is self-contained. The symmetry/commutativity aspect is a reasonable, bounded fix. The test case provided clarifies the expectations. Given all this, the sample is suitable for coding-benchmark purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14140": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is relatively well-specified, describing a clear bug in Django's Q object deconstruction logic when the Q object wraps an Exists expression. It details that the deconstruct() method incorrectly treats children as tuples but does not handle the case where the only child is an Exists instance, leading to a TypeError. While it does not reference exact filenames or line numbers, it is specific enough about the context (Q object, Exists child, deconstruct method, TypeError due to subscriptable issue) that an experienced engineer familiar with Django ORM internals could reasonably identify the site of the issue and what needs fixing. It could be more precise, but the nature of the bug and its trigger are clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves editing a single function (deconstruct in the Q object), removing special-casing for singleton children and ensuring all children are treated as tuples. The conceptual difficulty is moderate\u2014the engineer should have enough familiarity with Django's ORM query construction to reason about Q and Exists, but there are no fundamentally new algorithms, large refactors, or multi-file rewrites required. It would likely take less than 1 hour for an engineer familiar with Django's internals to identify and test the fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The bug and its context are clearly described. The only minor gap is the lack of explicit code references, but for benchmark purposes, the issue text is sufficient for an experienced engineer. There are no major issues preventing this from being used to evaluate coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14238": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The reporter describes the problem scenario (using a subclass of BigAutoField as DEFAULT_AUTO_FIELD) and the existing error (ValueError saying that the class must subclass AutoField, even though it does). A concrete code sample is provided. The cause is pointed to a specific area of Django's internals (AutoFieldMeta.__subclasscheck__ and its _subclasses property) and a proposed direction for the fix is given (allow subclasses of BigAutoField and SmallAutoField in the _subclasses property). This gives enough technical detail and context for an experienced Django developer to proceed confidently and implement the fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires understanding Django's metaclasses and how DEFAULT_AUTO_FIELD works, but the issue provides specific pointers and a code sample that reproduces the bug. The required fix (making __subclasscheck__ use issubclass against the existing subclasses) is a targeted, single-line change once the right place in the code is found. Implementing and testing shouldn't take more than an hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues exist. The test coverage and description are clear. It is a suitable sample for evaluating coding ability, specifically regarding understanding inheritance and Python's metaclass mechanisms in a real-world framework.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14351": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear direct example of the problem (a Django ProgrammingError due to a subquery returning multiple columns when only one is allowed) and refers to specific query construction via get_default_columns, Q object aliases, and the sources of difference between agent__property_groups__id__in and agent__property_groups__in. While it's not 100% explicit about the intended fix, an experienced Django engineer can deduce that the solution must ensure subqueries select a single column (likely pk/id), and will recognize the problem likely exists with Q object aliasing and subquery handling in lookups. The required outcome (avoid multi-column subqueries in these cases) is clear, even if the precise code location isn't named. Thus, there are some blanks for the developer to fill in, but it is not vague.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires a deep understanding of Django ORM internals, especially with respect to how Q objects, lookups, and subqueries interact. It involves tracing how querysets are constructed and aliased, understanding why get_default_columns behaves differently in these cases, and modifying potentially complex code and tests. The actual patch is not huge, but reproducing, validating, and confidently resolving the issue will require substantial investigation, familiarization, and potentially running several tests. This places it in the 1-4 hour range for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues are evident. The main risk is that the issue depends on deep familiarity with the Django ORM and may slightly favor those already knowledgeable about its internals, but it is otherwise a reasonable benchmark for experienced backend engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14493": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clearly and thoroughly specified. It describes the context (a custom subclass of ManifestStaticFilesStorage with max_post_process_passes=0), provides an easy-to-reproduce code snippet, and explains in detail the sequence of events and the resulting error (UnboundLocalError due to an uninitialized variable 'substitutions'). It also directly references the part of the codebase with links and line numbers, clearly pointing out where and why the bug occurs. The rationale for setting max_post_process_passes to 0 is also included. An experienced engineer can understand the bug, desired outcome, and context solely from the provided issue text. No ambiguity exists about what constitutes a successful solution: the crash should be avoided even when max_post_process_passes is 0.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue describes a small but non-trivial bug: a variable is referenced before assignment if a loop does not execute (i.e., when max_post_process_passes=0). The fix is conceptually simple (ensure 'substitutions' is initialized outside the loop) but requires familiarity with the logic and potential impacts. Investigating the code and confirming that initializing the variable globally is correct could take some time, especially to check for side effects. No extensive refactoring or multi-file changes are needed, so this would be a 15 min to 1 hour fix for someone familiar with Django's storage backends.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional issues with this sample. The bug, its symptoms, and the conditions that trigger it are all clearly detailed. Required changes are local, and the test-related patch affirms that the fix is testable. This sample is appropriate for benchmarking coding ability as it involves a concrete, reproducible bug that requires code and test changes to resolve.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14580": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it identifies a clear bug in the migration file generation process where 'models.Model' is referenced without the necessary import, leading to a NameError. The reporter provides an explicit minimal example of models.py, a generated migration file that shows exactly what output is produced, and describes the exact error encountered when loading the migration. The expected and actual behaviors are stated clearly. The target module (django.db.migrations.writer or related serialization logic) is suggested. There is no ambiguity as to what needs to be fixed: migration files referencing 'models.Model' (or similar) need to import 'models' as well. Any experienced developer familiar with Django migrations would be able to attempt a concrete fix from this report.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding how Django\u2019s migration writer serializes model references and where import statements are managed for generated migration files. Familiarity with Django\u2019s serialization and migration internals is useful but not strictly necessary, as the examples and explicit error are sufficient. The code change is very localized (adding an import to a special-case serialization), not requiring substantial refactoring. The expected fix could be reasoned about and implemented with focused study of the relevant serialization code, likely within 15-60 minutes for an experienced engineer with the codebase available.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues detected. The sample is suitable for use as a coding benchmark, as the problem is clear, has a concrete reproduction example, and its fix is tightly scoped.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14672": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is highly specific and provides all necessary context for an experienced engineer to implement an appropriate solution. It explains the technical background: the use of identity tuples for comparing ForeignObjectRel, the need for hashability because identity is hashed, and the role of make_hashable for tuple elements. It identifies that through_fields might be a list, which would not be hashable, and the redundancy (missing use of make_hashable) in the ManyToManyRel class. The issue includes a minimal reproducible example, the error message expected, and a clear suggested fix: ensure make_hashable is applied to self.through_fields in ManyToManyRel. The engineer does not need to make assumptions about 'what' needs fixing; both the problem source and solution are explicit.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires an understanding of how hashability affects Python classes, especially with respect to Django's internal modeling of relationships. The actual code fix is a small one-liner: applying make_hashable to self.through_fields when constructing the identity tuple. While knowledge of Django internals may be needed to recognize why this matters, the implementation does not require deep research or significant code changes. The provided reproduction and the pinpointed error narrow down debugging work. Thus, an experienced engineer familiar with Django ORM should be able to resolve this in 15 minutes to 1 hour, including testing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are present. The issue is suitable for benchmarking coding ability. The minimal reproduction ensures reproducibility and the solution is fully contained within the provided context. The only requirement is basic understanding of Django ORM internals and Python hash mechanics.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a clear and precise scenario showing a decorator being wrapped on a method using method_decorator, and the resulting error (AttributeError due to functools.partial lacking function attributes such as __name__). It also includes a minimal working example of how the error is triggered, making the issue tractable. However, there are still some blanks to fill in: for example, it is not specified precisely how the wrapper assignments should be preserved, or the precise requirements for compatibility with Django's method_decorator, but there is a sensible interpretation: ensure that the function returned by method_decorator preserves __name__ and other attributes as would be expected for a function. The problem and the goal are clearly inferred, but implementation details are left to the engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding how decorators and functools.partial interact in Python, and realizing that wraps() must be explicitly applied to the partial object to ensure function attributes are retained. The actual code change is very small (effectively a one-line fix), but it requires understanding the cause of the error and how metadata is handled in decorators. An experienced engineer familiar with decorators and Python standards should be able to diagnose and fix this in 15-60 minutes, including verifying with an appropriate test case.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The only complication is that context about method_decorator's place in Django is assumed, but an experienced engineer would be able to find the relevant function and understand the minimal fix needed. The example and explanation are direct, and there are no blockers to using this issue for the stated benchmark use case.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15104": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is detailed and clear, with an explicit reproduction of the problem, an explanation of its underlying cause, and a proposed solution. It names the relevant Django core function (only_relation_agnostic_fields in MigrationAutodetector), the precise line of code that fails (del deconstruction[2]['to']), and explains why this code fails with a custom ForeignKey field. The reporter provides a minimal example\u2014including custom model fields, deconstruct logic, and a test case\u2014that reproduces the error. The hints about a safe fix (switching from 'del' to 'pop') are also clearly articulated, supporting a definite and actionable direction for implementing a fix. All the how and why of the required solution are covered, so an engineer could immediately start investigating the specific file and line, reproduce, and target a solution. No major ambiguities remain.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue is well-scoped: it involves understanding custom deconstruction of model fields in Django, recognizing why a KeyError occurs when a key is missing from a deconstruct kwargs dictionary, and replacing a 'del' statement with a safer 'pop'. The patch only involves a one-line change. Familiarity with Django's migrations would help, but the reasoning required is constrained to a single function and error scenario. Although tracing through deconstruction in the Django ORM takes some thought, for an experienced Django engineer, identifying the failing line and implementing the defensive code fix is straightforward; the majority of the effort is understanding the context, which could be done in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would disqualify this sample from being a benchmark. The issue is well-described, and the patch aligns with the described fix. The related test code is self-contained and matches the issue scenario. This is a good candidate for evaluating coding ability pertaining to defensive programming and patching ORM internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15128": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is detailed and well-structured. It clearly states the problem: an AssertionError raised in Query.change_aliases when using the OR operator on QuerySets in a certain order. The models and the specific queries that trigger the bug are provided, as well as an explanation of the underlying cause\u2014conflicting alias renaming leading to change_map key/value intersections. There is also a clear request: fix the alias renaming so that this error does not happen, with suggestions for how this might work (by using different alias prefixes and updating alias maps). The description also gives thoughts on documentation improvements, but the main ask is clear and actionable. An experienced engineer familiar with Django ORM internals should have a clear understanding of what needs to be fixed and what behavior is expected, supported by concrete minimal reproduction code.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue involves a tricky and subtle bug in Django's ORM, a core and complex part of the framework. Fixing it requires deep understanding of query aliasing, how QuerySet operations combine queries and rename aliases, and the interplay between left-hand and right-hand sides of operations. The solution needs changes both to the core ORM logic (including new code for bumping alias prefixes and careful updates to ensure aliases don't conflict) and new/updated test cases. The edit is not massive in terms of lines but requires careful reasoning, understanding of edge cases, and ensuring backwards compatibility. Testing and verifying the fix also requires familiarity with Django tests and likely some manual trial and error. This is a classic \\\"medium\\\"-complexity patch that could easily take a few hours for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues beyond what is described. The sample is well-formed, relevant to evaluating advanced coding and debugging ability, and provides a direct reproduction and clear expected result. The only challenge is the need for detail knowledge of Django internals, but this is in scope for a strong engineering task.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15277": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The problem is described in detail: Value._resolve_output_field creates a CharField (django.db.models.fields.CharField), but does so with max_length=None, which results in a MaxLengthValidator being created with a None limit. This can cause a TypeError if 'clean' is ever called. The solution is proposed directly in the issue: update CharField.__init__ to only add the MaxLengthValidator if max_length is not None, following the precedent set by BinaryField. There are concrete code snippets and context, including relevant performance measurements. An experienced engineer could fully understand what needs to be done and where (CharField.__init__).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While this issue relates to core logic, the required code change is straightforward (an if condition around adding the validator). Recognizing the bug requires understanding Django field/validator internals, but the issue directly suggests the fix, and it only touches a small piece of code. Testing after the fix is also described and would be quick with the given guidance. For someone familiar with Django, the change and testing could reasonably be completed in under one hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues. The problem is isolated, well-specified, and the required solution can be precisely targeted. The only minor note is that understanding why this behavior matters may require some background in Django's model/field internals, but any engineer evaluating this for coding benchmarks should have that level of context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15280": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite well-specified and provides a clear context, detailed models, a representative test case, and precise information about the unexpected behavior (i.e., an extra query being issued even though the field was apparently prefetched). It explains both the setup and the exact evidence of the bug, including queries issued and what happens when attributes are accessed. The root cause is hypothesized, but the expected behavior is very clearly specified: nested prefetching should avoid unnecessary queries for already-fetched fields. The example, code structure, and what constitutes success (test passing without extra query) is unambiguous. An experienced engineer could work from this description alone.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue involves debugging a nuanced aspect of Django's ORM\u2014specifically, nested prefetching, deferred fields, and cache management in related descriptors. Although the root cause is hypothesized, arriving at the correct fix would require a solid understanding of Django's model internals and how prefetching interacts in these cases. The patch is concise, but determining the solution requires fairly deep investigation and comprehension of the query and cache logic. For an engineer familiar with Django internals, this would likely take several hours including setup, experimentation, and validation, but it doesn't rise to the level of very esoteric/large-scale refactoring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The issue is clear, testable, and the required code context is well-isolated and understood. The benchmark would work well for evaluating coding ability, especially around complex ORM logic and debugging nested relationships.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15315": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes the specific problem (that Field.__hash__ changes value after a field is assigned to a model class), explains why this is a problem (dictionary key lookups fail due to hash mutation), provides relevant context about the bug's origin (referencing #31750 and how equality for fields was previously broken), and gives a concrete recommendation (reverting the __hash__ implementation to make hashes immutable). Both the nature of the bug and the desired property (immutability of Field.__hash__) are unambiguous. There is sufficient information for an experienced engineer to identify the affected method (Field.__hash__), understand the context, and devise an appropriate solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding the subtle behavior of Python's __hash__ method, object mutability, and dictionary key lookups. However, it is localized: the offending method implementation and what needs to change are made clear in the report. The actual code change is minimal (adjusting the hash to only depend on creation_counter). Familiarity with the models/fields code is helpful, but the fix is conceptually straightforward and does not require extensive refactoring or editing multiple files. Including a new or broadened test is also standard procedure. This is likely to fall into a '15 min - 1 hour' change after reading the context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified. The description is clear, and the sample is a good candidate for benchmarking coding ability around subtle bugs with hash and object mutability semantics in Python.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15375": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue contains a clear description of the bug, detailed reproduction steps, minimal working code snippets, and specific version information. It points out a precise scenario in Django ORM where using an Aggregate with a 'default' parameter after 'annotate()' leads to a SQL syntax error. The expected versus actual behavior is demonstrated with example queries, and a workaround is provided, further clarifying intent. As an engineer, the requirements (fix aggregate with default after annotate so it does not crash, and works as the workaround does) are straightforward and actionable, with no ambiguity in what's to be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Although the issue is nuanced (involving QuerySet/annotation/aggregate mechanics), the root cause is isolated, and reproduction is easy. The fix is limited to understanding how default values are passed and constructing the correct SQL, likely requiring targeted edits to aggregation logic and some testing. For an experienced Django developer familiar with the ORM internals, this is an isolated change that could be resolved in under one hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is excellent for benchmarking: it is very reproducible, well-scoped, and tests will validate the fix. It tests understanding of a subtle ORM/SQL interaction, requiring both code and translation of an intent into correct SQL generation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15380": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified: it identifies a concrete sequence of user actions ('rename a model and a field in a single step' using makemigrations), describes the observed error (KeyError on lookup of the old model name), and explains the causality with a regression reference. However, it leaves blanks such as the precise internal steps and full context of the states involved, as well as the code paths that would trigger the bug. An experienced engineer familiar with Django's migration system should be able to reproduce and interpret the intended fix, though they'll have to fill some gaps on expected behavior.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is quite targeted: a KeyError due to referencing an old model name after a rename of both a model and a field, suggesting an issue in how states are referenced post-rename in the migration autodetector. While reproduction and understanding will require some familiarity with migration-state logic, the fix itself (as seen in the patch) only affects a very small part of the code, changing the model name used for lookup. For an engineer with an understanding of Django's architecture, this is a focused fix with limited blast radius, likely doable in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent. The scenario and requirements are plausible for an isolated test; the only caveat is the necessity for some contextual knowledge of the Django migration system, but this is reasonable for the intended coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15503": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well specified. It clearly states the observed incorrect behavior: has_key, has_keys, and has_any_keys lookups on models.JSONField() fail to find numeric keys when using SQLite/MySQL/Oracle, especially highlighted with the example using '1111' as a key. The working behavior is explicitly described for PostgreSQL. The minimal working (and non-working) example, including the Django version, database config, model, and test code, make the problem and its boundaries exceptionally clear. An experienced engineer would know what needs fixing: ensuring that JSONField's key lookups can successfully handle numeric string keys for all supported backends. No significant ambiguities exist regarding the 'what' of the problem or the expected outcome.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix requires a deep understanding of how Django's ORM compiles queries against JSONField for various databases, especially the differences in JSON handling between PostgreSQL (which works) and others (which don't). The issue will likely require changes at the SQL generator level (custom SQL compilation code in Django), care to not break existing behavior with array indices, and updating/writing new tests to confirm correctness across multiple database backends. The scope crosses both the core implementation and the test suite. While the general bug is clear, navigating Django's ORM internals and ensuring cross-database correctness represents a moderate complexity task (1-4 hours for a proficient Django developer, as seen in the patch).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues are present. The problem is isolated, well-documented, and has no apparent dependency on external clarifications, corner cases, or missing data that would hinder its use in an evaluation context. It is a solid exercise in backend framework internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15525": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It precisely describes the error scenario: loading a fixture with Django into a non-default database fails when natural keys with foreign keys are used, because the deserialization process attempts to access a related object (Author) in the secondary database before it has been created, causing a DoesNotExist exception. The relevant models (with all fields and custom manager methods related to natural keys) and an example of problematic serialized data are included. The error flow is explained clearly, including the ORM lookup step. An experienced engineer could reproduce and identify the bug, and understand the goal\u2014a fix that prevents premature foreign key resolution during fixture loading on alternative databases. No ambiguity remains about the technical context, the undesired current behavior, or the class/methods involved.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a deep understanding of Django's object deserialization process, custom manager methods, database state management (especially with multiple databases), and the natural key mechanism. The solution involves both understanding and adjusting the order/state in which deserialization and database assignments happen, as well as safeguarding against improper lookups on non-existent related objects. It may require stepping through the logic in Django's serializers, and could involve editing internals, ensuring backward compatibility, and writing/adjusting tests. Thus, while not an all-day task for an experienced Django engineer, it is more involved than a trivial fix or simple edit. 1-4 hours is a reasonable estimate for diagnosis, code and test development, and verification.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues are present, and the sample is well-suited for benchmarking coding ability. The issue has a clear setup and resolution criteria, a concrete bug manifestation, and sufficient technical detail across models, data, and error scenarios. It is a classic example of a regression/integration bug in a large codebase. The only caveat is that some familiarity with Django's serialization or fixtures system is assumed, but that's typical for a specialized benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15695": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description communicates the bug in sufficient technical detail: RenameIndex() fails when re-applied, particularly in the case where an unnamed index is moved backward (i.e., reverted as part of migration operations). A concrete example is provided showing the failure: after moving the migration backward and forward, attempting to reapply RenameIndex raises a ProgrammingError due to the index name already existing. The involved filename and context (tests/migrations/test_operations.py and OperationTests) are indicated, and the API (RenameIndex) and the error (\u201cindex ... already exists\u201d) are made clear. However, the issue assumes familiarity with migration operations and doesn't spell out all expected behaviors or edge conditions, requiring some background knowledge and assumptions about migration reversibility and idempotency. This leaves minor blanks to interpret but is sensibly actionable for someone with relevant experience.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is relatively contained: The engineer needs to ensure that RenameIndex behaves as a no-op if the old and new index names are identical, thus avoiding a crash. The solution involves adding a simple conditional early-exit in the relevant function, requiring understanding of migration operation flows, index naming mechanisms, and the involved Django migration APIs. The scope is limited to a minor logic adjustment and corresponding test coverage, which could be reasonably achieved in 15-60 minutes by an engineer familiar with Django migrations and schema editing. The patch is succinct and changes are minimal, indicating modest complexity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The task requires some prior understanding of Django\u2019s migration operations and the meaning of unnamed indexes and unique_together behaviors, but should be manageable in the benchmark setting for a Python/Django engineer. The example, error, and test context all support an actionable fix; no major obstacles or unsuitabilities are apparent. The requirement relates tightly to code correctness and migration reversibility, making it a meaningful benchmark for real-world bug fixing.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15732": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear picture of the problem: a unique_together constraint on a primary key ('id',) cannot be dropped properly during Django schema migrations because the migration logic anticipates only one constraint, but on PostgreSQL, two distinct constraints exist (the primary key and an extra unique constraint). The description is explicit about the observed behavior, the structure of the migration, and the database backend (PostgreSQL). However, there are small blanks\u2014such as the expectation that one understands Django migrations internals and constraint removal logic\u2014but these are sensible blanks for a Django engineer. There is a clear path to a successful solution: allow the migration logic to correctly remove the specific unique_together constraint even in the presence of the primary key constraint.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a solid understanding of Django's migrations framework, schema editor, and how database-level constraints are represented and named within Django vs the actual backend (PostgreSQL). The engineer must update logic to distinguish between constraints, ensure correct removal, and potentially update or add related index-naming logic for constraints. The resolution touches core Django internals and affects both code and database migration tests. The solution is more involved than a simple fix, requiring coordinated updates across schema management and tests, and a good conceptual model of Django's migration engine and PostgreSQL's constraint handling. This would take 1-4 hours for a well-informed engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is well-posed for engineers familiar with the Django codebase and provides a meaningful challenge for evaluating coding ability in a realistic codebase setting. The tests and patch show completeness and the sample covers both the logic and appropriate validation through testing.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15814": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and concrete. It describes the exact scenario that causes an error, including relevant environment details (Windows 10, Python 3.10, Django 4.0.5) and gives specific code examples of the involved models and the management command. The error is described precisely, and the issue text provides a clear hint at both the cause (primary key is not in the selected fields due to how select_related and only interact via the proxy) and the likely solution (using concrete_model._meta instead of _meta alone when processing models). Additionally, the filename and code line where the issue occurs are identified (django/db/models/sql/query.py, line 745), along with an explicit code suggestion. Any experienced Django engineer could work from this description to create and validate a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively small change in the Django ORM internals, requiring a nuanced understanding of proxy models and how select_related/only fields are resolved. Although it touches code in a widely used critical component (django/db/models/sql/query.py), the change is a single line. An engineer familiar with Django's internals would need a bit of time to verify edge cases, understand the model chain, and ensure correct behavior, but the scope is clearly defined and the solution isn't a substantial rewrite, so it falls into the 15 min - 1 hour range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues. The sample is well specified, the test is targeted, and the technical problem can be understood and addressed directly from the issue text. The link between the code and the desired behavior is explicit. The scenario is realistic for benchmarking coding ability relevant to framework maintenance.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15930": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well-specified. It clearly identifies a bug in Django when using a Case() expression with ~Q(pk__in=[]): the generated SQL is invalid because it produces a CASE WHEN clause with no condition. The minimal failing code snippet is provided, the observed behavior (syntax error), and what the user expected to happen (all rows annotated with True). There is also context about why ~Q(pk__in=[]) is used (acts as a sentinel). While further clarification on the internal expectations might be helpful (e.g., should all empty Q-like expressions in this context be 'always true'?), the text is detailed enough for an experienced engineer to understand and attempt a fix. Thus, I rate it a 1, as there are only small blanks to fill in, but the requirements are quite clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires understanding the handling of Q objects, SQL generation for CASE WHEN in Django's ORM, and how ~Q(pk__in=[]) ends up as a condition that matches everything and becomes an empty string in SQL. The solution involves checking for this case in the code that emits the CASE WHEN clause and substituting an always-true predicate. It is a small, localized logic change (in the as_sql method) and is not a substantial rewrite or multi-file change, though it does demand some familiarity with Django ORM internals. A skilled engineer familiar with Django ORM internals would likely be able to read, diagnose, and fix this within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified with this sample. The problem is well-scoped, testable, and the tests proposed clearly show the fixed behavior. The scenario is relevant for evaluating code comprehension and minor ORM logic modification skills.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15957": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly describes a specific problem with Django's Prefetch objects: currently, when prefetching related objects using a sliced queryset, it triggers an assertion error. The user provides a precise example code snippet and explains the intended use case: efficiently attaching a limited-size subset (a slice like [:3]) of related objects to a set of parent objects. While the core expectation (enable support for prefetching with sliced querysets) is clear and there's a concrete motivating example, some implementation specifics are left open. For example, the precise expected behavior regarding ordering, how to handle duplicates, or edge cases with complex querysets are not exhaustively specified. However, there is a sensible and well-scoped interpretation: \\\"make Prefetch work intuitively with slices, so that for each parent only the N-sliced subset is attached as desired.\\\" Thus, a reasonable engineer can move ahead, but will need to make some judgment calls.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue is of moderate complexity. Successfully resolving it requires a solid understanding of Django ORM internals, specifically how prefetch_related and Prefetch work, interaction with QuerySets, and the limitations around slicing. The solution requires non-trivial code changes to support efficient prefetch operations with sliced querysets, possibly utilizing SQL window functions, and must be robust to at least many-to-many and foreign key relationships, as evinced by the final tests. The solution in the gold patch involves new utility logic and careful handling with QuerySet filtering and windowing, which is not trivial. Overall, the problem is well-defined but involves more than superficial code changes, and requires thoughtful design and thorough testing. The estimated time (1-4 hours) reflects the need to first understand the relevant Django internals, design the logic, implement it, and write/adjust tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are evident beyond those discussed. The issue sample is suitable for evaluating coding ability in the context of ORM/query logic and framework extension.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well-specified: it describes the error encountered, the relevant models across multiple apps, and even includes the generated migration file that demonstrates the problem. The engineer is told exactly under what circumstances the issue occurs (using a ManyToMany \\\"through\\\" model specified as a string reference to another app), and what specific error is thrown (AttributeError: 'str' object has no attribute '_meta') during migration. It also clarifies that moving the intermediary model into the same models.py avoids the problem. The likely desired outcome (correct handling of through references across apps in migrations) is clear. However, the description does leave small blanks, such as explicit requirements for backwards compatibility or edge cases, and assumes some familiarity with Django's migrations systems, but any experienced Django developer would interpret the ask sensibly. Therefore, this fits best as a '1': blanks to fill, but a sensible, actionable interpretation.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue involves a non-trivial bug relating to Django's model resolution mechanism across apps during migrations, specifically how string references to intermediary models in ManyToMany fields are resolved. Understanding and fixing this requires knowledge of Django's ORM internals, migration dependency resolution, and how models are loaded and referenced. The actual patch is relatively contained but the investigation, understanding, and correct placement of the fix would likely take 1-4 hours for an experienced Django contributor because it requires setting up the reproduction, confirming the source of the bug, and understanding the knock-on effects throughout migrations. Test additions also suggest verifying multiple dependency states. Not a trivial or quick change, but doesn't rise to the level of multi-day work.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues\u2014the test and solution patches demonstrate that the issue is reproducible and addressable with the information provided. The scenario and requirements are within typical expectations for advanced Django work. The sample is suitable for evaluating coding ability, especially in the context of ORM/infrastructure fixes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16032": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It includes a concrete and minimal failing test case, specifies the versions of QuerySet functions being chained (annotate, alias) and relates to how '__in' with subqueries is not handling selected fields as expected. The OperationalError is precisely described, including the mismatch in the number of return columns. The filer path to place the test case is referenced, and the entire failure is deterministically reproducible from the description. This makes it clear what a successful solution would be: resolve the error so that the provided use case works as expected without raising the error.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"While the issue is well-localized and reproducible, the fix touches the internals of Django's ORM, specifically around how QuerySets handle selection, annotation, and aliasing with subqueries. It likely requires an understanding of Django's SQL generation and QuerySet machinery, which is nontrivial and may require tracing several code paths and understanding subtle interactions. The code change involves changing the logic in multiple locations to account for when select fields are set. This type of change could take an experienced Django developer 1-4 hours to fully understand, safely implement, and validate via tests, particularly since it deals with subtle ORM mechanics.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues block this from being a strong benchmark problem. The test case is realistic and well-targeted; the code change is meaningful but not overwhelming and requires a good understanding of the internals of a sophisticated open source project (Django ORM).\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16136": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The problem is described both in general terms (Django's async view system tries to 'await' a regular HttpResponseNotAllowed when it shouldn't, triggering a TypeError) and with a minimal reproducible example. The Django and Python versions are specified. There is a description of how to reproduce the bug within an empty project, and the exact error (500 server crash, TypeError due to 'await' on HttpResponseNotAllowed) is described. It's clear that the desired outcome is that asynchronous Views without specific method handlers should not crash on unsupported HTTP verbs, and specifically that async classes should not try to await a non-coroutine response in http_method_not_allowed. The relevant filenames and classes (django.views.View, Django's CBVs) are referenced and made clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires some understanding of Django's class-based views and async/sync mechanisms, but the bug is isolated and well-defined. The solution involves identifying the point where HttpResponseNotAllowed is returned for unsupported HTTP verbs and making it compatible with the async handler model. This is a small, focused change affecting a single class/method, and the reproducible example makes it easier to understand and test. An experienced engineer familiar with Django could solve this in 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is isolated, well-described, and supported with a clear MRE and context. The fix and its verification are straightforward and appropriate for inclusion in a coding evaluation setup.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16429": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly specifies the underlying bug: timesince() raises a TypeError when called with a datetime more than one month in the past and USE_TZ=True, due to subtracting a timezone-naive from a timezone-aware datetime. It explains the expected behavior, gives a specific test case (with the test method provided), points to the relevant part of the code (pivot creation in timesince.py), and even suggests a fix (include tzinfo in the datetime.datetime call). The issue is directly actionable, well-defined, and leaves almost no ambiguity regarding what needs to be fixed for a successful PR.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively localized bug fix. The issue includes a detailed description, isolated test case, points to the relevant code, and even gives a suggestion for how to fix it. For an engineer familiar with Django or able to orient themselves in the codebase, making the fix (adding tzinfo to the datetime object) and confirming with the tests would be a task of perhaps 15-45 minutes, so a 15 min - 1 hour estimate is appropriate. It does not require deep architectural changes or understanding of many files.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is clear, well-scoped, and the accompanying test ensures the intended fix is verifiable through automated tests. This makes it a good candidate for evaluating coding ability on this kind of bug fix.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16454": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, well-structured, and provides a concrete example demonstrating the problem with Django's management command subparsers not retaining error formatting. It describes exactly what currently happens (a traceback with CommandError instead of a usage message), why it's undesirable, and what the intended behavior should be (subparsers should copy relevant error formatting arguments, so errors result in friendly usage messages, not stack traces). It also provides information about relevant classes (e.g., CommandParser, add_subparsers) and an example of existing and problematic behavior. An experienced engineer would understand what needs to be changed in the codebase and what a successful outcome is.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change involves modifying the way Django's custom ArgumentParser subclass passes configuration (particularly related to error formatting) to subparsers created with add_subparsers(). Although familiarity with argparse and Django's management command infrastructure is needed, the scope is limited to ensuring correct argument passing during subparser construction and likely does not require changing more than one file. The patch itself is concise, indicating the change is straightforward but requires some deeper understanding of code structure and Python's argparse extension. Overall, an experienced engineer could likely implement and test this fix within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present which would make this sample unsuitable for the benchmark. The problem is clear, self-contained, and does not rely on ambiguous external context, making it a good fit for evaluating coding ability in the described setup.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16485": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified: it provides concrete examples (floatformat('0.00', 0) and floatformat(Decimal('0.00'), 0)), the relevant function (floatformat), the parameters passed, and the exception being raised (ValueError about the precision parameter). While it does not provide an explicit description of the desired output, it is clear from context\u2014the user expects floatformat to handle these cases without error, presumably returning '0' as output. There's only a small gap in that the issue does not state what the expected output is, but a sensible engineer can infer it from the function's general behavior and the context provided. Thus, there is a sensible interpretation for a successful solution, but it isn\u2019t perfectly explicit about output formatting.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding the floatformat function and its handling of edge cases; locating where ValueError is raised, and why p < 1 is an issue for precision (and how passing 0 interacts with the behavior). The code fix is very minimal (changing p < 0 to p <= 0), but making sure this doesn't break other uses and creating/adjusting tests does require some verification. All in all, it\u2019s a small change with a bit of reasoning\u2014likely a 15min\u20131hr task for a familiar engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The sample presents a concrete bug triggered by explicit input, and the paths to reproduce and verify it are clear. The only minor point is the lack of a precise specification for the desired output, but this can be reasonably inferred given the context and the function's purpose.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16569": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and precise. It gives all necessary information for a developer to understand and resolve the problem: it describes the unexpected behavior, the conditions under which it occurs, the relevant method ('add_fields') and attributes of the FormSet, the line of code with the bug, and the root cause (TypeError from comparing None with int). It also provides a code snippet that reproduces the problem and a proposal for how to fix it. An experienced engineer should have no trouble determining what is required for a solution from this description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small bug fix that requires correcting an edge case in a conditional statement. The root cause is clearly described, the affected code is pinpointed, and the solution is to add a guard for 'index is not None.' This will take a bit of reading of the relevant method surrounding logic, but for someone familiar with Django forms, it is a straightforward change that would likely take 15\u201360 minutes to fix, including verifying the fix with a test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The scenario is realistic, the bug is reproducible with the information provided, and the solution can be verified by appropriate test coverage. This makes the sample suitable for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16667": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it clearly describes the behavior that triggers the bug (passing extremely large integer values for dates via the SelectDateWidget), gives a precise traceback and Python code to reproduce the error, and points to the exact method implicated: value_from_datadict in the SelectDateWidget. It also specifies the exact lines of code where the conversion is attempted and the nature of the exception raised (OverflowError). There is sufficient information for an engineer to identify the problem and determine that the fix involves safely handling the OverflowError when converting user-supplied values to a date. The example provided is detailed enough that the engineer knows that invalid dates should not cause a server-crashing exception, and the underlying issue (lack of catching OverflowError) is well-explained.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding where user input is converted into a date object and ensuring that invalid input (such as excessively large numbers) does not crash the server. The solution is to add error handling (a try-except clause) for OverflowError in the appropriate part of the code. This is a small but meaningful code change, requiring careful reading of the relevant function and possibly minimal testing, but it does not require large code rewrites or touching many files. An experienced engineer should be able to implement and test the fix within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present: the problem is clear, the suggested outcome (avoid server crash, return validation error) is reasonable, and the necessary solution fits well in the context described. The fix can be localized to the exception handling in the relevant widget method with minimal risk or required changes elsewhere.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16950": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is detailed and provides all relevant model and admin code for the Django app, giving an explicit reproduction case. It clearly describes the erroneous behavior (IntegrityError due to unset UUIDField on inlines with the admin) and the circumstances under which it occurs. However, while the underlying bug is straightforward, the description does not deeply discuss the edge cases or desired outcome for all scenarios (such as intended behavior if multiple inlines or more complex relationships). It is, however, reasonable for an experienced engineer to infer the expected behavior: the UUIDField default should be set correctly when saving parent and child (inline) models together through the admin. Therefore, this is close to well-specified with some small ambiguities that a sensible engineer could reasonably fill in.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This bug touches on the subtle internals of Django's admin form handling, model defaults, and how inlines are saved. Understanding the full sequence of how admin inlines interact with model field defaults is non-trivial; it requires tracing through Django admin's model/form logic, particularly when structure involves multiple models and custom PK fields. The patch changes a conditional in a core Django file, signaling that the developer must localize and understand this inner process. Additionally, updating and adding relevant tests to cover all scenarios adds complexity. This is more than a trivial or quick fix\u2014it involves both debugging, patching framework logic, and supplementing tests. Estimated time 1-4 hours for someone familiar with Django internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected. The description and scenario are clear enough, especially alongside the example model/admin code. The only mild challenge is that someone unfamiliar with Django's admin internals might need longer to trace the issue, but for benchmarking coding ability in Python/Django, this is reasonable. The test patch also demonstrates what correct behavior looks like for both model and admin forms, strengthening the value of this sample.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-20859": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is quite well-specified: it provides a clear and minimal reproducible code sample that demonstrates the bug, a description of the actual outcome (TypeError when calling subfig.legend()), and an explicit expected outcome (calling subfig.legend() should work and produce a legend, i.e., SubFigure should support legends just like Figure does). The OP even suggests a specific area of code (a line in legend.py) and indicates that checking against FigureBase instead of Figure resolves the issue. This gives both the 'what' (make subfig.legend() work) and strong hints on the 'where' to look in the codebase. Precise library version and platform info are also provided. There are no significant ambiguities here for an experienced engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is quite localized: the source of the error is clearly identified as a type-check in legend.py\u2014changing the isinstance check from Figure to FigureBase. The issue is easier than average because the code path is highlighted, the logic is straightforward (just broaden the acceptable types for the parent object), and the context, including the right file, function and line range, is directly given. The only non-trivial part is confirming whether FigureBase is the proper superclass and checking for potential side effects. For someone familiar with Python and object-oriented principles, this is a 15-60 minute fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No concerns with sample suitability: The issue and its resolution are well-contained and the change is easy to test by running the supplied code. The test is clear and verifiable, and aligns directly with the issue's request. The sample is highly representative for coding benchmarks requiring reading, understanding, and small adjustment within a codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-22719": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description: (1) provides a clear minimal code example reproducing the bug (deprecation warning when plotting empty data with categorical units); (2) specifies the actual and expected outcome (either quietly produce no data or provide a clearer warning); (3) gives sufficient context regarding the underlying cause (handling of empty arrays in unit converters and the API change note); and (4) details the problematic warning and traceback. While some details about the internals (such as specific function handling) require inferring the proper location for the fix from knowledge of Matplotlib's code structure, the requirements for the solution are sensible and can be interpreted: suppress the deprecation warning for empty data or improve the warning. Thus, this is not fully \u201cThe issue is well-specified and it is clear what is required,\u201d but a competent engineer could proceed confidently with filling in a few implementation details.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core fix involves adding an additional conditional to skip or alter the deprecation warning logic when empty data is detected, as shown in the patch. This requires some understanding of how categorical units and unit conversion work in Matplotlib, but the required code change is limited (a small logic change in two places). Familiarity with the codebase helps, but the scope is contained to editing specific checks, as indicated by the gold patch. Therefore, this is a manageable task for someone experienced, requiring less than an hour and minimal edits across two functions, plus a simple test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no blockers to using this issue as a coding challenge sample. The test and gold patch show it can be expressed clearly in code and checked for correctness without ambiguity. The only difficulty is the need for some basic familiarity with Matplotlib's units handling, but the issue is otherwise clearly actionable from the provided text and patch. No external context needed. The error is visible, easy to reproduce, and fix.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, concise, and includes a minimal, executable code sample that reproduces the bug. The context describes the undesired behavior in terms of specific internal state changes (Gcf.figs being cleared) and the steps leading up to it. It also describes the expected outcome and why the current behavior is problematic (e.g., why plt.close(fig2) fails). Crucially, it specifies the environment and relevant versions, and describes the observed failure before and after calling get_backend(). There is no ambiguity about what constitutes a successful fix: the code given in the issue should pass (i.e., 'before == after' should succeed), and Gcf.figs should be unchanged by get_backend() after creating figures in rc_context. No further clarification would be needed for an experienced developer to address the problem.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding how rc_context interacts with backend selection and the internal state of figure managers (Gcf) within matplotlib. While it is not strictly trivial\u2014the developer must dig into how and why get_backend() is having side effects on Gcf when figures are created under rc_context\u2014the change itself is likely quite localized. The root cause centers on the handling of rcParams, particularly the 'backend' parameter. For someone familiar with matplotlib's architecture or willing to spend a short amount of time getting up to speed, this will require inspecting a couple of related functions and ensuring proper state preservation, but will not require broad redesign or extensive code rewrites. It's realistic to expect a solution within an hour given the information.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are evident. The issue is focused, the reproduction instructions are precise, and there is no requirement for obscure domain knowledge or additional context. This is a strong candidate for evaluating coding ability regarding debugging and bug-fixing in a mature open source library.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23476": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well specified: it describes a clear bug (the DPI value of a Matplotlib figure doubles after unpickling repeatedly on an M1 Mac) and includes minimal but complete code to reproduce the error, expected and actual outcomes, and relevant environment details (OS, Python, library version, backend). The intended correct behavior is given with explicit expectations in output. There are clear success criteria: after unpickling, DPI should not change from the original value. It is a technical bug report with precise reproducibility and a well-defined fix scope.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is fairly localized: it concerns state management and serialization/deserialization logic of figure DPI in Matplotlib when working with pickling. With sufficient familiarity with the Matplotlib codebase, an engineer would likely spend 15-60 minutes identifying where the DPI is set on unpickling, why it is changing, and implementing a fix to preserve the original DPI. The scope of changes is small, affecting only the serialization logic and a focused test. It is not entirely trivial because the engineer needs to reason about the interaction between DPI and device pixel ratio, but there are clear breadcrumbs in the issue description.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The sample is suitable for coding ability evaluation, as the requirements and bug reproduction are clear, and a code/test fix can be directly validated. No ambiguity remains.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24026": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, well-motivated, and provides both context and a reproducible example. It explains the user workflow that is failing: using color cycler alias ('C0', 'C1', etc.) for colors in different plot types, works for plot and Rectangle, but fails on stackplot because stackplot tries to set the Axes property cycler with the color aliases, which is not supported and raises a ValueError. The exact error scenario is described, including the ValueError message. The expected behavior is clear: stackplot should accept color aliases directly as colors, to be consistent with other pyplot primitives, and not try to update the property cycler using these references. The code example is concise and illustrates the issue precisely. This is sufficient information for a capable engineer to identify and produce a correct solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change involves a moderate understanding of the matplotlib stackplot function and how color cycles and color strings/aliases are handled. Locating where stackplot tries to set the property cycler and updating the logic to avoid putting cycle references ('C*') into the cycler does not require a major architectural refactor or dealing with complex interactions, but it does require reading and understanding the function and being aware of matplotlib color handling conventions. Since the change is localized (the stackplot function), the impact is clear, and the code edits are small but non-trivial. An experienced engineer, after reviewing the affected code, should be able to implement, test, and commit the solution within 15-60 minutes. The matching test patch also reflects how localized the change is.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The description is clear, the reproduction is feasible, and the test scenario directly follows from the issue. The problem is understandable and well-scoped for benchmarking coding ability closely related to color handling and property cycles in matplotlib.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24149": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly describes the observed bug: that ax.bar([np.nan], [np.nan]) raises an exception in matplotlib 3.6.1, breaking downstream usage in seaborn. The reproduction code is given and distinctions between prior and current (broken) behavior are carefully spelled out (3.6.0 returns BarCollection, 3.6.1 raises). The precise scenario is clear, including what is expected and what happens instead. Additional details about root causes and related debugging are provided (e.g., the StopIteration, differences between x and y NaNs). An experienced engineer would be able to identify what needs to be fixed, in which area, and how to verify correctness with the provided information. No significant ambiguity exists in what is being requested. The reference to specific behavior also points to the area needing investigation without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is small but requires a careful examination of the code handling NaN values in ax.bar, particularly the helper function that attempts to find the first finite element. The actual patch is localized: adding exception handling for StopIteration and falling back on the first element if no finite values exist. The scope is limited to a single function in a single file. An experienced engineer who is familiar with the matplotlib codebase would likely be able to identify the relevant area and implement this fix in 15-60 minutes, including testing. The change is not entirely trivial since it requires understanding the correct fallback for the edge case, but it is not complex or broad enough to take multiple hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is good for benchmarking: it involves debugging, edge case reasoning, and a targeted change. There are no major blockers or missing context. The issue is clear, tightly scoped, and has specific correctness criteria.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24970": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified. It provides a concrete code example that produces deprecation warnings in NumPy >=1.24, and explains the cause: matplotlib code does something that causes out-of-range integer conversion to uint8, which will not be allowed in the future. The expected behavior is clear (no warnings). However, while the issue points clearly to the warning, it doesn't explicitly pinpoint the code location or implementation details in matplotlib. The expected result ('no warnings') is somewhat interpretable since it refers to eliminating the deprecation warning, likely via code changes to avoid deprecated behavior, but some blanks may need to be filled regarding which functions are involved and the specifics of the fix. Still, it is quite a direct and actionable engineering issue for someone familiar with this codebase, and provides enough reproduction context to proceed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue would require an engineer to identify where in matplotlib uint8 conversions with potentially out-of-range values happen as part of the colormap handling code, and to adjust the code so that it avoids deprecated behavior in newer NumPy versions. The code sample and resulting warning make it easy to locally reproduce the problem using matplotlib and numpy, and the change is likely limited to a single or a few functions (as revealed in the patch). It requires some understanding of dtype operations, but is small in code footprint and complexity; thus, it would likely take between 15 minutes to 1 hour for a competent engineer to resolve.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers or missing elements in the issue description. The environment details and versioning info are complete, and the code to reproduce is minimal and specific. The only slight gap is the typical lack of code pointers, but reproduction is easy, so it shouldn't prevent a benchmarking scenario.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25311": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: there is a clear code sample that reproduces the problem (attempting to pickle a matplotlib Figure object after making the legend draggable raises a pickling error). The expected outcome is also stated: pickling should be successful. The actual error type (TypeError about non-serializable FigureCanvasQTAgg) is specified. This precisely scopes the bug and clarifies what constitutes resolution: after the fix, the provided code should succeed in pickling the figure. The required area of change (figure/legend interactivity and object serialization) is implied by the minimal but complete reproduction. No further clarification is needed to attempt a response or to judge a successful PR.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This bug requires understanding of how matplotlib manages interactivity and serialization, especially for interactive components like legends and their references to non-picklable objects (e.g., GUI backends). While it may take some investigation to identify that a direct object reference is preventing picklability and to refactor it safely (switching an attribute to a property), the required code change is relatively small and localized. For someone familiar with the codebase, after identifying the culprit, it is a focused fix likely to take between 15 minutes and 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major uncovered issues. The specification is sufficient, the code patch is focused, and the test patch shows an independent test verifying the solution. The only requirement is basic familiarity with Python's pickling mechanics and how matplotlib manages references to backend objects.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25332": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It includes a clear and minimal code sample to reproduce the bug (inability to pickle a figure after calling align_labels()), specifies the expected and actual outcomes (successful pickling vs. TypeError due to weakref.ReferenceType), and lists all necessary setup details (matplotlib version, OS, etc). An experienced maintainer can reasonably determine that the task is to ensure that calling fig.align_labels() does not introduce objects that break figure pickling, and that after the fix, the same code should work. The code and error message are sufficient to know where to start seeking the error and what success looks like (no TypeError, pickling successful after align_labels).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires understanding how Matplotlib's align_labels() affects the figure and why pickling fails\u2014specifically, tracking down the introduction of non-pickleable weakref objects in the alignment machinery. It likely requires some investigation into the code handling label alignment and potentially the serialization (__getstate__/__setstate__) of figure internals. While not a simple one-line fix, it is unlikely to require large-scale refactoring or deep architectural changes, but rather an extension to existing serialization logic. For an experienced engineer with knowledge of Matplotlib's architecture, this should be solvable in 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major concerns regarding suitability. The issue is specific, reproducible, and bounded. The original patch directly addresses the serialization of weak references, which was causing the pickling issue. Provided test patch modifies existing pickle-related tests to include this scenario, covering the regression. The issue is appropriate as a benchmark for coding ability involving bug diagnosis and serialization fixes.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25479": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified: it describes a scenario where a user creates a colormap with a particular internal name (from the object) and registers it under a different user-facing name using cm.register_cmap(), but matplotlib's pyplot interface uses the internal name, causing confusion and errors. The report provides working and failing code examples, describes the expected behavior (ability to reference by the registered name), and describes the observed error. Some blanks remain (such as precise locations in the code to fix, or whether there are other internal invariants that should be considered), but for an experienced engineer familiar with matplotlib, it is clear the lookup/registration logic should be fixed so that registered colormap names become the single canonical lookup and user-facing names. There is a sensible and direct way to interpret and address the issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding the colormap registration and lookup logic in matplotlib, and changing the registration process to ensure that a colormap registered under a new name can always be referenced by this user-facing name (instead of its internal name, which could differ). The code patch is small (modifying only a few lines), and the conceptual fix\u2014setting the registered name appropriately on registration and updating related equality logic\u2014is straightforward for a developer familiar with the code base. Implementing and testing the fix should take about 15-60 minutes for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues: the problem, patch, and corresponding test are all present and aligned. The only mild caveat is that a bit of prior knowledge of matplotlib's colormap subsystem is helpful, but this is expected for benchmark tasks using real-world libraries.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-26291": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The description provides (1) a minimal code example that triggers the bug, (2) the actual traceback error message and its context (AttributeError: 'NoneType' object has no attribute '_get_renderer' during rendering), (3) a detailed explanation of what specifically is failing (inset_axes in axes_grid1.inset_locator), and (4) a clear expected outcome (the user wants to create an inset axes as per the documented example, not get an exception). The environment details are also provided for context. An engineer familiar with Matplotlib can reproduce, understand, and attempt to resolve the issue based on this description, and the contained information is sufficient to define both the problem and the requirements for a correct fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug fix involves understanding the rendering pipeline and ensuring the renderer is correctly initialized in the context of inset_axes calls. The solution is conceptually straightforward and localized\u2014a small change to provide or initialize the renderer if it is None. The code change itself is concise, but requires reading and understanding relevant parts of the codebase such as inset_locator.py and how axes and renderers interact. An experienced developer could reproduce and fix this issue in 15 minutes to 1 hour, assuming basic familiarity with this part of Matplotlib.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major additional issues: the code and test patches show that the fix is clear and verifiable, and the problem is well-isolated without dependencies on ambiguous requirements. The original test covers the failure mode described.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-1724": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite precise. It specifies a UnicodeDecodeError occurring in Python 2.7.2 when the method argument is given as a Unicode string (u'POST') rather than a byte string ('POST') in requests.request. The reporter gives specific, copy-pastable code snippets for both working and failing cases; it also provides an analysis suggesting why the bug occurs (Unicode infecting the HTTP header generation) and references a specific line in sessions.py as a likely cause (req.method = method.upper()). The minimal reproduction example, error details, and a clear success criterion (no exception and successful request for Unicode method argument) means an engineer can address this issue without needing clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding how Unicode and byte strings are handled in Python 2, plus familiarity with how requests and httplib operate. However, the problem is tightly scoped to the handling of the method parameter in requests.request; it does not require extensive code restructuring, only tightening type conversion to bytes. Locating the fix and writing/regression test should be achievable in 15-60 minutes for someone familiar with Python 2 string types and the requests codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is tightly scoped, has a clear reproduction case, a reasonable fix, and measurable correctness. The sample is appropriate for benchmark evaluation of coding ability for engineers working in Python 2 legacy code.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-5414": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes the unexpected exception (UnicodeError) encountered when making a request to a URL with an invalid hostname (e.g. http://.example.com), and it states the desired behavior (raise InvalidUrl, ideally with a helpful message about an invalid label). It gives a clear reproduction case in code, an example of the expected message, and notes relevant lines in the codebase where similar handling takes place. An experienced engineer would understand what outcome is desired and where to apply a fix, as well as how to test that the fix behaves as intended. All required information is present to attempt a meaningful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is quite localized: it involves identifying where the UnicodeError exception is currently raised and ensuring it is translated to the correct InvalidUrl exception, possibly by editing/expanding the exception handling logic. The additional requirement is to improve the specificity of the error. Understanding the codebase to make this change and updating/adding tests is relatively straightforward for an experienced engineer. There are no indications of a need for broad refactoring or complex logic, but it requires enough care to identify the correct exception flow and add tests, placing it squarely in the '15 min - 1 hour' range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues or concerns. The sample is clear, relevant, concise and the solution is well-bounded. It is an excellent benchmark for coding ability in the maintenance/fixing context.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3151": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue describes a concrete failure: using yCoord = ['a', 'c', 'b'] with combine_by_coords triggers a ValueError because of non-monotonic indexes when combining datasets. It specifies the triggering input and the symptom (exception raised). However, it does not specify whether non-monotonic labels should always be permitted or under what conditions they are acceptable. There is ambiguity about what the desired behavior should be, but a sensible interpretation is that the bystander dimension (in this case y) can have non-monotonic order if it doesn't affect the merge along the concatenated dimension. An experienced engineer could reasonably infer what is needed, though some blanks remain about generalizing the fix and ensuring correctness. Hence, I rate it a 1: there are some blanks to fill in, but a sensible solution can be devised.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding the combine_by_coords implementation and why it fails when one dimension label is non-monotonic. Solving involves adjusting the code to check monotonicity only for the concatenation dimension rather than for all dimensions, and ensuring other bystander dimensions don\u2019t falsely trigger the error. The expected code change is relatively small and localized. For someone familiar with the codebase or with some setup time, the change can be made within an hour, probably on the lower end of that range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified that would prevent this sample from being used to evaluate coding ability. The core of the task is interpreting what dimensions need monotonicity checks, which is reasonably clear from the context and symptoms described, and the fix is specific, testable, and meaningful in terms of reasoning and code editing.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3677": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified: it outlines the behavior of the top-level 'merge' function versus the 'Dataset.merge' method, provides a code example showing the failure, and explicitly describes the error (AttributeError due to calling 'items()' on a DataArray). The implication is that the behavior should be consistent\u2014the method should support merging with a DataArray just as the function does. No further clarification is needed: the engineer is expected to update the method to handle DataArray inputs appropriately. The filenames (likely core/dataset.py) and methods involved are also clearly implicated in the description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively small change that requires modifying an existing method to check the input type and convert a DataArray into a Dataset if necessary. The scope is limited to the merge method and its relevant logic, with minimal impact elsewhere. The time and cognitive load are on the low to mid end for a codebase of this size\u2014mainly requiring understanding of current merge behaviors and a cautious change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue sample is suitable for benchmarking coding ability: it is clear, self-contained, and directly testable. The test case provided in the original PR confirms that the new behavior is as expected. No external context or undocumented details are required to resolve it.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4094": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it describes a bug where attempting to unstack a stacked array using `to_unstacked_dataset` fails for single-dimension variables, giving a MergeError. The author includes a minimal code example showing how to reproduce the bug, what happens, and what should happen instead (\\\"a working roundtrip\\\"). The essential context and affected API (`to_stacked_array`, `to_unstacked_dataset`) are directly named, and the failure is local to this workflow. While the issue could describe the desired 'correct' behavior a bit more fully and could mention whether metadata, order, or dtypes matter upon roundtrip, there's a very sensible interpretation: the round-trip from dataset -> stacked -> unstacked should return the original dataset (or something very close to it). Any experienced engineer working in xarray would know what is expected. Overall, one must fill in minor blanks, but it's clear what needs fixing and how to verify success.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem involves fixing the round-trip operation between stacking and unstacking a dataset in xarray. The failure is well-isolated and easy to reproduce, with a minimal code sample already provided. The cause is fairly likely to be a missing or incorrectly specified argument during selection or reconstruction of the dataset, making the fix likely to be a one-line change once tracked down, as evidenced by the patch. However, localizing the exact fix may require reading and reasoning about the code, so it's not completely trivial (i.e., <15 min), but it is not complex enough to be a multi-hour change for someone familiar with xarray. Thus, 15 min - 1 hour is appropriate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is suitable for the benchmark. The only slight caveat is that the issue could mention more explicitly that an exact roundtrip (dataset equality) is the desired output, and whether or not order or metadata matters, but the intent is clear enough for evaluation. Test coverage for this case is explicitly added in the test patch, ensuring evaluation is robust. No blockers identified.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4695": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it provides a clear, reproducible code sample, explicitly describes the unexpected behavior (an error occurs when the dimension is named 'method' and used with .loc), and gives context for the expected behavior (dimension name should be irrelevant to .loc). The text specifically says that the name collision is causing \\\"method\\\" to be interpreted not as a key but as the \\\"method\\\" parameter in .loc, leading to a ValueError. The Python and library versions are also noted for context. An experienced engineer can accurately diagnose and target the problem, and understand the intent of the required fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a modest code change: ensuring that .loc (likely via __getitem__ or sel) correctly distinguishes when a dimension name collides with parameter names, and doesn't misinterpret the key. The locus of the change is relatively simple and the root cause is easy to isolate thanks to the clear reproduction. The diff confirms that the fix is a small tweak in how the sel method is called to avoid passing 'method' as a kwarg. This would not take extensive time or require major refactoring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues present. The test added would ensure the bug is caught in the future, and the scenario is representative for evaluating bugfixing skill. The benchmark setup is suitable for this issue, since it evaluates a candidate\u2019s ability to diagnose a subtle but well-specified API behavior problem.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6599": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well specified. The user provides a clear and reproducible code sample that triggers the bug, as well as both the actual (erroneous) and expected (correct) output. The issue is isolated to a single function: 'polyval' in xarray. The description includes detailed version info, observed differences between specific releases, and a detailed explanation of how the output is incorrect (dimension swaps and vastly different output magnitudes). From the description, it is clear that 'polyval' is mis-handling input DataArrays with dtype 'timedelta64', treating their raw nanosecond values improperly during polynomial evaluation. To fix the issue, a solution should ensure that timedelta64 coordinates are properly cast to a numeric type in a way consistent with prior versions, yielding output consistent with the expected behavior. The scope and nature of the required change is well described and testable via the provided MVCE.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a small change that requires a bit of thought. The developer needs to understand how polyfit and polyval treat date/time and timedelta64 dtypes and ensure consistent behavior between versions by handling dtype conversion. Finding the correct place to convert or cast timedelta64 to the appropriate float representation requires reading through or understanding the relevant function internals (in this case, _ensure_numeric/data type handling in computation.py). The logic isn't completely trivial but also not very complex or spread across multiple files, and only requires a small well-targeted edit.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present that would disqualify this issue for use as a coding benchmark. The issue definition is clear, includes explicit testable code, and the solution is concise yet meaningful, requiring understanding of both the user-facing API and lower-level dtype handling.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6721": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and specific about the problem: accessing ds.chunks on an unchunked but lazily-loaded Zarr-backed xarray object causes the entire dataset to be loaded into memory. The example code, expected (lightweight/informational) behavior, and the actual problematic behavior (all data fetched over the network due to NumPy array conversion), are all described in detail. The relationship to the underlying library mechanics is discussed, and the problem is framed as a regression or behavioral bug. The expectation for correct behavior is explicit: ds.chunks should only inspect metadata, not read all data. Combined with the environment info, an engineer can reproduce the problem and knows what constitutes a correct solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change required is relatively small (one line replacing v.data with v._data), and the core of the solution involves understanding the appropriate attribute to access (._data instead of .data to avoid forcing data loading). This would take some codebase familiarity and examination of the difference between those attributes, but would likely take less than an hour, especially since the bug is easily reproducible and the fix is localized.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major concerns. The only caveat is reliance on familiarity with xarray and its data model (._data vs .data), but this is within the expected domain knowledge for a coding benchmark targeting experienced engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6386": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the bug: when using the short option '-v' for 'verbose' in pylint, the tool expects an argument, which leads to an incorrect error message and behavior. The text also specifies that the long option '--verbose' does not require an argument and works as expected, so the short option should do the same. The commands and outputs are included, making the expected fix and correct behavior explicit.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding how command-line options are parsed and handled in pylint. The engineer must locate where the 'verbose' option is defined and processed, and ensure the short form '-v' matches the long form's behavior by not requiring an argument. The patch involves modest code edits in the argument/configuration logic across a couple of files. While it's a multi-file change, it's not conceptually complex or large\u2014likely solvable in under an hour by someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be other issues with this sample. The bug is described in a straightforward way, and the expected result as well as reproduction steps are provided. The engineer does not need excessive domain knowledge or access to information not in the issue text. The sample is suitable for benchmarking coding ability as described.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6903": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides clear information about the root cause (multiprocessing receives a count of 0 due to miscalculation in _query_cpu), how it happens (in K8s with certain cgroup values resulting in cpu.shares=2 and period_us=100000), and the expected behavior (should never return 0 CPUs, default to 1 if calculation is 0). The issue also suggests a simple way to solve the problem. Specific filenames and functions involved (pylint.lint.run._query_cpu, run.py#L60) are named, making this issue clear and actionable for an engineer, even without discussion or further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Solving this issue requires identifying where the CPU count can become zero, recognizing that it needs to default to 1, and adding a small guard/fix in the code. The engineer may need a little time to understand the context and confirm whether the same logic should be applied elsewhere, but the main change is conceptually straightforward and small in scope. This fits well within the 15-min to 1-hour complexity range for an experienced contributor familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no critical blockers for this issue. The only ambiguity is whether similar logic is needed in other places, but the issue highlights this, so a reasonable engineer would check. Nothing prevents this from being a strong benchmark sample for coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-8898": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and provides clear information regarding the problem and the steps to reproduce it. The problem is that the bad-name-rgxs configuration for pylint is incorrectly split on commas, so regex patterns that themselves contain commas (such as quantifiers like {1,3}) are misinterpreted and cause pylint to crash. Concrete examples and the expected behavior are described explicitly, and the user provides configuration values and shows the erroneous behavior. The issue also offers hints at potential solutions (e.g., allowing escaping or otherwise fixing the behavior), which further clarifies the requirement. An experienced engineer would understand the root cause and what is necessary for a successful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires some knowledge of how the configuration is parsed and how to properly split regular expressions that may contain commas, but only a small, targeted change to the config parsing utility is needed. The provided patch demonstrates that the correct fix involves writing a smart split function (handling commas not inside braces). It affects a few functions and requires writing or updating tests. For an experienced engineer familiar with Python and parsing, this could be solved in 15 to 60 minutes, depending on familiarity with the codebase and desired thoroughness in test additions.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is appropriate for evaluating coding ability involving parsing logic, regular expressions, and understanding user configuration expectations. The test patch shows a clear, automated way to check correctness. No major issues or ambiguities remain that would disqualify this for use in a coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10051": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clearly explains both the problematic behavior and what the desired behavior should be. It identifies the conflicting relationship between caplog.get_records() and caplog.clear(), referencing relevant lines in the code where their interaction causes the divergence. The reproducible example demonstrates the failed assertion and pinpoints exactly what should be fixed: after calling caplog.clear(), get_records() should behave consistently with caplog.records (i.e., both should be cleared and reflect new log events consistently). The context is fully self-contained, and it is easy for an experienced engineer to interpret what a successful solution is.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue is clear in what needs to be fixed\u2014a synchronization problem between caplog.get_records() and caplog.records when clear() is called. This requires a small change to how records are managed by caplog and its handler (ensuring clear() resets both appropriately). The solution only involves minor code changes and understanding the structure, so an experienced engineer familiarized with the codebase should be able to resolve this in 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues stand out. The issue is clearly described, self-contained, and the fix will be covered verifiably by the provided test. This sample is suitable for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10081": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It explains the exact scenario (pytest with --pdb runs tearDown for unittest.TestCase classes decorated with unittest.skip), gives a minimal reproduction (test_repro_skip_class.py), specifies relevant versions, and provides console outputs both for the expected and faulty cases. The error observed (unexpected execution of tearDown during skipped tests in --pdb mode) is described, including the reason it is noticed (NameError due to placeholder code). The problem is narrowed down specifically to skipped classes, as opposed to individual test methods, and the relationship to a previous similar issue at the function level is cited. An experienced developer with access to the codebase would know exactly what to reproduce and what constitutes a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty level is 1 (15 min - 1 hour) because the issue involves adjusting logic in the pytest codebase that decides whether to run tearDown for skipped unittest classes under --pdb. The root cause has already been triangulated by the issue reporter, and a minimal reproduction is provided. The fix relates to guarding a specific teardown call, which should be achievable with a small conditional logic change after referencing the relevant code handling skips. While familiarity with pytest's internal handling of skips is helpful, it is not a large or highly complex change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being suitable for the benchmark. The issue is clear, concrete, and verifiable with tests, and does not require external clarification or undocumented context.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5262": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and provides both a detailed explanation of the bug and a concrete, minimal reproducible example. It pinpoints the unexpected behavior that occurs when pytest is run with youtube-dl: specifically, that _pytest.capture.EncodedFile incorrectly exposes a file mode containing 'b', causing client code to treat the stream as binary, whereas its write method requires text. This leads to a predictable TypeError. The requirements for a solution are specified: EncodedFile's .mode property should not include 'b', and it should represent itself as a text stream. The provided stack trace and reproduction details let an engineer quickly verify the problem and test the fix. References to specific objects/class names (_pytest.capture.EncodedFile, .mode, .write) further clarify the scope of the fix. The example test.py is executable and will demonstrate the bug, given the versions listed. Altogether, the issue is well specified and actionable for an experienced engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial bug fix: the solution involves overriding or correcting the .mode property in the EncodedFile wrapper so that it accurately describes the object as text and does not contain 'b'. It requires understanding Python's file interface, subclassing/wrapping patterns, and handling the interaction with third-party code that introspects file attributes. However, the issue is tightly scoped to a single property and its interaction, and the fix involves a small code change (as the gold patch confirms). Familiarity with the relevant class will allow an experienced engineer to resolve it within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue is suitable for evaluating coding ability: it requires careful reading, identifying a mismatch in interface expectation, and implementing a wrapper property. The engineer must understand Python IO text-vs-binary mechanics and property overriding but does not require knowledge beyond what is typical for a Python engineer. No additional issues present. The contextual information provided (test, pip list, minimal example) further improves reproducibility.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5631": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly explains a regression involving pytest, unittest.mock.patch, and numpy arrays. It details the stack trace and underlying problem (a 'ValueError' due to improper boolean evaluation of numpy arrays during internal sentinel checks after an upstream change). While it doesn't specify exactly which function should be changed, it provides enough technical context (repro code, version numbers, and a clear description of \\\"what goes wrong\\\") for an experienced engineer familiar with the pytest source to locate the problematic logic and devise a solution. Some minor blanks (where exactly the fix should go) must be filled in, but the requirements for a successful solution (avoid boolean context comparisons with potentially array-like values and fix the sentinel logic) are clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug fix is modest in scope: it requires adjusting equality/identity comparison logic to avoid ambiguous boolean comparisons with numpy-like objects when checking patch sentinels. This involves identifying where the check occurs and updating the logic, likely in a single function. The core challenge is understanding why the ValueError arises and devising a safer way to check for sentinels, but this should not take more than an hour for someone familiar with the codebase. The patch final solution is concise and restricted to one file.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue provides a clear minimal reproducible example and the use case is relevant to realistic testing scenarios. The test patch also sensibly verifies the fix. This is a valid coding benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5787": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and includes a clear explanation with step-by-step reproduction tests. The user describes two minimal test cases for chained exceptions, describes expected behavior both with and without xdist, and notes a regression (or missing feature) in the xdist output. The logic of what should be displayed is spelled out in plain language: 'pytest shows the entire chain', but xdist shows only the last exception. There is enough detail for an experienced engineer to both reproduce, diagnose, and understand the required behavior, which is to ensure exception chains are displayed/serialized correctly when running with xdist. No ambiguous requirements, and explicit stack traces/outputs expected. The environment (pytest versions) is specified as well. Any engineer familiar with pytest would have a clear specification of the requirements here.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue required editing multiple files and dealing with the serialization and deserialization of complex exception objects, including chained exceptions\u2014which are not trivial data structures. The solution involves moving/refactoring existing logic (de-duplicating from xdist code into core pytest), understanding the serialization format, and updating both code and involved tests (including adding new tests to confirm correct handling of chained exceptions, likely requiring a deep understanding of both representation and reporting in pytest). The fix is not merely a few lines\u2014it involves understanding the flow throughout pytest reporting, modifying both internal interfaces and public-facing API (output), and ensuring no breakage. While not a huge rewrite or extremely esoteric, it's a solid medium-complexity feature/fix that would take 1-4 hours for a skilled developer already familiar with the project, especially given the need to update and check several parts of code and corresponding tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues\u2014the specification and required fix align well. The test and code patches support automated checking in a benchmark scenario. The sample is suitable for use in evaluating coding ability, as it exercises serialization, detailed debugging, test writing, and understanding of core data flows in a moderately challenging open-source Python project. The issue itself is a good example of both a real bug and a feature requirement, with clear criteria for success.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5809": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear about what the defect is, the conditions that trigger it, and how it can be fixed. It specifies that the use of \\\"lexer=python3\\\" in the pastebin feature causes HTTP 400 errors from bpaste.net for certain data, and that changing the lexer to \\\"text\\\" resolves the issue. It also provides a rational explanation for the fix, indicating that pytest output is arbitrary text, not Python code. The file and code context are provided via links, so an engineer would understand where to make changes. As such, there are no significant ambiguities or missing details.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This fix is a straightforward parameter change: replacing the value of a dictionary key from \\\"python3\\\" to \\\"text\\\" for the 'lexer' argument before posting to bpaste.net. The description, rationale, and expected output are all clear, and the change touches a single location. The engineer just needs to identify where this parameter is set in the code and adjust it, which is trivial for an experienced developer familiar with Python codebases.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent the use of this sample for evaluating coding ability. The test changes are straightforward and directly related to the parameter change, and the issue is isolated, reproducible, and clear. No additional context from the repo is necessary to understand or solve the problem.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5840": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides the relevant context: after upgrading pytest from 5.1.1 to 5.1.2 on Windows, the path is forced into lowercase, causing the import system to look for a non-existent top-level module ('python'). The core symptoms are clear and reproducible, and the effect (import failure for conftest.py) is explained. However, there is some ambiguity: the exact mechanism that leads to the path lowercasing and exactly where in the codebase this takes place is not spelled out, nor is the precise expected new behavior beyond 'should not lowercase the path'. It is up to the engineer to determine the best way to resolve this Windows path casing issue in pytest's import handling. Thus, while there are a few gaps, they are minimal and the requirements are interpretable for a reasonably well-scoped fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix involves changes across multiple files, including introducing a more robust solution for path handling compatibility on Windows (case-insensitive filesystem), and updating related logic and tests. The root cause analysis and ensuring no regressions in import logic for conftest.py modules may take 1-4 hours, especially considering the nuances of platform differences and rigorous testing required for compatibility. It's not a trivial one-line fix, but not a huge refactor either.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues detected. The scenario is relevant for real-world cross-platform testing and exercises the candidate's ability to reason about filesystem paths, Windows/Linux compatibility, and subtle import mechanics. The test coverage also seems sufficient. The test and code changes adequately check that the regression is fixed and should be applicable for evaluating coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6197": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified. The reporter gives a minimal reproducible example, details the environment, and explains the precise regression: pytest is attempting to import __init__.py files during test collection under certain circumstances, causing errors if those files are not importable on the current platform. It is clear that the expected solution should prevent pytest from importing unrelated __init__.py files during test collection unless necessary. However, it does leave some implementation details up to the developer (for example, exactly when and how __init__.py files should be imported, and whether that import may ever be desirable). This means the issue is slightly open to interpretation, but a sensible solution can be attempted from the information provided.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a good understanding of how pytest collects test modules and packages, and why __init__.py files are imported during collection. Debugging the minimal example, tracing test collection logic, and understanding internal design decisions (especially related to import mechanisms) requires substantial codebase familiarity. The solution involves editing core collection logic and writing/adjusting tests, which takes significant analysis and careful coding to avoid breaking normal package test discovery. This is not trivial, but also does not seem to require days of research.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The sample is legitimate for benchmarking coding ability in the context of regression fixes and framework internals. The issue is technical but ultimately clear and grounded in realistic user impact, and the associated tests directly validate the regression fix.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6202": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and unambiguous. The user identifies that the string replacement '.[' to '[' in getmodpath (in src/_pytest/python.py) is causing an incorrect test headline, specifically changing 'test_boo[.[]' to 'test_boo[[]'. They explain they traced the problem to 'return s.replace(\\\".[\\\", \\\"[\\\")' and propose that simply 'return s' is correct. The example test case, the filenames, and line references all make clear what needs to be fixed. There is a clear description of the undesirable behavior, a proposed fix, and explicit mention of success/failure criteria (tests/linting). The test patch further shows the exact behavioral change expected. All this makes the issue very clear and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the issue requires familiarity with pytest's internals and understanding parameterized test nomenclature, the actual code change is limited and clearly scoped (removing a string replacement in getmodpath). The most tricky part is tracing the code path and validating that removing the string replacement doesn't break other behavior, which is what an engineer would likely spend time confirming. The implementation itself is just removing/replacing a single line. This places it in the realm of a small change requiring a bit of thought (15 min - 1 hour).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue is a good benchmark sample: it involves tracking down a specific section of code causing a bug, has a self-contained and well-explained fix, and tests to verify the outcome. The comprehensive references in the issue and the test patch further support its suitability.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7205": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly states the version of Python and pytest involved, provides an example test case that triggers the issue, describes the command to reproduce the failure (with the '-bb' flag to turn BytesWarning into errors), and explains the underlying cause (the code uses str() implicitly on a bytes object). Furthermore, the reporter suggests a concrete solution direction ('Shouldn't that be using saferepr or something rather than (implicitly) str()?'). The file and line where the error occurs is also specified. For someone familiar with pytest's codebase, all the information needed to locate, understand, and resolve the problem is present.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required is localized and clear: replace the incorrect conversion of a parameter (using str()) with a safer representation (using saferepr) to avoid BytesWarning, as suggested in the issue. The required understanding of fixture setup reporting and the relevant file (src/_pytest/setuponly.py) is straightforward for an engineer familiar with the codebase. This kind of bug fix, though it requires awareness of corner cases in Python 3, is not complex: it is a 15-60 minute task to implement and verify, especially with the provided reproducible example and explicit pointer to the fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues. The issue is suitable for evaluating coding ability in the proposed benchmark setup. It has a clear input, well-scoped context, and an objectively testable solution (as evidenced by the associated test patch).\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7236": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it describes the environment, the minimal test case, the current (buggy) behavior (tearDown is called on skipped unittest.TestCase tests when running pytest with --pdb), the expected behavior (tearDown and setUp should NOT be called for skipped tests, even when --pdb is set), and the precise regression window (between pytest 5.4.1 and 5.4.2). It also describes the consequence (NameError, unexpected debugger invocation). There is no ambiguity about which behavior is expected, and the reproduction steps are clear. An experienced engineer could directly implement and test a fix based solely on this information, without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue involves a fairly small change in logic to prevent tearDown being executed for skipped tests when --pdb is set. The primary challenge is determining the correct skip state and ensuring this logic is coordinated in the internal flow of pytest's unittest plugin. However, the test case is minimal, the expected and erroneous behaviors are clear, and the fix as shown in the patch is localized and conceptually straightforward. Familiarity with pytest's test collection and execution models is moderately helpful, but the engineering difficulty is in the 'requires a bit of thought' range, not much more.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are apparent. The issue is specific, concrete and testable, and the minimal reproducing example provided in the description ensures the regression is measurable. The expected behavior and requirements are unambiguous. This is a suitable benchmark case for coding ability with no disqualifying factors.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7324": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a clear minimal reproducer, the exact symptom (assertion failure in compile.c:3559 due to the usage of expression.compile(\\\"False\\\")), and hints at the underlying cause (usage of 'False' as a name identifier is illegal in this context for Python 3.8+). While it references related issues elsewhere, all critical information needed to understand and address the issue is present in the description. However, a small gap exists in that it does not explicitly specify what the correct, fixed behavior should be (e.g., should 'False' be allowed, or handled differently?), so there is a minor judgment call required. But for an experienced engineer, the path forward is reasonably apparent: prevent the crash by not misusing reserved Python keywords/identifiers in the AST transformation process.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue is non-trivial but localized: it requires understanding how the expression parsing and AST generation works in pytest's mark expression system, specifically to translate match expressions so that reserved identifiers (True, False, None) are not used in places where Python prohibits them. The actual fix, as shown, involves prefixing such identifiers during the conversion to Python AST, impacting a small region of code. For an engineer familiar with AST manipulation and the pytest codebase, this represents a small, contained change with a bit of head-scratching, easily achievable in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues disqualifying this sample. The reproducer is minimal and the failure is specific, and the scope of the underlying bug is clear for evaluation of coding ability. The minor judgment required about the specification (how to handle reserved identifiers) actually makes it a reasonable real-world style challenge compared to overly prescriptive issues.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7490": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is quite specific: it describes a regression in pytest's handling of dynamically added xfail markers (via request.node.add_marker(pytest.mark.xfail)) between versions 5.x and 6.x. It includes a minimal reproducible test, explicitly says what the behavior used to be, and what it is now, and what the expected behavior is. However, it doesn't explain why the underlying behavior changed or if there are architectural constraints, so there is some need for the engineer to understand how pytest handles marks (and the relevant code paths) to design a robust solution. Still, for someone familiar with the codebase, the practical, test-based expectations are clear. The engineer will need to investigate handling of dynamic xfail marks during test runtime.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue is not a trivial fix: it requires understanding how pytest internally manages xfail/marks, and why this regression occurred between versions. Solving it involves not just restoring behavior, but ensuring correct mark evaluation both at collection and during runtime, which can involve multiple hooks and internal state management. While the code patch is relatively short, it demonstrates non-trivial understanding and a need to update or reason about pytest's test running core. It would likely take at least an hour or two for an experienced contributor to comprehend and fix, especially with investigation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues stand out. The issue is about a specific regression, and expects a clear behavioral change validated by a focused test. The use case is well illustrated. The sample is suitable for evaluating coding ability, particularly around debugging and regression fixing in mature codebases.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7521": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and gives a clear problem statement. The reporter provides detailed context: the pytest version upgrade, the specific test that fails, and the observed versus expected behavior. The distilled reproducer further isolates the problem: when using capfd.readouterr(), carriage returns ('\\\\r') are converted to newlines ('\\\\n'), differing from previous pytest behavior. The failing assertion's code, the input, and expected output are all made clear. Even without knowledge of the PR, an experienced contributor would know that the capfd capturing mechanism needs to preserve written '\\\\r' characters, as in prior versions. File/module names (likely capfd/capture functionality in pytest) are referenced, and the scope is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires knowledge of how Python file I/O translates line endings and how pytest's capfd fixture captures output. The root cause is likely related to the mode used for capturing output streams or the newline setting. Fixing this is not trivial but would not require more than an hour for someone familiar with pytest internals: examining the change from one version to another, understanding the capturing implementation, and ensuring compatibility. The solution is a one-line change but finding the required newline handling and regression might require moderate code exploration.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present beyond what is described in the prompt. The issue, context, and expected output are well-defined, allowing for objective evaluation in a benchmark setting. The only challenge is technical investigation of file I/O settings and the capfd implementation.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The problem is clearly described: the RidgeClassifierCV class in sklearn.linear_model does not accept the store_cv_values parameter, leading to a TypeError despite documentation indicating that it should. The code to reproduce the problem is provided, the expected and actual results are clearly described, and the desired outcome (adding store_cv_values support to RidgeClassifierCV) is explicit. Both the motivation and technical context are clear, and the engineer knows exactly where to look (RidgeClassifierCV class) and what to change (parameter to add and its expected behavior).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted enhancement to add a missing parameter to a class, following the pattern already present in a related class, RidgeCV. While some understanding of scikit-learn's estimator patterns and careful integration/testing is needed, it is conceptually straightforward. It primarily involves adding a parameter and ensuring it's used consistently and passed to the base class. The added test coverage is targeted. For an experienced engineer familiar with sklearn's structure, this should take between 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The only mild consideration is that proper understanding of scikit-learn's constructor and inheritance patterns are needed, but this is within normal expectations for a contributor at this level. The issue is well scoped for evaluation purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10908": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The report provides clear code examples demonstrating the unexpected behavior in CountVectorizer's get_feature_names method when a custom vocabulary is provided but the vectorizer is not fitted. It outlines both the expected and actual behavior, explicitly requesting that get_feature_names should not raise NotFittedError if a vocabulary was provided at initialization, since the vocabulary required already exists. The explanation is tied directly to observed mechanics (the presence/absence of vocabulary_, how _validate_vocabulary is called from transform, and suggesting its use in get_feature_names), making it clear what needs to be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is fairly contained: adjusting get_feature_names so that it checks or initializes vocabulary_ if a custom vocabulary is provided, mimicking the logic in transform. The problem may require a bit of investigation into the initialization and state management of CountVectorizer and underlying methods (_validate_vocabulary, etc.), but the scope is limited to a small section of one class. An experienced engineer familiar with scikit-learn or similar APIs should be able to reason through and resolve this in under an hour, including writing or updating relevant tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The bug and expected resolution are clearly described, the logic is self-contained, and it is a good fit for evaluating coding ability on this type of codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-12585": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear problem statement: the 'clone' function fails when instance parameters are estimator classes (types, rather than instances) in scikit-learn. The submitter includes a minimal code snippet that reproduces the problem, describes the expected behavior (no error should occur), and specifies the actual error seen (TypeError from get_params called on a type). A concrete suggestion for a code change is given, including the relevant filename (base.py) and line number. There is enough information to unambiguously understand what the bug is, what successful behavior would look like, and what kind of fix is likely required, even without the suggested change. An experienced engineer could directly attempt a patch based on this information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix only requires a small modification to the logic of the 'clone' function, plus at most adding a new test case. The engineer needs to understand how 'clone' interacts with estimator parameters and the difference between instances and types in Python. The issue is localized and well described, with a clear suggestion; testing is also straightforward. For someone with moderate familiarity with scikit-learn (after a short ramp-up), implementing and verifying the fix should take 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified. The sample is clearly described, the problem is localized, and it is appropriate for evaluating coding ability, especially regarding understanding Python class-versus-instance distinctions and minor control flow edits.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13135": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the problem (KBinsDiscretizer with strategy='kmeans' fails for higher n_bins due to unsorted bin_edges), explains the effect (ValueError from np.digitize due to unsorted bins), and gives a concise minimal code example to reproduce the bug. The expected result ('No error is thrown') and the actual result (ValueError from np.digitize about unsorted bins) are clearly described. Enough context about where and how the error occurs (fit_transform, kmeans strategy, bin_edges) is provided. An experienced engineer should have all the information needed to locate and resolve the problem. The issue text gives relevant file/function references and steps for reproducing and understanding the bug. The requirements for a successful solution are clear: make sure bin_edges are sorted before passing to np.digitize.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial change. Identifying the root cause is straightforward given the code references and the example; the engineer needs to ensure kmeans-derived bin edges are sorted before being used. Making this fix would usually take 15-60 minutes (level 1), as it involves reviewing how the centers and bin_edges are computed, inserting a sort at the correct spot, and verifying it works. The patch required is very short, and the new tests are a minor extension to an existing test. The technical concepts involved (kmeans, np.digitize, sorting arrays) are standard for a machine learning codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present. The issue is direct, focused on a specific bug, and all necessary background is included. The example is clear and the failure scenario is reproducible on its own. The issue and required solution are both good fits for a coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13142": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-structured. It clearly states the observed bug: that fit_predict(X) and predict(X) on the same data disagree when n_init > 1 in GaussianMixture, while they should agree. It also provides self-contained reproducible code, the expected behavior (no AssertionError), and the actual result with a specific mismatch statistic. There is an explicit reference to the current unit test and why it doesn't catch the issue. The purpose and context are unambiguous, and the relevant class (GaussianMixture) and parameters are named. All this information is sufficient for an experienced engineer to attempt a correct and meaningful resolution without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires understanding how fit_predict and predict work and why they might give different outputs when n_init > 1. This is a moderately tricky debugging problem, but thanks to the clear, minimal reproducing example, the surface of investigation is well-bounded. The change required is conceptually straightforward (ensuring a final e-step to synchronize the code paths), which involves a small modification in a specific function. Given some familiarity with scikit-learn's codebase, an experienced developer could diagnose and implement the fix within 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The test coverage is present and specifically checks the corrected behavior. No ambiguous requirements or major edge cases are left unspecified, making this a strong candidate for coding evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13328": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it provides concrete code to reproduce the error (fitting HuberRegressor with boolean data), shows what works and what fails, and describes the expected vs actual results clearly. It explicitly states the expectation: HuberRegressor.fit should work with boolean valued X (casting to float as in LinearRegression), and currently fails due to a TypeError involving numpy array negation. The specific class/method involved (HuberRegressor.fit) is named, the input types are shown, and the desired behavior is made explicit. Any competent engineer could attempt a fix based on this, and the unit test can be directly inferred from the issue text. The class and file to edit (fit method in sklearn/linear_model/huber.py) are obvious given the minimal code context.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change requires a small but thoughtful edit: ensuring that boolean input to HuberRegressor is safely cast to float (similarly to LinearRegression) before processing. While an engineer would need to locate where X is checked/cast, and ensure the behavior matches expectations (possibly checking related estimators or helper methods), it is not a large or esoteric edit, and is localized to the input validation code. The fix is straightforward and does not require editing multiple files, large-scale refactoring, or deep algorithmic changes. This is not a totally trivial fix (there is a root cause to identify, relevant scikit-learn internals to understand, and some consideration to matching expected casting behavior), but it is essentially solved with a couple of lines of code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a well-suited issue for a coding benchmark. The failure and desired behavior are clear and reproducible. The engineer is expected to understand basic numpy/scikit-learn input handling and make a focused patch, which is a valid test of library-hacking ability. The solution is not so trivial as to be meaningless, nor so open-ended as to be ambiguous. The test's reliance on scikit-learn conventions is explicit in the issue and reasonable. No major issues noted.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13779": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the problem (an error occurs when passing sample_weight and one of the estimators in VotingClassifier is set to None), the reason (fit is called on None), and includes a code snippet that demonstrates the bug. This allows an engineer to both reproduce the issue and understand exactly what a successful fix should do (ensure that fit is not called on None estimators, especially in the context of handling sample_weight). The context is specific to VotingClassifier (and by analogy, VotingRegressor), and the fix is scoped within their fit logic. No filenames or function names are explicitly referenced, but the description is sufficient to unambiguously identify the relevant code area for someone familiar with scikit-learn. The problem is local and well-delineated.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem requires a small but thoughtful change to the VotingClassifier (and possibly VotingRegressor) fit logic to skip None estimators when handling sample_weight. The change is conceptually straightforward but requires basic familiarity with the ensemble voting code. This likely takes less than an hour for someone familiar with scikit-learn's codebase\u2014after reproducing the error, the solution is to add a simple check for None before calling fit or verifying sample_weight support. The gold patch confirms that the actual code modification is very minimal (a 2-line guard clause).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues. The sample is well-contained, isolated, reproducible, and the change is small but targeted. The code to be written requires understanding scikit-learn's conventions, but nothing unusual.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14053": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and contains all the necessary details to understand and reproduce the error. It identifies the function and context (sklearn.tree.export.export_text), the cause of the problem (IndexError when there is a single feature), and provides reproducible example code so an engineer can trigger the error directly. It explicitly states that the error is due to the function trying to access an element out of range in the provided feature_names list. An experienced engineer would be able to infer from this what needs to be fixed: handling cases with a single feature so that export_text doesn't throw an IndexError.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted bug fix requiring a small code update to handle edge cases in the export_text function. The scope is narrow\u2014understanding the code path for feature_names and ensuring the function does not access invalid indices. The issue comes with a minimal reproducible example, so verifying the fix is straightforward. With a few minutes to locate the relevant logic, an experienced engineer could craft and test a patch in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The issue is clear, reproducible, and focused. The relevant code and test files are likely discoverable by name or via the stack trace of the error when running the example. It is suitable for a coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14087": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The issue describes a specific error (IndexError: too many indices for array) arising from using LogisticRegressionCV with refit=False. There is a clear minimal reproducible example, including the code, parameters, and data generation. The expected behavior is explicitly stated: fitting should work without error. The actual cause of the error is concisely described, indicating the bug results from trying to average coefficient paths across folds by incorrect indexing. A skilled engineer would know which class and method (LogisticRegressionCV.fit) to look at and would have all necessary information to attempt a fix. There is no ambiguity about the bug or the desired outcome.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is more involved than a trivial change, but it is limited in scope: it entails fixing the logic in LogisticRegressionCV.fit so that it correctly handles averaging coefficient paths (and associated attributes) when refit=False, avoiding the incorrect indexing that leads to the IndexError. Diagnosing and implementing the correct fix likely requires careful code inspection and understanding the relationship between penalty type, folds, and how best indices and l1_ratios are being handled. This will involve small logic changes, but does not require substantial rewriting, architectural changes, or in-depth research. An experienced engineer familiar with numpy and scikit-learn will likely spend 15m-1h on this task, including validation with the provided test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The test patch adds parametrization and covers different penalty and multi_class values, further clarifying the required coverage and making this a strong candidate for a benchmark sample. The description, expected behavior, and requirements are clear. The sample is domain-specific (scikit-learn and numpy knowledge needed), but otherwise straightforward.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14629": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It describes a concrete problem: calling cross_val_predict with method='predict_proba' on a MultiOutputClassifier raises an AttributeError because MultiOutputClassifier does not have a classes_ attribute directly. The issue gives a minimal reproducing example, showing the precise code that fails and the error encountered. The expected and actual behaviors are clearly stated. Furthermore, the reporter pinpoints the likely problematic code section and suggests the general direction for a fix (accessing classes_ through underlying estimators). All the information required to understand, reproduce, and resolve the issue is present. Filenames and class names are either explicit or easily inferred in the sklearn context. An experienced engineer would know exactly what to attempt without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, self-contained change in behavior: the fix consists of adding a classes_ attribute to MultiOutputClassifier by aggregating the classes_ attributes of the contained estimators, as well as ensuring compatibility with cross_val_predict. Understanding the problem and direction cited in the issue, implementing the attribute, and writing/adjusting tests would fit comfortably into a 15-minute to 1 hour bucket for an engineer familiar with scikit-learn. The change is conceptually simple, does not require complex research or architecture change, but needs a reasonable understanding of the estimator structure in scikit-learn.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major concerns. The sample is suitable for benchmarking problem-solving and implementation abilities related to Python, object-oriented code, and familiarity with scikit-learn patterns. The only note is that it tests both understanding of API gaps and modification of both core and test code, which is appropriate for a practical code challenge.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14710": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a reasonably clear account of the bug: when calling gbrt.fit(X, y), an error occurs during early stopping because of a mismatch between the types of true labels (integer-encoded) and predicted labels (string class names). This leads to NumPy attempting to compare strings and floats, ultimately causing a TypeError. An experienced engineer could deduce that dealing with label encoding for scorers is the core issue. However, the exact required behavior (should the scorer always receive decoded string labels? only in early stopping? etc.) is not fully spelled out, so some assumptions must be made. Still, it is clear enough to attempt a targeted fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue appears to be a small-to-moderate bug fix likely confined to a single place in the codebase that handles early stopping scoring for classifiers. It requires identifying where label encoding/decoding occurs and ensuring string labels are properly passed when needed. For an engineer familiar with the codebase, this would likely take between 15 minutes and 1 hour, as it involves tracing the data flow, implementing a simple transformation, and verifying test coverage.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being used to evaluate coding ability. The issue is reproduceable and has a clear point of intervention, and the test patch defines the relevant regression test. The needed assumptions (properly decoding labels before scoring) are sensible. The sample represents a real-world bug fix that would be encountered in practical software engineering.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14894": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is extremely clear and well-specified. It includes a concise summary of the bug (ZeroDivisionError in _sparse_fit when support_vectors_ is empty), a detailed minimal reproducible script that triggers the error, the expected and actual outcomes, and a good technical explanation pointing at the root cause (division by zero due to n_class==0). The information provided directly points to the relevant function (_sparse_fit), describes the error path (division by zero at a specific computation), and specifies the desired behavior ('no error, dual_coef_ = sp.csr_matrix([])'). All information necessary to implement a successful fix is included. An engineer would know exactly where to look and what change is required, without needing further clarification. There is also environment/version info to help reproduce. Overall, the issue is a model example of a well-specified, actionable engineering bug report.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty of this issue is moderate. The bug is already well-localized: the error is in _sparse_fit and happens under a rare scenario (when there are no support vectors after fitting on sparse input). The description and reproducible script allow quick reproduction and debugging. The fix does require understanding some context (meaning of dual_coef_, support_vectors_, and sparse matrix construction in scikit-learn's SVM implementation), but does not require deep architectural changes or touching multiple files. The fix, as in the patch, is a small conditional check before creating the sparse matrix, which is a one-line logic addition. For an experienced engineer familiar with numpy/scipy and scikit-learn, this is easily in the <1 hour range, most likely 15 to 30 minutes including finding appropriate location, reproducing, implementing, and validating. It may take a little longer to set up the environment if unfamiliar, but the directness of the bug and fix makes it suitable for this level.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue text is specific, actionable, and well-formed for coding evaluation. The test case in the patch corresponds closely to the reproduction script in the issue and checks for the correct post-fix behavior (attributes are empty sparse matrices). The scope of the required knowledge is moderate (numpy, scipy, SVM internals), appropriate for benchmark purposes. This makes an excellent inclusion candidate.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25747": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a specific error: when using transform_output=\\\"pandas\\\", a ValueError occurs due to a length mismatch between the expected output index and the DataFrame returned by a transformer\u20144 rows produced, but the index is 96 rows long. The issue text precisely describes the scenario that produces the bug (transform_output=\\\"pandas\\\") and the nature of the bug (DataFrame/Index length mismatch), making it clear what problem needs to be solved. However, it doesn't specify the exact location or name of the function responsible, so the engineer must still investigate where pandas wrapping occurs or which utility/helper is used in the codebase for transforming outputs (likely _wrap_in_pandas_container, but not named). There is also no suggestion for a desired behavior, but the implication is that the returned DataFrame should have a matching index or not forcibly override an index when a DataFrame is already returned. Thus, there are a few blanks to fill, but a sensible interpretation is possible, and it is clear what outcome is required for a successful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized and conceptually straightforward: the bug arises when attempting to assign a new index to a DataFrame returned by a transformer, but the row count of the output differs from the original input. The solution is to avoid overwriting the DataFrame's index if it is already a DataFrame, which is a small conditional change (as seen in the gold patch). Arriving at this requires some knowledge of pandas, index alignment, and the code path where set_output and DataFrame wrapping happen. It may take a bit of debugging and inspection to confirm the proper place for the fix, but for someone familiar with scikit-learn/pandas conventions, this wouldn't take long. It is more than a trivial change (since you need to understand the wrapping logic and intended invariants) but not much more than that. Likely 15 min to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The only limitation is that details about the test scenario (what object or transformer causes this in practice) are omitted, but the underlying cause, how to reproduce, and failure mode are explicit. The test patch both fixes the bug and adds a non-regression test. The engineer may need to investigate to find the correct function to change, but otherwise this is suitable as a benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25931": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, detailed, and demonstrates a concrete, reproducible bug. Specific code to reproduce the warning is given, including the parameters and environment setup. The expected and actual results are explained, making it unambiguous what the desired outcome is (no warning about feature names when fitting IsolationForest with a DataFrame and non-default contamination). The text provides enough context for a knowledgeable contributor to identify the bug, understand when and how it manifests, and verify that any patch resolves the problem. No further clarification is needed beyond the current description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the root cause may require some investigation (the difference in feature name validation paths when using a non-default contamination parameter), the actual code change is confined to distinguishing between input validation paths and ensuring feature names are preserved. The patch is a straightforward fix once the problem is localized, requiring an understanding of the codebase but not a deep architectural change or multi-file rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being suitable for a coding benchmark. The problem is well-defined, and the fix can be clearly attributed to the described bug. The test verifies the resolution unambiguously. The issue is suitable for benchmarking coding ability in the context described.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25973": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It gives a precise example, including code to reproduce the bug, describes both the expected and actual results, and gives context on the scikit-learn component being used (SequentialFeatureSelector and the cv parameter). It also specifies the exact error encountered (IndexError, list index out of range) and under which conditions. The user is explicit about the requirement: SequentialFeatureSelector should accept an iterable of splits for the cv parameter and work without raising errors.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change involves ensuring that the SequentialFeatureSelector appropriately handles an iterable of splits for the cv parameter (by explicitly passing it through check_cv before using it). This requires understanding how other scikit-learn components handle cv parameters, tracing where the parameter is consumed, and updating function signatures and usages accordingly. For an experienced contributor familiar with scikit-learn patterns, this would take modest investigation and code adjustment, likely between 15 minutes and an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional issues that would preclude this sample from being used in the benchmark. The issue is self-contained, has a clear reproduction case, and is relevant to evaluating the ability to read, interpret, and debug code that interacts with standard library conventions (like cv in scikit-learn).\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10323": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear about the problem: when using the Sphinx 'literalinclude' directive with the ':prepend:' option, leading whitespace is removed, causing the resulting code's indentation to be incorrect. The description gives a concrete example using XML snippets, specifies expected (and incorrect) output, and lists a failed workaround with :dedent: and describes why it doesn't work. The desired behavior\u2014preserving leading whitespace on the prepended content so as to match included content's indentation\u2014is unambiguous. An experienced engineer would have sufficient information to understand what a successful solution should do.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change is conceptually straightforward but non-trivial: it involves adjusting the order of filters (prepend/append/dedent) applied to the included file content when processing the Sphinx 'literalinclude' directive. Understanding the correct sequence and the code path requires checking how Sphinx processes these filters, but it does not entail a large or deeply complex refactor. The change is small but requires careful thought to avoid unintended side effects. This would probably take 15 minutes to 1 hour for a capable engineer familiarized with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent\u2014reproducibility, testability, and suitability for use as a benchmark for coding are soundly covered by the issue, the patch, and test changes. The modifications are self-contained and justified.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10673": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified: it clearly describes the current undesired behavior (Sphinx warns about toctree entries referencing 'genindex', 'modindex', or 'search' as if they do not exist when those are included in the toctree), and spells out the desired behavior (those entries should work and not raise errors). The recommendation is to allow such entries without warnings. The description includes user motivation, examples of what users do, and what currently breaks. However, there is a small gap: it does not explain exactly what the correct/working user experience should be (are those entries to show up as proper links?) or whether there are any edge cases, so an engineer must make some assumptions (e.g., that those names refer to special documents auto-generated by Sphinx). Still, there's a sensible and direct interpretation of what success looks like: allow 'genindex', 'modindex', and 'search' in toctrees without errors or warnings.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix involves understanding not just the HTML builder or toctree directive in Sphinx, but also the concept of generated pages that don't physically exist as source files but are created at build time ('genindex', 'modindex', 'search'). The engineer must update how Sphinx checks for the existence of referenced documents when parsing toctrees and adapt code in multiple places to treat these auto-generated pages as valid. The patch touches several files, involves Sphinx's domain and environment systems, and careful handling of document references. This requires non-trivial code reading, understanding Sphinx's build environment and document processing, and testing. For an experienced engineer new to this codebase, this will likely take 1-4 hours including testing and edge-case handling.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is suitable for evaluating coding ability. It is fairly scoped, yet requires understanding of both domain concepts (Sphinx, generated docs, toctree) and system internals. The user story is realistic and often-encountered.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7440": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it describes a concrete bug encountered when building documentation in Sphinx due to duplicate glossary entries for the term 'mysql' with different cases ('mysql' and 'MySQL'). The error location is referenced, and instructions to reproduce the issue are enumerated step-by-step. The expected behavior is implied but not fully explicit (the issue author asks if 'MySQL' != 'mysql'), leaving a small interpretative gap. However, an experienced engineer can reasonably infer that glossary terms should be case-sensitive, so 'MySQL' and 'mysql' are considered distinct. The scope of the fix and its check is clear enough to attempt a solution, but not at the level of absolute clarity (e.g., not 0) due to a hint of ambiguity about intent.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change requires modifying where/how glossary terms are registered and referenced in Sphinx to make them case-sensitive (removing the forced lowercase transformation). The code patch shows a straightforward change: a couple of lines in two files (domain logic and test updates). The conceptual understanding takes a few minutes, but implementing and ensuring no unintended side effects could take a little longer, especially with testing. This places it solidly within the 15 min - 1 hour range, closer to the lower end.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified. The issue is a representative example for a bugfix relevant for documentation tooling and case-sensitivity handling, and is suitable for benchmarking coding ability. The only slight ambiguity comes from the phrasing of the expected behavior, but it's minor and resolved through reasonable inference.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7462": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is clearly described with a concrete example (the Tuple[()] annotation), the error message observed (IndexError: pop from empty list), and specific instructions to reproduce the issue including the exact Python code and Sphinx configuration. The user explicitly identifies the location in the code where the problem occurs (the unparse method in python.py) and describes the expected behavior (documentation builds successfully and the function foo is rendered with valid annotations). All necessary context, such as environment details, is provided. An experienced engineer would know immediately what is wrong, what kind of fix is needed, and how to verify the solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is relatively contained: it manifests in a specific context (type annotation rendering of Tuple[()]), appears in only one or two places (the unparse methods), and only requires modest logic to handle the empty tuple case without raising an IndexError. The solution involves changing a conditional structure to properly handle the empty list. This is a small code change, but it does require some understanding of how AST handling and docstring parsing works, which could take a bit of time to trace. Implementing the solution, writing/adjusting tests, and confirming the fix would be doable in less than an hour for an experienced engineer familiarized with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues: the user provides all necessary context, a clear motivation, a reproducible example, and explicit expected behavior. The solution requires code and test changes, both of which are typical for evaluating coding ability. The only minor challenge is that the affected logic is somewhat specialized (AST parsing and doc rendering), but all requirements are clear.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7985": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is relatively well-specified, but it leaves some details to the engineer's judgment. It reports that Sphinx's linkchecker currently checks external links (such as 'https://...') but not local (internal) links defined with a reference like '.. _local-link: doesntexist'. The report provides a way to reproduce the problem using minimal changes to index.rst, details the output observed, and has a clear \\\"expected results\\\" statement: linkcheck should also check the local link. However, it does not fully specify whether symlinks, absolute paths, or other Sphinx-specific references (such as image or include directives) should be handled identically, nor does it detail what constitutes a \\\"local link\\\" (relative vs. absolute, directory traversal etc). Therefore, there are some blanks to fill in, but the main ask is \\\"make linkcheck also check internal/local (non-external) links for validity\\\".\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change requires understanding how the linkcheck builder in Sphinx distinguishes between external and internal links, and adding logic to test whether local (file-based) URIs resolve to an actual file in the documentation source directory. The fix is conceptually straightforward for someone familiar with Sphinx or Python\u2014involves adding some checks in the relevant code path\u2014but requires care to avoid false positives/negatives. Updating tests, as seen in the gold patch, also takes some time but not a substantial rewrite. Thus, it's more than a trivial fix but not a major refactor or conceptually difficult: about 15 minutes to 1 hour's work for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The single ambiguity is a thorough definition of all 'local link' edge-cases, but the request and its reproduction case make the core ask clear enough for a meaningful benchmarking sample. The example is sufficiently isolated and does not reference private or external data.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8269": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It describes precisely what the bug is: when linkcheck_anchors is True and the server returns an HTTP error code (e.g., 404, 500), linkcheck reports only \\\"anchor not found\\\" instead of the real HTTP error. It provides clear reproduction steps, an example of the actual and desired behavior, including what the expected output should be. The context about configuration and environment is sufficient to attempt the fix. This gives a clear idea of what needs to be changed: report HTTP errors before checking for anchors when the server did not succeed. All the information needed to create a successful solution is present without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change that requires some understanding of both how linkcheck works and how HTTP errors are currently handled, but it does not require rewriting large parts of the code or deep architectural changes. The actual fix (adding response.raise_for_status() to ensure HTTP errors are raised and reported) is a straightforward update once the control flow is understood. Setting up and updating a small corresponding test is also straightforward for an experienced engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue provides a concrete, single behavioral change request with environment details and a clear expected output. There are no major blockers or ambiguities present. The issue is suitable for benchmarking coding ability within the described framework.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8551": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear and concrete example using Sphinx reStructuredText markup, showing the problem: unqualified references in :type: and :rtype: info fields do not resolve as expected and cause ambiguous class lookup warnings. The current and expected behaviors are described, with detailed sample input and the observed/expected output. However, the root cause of the behavior, and whether it is a documentation, code, or configuration issue, is left to the engineer to determine. The general requirement\u2014make unqualified name resolution in :type:/:rtype: mimic explicit xref resolution\u2014is clear. There is a sensible interpretation of what a successful fix would do, but someone unfamiliar with Sphinx's internals or these fields' mechanics might have to make some guesses.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix affects logic around cross-reference resolution within Sphinx's documentation generation for Python projects. The engineer would need to understand Sphinx's info field handling and cross-reference internals, identify where the :type: and :rtype: implicit xrefs are created, and adjust their module/class lookup context to match explicit xref behavior. The provided test update suggests edits were needed in at least two subsystems (domains/python.py and util/docfields.py), and a new test had to be added. Overall, this is a non-trivial, cross-cutting bug that goes beyond a simple one-line fix. For an engineer new to this corner of Sphinx, getting familiar, debugging, and validating the change could easily take 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues detected that would impact the suitability of this issue for the benchmark. The sample is well specified and testable, as long as the candidate is given the same context as the PR author had. The only minor concern is the need for Sphinx/reStructuredText familiarity; however, the problem is clearly scoped in the provided text and test cases.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-9711": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the bug: the needs_extensions version check is performed as a string comparison rather than a version comparison, which results in errors, e.g., '0.10' is regarded as less than '0.6' when compared as a string. It walks through specific reproduction steps, the expected behavior, and clarifies the nature of the bug and what constitutes correct functionality. For an engineer familiar with dependency/version checking, the problem and required solution are obvious: the code should compare versions semantically rather than as strings. No external clarification is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the issue is very clear once read, implementing the solution involves identifying where the comparison is performed and replacing a string comparison with version-aware comparison. This requires importing a library like packaging.version and making minor code adjustments, as indicated by the gold patch. The patch only affects a single function, with a small amount of added logic and a new import. Adding relevant tests solidifies the fix. This would likely take between 15 minutes and 1 hour for an experienced engineer, assuming some setup time and verifying the fix with tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers or ambiguities with this sample. The problem is common and clear, reproduction steps are provided, and the expected behavior is described precisely. The solution and its validation are straightforward for a skilled developer. The only minor note is that knowledge of version-parsing libraries (e.g., packaging.version) is assumed, but this is reasonable for an engineer working on a project like Sphinx.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13372": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is relatively well-specified. It provides a clear reproduction scenario (calling evalf on Mul(Max(0, y), x, evaluate=False)), explains the specific exception (UnboundLocalError due to the uninitialized variables reprec and imprec), and gives insights into where the error arises (in the multiplication evaluation routine). Furthermore, it suggests a plausible solution approach (add an else: raise NotImplementedError clause in the elif branches). While the description doesn't specify every detail (for example, it doesn't specify the expected output or what exactly NotImplementedError should signal), it gives adequate context for someone familiar with the codebase to understand both the bug and how to fix it. The engineer would need to review the relevant evaluation logic to properly implement and test the fix, but the issue is concrete and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change required is relatively small and direct: it involves adding a missing else clause to ensure that the handler failure for previously unaddressed cases is explicit and does not result in uninitialized variables. The engineer needs to understand the control flow and the point at which variables are initialized, but overall, this is a small patch that wouldn't require more than 15-60 minutes for someone already somewhat familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are present. The issue is focused, has a clear reproduction step, indicates exactly where the breakage is, and suggests a sensible approach to a fix. The only minor ambiguity is whether NotImplementedError is the proper exception in the context, but this is a minor gap that does not detract from its suitability for the benchmark. The included context is sufficient for use in coding evaluations.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13480": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it describes a runtime NameError arising from referencing an undefined variable 'cotm' in sympy's hyperbolic.py at a specific code location (line 590). It also provides clear reproduction steps (code snippet using coth(log(tan(x))).subs(x, 2)), describes the triggering input values, and notes the error message and the origin of the bug. This allows an experienced engineer to identify the cause and location of the bug and what needs to be corrected (ensure proper variable use in the conditional and reference 'cothm' instead of 'cotm'). Thus, the issue is clear in both what is wrong and what a successful solution should address.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a very straightforward bug fix: the issue results from a typo or misnamed variable ('cotm' instead of 'cothm'). Identifying and correcting this should take less than 15 minutes for an engineer familiar with the codebase or after a short codebase review. The error points to the precise file and line, and the provided snippet is enough for quick reproduction and confirmation of the fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues are present. The bug is clear, has a precise reproduction method, and a simple targeted fix. The setup mirrors the type of bug-fixing encountered in real-world coding assessments, so the sample is suitable for benchmark use.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13877": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a reproduction of the problem using specific code snippets. It explains that computing the determinant of certain symbolic matrices in sympy leads to an invalid NaN comparison error and that the Bareiss algorithm, which is typically valid for integer matrices, seems to fail when used for symbolic values here. The reporter suspects a misapplication of the algorithm and gives concrete, reproducible examples for different matrix sizes. However, there are some blanks to fill in about the precise fix: while it's clear that determinant computation for certain symbolic matrices should not raise an error or return NaN and should instead return 0, the issue does not specify the exact expected behavior or whether a change in algorithm, a patch to input validation, or an error handling fix is expected. An experienced developer could reasonably infer the desired outcome, but some interpretive work is required.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This task requires the engineer to understand the sympy matrix determinant computation and the limitations of the Bareiss algorithm with symbolic (non-integer) entries. The developer must trace through the logic, discover where invalid NaN comparisons arise, and design a fix (in this case, supplying an improved zero-check function for pivoting). This requires examining non-trivial code, understanding both mathematical and software subtleties, and ensuring the fix does not break other scenarios. The actual code changes are compact, but arriving at the solution involves deeper analysis and understanding of both the algorithm and sympy's structure. This is a 1-4 hour class issue for someone moderately familiar with sympy's codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blockers are apparent. The issue is clearly, if somewhat minimally, reported and is plausible as a benchmark sample for coding ability in a technical domain (symbolic computation). There are no external dependencies or context required outside the repo and the information given.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-16792": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it provides a minimal reproducible example, shows the error raised and its cause (i.e., incorrect C signature generation for unused array arguments), contrasts the failing and working scenarios, and clearly frames the expected behavior (the array argument should still have the appropriate signature even if unused in the expression; this is needed for interoperability reasons). The issue also identifies the likely source of the bug (in codegen) and spells out a real-world use case. An engineer could confidently attempt a precise fix based only on the information given.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, focused bug requiring the addition of some logic to ensure that all array arguments are correctly represented in the generated function signature regardless of their use in the expression. The core challenge is understanding how code generation currently handles argument types and ensuring that array arguments are processed consistently. For an engineer familiar with the codebase, this should be a manageable change involving minor adjustments to the code generation logic, as supported by the compact size of the gold patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is a clean fit for the benchmark: it describes a reproducible, understandable bug with a clear expected output, and the solution is self-contained (affecting codegen logic and its tests). No significant ambiguities remain, and the context is ample for the task.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17139": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text is well-specified. It clearly demonstrates both the code that triggers the error (calling simplify on cos(x)**I in SymPy) and explains that the error is due to an invalid comparison between a complex exponent (I) and zero during the simplification process. This provides enough detail for a developer to understand both the nature of the bug (TypeError due to invalid ordering comparison for complex numbers) and the expected outcome (no TypeError, and the expression should be returned unsimplified if the exponent is complex). File and function hints are also clear, and the behavioral expectations can be inferred directly from the examples provided.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding of how simplification routines handle powers and exponents in SymPy and where in the code the comparison occurs, but the fix (adding a check for the exponent's reality before performing the comparison) is straightforward once the problematic area is identified. The reproducibility is easy, the cause is described, and the fix is small and targeted. It would likely take 15 min to 1 hour for an engineer familiar with SymPy or who spends a bit of time reviewing the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The bug is well-scoped to a specific behavior, and the task is well-suited for assessing basic debugging, code reading, and logic skills around special-case handling of types in symbolic math libraries.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17318": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly identifies the function is sqrtdenest, what input triggers the error, what the erroneous behavior is (an IndexError is raised), and what the correct behavior should be (the expression should be returned unchanged if it cannot be denested). It also gives explicit examples\u2014both the code that fails and the expected new result, so there is no ambiguity as to what a fix should accomplish. The function and files involved are unambiguously specified by name, and the correction criteria are explicit and testable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This should be a relatively small change for an experienced engineer. The issue is due to an unhandled case causing an IndexError, so the fix will revolve around properly checking the cases, perhaps by validating the input or adjusting conditionals. The key requirements and location are described, and the logic change needed is relatively small but may require some thought about safe checks and about the intended math behavior of the function. Fitting a 15-60 minute window is realistic for someone already familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues with this sample. The description is clear and the change is isolated and testable. The error, desired behavior, and relevant context are all provided. The test patch confirms the relevant checks for the fix and handles the specified corner cases.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17630": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly demonstrates the problem with block-multiplying BlockMatrix objects containing ZeroMatrix blocks in sympy: upon repeated multiplication, zero blocks are returned as the scalar 'Zero' instead of as 'ZeroMatrix', leading to an AttributeError when methods expect a matrix-like object with attributes (like 'cols'). The author gives a concrete, minimal reproducible example with explicit code showing both the expected and actual behavior, along with the error message. While there is a slight gap (the reporter does not specify the expected behavior in every edge case or offer a desired test/output in full detail), it is very straightforward to infer what is required: ensure that zero blocks in block matrix multiplication remain ZeroMatrix, not Zero scalars. This is a sensible, actionable requirement, making the issue well-specified for a competent engineer working on this codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The primary challenge in this issue is understanding how SymPy's matrix expression and block matrix multiplication internals treat ZeroMatrix and Zero, diagnosing where the incorrect type conversion is happening, and ensuring that operations on blocks preserve ZeroMatrix when appropriate. The fix is localized: it only involves adjusting the return logic in the internal post-processing function in matexpr.py. No major refactoring or architectural changes are needed; implementation is straightforward once the problem is understood. However, some knowledge of sympy's block matrix internals is helpful, so it's a bit more than a trivial fix, but not complex.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues preventing this sample from being used. The reproducible example in the issue and the resulting test cases are simple and direct. The expected behavior is clear. There may be some nuances about matadd or scalar-matrix operations, but these are tangential or covered in the PR tests, and not ambiguous in the context of the main issue. The reporter correctly gives Python and SymPy versions\u2014no ambiguity due to version differences.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17655": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and specific about the undesired/buggy behavior encountered and the expected behavior. It specifies the types involved (geometry.Point and numbers from sympy), shows the exact code that leads to the problem, and gives context about how Sympy's addition logic is mishandling multiplication. The sample provides a precise reproduction and describes the error path (GeometryError when combining a Mul and a Point). This information is sufficient for an experienced engineer to understand what is wrong (that Point should correctly work with left-multiplied scalars, i.e., scalar * Point, just as right-multiplied) and what a successful solution would need to address.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is to add reverse multiplication (i.e., __rmul__) to the Point class so that scalar * Point works, mirroring existing __mul__ functionality. This is a well-understood Python protocol and does not involve deep architectural changes or multi-module edits. It would take some time to check correctness and possibly touch tests, but for someone familiar with Sympy it is a targeted fix that should be achievable in 15 min to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major shortcomings in the issue description, nor in the suitability of the sample for coding benchmarks. The problem and the required general logic change are both clear from the description and code context. The task has a well-defined bug fix nature that also exercises understanding of Python dunder methods and operator overloading, making it appropriate for coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-18211": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It explains the current undesired behavior (NotImplementedError raised by solveset when using as_set() on certain equations), gives a concrete code example (Eq(n*cos(n) - 3*sin(n), 0).as_set()), and describes the expected behavior (return a ConditionSet representing the solution set, rather than raising the error). It also gives the exact expected output, including the form of the ConditionSet. The issue gives sufficient context to identify the relevant code area (solveset, as_set method, returning/handling ConditionSet) and outlines what needs to change. No additional clarifications or requirements are needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue likely requires identifying where NotImplementedError is raised and changing the behavior so that a ConditionSet is returned instead. The scope is contained: understand the current call stack, add a try/except for NotImplementedError, and wrap the result in ConditionSet as needed. The patch touches only a single function and adds a straightforward exception handling block. This is a focused change, but requires some familiarity with the call hierarchy and expected API behavior, so it's not entirely trivial, but is a manageable fix for an engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major reasons to exclude this sample. The specification, expected output, and solution space are clear. It exercises a realistic code path and requires a well-justified reasoning for choosing return value over error. There are no multiple interpretations, ambiguous requirements, or missing context.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20428": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes a concrete bug: calling Poly.clear_denoms() on a certain polynomial produces a Poly(0, x, domain='EX') object that acts inconsistently \u2013 for example, bad_poly.is_zero is False, but bad_poly.as_expr() is 0, and bad_poly.terms_gcd() fails. It includes a minimal working example (with full code to initialize the input), shows the wrong output, traces the underlying representation (the DMP having [EX(0)] instead of []), and explains broader implications (like primitive() behaving oddly). As an engineer, it is clear what behavior needs to change, namely, ensuring zero polynomials are correctly normalized to the canonical representation so that Poly(0, x, domain='EX').rep is DMP([], EX, None).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is localized: it is about correctly normalizing the zero polynomial when calling clear_denoms() (and elsewhere). The description provides a clear example and the required adjustment (normalizing internal DMP to empty for zero), so the engineer would need to identify and patch the relevant code. While this takes understanding of the domain-specific code and data structures, the fix itself (as seen in the patch) is a small change \u2013 likely less than an hour's work for someone familiar with SymPy's polynomials. No substantial rewriting or design is required, just a targeted bugfix informed by the repro case.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue scope is sufficiently clear, it is a real functional bug, and the code/test patches validate that it's been addressed with minimal risk of ambiguity. The main technical complexity is around understanding SymPy's internal polynomial reps, but the issue and patch leave little room for misinterpretation.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20438": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides specific Python code examples that demonstrate an inconsistency or error in the implementation of 'is_subset' and related methods for ProductSet and FiniteSet interactions in SymPy. While the underlying desire is fairly clear\u2014to have is_subset return the correct result and to avoid an AttributeError when simplifying Eq(b, c)\u2014there are some blanks to fill in for a newcomer: the precise expected result for each of the provided code examples isn't always explicit (though it's usually implied), and the connection between the Complement.equation error and the set logic bug requires careful reading. Also, the symptom and cause (the AttributeError) are described, but the desired fix is somewhat implied. Nonetheless, for an engineer familiar with SymPy's structure, there's a sensible interpretation: fix is_subset for ProductSet vs FiniteSet, and handle the error in Eq(). Overall, it is clear enough for a solution, but a few details would benefit from being more explicit.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue involves extending the logic of set relationships in the SymPy library, specifically ensuring that ProductSet and FiniteSet interact as expected in is_subset and equality checks. The solution requires adding a new handler with a multipledispatch decorator, as well as small fixes in relational equality logic and safely handling non-Expr types in Eq.simplify. The engineer would need to understand the dispatch system, the hierarchy of SymPy sets, and some subtleties of equality checks and simplification. This is not trivial and involves editing logic in multiple files, understanding core design patterns, and writing accurate tests. While not a large code change, it requires time to thoroughly understand the bug and verify correctness, making it a 1-4 hour task for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No notable fundamental issues that would prevent using this as a benchmark. The issue is focused, has an appropriate amount of technical depth, and is well-aligned to judging coding ability in a scientific Python context. The only minor caveat is that some of the code examples require careful thought to fully understand their intent, but this is a normal part of an advanced engineering benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20590": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified. It clearly identifies a behavioral difference in the Symbol class between SymPy versions 1.6.2 and 1.7: previously, Symbol instances did not have a __dict__ (as indicated by the use of __slots__), but in 1.7, Symbol instances unexpectedly gained a __dict__ attribute. The reporter suggests this breaks the intended use of __slots__ and is likely a bug. While the description is explicit about the observed problem, it is not completely clear about the root cause or the exact place to fix it; an engineer would need to verify where __slots__ are defined or missing in the class hierarchy. The example with sympy.Symbol('s').__slots__ and __dict__ provides a testable repro. The 'sensible interpretation' is to restore the __slots__-only behavior so Symbols no longer have a __dict__.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual patch is a small change (adding __slots__ = () to a base class), but understanding the issue requires some knowledge of Python's __slots__ mechanism and inheritance. For an engineer familiar with SymPy or Python internals, this task would generally take between 15 minutes to 1 hour to validate the inheritance structure and fix the issue.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no further issues. The issue is suitable for a benchmark scenario testing understanding of Python class mechanics, especially __slots__. The problem could be confusing for someone unfamiliar with __slots__, but it's reasonable within the targeted skill set.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-21379": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well specified. It provides a clear description of the error that occurs when using `subs()` on an expression in sympy with piecewise and hyperbolic functions, and details the circumstances under which the error appears and disappears. It gives a minimal working example, lists variations, and what seems to trigger or not trigger the error. However, the expected outcome is only implicitly specified: that the substitution should work without raising a PolynomialError, which is a sensible and unambiguous interpretation. The root cause is somewhat speculative, but the details provided are sufficient for a competent developer familiar with sympy to attempt a meaningful fix. Only minor blanks are left, mainly confirming that the absence of the error is the desired behavior, not that some specific value must be returned.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While this bug touches on subtle interactions between symbolic assumptions and how they propagate through sympy's internals, the patch and the description suggest the fix is relatively localized\u2014catching a specific error and guarding against it. Given the clear minimal example and the pattern of the fix, an experienced developer familiar with Sympy's core modules would likely identify where the error is raised and protect against it within 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major problems\u2014this is a technical, reproducible issue with a clear symptom and reasonable context. The main complexity is understanding the interaction between different sympy modules, but that is within the remit of a benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-22714": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well specified. It clearly states the context (using sp.evaluate(False) in sympy), the operation being attempted (construct a Point2D from two integers using a sympified expression), and the crash/error message encountered (ValueError: Imaginary coordinates are not permitted.). Example code is given that triggers the issue, and it is clear that the expectation is for this operation to not raise an error when imaginary coordinates are not present. The error message is quoted verbatim, guiding the engineer to the relevant source. In summary, both the reproduction and the desired behavioral change (not raising an error in this real-valued context) are clear and unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue involves tracking down why a ValueError is raised when it shouldn't be during unevaluated construction of Point2D with real coordinates, and then fixing the condition under which imaginary coordinates are disallowed. This likely involves inspecting a specific function in the codebase (the Point2D constructor or its helpers) and tweaking a conditional. The patch shows this is only a single-line logical refinement. For an experienced developer, this is more than a trivial fix (requires correct symbolic logic), but doable in under an hour: a small change requiring thought.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent: the bug is straightforward and well-localized, and the context is clear. The test patch demonstrates that the issue is reproducible and testable with a clear pass/fail criterion using unit tests. The reproduction context, error, and expectation are all explicit.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23824": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. The description provides a clear statement of the problem, identifies the relevant function (kahane_simplify) and its expected behavior, gives detailed examples that reproduce and clarify the bug (including both input and expected output), and even briefly explains the likely source of the bug and fix strategy. There is no ambiguity about what needs to be fixed: contracted gamma matrices should be simplified while preserving the order of any leading uncontracted gamma matrices, and the provided tests make it explicit what constitutes correct behavior. An experienced engineer could confidently implement a fix and verify its correctness from this description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Although the issue is mathematically domain-specific, the fix is relatively contained: it involves adjusting the algorithm by which uncontracted leading gamma matrices are re-inserted after simplification. The problem is well-localized, and test cases directly illustrate the incorrect and correct behavior. For someone familiarizing themselves with the codebase and the mathematics, this would likely take 15 minutes to 1 hour, mostly spent on verifying the logic and running the existing tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is exemplary for benchmark use: it is narrowly scoped, testable, and well-motivated, and does not require information outside of what is provided in the issue description (no need to consult external discussions or documentation). The level of math and familiarity with the codebase required is moderate but not prohibitive. There are no concerns that would prevent its use in a coding ability benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23950": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly explains what is wrong: Contains.as_set() currently returns a Contains object, but it should return the set argument of Contains, because Contains is not itself a set but a boolean. The description even gives a concrete example (Contains(x, Reals).as_set() should return Reals), as well as downstream consequences (errors in Piecewise construction). The relationship between Contains, its arguments, and what as_set() should deliver is unambiguous. Any reasonably familiar engineer would know what code to write to fix this bug. The files, function, and the general class of methods involved are all clearly described. The test also leaves no doubt as to the expected behavior after correction.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change is small and localized: it just requires as_set() to return the .args[1] (the set object inside Contains). The bug is not due to a deep or subtle issue, but just an unimplemented/incorrect method. The fix doesn't require modifying multiple parts of the codebase or understanding complex interactions. While one may want to check if there are related uses, this is simple for someone familiar with the codebase. Writing or editing the associated unit tests is also straightforward, as there is already a testing structure in place for this behavior.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue and its solution make for a good benchmark: the task requires the engineer to think about the data model and API (distinguishing boolean functions from sets), but the implementation is simple once the problem is understood. It very directly tests reading comprehension and basic reasoning.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24066": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The author provides reproducible code snippets demonstrating the problem, specifically showing how SI._collect_factor_and_dimension(expr) works with a clearly dimensionless quantity, but fails in the context of an expression used as the exponent to exp(), raising a ValueError. The text precisely identifies which function has the problem (SI._collect_factor_and_dimension), what is going wrong (failure to recognize a dimensionless exponent), and demonstrates the expected vs. actual behavior. It is clear that a successful PR must make SI._collect_factor_and_dimension recognize dimensionless expressions inside mathematical functions (like exp). No additional context beyond the codebase itself is required to start implementing a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty level is 1 (15 min - 1 hour). The problem affects a single function and requires understanding how dimensions are propagated when mathematical functions with arguments are handled. The issue is localized and reproducible; the solution involves adjusting logic inside _collect_factor_and_dimension to properly return Dimension(1) for dimensionless exponents. While the mathematics of physical units require care, the expected change is in the conditional handling for Function instances, as highlighted by the code patches. This is a moderate fix requiring thoughtful handling, but not a fundamental rewrite or a deep change across the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blocking issues are present. The issue is tightly scoped, reproducible, and the success criteria can be automatically tested with the provided examples. This makes the sample suitable for benchmarking coding ability in a controlled environment. The test patch ensures regression protection.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24443": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well specified. It clearly states that a ValueError is raised when using the homomorphism function with PermutationGroups, specifies the repro steps in code, explicitly points out that the problem arises from the _image() function (linked in the description) when handling inverted generators (r[i]) in the gens test, and hints that the logic is unnecessarily complicated. However, it doesn't spell out the precise expected behavior for inverted generators or exactly what the overall correct solution is supposed to look like\u2014in other words, it tells you the 'what' (a bug is thrown in a particular scenario) and some of the 'where', but leaves 'how should this work' up to the implementer's common sense and domain knowledge. Thus, there are some blanks to fill in, but it supports a sensible interpretation for a meaningful fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This bug requires a reasonable understanding of the combinatorics homomorphism internals, the _image() logic, and group theory, especially related to how elements and their inverses are represented in PermutationGroups. The fix involves substantially rewriting the internal mapping logic to correctly handle inverted generators and is not a trivial addition or surface-level change. Modifying logic that interprets generator relationships within algebraic structures (while ensuring the solution remains general and correct for all users) takes careful reasoning and nontrivial testing. Thus, while not an all-day project, it's more than a quick or small patch and likely to take over an hour when factoring in analysis, implementation, and test writing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major blockers for using this issue as a benchmark sample. The repro is specific, the bug is demonstrable, the impacted code section is identified, and a reasonable candidate for a correct fix is possible for an engineer with suitable background.\",\"q2_5_confidence\":5}"
    }
]