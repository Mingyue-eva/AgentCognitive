[
    {
        "astropy__astropy-13033": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and sufficiently detailed for an engineer to reproduce the bug and understand what successful behavior looks like. The steps to reproduce walk through creating the condition (custom required columns, removal, and the misleading error), and the expected/actual/desired behavior is described in plain language. The intention is that removing any required column (not just 'time') should yield an informative error message about which required columns are missing or mis-ordered, not just defaulting to a misleading message about the 'time' column. The description is specific to the TimeSeries object, the mechanics of required columns, and how errors should be reported. This leaves little ambiguity about the goal: improve the error-checking/reporting logic for required columns in TimeSeries.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small refactor in the error-checking logic of a function that validates required columns in TimeSeries objects. The main task is to ensure that when columns are missing or incorrectly ordered (especially if additional required columns are present), the exception message correctly reflects which columns are required and what was found, rather than always assuming 'time' is the only required column. This likely involves only localized edits and a bit of code to format messages, and is apparent from the supplied patches as well. It is not a trivial one-liner, as it requires thinking about the edge cases and test coverage, but it's well-contained and shouldn't require major time investment.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues with this sample. The information provided is comprehensive, relevant, and the problem is both logical and concrete. The setup fits well for a coding benchmark assessment since it asks for precise changes rather than vague or underspecified improvements.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-13977": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is clear in outlining what the undesired and expected behaviors are, and it provides a detailed reproduction scenario. The user describes precisely the problem\u2014a ValueError is raised by astropy Quantity.__array_ufunc__ when handling duck-types during reflection, when NotImplemented should be returned to allow __radd__ to be called. There is a precise suggestion for the fix: to return NotImplemented under specific conditions, instead of raising an error. However, there are a couple of blanks: The exact location(s) and implementation details inside __array_ufunc__ are left somewhat open, especially concerning edge cases and compatibility. Nevertheless, an experienced developer familiar with NumPy and duck-typing could craft a meaningful fix with this information.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This change requires a solid understanding of advanced Python features: NumPy ufunc dispatch, duck-typing, operator overloading, and the internals of astropy's Quantity. The fix mandates carefully altering a key method, __array_ufunc__, to distinguish between cases that should raise errors and those where NotImplemented should be returned, which requires thoughtful handling of inputs and possibly changes to tests as well. This is not a trivial change but does not require rewriting large amounts of code across multiple modules. Estimate: 1-4 hours for an experienced engineer after familiarizing themselves with the code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would preclude this from being a useful sample for coding ability evaluation. The problem is practical, testable, and the solution requires careful consideration but remains within a single (critical) method. The reproduction scenario and expected result are clear. As always, test coverage is essential for subtleties in error handling and dispatch, but the test patch is present.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14096": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clear about what the problem is and what needs to be addressed. It explains that when subclassing SkyCoord and adding a property that accesses a non-existent attribute, the AttributeError message incorrectly claims the property itself doesn't exist, rather than reporting the missing underlying attribute. The description provides a concrete reproduction scenario and explicitly states the expected versus current exception message behavior. This gives a knowledgeable engineer all necessary context regarding what must be fixed (the propagation of the correct AttributeError) without ambiguity. There is no reference to missing code, specific filenames, or lines, but the behavioral expectations are clear enough for someone familiar with Python's property and attribute access delegation mechanisms to identify and fix the issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a small behavioral fix, requiring investigation of the __getattr__ and property machinery in the SkyCoord class. The needed change is targeted (adjusting how missing attributes are handled so that the AttributeError correctly reflects the underlying missing attribute), and does not demand broad restructuring or extensive edits. An engineer familiar with Python's attribute access model could identify the problematic exception handling and correct it with a small code change, likely within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues present. This is a well-specified, isolated bug with a clear expected outcome and no dependency on non-public information or external clarifications. The sample is suitable for coding ability assessment as it tests a candidate's understanding of Python attribute access and exception propagation.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14182": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified. It describes the expected behavior in ReStructuredText (RST) table output when using multiple header rows, giving both the desired output and the problematic exception encountered. The actual table data, header names, and units are specified, and the distinction between fixed-width ASCII and RST output is articulated. However, it does leave a few details for the implementer to infer, such as exactly how header rows should be rendered in the RST format and any format-specific edge cases. Still, there is a clear and sensible main interpretation that an experienced engineer can implement without further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This task requires editing core writer logic to support variable header rows in the RST output, which involves understanding and modifying how headers are processed and rendered. The change involves alterations to multiple parts of a module, appropriate handling when different header_row arguments are passed, and ensuring round-trip compatibility for reading and writing tables. The test addition is nontrivial and requires an understanding of the codebase conventions. While not exceedingly complex, it is more than a quick fix and could take a few hours for someone not already deeply familiar with the code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues; the sample is suitable for evaluating coding ability and follows the problem description closely. The core challenge is in generalizing the output logic and test coverage, which is appropriate for benchmarking.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14309": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly explains the observed error (IndexError due to tuple index out of range) and provides the context in which it occurs: running format detection on a filename with a non-FITS extension causes the FITS connector logic to assume a positional argument exists when it does not. The steps to reproduce the issue are specific enough for an experienced engineer to replicate the failure. While some familiarity with the `identify_format` machinery and the FITS connector in the Astropy Table IO system would be needed, the description contains a sensible interpretation of what is required: the FITS identifier should not assume positional arguments are present, and should not attempt to apply FITS-associated logic if the filename extension does not match FITS types. The mention of a recent change in Astropy further helps narrow down the cause. There is some need to fill in implementation details, but the \\\"what needs fixing\\\" is clear, so this is rated as level 1 (some blanks, but clear expectation).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires adjusting the FITS identification routine to not index into args if the provided filepath is not FITS-like, which is a relatively small code change. An experienced engineer should be able to identify the function in question, clarify its usage, and validate behavior via existing or new tests in roughly 15-60 minutes. The patch itself is small and targeted. A bit of care is needed to ensure correct behavior for all input signatures, but the difficulty is modest and does not involve major refactoring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent with this benchmark sample. The issue is specific, the required change is well-focused, and there are clear testable outcomes. The scenario also mimics a real-world bugfix context suitable for evaluating coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14365": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It explains that the current ASCII QDP file reader in Astropy expects upper-case QDP commands, although the QDP format itself is case-insensitive. There is a concrete example: a file line 'read serr 1 2' (all lowercase) currently causes the parser to crash, but it should work the same as the upper-case equivalent. The steps to reproduce the problem are described, along with information about the error behavior and the expected behavior (i.e., the table should be loaded with error columns, not throw an error). This is sufficient for an engineer, familiar with the codebase, to correctly interpret the required functionality and devise a solution, without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Implementing this fix requires a basic but careful change: making the command parsing case-insensitive in the QDP file reader. An experienced engineer would need to trace where QDP commands are processed and ensure that both the regular expressions and any string comparisons treat command tokens without case sensitivity. The changes are minor in scope, mainly involving regular expressions, and are straightforward. This should take less than an hour, including verifying with new/modified tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a well-posed issue, focused, and has no major drawbacks. The tests show how to validate success (reading QDP files with lowercase commands). The only complexity is ensuring correct scope of the case-insensitivity, which is manageable for a benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14508": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified, clearly defining both the problem and the expected solution. The reporter explains that the io.fits.Card class in astropy sometimes uses a string representation of float values that is longer than necessary, leading to comment truncation when constructing header cards. The expected behavior is also explicit: the user should be able to create any valid FITS Card, with the value and comment preserved. The reproduction steps are thorough\u2014starting from examining a FITS file with a specific float value and comment, attempting to duplicate it via code, and confirming undesired behavior (including the type of warning and nature of the truncation seen). All inputs and expected observations are well described for a developer familiar with the codebase to implement a solution, likely in the io.fits.Card or its float formatting logic.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is relatively contained: the issue is about how floats are converted to string representations for FITS header cards. This would require finding the piece of code responsible for formatting floats, making adjustments so that the float string is no longer unnecessarily verbose, and ensuring comment truncation does not occur unnecessarily. Such a change may require careful consideration of edge cases (e.g., scientific notation, precision) and ensuring compliance with FITS standards, but the actual code edit is modest and largely localized. An experienced engineer would likely solve this in under an hour, including rerunning or augmenting relevant tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further blockers or issues: the sample is clear and representative of typical formatting/serialization bugs found in scientific Python libraries and fits well with the benchmarking setup. Testability is also straightforward given the test case structure described.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14995": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified: it describes when and how the bug appears, provides explicit instructions on how to reproduce it, states what the expected behavior should be, and pinpoints the context (mask handling in NDDataRef arithmetic operations when only one operand has a mask, and the use of handle_mask=np.bitwise_or). The reporter even notes that the problem occurs in v5.3 but not in v5.2, and clarifies what the mask propagation should do in these cases. However, there's a small degree of ambiguity: the exact intended semantics for mask propagation could be debated (is 'just copy the mask that exists' always valid?), and the engineer may need to look at/interpret the codebase and 5.2 implementation to fully confirm the ideal fix. Still, overall, it's clear enough for a meaningful engineering attempt without requiring external clarification beyond what's provided in the issue text.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change only requires a small tweak in how the mask is propagated depending on which operand has a mask; the main logic already exists, so it is a matter of handling the None case correctly. This involves understanding the arithmetic mask handling in NDDataRef, replicating/adjusting prior version logic, and writing/adjusting relevant unit tests. An engineer familiar with the codebase/editing similar code would likely solve it in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue provides enough information and context, as well as explicit reproduction instructions and a clear expected behavior. The only possible challenge is confirming the desired mask propagation semantics, but the issue plus reproducing in v5.2 should resolve this. The sample is suitable for use in the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7336": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified for an engineer to attempt a solution. The issue details a specific decorator, 'units.quantity_input', and a specific use-case: decorating a constructor (__init__) in Python that is type-annotated to return None. The user reports a concrete error: the decorator attempts to call 'to' on a None value. A minimal reproducer is provided with clear indications of Python version, Astropy version, NumPy version, and the context. The desired behavior and a potential fix are also suggested directly in the issue, making the requirements for a solution unambiguous. The file/class/function in question is clear from the context.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding how Python decorators work, introspecting type annotations, and handling a specific edge case in the decorator logic. The actual fix is a small change to an existing if-statement that checks the return annotation, with the addition of another case for 'None'. Familiarity with the codebase helps but is not strictly necessary as the change is localized and the problem is well-scoped. Design/testing considerations are straightforward. The whole process (including running/adding a test) is likely to fit within 15 min to 1 hour for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers or ambiguities in the issue statement that would disqualify its use for evaluating coding ability. The minimal reproducer and suggested fix both help clarify the requirements. While user experience with Python decorators and type annotations is assumed, this is reasonable for the context. The only challenge is understanding the decorator's internals, which is reasonable for this benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7606": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes a concrete problem: comparing a Unit instance representing an unrecognized unit (created with silent parsing) to None leads to a TypeError, but the expected behavior is for the comparison to return False (i.e., Unit == None should yield False, rather than raising). The reproduction steps are explicit and clearly explain how to trigger the problem. The expected and actual behavior are both stated unambiguously. The engineer tasked with this issue would know exactly what condition to fix and what the desired output should be without needing to ask for external clarifications. The relevant context for the bug is provided, focusing on equality/inequality comparisons involving unrecognized units and None values.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The root cause of the bug is straightforward and limited to the equality methods of specific classes related to units. The fix involves changing exception handling in the __eq__ implementation(s) and returning NotImplemented to allow Python to handle the comparison and return False. This only requires a small code change in 1-2 methods and does not require a deep understanding of the entire codebase, just familiarity with equality comparison patterns in Python and the project\u2019s code structure. Including or adapting a relevant test is also straightforward as the required test case is directly suggested by the issue. Thus, for someone proficient in Python and after a brief codebase familiarization, resolving this should take no more than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues or caveats with this sample. The required fix and test addition are clear, the relevant classes and behaviors are referenced or easy to deduce, and there are no ambiguities about the expected result. The issue is suitable for evaluating coding ability on the described benchmark setup.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7671": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified in diagnosing the underlying problem: calling minversion with a version string like '1.14dev' causes a TypeError when using distutils' LooseVersion, due to incompatible comparisons between int and str. The description gives a minimal example, describes the root cause, and compares behavior with a previously used function (pkg_resources.parse_version). However, it does not spell out exactly what the replacement logic should be, or how to update minversion\u2014should support for all PEP440 dev/release version strings be included, or only special-case 'dev'? That said, with the information given, an engineer can reasonably infer that the goal is to prevent TypeError and replicate the behavior seen with pkg_resources.parse_version for such cases. The desired behavior and scenario triggering the bug are clear. Some small blanks for best practices remain, but the main requirement is understandable for a meaningful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires a small modification to the minversion function to handle version strings in a way that prevents the TypeError from LooseVersion. This does not require substantial rewriting, just a pre-filtering of the version string or use of regex to extract a compatible version substring. The main challenge is noticing and working around the limitations of LooseVersion for dev/rc labels, as described. The scope of the change is localized and should be achievable within an hour for a familiar engineer. It requires some thought and care to not introduce side effects but is not fundamentally difficult.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent. The sample is usable as a benchmark for coding ability, as the bug is clearly described, the symptoms and probable root cause are given, and the fix is a focused code task. This is a classic small bugfix suitable for intermediate testing. The test update in the patch verifies that the new edge case is handled as required. There are no extraneous complexities or ambiguities that prevent it from serving as a coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-10554": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified: it describes a reproducible scenario in Django ORM involving combining querysets using union() with an ordering on the second queryset. It spells out the setup, the observed result, and the error message when inspecting the queryset after clearing orderings. However, it does not specify any class or file names directly, nor does it reference implementation details, but an experienced Django developer could reasonably narrow the focus to the query combination and ORM SQL compilation logic. The scenario is specific (ordering with union and clearing orderings), making the desired fix deducible, but requires some interpretation about what the correct long-term behavior for such combinations should be.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue likely requires understanding of Django's ORM internals, specifically how querysets are built, combined, and executed (especially SQL compilation and order by handling). The fix itself is moderate in code size, but non-trivial because it involves manipulating the queryset structures to ensure select fields match the columns used in ORDER BY after unions\u2014a subtle and easy-to-break area, as evidenced by the edge-case error. Tests must carefully cover multiple permutations. Overall, it would take a few hours (1-4 hrs) for an experienced developer to fully trace, reproduce, devise and implement a robust fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further major issues are evident. While a bit domain-specific, the scenario is representative of realistic ORM bugfix work and provides both a clear repro case and observable error, making it a solid benchmark for coding ability\u2014even though it requires a moderate amount of knowledge about how Django handles ordering and SQL compilation with unions.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11087": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified: it describes the environment, steps to reproduce the problem, root cause analysis, and specific suggestions for improvement. It clearly identifies two closely related problems: (1) the UnicodeDecodeError due to differing unicode handling in MySQL client libraries between Python 2 and Python 3, and (2) the sub-optimal behavior of Django's .delete() cascading queries which select fields that are not truly necessary for the delete operation. The main required fix is identified: optimize .delete() to only retrieve the fields required for cascading deletes, thereby avoiding the problematic text_log_error.line field and improving performance. However, there are some details (e.g., exactly how to identify required fields, how to integrate with Django's ORM codebase) that require the engineer to make design decisions and interpret what is meant by 'referenced fields.' Thus, there is a sensible interpretation and a clear target behavior, but there is room for blanks to fill in around implementation.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a solid understanding of Django ORM internals, particularly the deletion and cascading logic. The fix is non-trivial: the engineer must analyze how fields are selected for queries during .delete() cascades, identify which fields are minimally necessary (primary/foreign keys and referenced fields), and alter the code so only those are queried if no signals/listeners are present. The engineer must ensure not to break deletion semantics, including corner cases (such as when signals are connected). The patch touches core ORM code and requires the creation of detailed tests. Familiarity with the Django project structure and test suite is needed. The changes are moderate in size and require both a careful change and comprehensive test coverage, making it a 1-4 hour task for an experienced developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain unaddressed. The sample is appropriate for benchmarking coding ability: the problem is clearly linked to code and test changes, the initial issue description gives enough context, and the challenge is meaningful without requiring esoteric knowledge. One possible minor caveat is the need to reason about ORM design and not just implement a local fix, but that is within bounds for evaluating expertise.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11265": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear, concrete example of what does not work: applying exclude() to a queryset annotated with a FilteredRelation raises a FieldError, whereas the expected behavior is to return no results (if the annotation is non-null). The specific part of the Django ORM, QuerySet annotations, and FilteredRelation usage are called out, and the problem occurs when using exclude(book_alice__isnull=False) after such an annotation. The reporter also points to a possible faulty function (split_exclude) and explains the break: the new query loses extra data including filtered relations. However, details like exact traceback or all contextual code of the models and test setup are not present in the description, so implementers will need to make reasonable assumptions about test structure and model fields. The expected fix is clear: ensure FilteredRelations propagate correctly for excludes, but some blanks about edge cases or related changes to other areas persist. Thus, the issue is well interpretable for someone familiar with Django internals, but not perfectly fully specified.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a good understanding of Django ORM internals\u2014specifically, how annotations and FilteredRelations are handled in query construction and exclusion processing. The fix touches core query-construction code and involves understanding why certain context (filtered relations) is lost when building a subquery for an exclusion. It also needs tests added or adapted to cover the fixed behavior. For an engineer experienced with Django but not intimately familiar with this part of the codebase, time would be taken to read through related Query and annotation handling logic, reproduce and understand the failure, design the propagation of filtered relations, and validate with appropriate ORM-level tests. Therefore, this is a non-trivial fix that would span more than an hour but likely less than a day for an experienced engineer. It is substantially rewriting and modifying function internals, not just a small or trivial change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers for using this sample\u2014the issue is narrowly scoped to a feature of the ORM, the fix required a nuanced codebase understanding, and a relevant test is available. No extraneous dependency on external discussions. The main limitation is that it presumes understanding of filtered relations in Django, but this is acceptable for a benchmark covering framework internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It gives a clear description of the bug: Django is generating SQL check constraints on SQLite and Oracle that incorrectly use fully qualified field names when there is a mix of OR and AND clauses, causing migrations to fail. The reproduction section provides concrete steps to demonstrate the issue, describing the model fields, the form of the constraint, and the behavior observed during migration. The expected behavior is also explicitly described: the check constraint in the CREATE TABLE statement should not include table name qualification, just the raw column names. An experienced engineer would know which parts of the codebase (e.g., model constraints generation, schema editing for migrations) to focus on, and what successful output (clean migration with unqualified field references) looks like. There is little ambiguity about what is wrong and what is required for a fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This requires a deep understanding of how Django constructs SQL constraints, interacts with schema editors for different databases, and how column qualification occurs within constraints. Although the issue is clearly described, the solution likely involves navigating internal query/model constraint-building mechanisms and making non-trivial changes; the patch touches low-level query construction code. The engineer must ensure database compatibility and probably update or add relevant tests. This represents more than a quick or small change but does not seem to require days of research or vast refactoring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues prevent this sample from being suitable for evaluating coding ability. The problem is clearly described and solvable with the information provided, and it tests understanding of ORMs, SQL generation, and database-specific quirks.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11532": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it describes the exact problem (\u201cEmail messages crash on non-ASCII domain when email encoding is non-unicode\u201d), the steps to reproduce (setting a non-ASCII hostname, running Django mail unit tests), the expected behavior (Django should convert non-ASCII domain parts in the Message-ID header to punycode, e.g., 'xn--p8s937b'), and where the change should take place (in django.core.mail.utils or django.core.mail.message). Furthermore, the test scenario is clearly explained, referencing how DNS_NAME is patched and EmailMessage constructed, and the expected assertion. The runtime error is described (UnicodeEncodeError) and specific lines/files are mentioned. All essential details for implementing and validating a solution are present.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a moderate amount of effort because multiple files and functions are involved (e.g., message, utils, validators, html, encoding). It involves understanding how Django handles EmailMessage creation, header encoding, and how to replace domain parts with Punycode when needed. To properly fix the issue, the engineer would need to trace all the usage of domain names throughout the mail system, introduce a helper function, and update several key spots. Testing requires patching and running a non-trivial test. This is not a trivial or single-function fix, but also doesn\u2019t require deep research or architectural changes, making it an appropriate '1-4 hours' or '2' on the scale.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional major issues. The issue is well-scoped, the context and reproduction steps are provided, and there is a direct testable outcome. The sample is suitable for evaluating coding ability, especially with respect to Unicode handling, encoding, and standards compliance (punycode in email headers). The only minor challenge might be broader understanding of the IDNA (punycode) encoding and mail header construction standards, but this is within the expectations for an experienced engineer working on Django's internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is highly detailed and well-specified. It describes the regression bug in admin list_display validation that appeared after upgrading from Django 2.0.7 to 2.2.1, explains exactly what is failing (the E108 error issued when a PositionField is used in list_display, although this worked before), and provides a systematic breakdown of expected versus current/broken behaviors. The relevant function (_check_list_display_item) and the underlying logic are thoroughly analyzed, along with what changed in the codebase and why that broke existing behavior. Sample models and fields are given. The correct desired logic and justification for it are clearly specified, plus a sample code fix is provided. This means any experienced engineer could understand both the bug and the proper end state directly from this report.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The underlying issue is non-trivial as it involves nuanced understanding of Django model field descriptors and the admin's validation mechanism, but the required code fix is confined to adjusting the control flow and exception handling in a relatively small internal function. The description and test case provide clear guidance, so for an experienced developer with knowledge of Django's admin internals, this change could likely be implemented in under an hour. It is more than a trivial single-line change, but does not require deep rewrites, and is localized to a specific method.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues are apparent. The report is self-contained, makes no assumptions about private context, and does not require external discussion to resolve. Both the test and patch are targeted and reproducible given the provided description.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11734": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is somewhat well-specified: it describes clearly a situation in Django ORM where using OuterRef within an exclude() or ~Q() operation raises an unexpected ValueError. It also lays out a reproducing test scenario and describes the expected vs. actual behavior. However, while the issue does explain the models and the type of annotation/subquery used, it requires the reader to infer some specifics about how the ORM is supposed to treat OuterRef objects within exclude()\u2014this is likely apparent to an experienced Django contributor but leaves out some details an absolute outsider might need. There are, however, sensible interpretations for how the ORM should behave, and the reproducer is clear enough. It is not as crisp as the pylint ignore-paths example, but certainly clearer than the ambiguous or vague examples given in the guide. Thus, this best maps to '1: There are some blanks to fill in about the issue, but there is a sensible interpretation of what is required for a successful solution.'\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a fairly deep understanding of how Django's ORM constructs subqueries and handles outer references, especially in the context of exclude() and Q objects. The solution touches multiple locations in the codebase, including field preparation and query construction. The developer must comprehend not only how to avoid the error, but also ensure the logic maintains semantic correctness for ORM queries using OuterRef. It is not a huge patch but is non-trivial and subtle, requiring careful consideration\u2014especially since it crosses multiple core files. The time estimate (1-4 hours) is warranted both for diagnosis and for writing tests to verify the fix across various subquery cases.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues were identified. The setup is valid for benchmarking coding ability: the issue text meets the minimum clarity threshold, with a concrete reproducer provided and clear expected-and-actual outcomes. The problem is well motivated, directly relevant to Django users, and testing verifies correct behavior pre- and post-fix. The main caveat is that deep ORM knowledge is required, but that is a fair aspect of evaluating coding ability in this domain.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11740": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. The description gives a clear minimal Django project structure (two apps, models with UUIDs, a field changing from a plain UUID to a ForeignKey), and the expected behavior: the migration should include a dependency on the target app when a ForeignKey is added. The problem is described with relevant context (Django 2.2, PostgreSQL, UUID PK, SET_NULL). However, it doesn't provide actual source code, filenames, or the exact migration filenames, so the engineer would have to reconstruct the minimal setup. The expected outcome (dependency appears in migration, error without it) is clear, but minor assumption-filling is needed around project layout and specifics. (Compare with example: interpretation is clear, but blanks must be filled in.)\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires a good understanding of Django's migration system and how field changes should trigger cross-app dependencies. One would need to read the migration autodetector logic, check how dependencies are gathered, and ensure altering a field from a UUID to a ForeignKey triggers the right dependency. This could involve understanding and editing a significant portion of migration generation logic, writing/adjusting tests, and careful manual validation. For an experienced Django engineer, this would take a few hours: not trivial, but not a deep/large refactor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The main vagueness is in the fact that the issue text doesn't give explicit code fragments or filenames, but the high-level requirement is clear and sensible. No major blockers to using it as a coding benchmark, provided the engineer is familiar with Django app and migration conventions.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11815": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It describes a clear and concrete defect: when an Enum is used as a default value for a CharField in Django migrations, the generated migration records the enum's translated value, not its constant name. This leads to failures when the migration is run under a different locale, as the translated value won\u2019t match any enum member. The problem, reproduction steps (defining an enum with translated values, using as a default, running makemigrations, and then changing locale), symptoms (ValueError about an invalid enum choice), and the desired behavior (the migration should use the enum constant/key, not the translated value) are all explicitly detailed. There\u2019s no significant ambiguity about what needs to be fixed in the codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is rated at 1 (15 min - 1 hour). The fix requires understanding how Django serializes Enum members in migrations and updating the serializer to refer to the Enum member by its key (name) instead of its value. While some familiarity with the Django migration system and serializers is needed, the core logic change is scoped to a specific serializer and is not overly complex. It requires careful review to prevent breakage and good test coverage, but not major refactoring or research\u2014hence a moderate but not trivial level of difficulty.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The scenario is realistic for an engineer to encounter and solve. The description is specific about the problem, steps to reproduce, and expected behavior. Testable outcomes are clear, aligned with the intended use in this benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11964": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear about the observed and expected behaviors, specifically in the context of Django's TextChoices/IntegerChoices fields. It provides concrete reproduction steps involving model creation, assignment, inspection of values, and the discrepancy between types when accessing values immediately after creation versus after a database fetch. The main requirement\u2014that str(MyChoice.FIRST_CHOICE) should return the raw string value\u2014is explicit. The runtime error and its consequences are also described, and there is enough detail for an engineer familiar with Django, enums, and Python class models to know which area of the code to investigate.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is conceptually simple: it requires overriding the __str__ method to ensure string conversion of enum members behaves as expected within model fields. An experienced Django/Python developer familiar with enums should be able to identify the issue, locate the relevant Choices/Enum class, and implement the required method rapidly. The required change is localized and does not demand complex logic or a deep rework, just a correct override and confirmation that it addresses both immediate usage and after database fetching.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The issue and solution are technically clear, and this sample is well-suited for evaluating a candidate's ability to reason about Python enums, Django fields, and string conversion. The required knowledge base (Django enums and model fields) is common for intermediate Python web engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12155": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well specified. It describes the precise problematic workflow: rendering documentation in Django admin fails if the docstring does not start on a blank line, while it works if there is an empty first line (the Django style). The expected vs. actual behavior is explained explicitly. The key line of code responsible for the failure is identified and explained:\\nindent = min(len(line) - len(line.lstrip()) for line in lines if line.lstrip())\\nThe cause of the failure (indentation is 0 when the first line is not empty) and the explicit solution (skip the first line for the calculation) is described. Thus, any engineer\u2014even unfamiliar with the codebase\u2014would know exactly what to do: modify how indentation is calculated by skipping the first line. \\nRelevant filenames, function names, and the code area are all either indirectly described or trivially searchable, making the implementation clear and not open to substantial interpretation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is conceptually straightforward once the bug is understood. The responsible line and the reason it fails are directly called out, and even a corrected version of the code is proposed in the issue description. An experienced engineer can quickly test this, update the function (likely in a single file), and ensure compatibility. Additional diligence to check call sites would be prudent but not time-consuming. The total change is minor, but some thought is needed to verify the root cause and to ensure all usages are updated accordingly. Overall, this fits into the 15 min - 1 hour category.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues or blockers for including this sample. The problem and the solution are constrained enough for a benchmark, and there is no ambiguity in interpretation. The test file update is straightforward and adds relevant coverage.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well specified. The problem description makes it clear that custom template tags (and inclusion tags) in Django raise a TemplateSyntaxError when they receive a keyword-only argument with a default value, even when that argument is provided correctly. It further highlights that supplying such an argument twice does not raise a 'multiple values' error as expected. There are some possible blanks\u2014for example, exact edge case expectations and how the argument parsing should behave in interaction with other forms of arguments\u2014but the issue gives enough information, including specific error scenarios, for an experienced engineer with awareness of Django's template tag system to make a sensible fix. The part that could use clarification is the full spectrum of related behaviors and whether similar issues could happen with other types of arguments, but the core requirement is clear: fix the handling of keyword-only arguments in custom (and inclusion) template tags so that they are recognized correctly and errors are raised as expected in conflict situations.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Most of the work involves finding where Django's template tag parsing logic distinguishes between regular and keyword-only arguments and ensuring that keyword-only parameters with defaults are properly accounted for. The actual change, as shown in the patch, is quite localized\u2014a single conditional adjustment in one file. Writing and updating the corresponding tests is straightforward. For someone already familiar with Django\u2019s templating system and test suite, this kind of issue would take less than an hour to diagnose, fix, and test. The challenge is more about understanding the domain than making extensive code changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers or other issues for using this issue as a benchmark. The scenario is concrete, connects clearly to user-facing errors, and is backed by a targeted test suite. The requirement is not esoteric and would be intelligible to engineers familiar with Django or broader Python template systems. The only challenge might be for those unfamiliar with Python 3's keyword-only arguments, but that's still within reasonable expectations for the target benchmark audience.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12325": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete example involving Django model inheritance with multiple OneToOne relationships and how the order of field declarations causes inconsistent behavior and errors. The description makes it clear that the ordering should not matter, especially since an explicit parent_link is present. While the terminology and context (e.g., parent_link, OneToOneField) are clear to someone familiar with Django internals, there are some details that require familiarity with the ORM's internals and model inheritance. The description would be confusing for those without Django experience, but for a Django developer, it is specific enough to understand both what the problem is and the high-level expected direction for fixing it. There is some room for interpretation in how to approach the internals, but the problem and expected result are clear. Thus, it's a level 1 (some blanks to fill, but a clear goal).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Diagnosing and fixing this issue requires a solid understanding of Django's model inheritance system, especially how parent_link and OneToOneField are resolved in the metaclass machinery. The solution involves modifying key internals (like base model metaclass instantiation and model option preparation). Additionally, updating and potentially understanding the relevant tests, and ensuring backwards compatibility would take time. Thus, for an experienced Django engineer, this is likely a 1-4 hour task, not just a small fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major blocking issues for benchmark suitability. The issue is self-contained and the patch + tests directly relate to the described problem. The main challenge would be for those not familiar with Django internals; for the intended audience, the issue is appropriate.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12663": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified: it describes an apparent regression with using SimpleLazyObject in Django queryset annotations and model field comparisons. The problem is described in detail, including model definitions and a step-by-step explanation to reproduce the bug scenario. It specifies what used to work, the expected behavior, and the current failure mode (type error on int(SimpleLazyObject)). The only thing left somewhat implicit is the exact mechanism or location in the codebase that caused the regression\u2014this requires the engineer to investigate the code introduced by the specified commit. Nonetheless, it is fairly clear what constitutes a successful resolution: filtering a queryset annotated with a subquery via SimpleLazyObject (as per the process described) should work, not error. Occasionally, the precise fix for a regression can be ambiguous if the broader intent isn't stated, but in this case, the failure is specific enough.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Understanding and fixing this regression requires a nuanced understanding of Django's ORM internals, subquery annotations, and how Django prepares values for lookups. Finding the root cause involves tracking changes after a specific commit and ensuring the handling of wrapped objects like SimpleLazyObject matches expectations. The solution needs careful adjustment in a core ORM component (the query system) and additional testing. Such work generally takes significantly more thought and care than a trivial bugfix, and involves multiple interrelated ORM concepts\u2014hence 1-4 hours is an appropriate estimate for an experienced contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other outstanding issues that would prevent this issue from being a good benchmark sample. The testable reproduction steps and the clarity about erroneous vs. correct behavior make it suitable. The Django context is advanced, which increases difficulty but is not a fundamental problem for evaluative use.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12708": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is generally clear: when there are both unique_together and index_together constraints on the same fields in a Django model, and only index_together is removed, Django migrations fail because Django cannot distinguish between the unique and normal index when dropping the index. The reporter explains both reproducible steps (create a model with both constraints, then remove only one and migrate), expected behavior (index is dropped, migration completes), and observed (ValueError due to ambiguous constraint). The issue also touches on a related desired behavior about refactoring to use the newer 'indexes' feature, but the main actionable point is about safely deleting index_together when unique_together remains. While the description could specify more implementation detail (e.g., what exact error message is seen, relevant code context), it provides enough concrete info and reproduction to develop a sensible fix, subject to a small amount of interpretation.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a deeper understanding of Django's migration system, particularly how constraints are managed and identified during schema alterations. Creating a clean solution requires navigating Django's ORM internals, understanding how constraints are introspected and altered, and ensuring other scenarios are not broken. It also calls for amending or extending relevant tests. The fix itself isn't lengthy, but the challenge lies in identifying the subtle bug and ensuring correctness with multiple constraints. For an engineer familiar with Django's schema editor/migrations, this is likely a multi-hour (1-4h) task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The bug and solution are tightly scoped, relate to core migration behavior, and have both clear cause and reproducible behavior. No major issues prevent using this as a coding benchmark. It does require a respondent who is comfortable reasoning about database/index constraints and Django's migrations, but this is appropriate for a senior developer benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12754": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a concrete scenario, with steps to reproduce, an explanation of the expected result, and details about the error encountered. The mention of order-dependence in migration operations and the specific models/fields involved (Readable, Book, title field) makes it clear what the bug is and what the correct behavior should be. An engineer familiar with Django migrations will have enough information to attempt an effective fix without needing further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue involves understanding Django's migration autodetector logic, especially regarding model inheritance and operation ordering. Fixing it requires working within the migration system's internals, writing a dependency-detection patch, and likely adding or modifying a test. This is not trivial, but also not a full-system rewrite; it reasonably fits into the 1-4 hour range for an engineer already familiar with Django's codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue is clear, and the test case validates the fix properly. This is a good sample for evaluating coding ability around framework-level logic.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12858": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified for someone familiar with Django's system checks and model field lookups. It directly reports the check error (models.E015) and the situation: ordering refers to a lookup (supply__product__parent__isnull), and describes the model relations, as well as the regression (\\\"used to be accepted before the change in issue #29408\\\"). However, the reporter does not propose a direct ideal solution or a clear acceptance criterion beyond mentioning that the order-by in queries works as expected in practice. The issue requires some inference: the engineer must recognize the bug is in the validation logic (system check), not actual model querying, and must fill in the gap that the system check should accept lookups, not just transforms. Thus, it is not maximally explicit, but a sensible solution is possible from context, earning a 1 ('some blanks to fill in').\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires an understanding of Django field lookups, ordering, and the system check framework. Once identified, the change is not substantial\u2014adjusting the validation logic to accept lookups as well as transforms\u2014but does require a bit of thought about what constitutes a lookup versus a transform. It's likely someone familiar with Django internals could make the fix and add a corresponding test within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues. The issue sample is suitable for use in coding evaluation, as it tests understanding of ORM concepts and internal checks. It does not depend on external code or require unspecified information. The only minor concern is the need for some background knowledge of Django's internal model validation logic, but that is inherent to the nature of the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13012": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified. It gives a concrete description of an ORM query flow and details when the bug occurs: when using an ExpressionWrapper with a constant expression in the context of Django ORM annotation, grouping, and aggregation. The issue explains that the bug leads to incorrect SQL generation (with constants being placed in the GROUP BY clause) and contextualizes why this is a problem (Postgres rejects it with ProgrammingError). There's also a mention of how not using ExpressionWrapper avoids the issue. However, while the high-level requirement and expected behavior are clear, the affected function/class isn't named directly. The engineer must infer that this relates to ExpressionWrapper's get_group_by_cols (or similar grouping logic). There is some room for interpretation, but sufficient information is present for an experienced Django developer to create a sensible fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires identifying the point in Django's expression system where ExpressionWrapper interacts with ORM grouping logic, and then overriding or modifying its behavior so constants do not generate incorrect GROUP BY clauses. This is a targeted change (likely to a single class and a method override), but requires understanding how SQL grouping is computed in Django and some knowledge of Django's ORM internals. For someone familiar with Django's ORM and codebase, this would take between 15 min and 1 hour. It is more than a trivial fix, but limited in scope to a very specific behavior.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified. The sample is a good candidate for benchmarking: the problem is isolated, testable, and relevant to realistic Django ORM use-cases.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13028": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description contains enough information to understand the problem: filtering on a model field when that field is related to another model which has a boolean attribute named 'filterable' set to False causes a NotSupportedError in Django 3.0.7, whereas changing the attribute name resolves it. The relevant models and expected operation are both described, and the nature of the error is clearly explained. It is sufficiently clear for an experienced engineer to identify the underlying issue (namely, that Django is looking for a field-level (or model-level) 'filterable' attribute and wrongly interprets user attributes), and to propose a code solution. However, the report mixes the symptom and what fixed it (renaming 'label' to 'filterable_test'), and it only hints that either documentation or a code change may be required, so there is a small amount of ambiguity in whether to document or code-fix. But a sensible fix is possible from the description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core issue is limited in scope: identifying how Django's filter mechanism checks for the attribute 'filterable' on model fields or related objects, and ensuring that only appropriate objects trigger the NotSupportedError. The actual code change is just a guard to check if the attribute belongs to a filter expression rather than any object, and the new test covers the edge case. An experienced engineer familiar with Django, or at least its ORM internals, could identify, fix, and test the bug in well under an hour. The problem requires debugging and reading ORM code, but does not involve substantial rewrites or architecture changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other outstanding issues are present with this sample. The problem is well-formed and can be solved based only on the provided description. Test coverage is clear, and the code patch scope is limited. The ambiguity about whether to document or fix in code is minor and does not preclude suitability for evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13112": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The report provides explicit reproduction steps: create a Django app with a mixed-case name, set up two models (one referencing the other by ForeignKey), and run makemigrations/migrate. It clearly identifies both the trigger condition (mixed-case app name) and the regression (worked in Django 3.0, fails in 3.1b1), plus quotes the error message. It includes all relevant code snippets (settings.py, models.py, apps.py) so a developer understands both the configuration and how the ForeignKey is set up. The expected outcome (successful migration) and the actual problem (migration halts due to case-mismatch in app label resolution) are obvious. All this makes it clear what the bug is and what a successful solution must accomplish.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem requires a fix to how Django serializes/deserializes ForeignKey string references in migrations, ensuring app label case sensitivity/preservation. Although understanding Django's migrations framework and how app labels are resolved is non-trivial, the scope of the required code change is clearly localized (e.g., in deconstructing and reconstructing model references), and the report and error trace point the developer toward a specific area of code. The patch confirms it's a small, targeted logic fix. An experienced engineer could likely investigate, adapt tests, and write the patch in under an hour, although familiarity with Django's codebase and migration internals would help. Not a trivial fix, but not an all-afternoon project, either.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The sample is well-suited for a coding skills evaluation in the context described: it is well-scoped, self-contained, and tests are provided. The only potential challenge is if the participant is not familiar with Django internals, but that's expected from a suitable benchmark of intermediate complexity.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly clear in scope. It provides a concrete example scenario (an Experiment model with two DateTime fields, start and end), describes the expected behavior (annotating each record with the interval between end and start), and gives specific information about the cause of failure (mixing datetime and duration fields requiring an explicit output_field). However, the description assumes familiarity with Django ORM expressions and its handling of temporal arithmetic, and does not specify *where* and *how* the fix should be implemented\u2014leaving some minor blanks for the implementer to fill (e.g., which part of the ORM expression system should be modified). Still, it is well within the norm for engineering issues and is overall actionable for an experienced developer. Thus, it merits a '1'.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix requires substantive understanding of the Django ORM expression internals and how different database backends handle temporal arithmetic. The solution goes beyond a trivial change: it involves editing and refactoring the core logic in expressions.py and updating a wide swath of related tests. This demands careful reasoning about type handling, output field determination, and correct SQL translation for temporal data types. Thus, it would likely take in the range of 1-4 hours for an engineer familiar with Django internals, but not so long as to suggest it is a very esoteric or massive change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues. The issue is sufficiently precise for an evaluation benchmark and the associated tests clearly measure the defined requirements. This issue is appropriate for coding ability benchmarking, especially for developers familiar with ORM or Django-like systems.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13297": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is generally well-specified for an experienced engineer. It gives a precise setup to reproduce the problem, references the relevant classes/methods (TemplateView, get_context_data, kwargs), explains the observed change in behavior between Django 3.0 and 3.1, and identifies the problematic object type (SimpleLazyObject). It clearly states the necessary workaround (casting the lazy object to string before lookup), which suggests what a patch should do. However, it does not specify precisely where in the framework the fix should be, nor the desired long-term behavior (should kwargs always be plain strings, or should all users cast?), so the fix location and precise approach must be inferred. Still, the description is sufficient for a reasonable solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem can be understood and localized fairly quickly once familiar with Django internals, especially since the issue describes how to reproduce it and the workaround. The required code change is quite small (essentially, ensuring that kwargs passed into the context are eagerly evaluated, not left as SimpleLazyObjects), and the impact is in a known area (context data for TemplateView). Some care is needed to avoid breaking deprecation warnings and to ensure backwards compatibility, but overall, this is a small fix that an experienced Django developer could resolve in 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is suitable for evaluating coding ability, as the bug is sufficiently clear, has direct practical implications, involves a known Django extension point, and the solution requires knowledge of Django's LazyObject system and context passing. There are no other major blockers for using this issue.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13406": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly describes the use-case: pickling and unpickling a Django queryset which uses values()/values_list() and annotate for GROUP BY-style queries. It outlines the expected vs. actual outcomes after unpickling and reassignment, identifies the specific error message (AttributeError: 'NoneType' object has no attribute 'attname'), and provides concrete reproduction steps. The problem is scoped to how the queryset's result type unexpectedly changes after deserialization, which allows the engineer to drill into the serialization/deserialization logic and queryset mechanics, especially focusing on ValuesIterable. The sample provides all information needed to produce or test against the bug and to know when it is resolved.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required is relatively small: one needs to ensure that upon query assignment (after unpickling), the queryset's iterable class reflects whether values()/values_list() was used by inspecting the underlying query object. This requires understanding how Django querysets manage iteration and result type, and how pickling/unpickling works, but the required patch is localized (a conditional assignment in the query setter). An engineer familiar with Django ORM internals would solve this in under an hour \u2013 the main challenge is understanding where the conversion to ValuesIterable must be enforced.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no concerns for suitability as a benchmark issue: the description is precise, the expected behavioral change is clear, and the solution is meaningful for evaluating an engineer's ability to reason about Django query internals, serialization, and iteration behavior. No external context is needed beyond a reasonable understanding of Django ORM internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13449": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The user describes in detail how to reproduce the bug, including Django version, database (SQLite), a model schema (fields: DateField, FloatField, DecimalField), and the queryset using the Window and Lag functions that triggers the problem. The user also explains the current and expected behaviors, and why the SQL error occurs. Concrete examples are provided, including the SQL generated and workarounds that succeed. A reader can clearly understand both the problem (how Django incorrectly casts the LAG window function for DecimalField/SQLite) and the desired outcome (correct SQL generation with casting around the window as a whole, not just the LAG()). This makes it clear what needs to be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a targeted fix within the ORM expression rendering logic. It requires understanding of how Django ORM generates SQL for window functions, how data types are handled per-backend (SQLite here), and how/where to adjust the casting. For someone familiar with Django's codebase and the SQL rendering path, it's a localized change (see the gold patch: a new mixin added, a method override). The test changes are simple\u2014adding a new test case. While requiring care to avoid regressions, it is unlikely to take more than 1 hour for an experienced dev after initial familiarity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"All preconditions for using this issue as a coding benchmark are met: clear requirements, reproducible bug, clear expected outcome, and concrete test. No other issues are present.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13568": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is relatively clear in describing the problem: if you set a custom user model with a USERNAME_FIELD that is not unique (i.e., unique=False), but instead ensure uniqueness via a UniqueConstraint in the model's Meta, Django's system checks still raise an error, insisting the field must be unique. The reporter asks for the checks system to support UniqueConstraint on the USERNAME_FIELD as a way to enforce uniqueness, rather than requiring unique=True directly on the field. It specifies the context (custom user model, UniqueConstraint, implications for PostgreSQL indexes), and gives concrete details about the error. While not giving file paths or functions, it unambiguously describes the behavioral expectation and required improvement: changing the system check to accept either unique=True or a UniqueConstraint. There is some ambiguity about precise implementation (should partial/conditional uniqueness count? Must it be a total constraint?), but overall, a sensible interpretation is possible for a successful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a modest code change: adapting a system check to recognize an additional pattern in how uniqueness could be specified (i.e., via UniqueConstraint as opposed to unique=True directly on the field). For an engineer familiar with Django's codebase, this involves examining the model meta options, changing the logic for the system check, and updating or adding relevant tests. It is conceptually straightforward but requires some familiarity with Django's model and system check machinery. Total scope would likely be within an hour, as shown by the small patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are found for this sample. The problem is reasonably clear, not too trivial, has concrete behavioral requirements, and is solvable without needing clarification from outside the issue text. The only mild ambiguity is about handling partial/conditional UniqueConstraints, but the desired intent (recognize non-field unique constraints when checking for uniqueness) is sufficiently clear for a meaningful solution and evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13807": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes the context (Django's manage.py loaddata fails on SQLite when model/table names are SQL reserved keywords), expected behavior (the fixture should load successfully), the actual observed behavior (OperationalError due to malformed SQL), and root cause (table name not quoted, leading to syntax error in 'PRAGMA foreign_key_check(<table>)' and subsequent SQL statements). The location in code (base.py: line 327 in check_constraints), specific SQL statements, and relevant interaction with the database are all included. An engineer could directly reproduce, understand, and attempt a fix with this information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, targeted fix requiring an understanding of how Django interacts with SQLite and how quoting should be handled for SQL identifiers. It likely requires reading relevant code paths and making small edits, but the problem and solution space are both fairly narrow and well-described. Testing and verifying would also be simple given the reproduction steps and cause.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues. The sample is suitable for evaluating coding ability: it is clear, self-contained, and requires a non-trivial but modest change.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13810": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified, though not absolutely precise. It describes the context (ASGI, Django, middleware interaction), what is observed (type error due to non-awaitable response in the middleware chain), under what conditions it occurs (when a sync middleware returning a synchronous response is hit in an ASGI context), and makes a clear suggestion about where in the codebase the problem likely resides (cites django/core/handlers/base.py#L58 and the use of handler/adapt_method_mode in the middleware loop). The issue also specifies expected vs. actual behavior and outlines why the current handling is surprising. However, the reporter is not 100% certain about some ASGI/Django internal expectations, and leaves open exactly how and where the fix should be applied, as well as any needed test coverage, making this a 1 rather than a perfectly specified 0.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a good understanding of Django's middleware processing, the sync/async split in Django's handler code, and some awareness of how the ASGI and WSGI interfaces interact--especially in corner cases. While the required code change in the end is not huge, pinpointing precisely where and how to adjust the handler assignment to avoid 'poisoning' the middleware chain is nontrivial: one needs to work through the logical flow, confirm async/sync adaptation, and ensure the fix is robust. Also, updating or creating new relevant tests to validate the fix is necessary. For an experienced engineer, it is realistic that this could take 1-4 hours including debugging, coding, and validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The main risk is that full context (such as the boundaries of support for async vs sync middleware in Django ASGI apps) is complex and subtle, but the test patch and description together provide a sufficient framework for a fair coding benchmark. The issue is decently challenging but appropriate for experienced Django/async engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14017": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified, though not to the level of absolute precision. It clearly explains the observed problem (the non-commutativity of the & and | operators between Q and Exists objects in certain orderings), gives a description of the TypeError that is raised, and hints at the probable reason (missing __rand__). It provides a reasonable reproduction case and articulates the expectation (commutativity should hold). However, to recreate the exact test, a developer may have to fill in some details about what 'Exists' and 'Q' are (likely Django query constructs), but in context it's unambiguous for someone familiar with Django ORM. The fix and the desired behavior are strongly implied, but not spelled out line-by-line. Thus, while there are a few blanks and need for interpretation (e.g., precise operator overloading mechanisms needed), it's clear enough for an experienced engineer to form a good solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a targeted patch in the commutative handling of query objects. Specifically, it's about adjusting type acceptance or operator resolution logic so that Exists(...) & Q(...) and Q(...) & Exists(...) are both accepted and yield equivalent behavior. The need for relatively deep familiarity with how custom expressions and operator overloading work in Django ORM is required, but the surface area of change is limited, and the root cause (missing __rand__ or equivalent type checking modification) is described. An experienced Django developer could reasonably investigate, verify, and patch this in less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues were found that would disqualify this sample for use as a benchmark coding exercise. The problem is localizable, reproducible, and scoped well for an evaluation task. The only caveat is that familiarity with Django or similar ORM patterns is assumed, but that is reasonable for Python-based coding tests.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14140": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides clear context and concrete reproduction steps for the Q object's deconstruct method in Django, noting the inconsistent treatment of single-child Q objects compared to multi-child ones, and the runtime error when using non-tuple children such as Exists. It also specifies the expected result: single and multi-condition Q objects should be handled the same way on deconstruction. However, the specification does not dictate exactly which approach to take to fix the inconsistency, and it references an external PR (whose details are ignored). Overall, the issue is understandable and solvable, but minor interpretational blanks remain\u2014such as whether to pursue perfect backward compatibility\u2014making it a level 1 for specification clarity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue fix requires understanding the behavior of the Q class in Django and its deconstruct method, and then applying a consistent treatment for single and multiple children\u2014removing a special-casing branch. The patch is localized to a single function, with a precise logic edit and small associated test suite changes. For an engineer familiar with Django's ORM concepts, after reading the description and exploring Q/deconstruct, the change is small (a few lines), though it does require careful adjustment to both the implementation and corresponding tests. Thus, it is a 15 min to 1 hour task for a competent engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues remain. The issue is directly concerned with a documented and testable inconsistency in Django ORM internal logic, with clear reproduction steps and testable outcomes. One potential area of ambiguity is backward compatibility, but the candidate is free to make a sensible engineering judgment here.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14238": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the setup to reproduce the problem (using a custom subclass of BigAutoField as DEFAULT_AUTO_FIELD and defining it in your app's models module), what the expected result is (Django should recognize the custom field as a valid PK type), what the current incorrect behavior is (Django raises an error because its subclass check fails), and even where in the code to look to fix the problem (AutoFieldMeta.__subclasscheck__ and the _subclasses property). The specificity about the code and intended behavior makes it clear what is required for a successful solution without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted bug that requires understanding Django's model field machinery, specifically around meta-class subclass checking, but the code change is relatively contained (~1-2 lines), and the description gives a strong pointer to the solution area. It is more than a trivial change but should require less than an hour for an experienced developer familiar with Django internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no significant issues preventing this issue from being used as a benchmark. The scenario and required behavior are clearly described, and the affected code area is pointed out. The only minor note is that knowledge of Python metaclasses is assumed, but that is reasonable for the domain.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14351": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly thorough and gives clear context around the error, including sample code, SQL output, and iterative attempts to work around the bug. It describes the key difference between the 'agent__property_groups__in' and 'agent__property_groups__id__in' lookups, the resulting ProgrammingError due to an incorrect number of columns returned, and provides contextual clues (e.g., referencing specific Django versions, and even a manual hack in the codebase). However, some blanks remain: The exact intended fix is not explicitly described (though clues are abundant), and familiarity with Django's ORM internals is almost assumed\u2014which could make the issue ambiguous for less experienced engineers or those new to the codebase. Still, an experienced Django developer would have a reasoned path to the correct patch. So, I rate this a 1: some interpretation is needed but the requirements are largely clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This bug requires a solid understanding of Django ORM internals, subquery generation, and how Q objects and lookups are translated to SQL. The issue text provides real SQL outputs, examples, and code fragments, but resolving the issue likely involves non-trivial debugging and potentially touching deep ORM logic. The corresponding patch, while not very large in line count, operates at a core and nuanced part of query generation, and the tests involve working with advanced queryset annotations and filters. This puts it well above a quick fix. For someone familiar with the codebase and concepts, it might be doable in a couple of hours, but it's substantial and tricky enough to be a 2: 1-4 hours task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues that obviously disqualify this issue as a benchmark candidate. The required context is present in the description, and testability is good, given the provided test case.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14493": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is well-specified in that it pinpoints a specific problem: an UnboundLocalError is triggered when a custom storage class based on ManifestStaticFilesStorage has max_post_process_passes set to 0, due to the variable 'substitutions' being referenced before assignment. The reproduction steps are clear: subclass ManifestStaticFilesStorage, set max_post_process_passes to 0, set STATICFILES_STORAGE, and run collectstatic. The expected and actual behaviors are described. However, there is a minor blank: what the proper outcome should be when max_post_process_passes == 0 is implied (collectstatic should run without error, post-processing step finishes, output exists), but is not spelled out in detail. There is reference to motivation via a Django ticket link, but the immediate problem and a sensible fix are evident from the context. Thus, it is category 1: some blanks to fill in, but a well-interpretable bug report.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The underlying bug is quite localized: the fix only requires initializing the substitutions variable prior to a for-loop, avoiding its use before assignment when the loop has zero iterations. Understanding why the error occurs does require inspecting the logic flow and a bit of reasoning about the loop's execution, but fixing it is a small, targeted change. An experienced Django developer can diagnose and patch this within an hour\u2014even without deep expertise.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is suitable for a coding benchmark. It tests the engineer's ability to read a traceback, understand control-flow in Python, and make a minimal, targeted fix. There are no additional blockers: all information required to fix the bug is present. The main challenge is subtle (implicit) but reasonable for the benchmark context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14580": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and clearly describes both the problem and the expected outcome. The core bug is that Django's migration writer fails to import 'models' in generated migration files that reference 'models.Model', causing a NameError. The steps to reproduce\u2014including details about custom fields, mixins, and the model structure\u2014are sufficient for an engineer to replicate and understand the context. The expected behavior (migration file should include the proper import and be valid Python) is explicitly contrasted with the current faulty behavior. The location within Django responsible for generating migrations is even suggested as being relevant (django.db.migrations.writer). The task for the engineer is clear: ensure that migration files referencing models.Model have 'from django.db import models' present in the generated code.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual code and test patches indicate this is a focused change: updating the serialization logic to add the missing import when models.Model is serialized for a migration, plus a new test. For an engineer versed in Django\u2019s codebase and its migration system, reading the issue and identifying where to introduce this import is straightforward, requiring some care but not extensive refactoring or research. The main effort lies in understanding how type serialization works and adding the required import line, which is not trivial but is well-contained. Overall, the issue would likely take less than an hour for an experienced contributor to investigate, write the fix, and add the relevant test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other issues that would block this sample from being used in the benchmark. The bug is clear, well-contained, and the required fix and test are both well-aligned with the description. The sample demonstrates debugging/diagnosis, code modification, and test writing in a realistic open-source setting, making it appropriate for coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14672": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides clear context for the problem, referencing specific Django internals (such as the ManyToManyRel class and its identity property), the introduction of hashability for relation identity tuples, and the missing application of \\\"make_hashable\\\" on the through_fields attribute. The description includes both a minimal reproducible example and a direct statement of the error (TypeError due to unhashable list), along with an explicit one-line solution. An experienced engineer would have sufficient detail about what code needs to be changed (the application of make_hashable to self.through_fields in ManyToManyRel), why it must be changed, the error that will be avoided, and how to test the correctness (by running the Django model system checks with a constructed model as described).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change, while requiring familiarity with Django's ORM internals, is localized and incremental: it consists essentially of invoking make_hashable on self.through_fields when constructing the identity tuple in ManyToManyRel. The engineer must read the relevant code to find where identity is composed, verify the type of through_fields, and ensure hashability (with a call to make_hashable). The additional effort lies in understanding the context\u2014why hashing is needed for identity, and why through_fields needs to be converted. However, with the clarity of the issue description and the minimal repro, this can likely be accomplished in under an hour, and mainly involves a single code modification and validation with the appropriate test. Thus, this is a 15min-1hr fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major unresolved issues for using this sample. The issue is clearly scoped, the required code modification is straightforward, and the connection between cause, effect, and solution is unambiguous for a knowledgeable engineer familiar with Django or similar Python codebases.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14787": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It explains the context (method_decorator not preserving function attributes), provides clear reproduction steps (defining a logging decorator, applying it via method_decorator), the expected result (no error, correct logging with method information), and the actual error encountered (AttributeError due to functools.partial missing __name__). The problem is concrete, and a clear understanding of Python decorators, functools.partial, and attribute preservation is enough to attempt a PR. No ambiguity about what 'success' looks like: method_decorator should preserve wrapper assignments so that function attributes (__name__, __module__) are retained when decorators are applied this way.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change requires understanding how method_decorator interacts with function objects and decorators, especially when functools.partial is involved. The fix is small (just a call to wraps()), but requires sufficient knowledge of Python's functools and decorators. For someone familiar with Python internals and Django decorators, this should take 15-60 minutes including code/test edit. The change is localized and conceptually simple.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The problem is clear, solution success is objectively testable, and it is representative of real-world decorator/wrapper attribute issues in Python. The only minor implicit knowledge required is familiarity with Python's wraps() utility.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15104": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly specifies the context (Django migration autodetector), the nature of the problem (KeyError 'to' caused during deconstruction of a custom ForeignKey with hardcoded 'to' in the constructor and removed in deconstruct), and an exact line where the fix is needed (replace del with pop). The scenario is carefully described, including reproduction steps and reasoning, though it leans on familiarity with Django internals and custom fields. While the root cause is well-explained, some blanks remain regarding the full scope of custom field behaviors and surrounding expectations. However, the main requirement (robust handling of cases when 'to' key is absent) is clear, making the issue well-specified with only minor interpretative blanks for a competent engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual code fix is minimal (replace del with pop), and the underlying Python and Django concepts, while potentially specialized, are well-defined for someone with reasonable Django experience. Identifying the root cause takes some reading and understanding, but the steps to reproduce and fix the issue are provided. It is neither totally trivial nor complex: it requires understanding why the KeyError occurs and the implication of the fix, but not major rewriting or architecture changes. Thus, this is a 15 min - 1 hour issue.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues have been identified that would prevent this issue from being a good benchmark for coding ability. The test demonstrates the problem and the fix's effectiveness without ambiguity, and the scope of the issue fits the evaluation setup well.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well specified for an experienced engineer familiar with Django's ORM internals. The description provides a reproduction scenario, explains the cause of the problem at a technical level (alias conflicts during QuerySet `OR`/union operations), and even suggests where the assertion is triggered and what should be done at a high level (avoid intersecting keys and values in change_map, enhance aliasing logic). However, some minor blanks exist: the full reproduction code is not given, and the precise expectation on documentation/comment changes is left somewhat open. Nevertheless, the technical requirements for fixing the bug are clear, as are the areas of code to examine (Query.combine, QuerySet.change_aliases, etc.), so a sensible solution can be developed from the description.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The difficulty is rated as 2 (1-4 hours) because solving this issue requires a good understanding of Django's ORM internals\u2014specifically, how query aliasing, query combining, and related assertion checks are implemented. The fix entails non-trivial modification to a core logic (alias bumping/prefixing logic, careful relabeling), adding (or improving) code comments for assertions, and extending/fixing test cases. It may involve exploring impacts across different cases. Such changes, while focused, are not trivial and require careful reasoning, code browsing, and confidence in subtle details of the ORM core.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present that would prevent the sample from being useful in a coding benchmark setup. The issue is rich enough to be non-trivial, isolated in ORM logic, and its expected solution and testing are both well described. The only caveat is that a high level of familiarity with Django internals is assumed, but this is not a problem for assessment purposes. The tests added in the PR appear to directly address the described bug.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15277": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is detailed and precise, providing both the problematic area (Value._resolve_output_field and CharField.__init__ in Django), the underlying cause (attaching a MaxLengthValidator even when max_length is None), and the side effects (TypeError when validating). The expected behavior is clearly laid out \u2013 do not attach a MaxLengthValidator if max_length is None \u2013 and the suggested solution is spelled out. Specific class and function names are included, and an experienced engineer would know both what needs to be fixed and where to begin looking. The problem demonstrates sensible reproduction steps and micro-benchmark data, making it clear how to validate a solution as well. There is almost no ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted fix in the Django CharField class, requiring a developer to add a simple conditional before appending the validator. The issue provides the reasoning and even mentions a similar precedent (BinaryField.__init__), allowing the developer to follow established patterns. Familiarity with Django field validators is helpful, but the required code change itself is straightforward, would likely not require editing multiple files, and could be done well within an hour. The engineer might want to add tests, but that's routine complexity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a well-contained and highly representative bugfix issue for an engineering benchmark. It is Python/Django specific but otherwise poses no major concerns for use in a general coding ability benchmark. The test patch provided already covers the core change with a good breadth of data types, ensuring test coverage is not an issue.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15280": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides concrete model definitions, a clear reproduction scenario, the sequence of queries expected/executed, and the specific bug (extra query occurring when accessing a deferred field). It demonstrates familiarity with how Django ORM prefetching and deferred fields should work, and describes the user's expectation versus actual behavior in detail. However, the root cause and required changes (what code to alter) are not fully spelled out and rely on the developer having a good grasp of Django's ORM internals. Although the report is precise, an engineer is left with some blanks to fill in (e.g., exactly where and how to patch the cache/field inheritance behavior). Thus, it is well-specified with minor blanks, suitable for a competent engineer to solve.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue exercises sophisticated Django ORM internals, specifically around prefetching with deferred fields and how models cache related objects in complex querysets. It requires understanding lazy field evaluation and the prefetch_related API, which goes beyond trivial model/query usage. The final patch is a relatively small conditional guard, but finding where to insert it and writing a robust, isolated test case is nontrivial. An experienced Django developer would likely need 1-4 hours to understand, track down, solve, and test the issue\u2014especially if they weren't already familiar with the related descriptor code and the subtleties of object caching in prefetching scenarios.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major blockers: the issue is reproducible, the expected and actual behavior are described, and the required code context is included. The test changes show how a good regression test can be constructed, but this does not introduce any fundamental barrier to using the sample as a coding benchmark. The scenario is representative of real-world, medium complexity ORM issues.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15315": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and clear. It describes an unexpected change in the hash value of a Field object when it is assigned to a Django model class, leading to failures when the object is used as a key in a dictionary. The reproduction steps are explicit: create a Field, store it as a dict key, assign it to a model, then attempt the lookup. The impact and rationale are explained, as is a pointer to the codebase area (Field.__hash__). There is also background on why this undesirable behavior was introduced. An engineer can confidently create a test and code change to ensure Field.__hash__ is immutable across assignment to a model, as requested.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a modest difficulty bug. The issue is isolated and the fix is clear: adapt the __hash__ method on Field so that it is not affected by model assignment. While some digging into how Field assignment or creation_counter works may be required, and a new test likely needs to be added or updated, it is not a complex or sprawling change. An experienced engineer could reproduce, fix, and test this within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The problem is conceptual, not requiring a specialized domain expert, and the request is directly testable using the provided example scenario. Both the issue and the required code/test changes are clear and bounded.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15375": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly identifies a specific bug: that using aggregate() with the 'default' parameter after annotate() causes a SQL syntax error, due to an empty SELECT list being generated. The reproduction steps are detailed (mentioning the relevant models, ORM steps, and error message), and the workaround using Coalesce is also presented. However, the report lacks minor details \u2013 for example, the precise expectation (should defaults always work after annotation? Should the SQL always be valid?) and whether any edge cases need coverage. Nonetheless, the desired behavior is sensible to infer (aggregate should work with default regardless of annotation) and the associated models/functions (Book, annotate, aggregate, Sum, Coalesce) are referenced, so an experienced engineer could make a reasonable fix without further clarification. Hence, this is a level 1: some blanks but clear intent.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue appears to require investigating the code generation path for Django ORM's aggregate and annotate operations, specifically when the 'default' argument is used on the aggregation function following an annotate. The fix in the gold patch is a small change: ensuring that the constructed Coalesce gets the is_summary attribute transferred from the wrapped aggregate. This is a subtle issue but once located, the change is small and modular (a couple of lines in one method), requiring reading and understanding only a small part of the codebase. For an engineer familiar with Django ORM concepts, this is a 15 min - 1 hour change (level 1).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues beyond what has been discussed. The sample is a clear, contained ORM-layer bug, has an easily reproducible scenario, and no major ambiguities or missing context that would prevent using this issue for benchmarking coding ability. The test patch also demonstrates targeted, meaningful coverage.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15380": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well-specified: it describes the concrete user action (renaming a model and one of its fields simultaneously during Django migration creation), the observed outcome (the process crashes with an internal key lookup error after confirming the rename), and the expected result (a migration file should be generated). There is some ambiguity about which field is to be renamed and about specifics of the usage context, but a sensible engineer would interpret this as applying to any field of the renamed model. Also, it references a clear example (the 'MyModel' model in the 'test_one' app), making it relatively straightforward to reproduce. Thus, while a few minor details are missing, the problem statement is actionable and suitable for a meaningful attempt at a solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a fair bit of understanding of the Django migration system internals, especially how the autodetector resolves renames for models and fields and manages in-memory state mapping. While the code change itself is small, identifying exactly where the mismatch occurs and how to maintain the correct internal mappings between old and new model names/fields is not trivial. An engineer would need to read relevant parts of the autodetector logic, reproduce the error, and write/adjust suitable tests. This work\u2014including investigation, patching, and verifying\u2014would reasonably take 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no blockers or major issues that would prevent using this sample for evaluating coding ability. The scenario is well-grounded, and the supplementary test changes show that the fix can be verified. The problem tests both understanding of fragile, stateful migration logic and test-driven bug resolution.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15503": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"This issue is well-specified. The problem is clearly described: JSONField lookups like has_key, has_keys, and has_any_keys fail when searching for numeric-string keys (e.g., \\\"1111\\\") on SQLite (and possibly MySQL/Oracle), even though these work on PostgreSQL. The issue explains exactly how to reproduce it, what is expected (both JSON fields with string and numeric-string keys should be queryable), and what fails (lookup for the string \\\"1111\\\" returns 0 records when it should return 1). There is no ambiguity about which Django features are impacted (models.JSONField, database backends), and a sensible test case is given. An engineer familiar with Django and databases would know which files and lookups this touches (likely django/db/models/fields/json.py and tests in test_jsonfield.py), as also seen in the patches. No further clarification is required to scope the work.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This is a '1-4 hours' task (level 2), as it requires a good understanding of Django's ORM internals, particularly how JSONField lookups are implemented for different backends (SQLite, MySQL, Oracle, Postgres). The fix involves not just a trivial change but refactoring the way key paths are generated and interpreted so that numeric-string keys are handled correctly and not confused with array indices (which is a subtlety in JSON path expressions). The correct solution also involves design decisions and updating or extending existing lookup classes. This is not trivial but doesn't rise to the complexity or time commitment of a multi-day research+engineering effort.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues preventing this from being used in the benchmark. The issue is clear, has a focused scope, and is testable based on the description alone. The difficulty is appropriate to distinguish intermediate to advanced ability with Django/ORM internals. It exercises understanding of database backends' JSON support and test-driven debugging. A minor note: the solution requires a working Django dev environment with SQLite to validate.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15525": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly describes the observed problem: loading a fixture containing models with natural keys and foreign key relationships into a non-default database via Django's loaddata causes a failure that does not occur in the default DB. It provides concrete reproduction steps\u2014including sample models, the fixture data, and reproduction instructions. The affected methods and error (DoesNotExist) are directly referenced, and the expected and observed behaviors are both described. An experienced engineer should know exactly what to focus on and investigate, as well as what constitutes a valid fix (successful loaddata into the secondary DB with the provided models and fixture, mirroring the working behavior on the default DB).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires not just a trivial code tweak but understanding Django's serialization/loading mechanism in multi-database environments and how natural keys and dependencies are handled. The fix involves updating how instances are created and looked up with database context during the import, which may require careful tracing, updating core logic, and nontrivial test coverage. It's necessary to pinpoint where database selection is incorrectly handled, devise a change that doesn't break default-database behavior, and write/adjust targeted tests to cover the scenario. This would likely take 1 to 4 hours for a proficient Django developer familiarizing themselves with this subsystem.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The sample includes all relevant info for code and test changes, the error is reproducible, and the expected behavior is unambiguous. The task is a precise fit for evaluating ability in debugging, reasoning about foreign key/natural key serialization, and handling Django's multi-database architecture. The engineer would have to deal with core logic and testing in a major Python web framework\u2014excellent for a benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15695": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well-specified: it clearly states that applying RenameIndex() twice, on an unnamed index (auto-generated name) associated with unique_together, results in a crash because the target name already exists. The description includes a minimal code diff and a reproduction recipe, outlining the sequence of backward/forward migrations that exposes the bug. However, while the general problem and context are clear, precise details about the naming conventions, which platforms and edge cases are affected, and whether this behavior is intended for all databases are lightly assumed. An experienced Django engineer would interpret the requirements sensibly. There are still some blanks to fill, but a practical and correct fix with testing is quite feasible and the success criteria (no crash, idempotent RenameIndex) are clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue likely requires understanding the internals of Django's migrations (specifically RenameIndex), how index names are auto-generated, and database schema operations. However, it is essentially a conditional early return if the index names don't change, which is conceptually straightforward and apparent once the bug and context are understood. The patch is concise, affecting a single point in the migration operation, with a small test. Thus, an experienced contributor could implement and verify this fix within 15 minutes to 1 hour, after understanding the context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The issue is focused and the patch/test support the correctness and specificity of the scenario. As with most migration ops, care should be taken with any database-specific behavior, but nothing prevents the sample being a solid test of core engineering skills.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15732": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text is quite clear in terms of what is happening: attempting to remove a unique_together constraint from a field which is also defined as unique in Django (specifically the PK or a unique field), causes migration failures because the migration system finds multiple unique indexes (one belonging to the PK/unique field, and one from unique_together) and cannot resolve which one to drop. It also specifies the backend (PostgreSQL) and the relevant schema details. Although it does not point to concrete filenames or explicit classes/functions, it does clearly communicate the core problem, the expected behavior, and the observed failure mode. An experienced engineer with knowledge of Django's migration and schema systems would be able to infer that the bug is in the migration/schema logic and what needs to be fixed. There could be some blanks to fill in around exactly what code to modify, but the intent is clear and actionable.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This requires understanding both Django's migration framework and how unique constraints are tracked and generated, as well as details of database backends, particularly PostgreSQL. The solution is not trivial\u2014it involves modifying logic in the schema editor to correctly identify and remove the proper unique constraint rather than mistakenly assuming a single one. The solution patch spans multiple functions and introduces helper methods and conditional logic, so it's more involved than a simple fix or a one-liner. For an experienced Django contributor, this is likely a 1-4 hour fix, especially once familiar with the relevant code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues prevent this sample from being used for evaluation. The test patch is comprehensive and focuses on the core failure mode. The description also limits itself to a single, focused bug for a commonly-used framework and is not reliant on external explanations or links. The only caveat is that experience with Django's internals is required, but that is suitable for a code benchmark of this nature.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15814": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete example: models are defined (CustomModel, ProxyCustomModel, AnotherModel), and the failing code usage pattern is described in detail (select_related followed by only on a proxy ForeignKey, error about primary key 'id' not selected). The specific error encountered, the context (Windows 10, Python 3.10, Django 4.0.5), and even a code pointer to the Django internal file and a proposed workaround/fix are included. However, there are minor blanks to fill in: not every single environment or input is exhaustively described (e.g., are other fields or relations present?), but the meaning is clear and a sensible fix is obviously possible. There is little ambiguity about the desired result: making the Django queryset operation work as expected for proxy models with select_related and only. Thus, the sample scores a 1 (some blanks, but a sensible task).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug involves a moderately deep feature of Django's ORM (using select_related with only on proxy models), and the solution requires understanding how Django handles model metadata (particularly _meta and concrete_model). The root cause is described in the issue, with a pointer to the relevant code location and a suggested fix. For an engineer familiar with Django internals, it would likely require some careful reading of the relevant QuerySet/ORM code, but the change itself is small (one line), and there's a direct mapping from the issue description to the code change. Thus, I estimate this as a 15min-1hr task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is suitable: the issue is reproducible, there is a clear desired behavior, and the provided test covers the scenario. The example is realistic and well-targeted for evaluating ORM/scaffolding bug fixes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15930": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is clearly described and includes concrete reproduction instructions, describing how to cause the bug using Django's ORM with a Case expression and a condition involving ~Q(pk__in=[]). The expected and actual behaviors are precisely outlined, including the generated SQL fragment and its empty WHEN clause leading to a syntax error. The context for why ~Q(pk__in=[]) arises is mentioned, giving additional background. An experienced engineer reading this would understand that the task is to fix Case's SQL generation so that always-true (or empty) conditions are rendered correctly in SQL CASE WHEN expressions. There is no ambiguity about the required behavior or the scope of the bug, and the solution's correctness can be tested by checking if the error is resolved and the logical meaning is preserved.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small but nontrivial change: updating the SQL compilation logic to handle the special case when the compiled condition is an empty string (meaning always-true). It requires understanding of how Django ORM compiles queries, particularly Case expressions and conditions, but is local to one function and does not require large-scale refactoring. There may be some need to research how filter conditions are rendered as SQL, but the edit itself is quick and targeted. Overall, it's a less-than-one-hour fix for someone with a bit of experience in Django's codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues stand out. This is a well-contained bug with a well-defined fix and clear tests, suitable for a coding benchmark. The only caveat is that it presumes familiarity with Django ORM SQL compilation, but the reproduction and explanation are clear enough for an experienced engineer to pick up.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15957": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description details a concrete technical shortcoming in Django's prefetch_related functionality: when a queryset used for prefetching is sliced, Django raises an assertion failure and will not run the query. The user expectation is clear\u2014being able to prefetch only a limited number of related items (e.g., 3 posts per category). The current limitation is presented, an example rationale (show only a few related items per parent) is given, and the problem is described in practical terms. However, the description doesn't provide examples of the expected code (models, queries, etc.), nor does it specify exactly how the interface for this should behave (e.g., what methods or options should or should not be available regarding slicing inside prefetching). An implementer must interpret these specifics and decide how to approach the patch\u2014but there is a sensible, technical reading that enables meaningful work toward a solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires a fairly deep understanding of Django ORM internals, how querysets work, the mechanics of prefetch_related and slicing, and the performance/SQL implications. The solution in the gold patch introduces a custom method to apply slicing at the per-parent level, leverages window functions (RowNumber), and touches several core logic paths of Django's related field descriptors. This is a substantial change (impacting how relationships and limits interact) and must be thoroughly tested. Solving this would require research, significant thought, and changing more than a trivial amount of code. Estimated time for an experienced engineer (familiar with Django): 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues found. The sample is technically focused and would be suitable to benchmark advanced coding and software comprehension skills involving ORMs and back-end logic. As with many framework internals, strong familiarity with both the domain and codebase is required, but this is expected for such a benchmark scenario.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides sufficient context to reproduce the problem: it names the apps, describes the model layout for each app, specifies the many-to-many field with a 'through' argument referencing a model in another app, and describes the expected and erroneous behaviors during migrations. The reader understands both how to set up the scenario and what the error is. However, the description does not include the exact code (e.g., model definitions), so there is a minor degree of interpretation required to fully reconstruct the scenario (e.g., precise field names, which classes to use). Nonetheless, any Django-knowledgeable engineer can infer what is meant, making the problem mostly well-specified, with only some blanks to fill in.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue centers on a moderately subtle bug related to model referencing and Django migrations internals. Once the cause is understood (incorrectly resolving a string reference instead of a model class), the fix is conceptually straightforward: use the correct reference for the 'through' relation. The actual code change is small (one line), but correct diagnosis requires a solid understanding of Django's model/field resolution and migration machinery, which could take a bit of research or code tracing. An experienced Django contributor would likely solve this within an hour, but it isn't entirely trivial at first glance.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues: the problem is clearly reproducible from the description, the reproduction steps are robust enough for benchmarking, and it checks for real debugging and framework understanding, making it suitable for evaluating coding ability in this context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16032": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete scenario for reproducing the bug: working with Django QuerySets, using filter, annotate, and alias, and then performing a filtered query across related models. It specifies the necessary model fields (Book with pages, Publisher) and the steps required, as well as what the correct outcome should look like (exact publisher names). The description gives a clear direction for identifying and understanding the bug (database operational error due to wrong column output in a subquery). However, it does not explicitly state what the underlying code or ORM logic is, nor does it cite precise locations for a fix. This leaves some details up to the implementer - for example, exactly how the aliasing and select fields interact in the ORM internals. Thus, there is a sensible interpretation of what is required, but minor gaps remain in how, internally, the ORM should handle the 'alias' and 'annotate' fields in subqueries, which the engineer must deduce. Therefore, the best choice is 1.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue would likely take 1-4 hours for an experienced engineer familiar with Django's ORM to resolve. The solution involves understanding and debugging the logic around how annotated and aliased fields in a QuerySet affect what columns are selected in subqueries, looking into the internals of Django's QuerySet/SQL generation, and modifying core code. It requires working knowledge of the ORM and confidence with debugging database issues. While the actual code change is not massive, the time is spent understanding a nuanced bug and ensuring tests pass. Interactions between annotate, alias, and QuerySet internals are not trivial. Plus, writing new tests to ensure the fix covers the reported bug is required. Therefore, 2 (1-4 hours) is appropriate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The description would be accessible to someone with Django knowledge and the reproduction steps are clearly defined. The only minor concern is familiarity with ORM internals may be necessary, but this is suitable for a coding benchmark targeting mid-to-senior engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16136": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear and well-specified. It provides a concrete scenario to reproduce the bug, including exact software versions (Django 4.1.1, Python 3.10.6), precise steps (creating a View with async post, registering it, and performing a GET request), the observed and expected behavior (server crashing with a 500 Internal Server Error and type error vs. a 405 Method Not Allowed response), and describes the error message. An experienced engineer can reproduce the described error and clearly understands the intended correct behavior, leaving no significant ambiguity in requirements. It even pinpoints the relevant method in the codebase likely to require attention (the handling of non-allowed HTTP methods for async views).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue involves understanding the async/sync mechanics of Django's view dispatching and error handling, but it only requires a small code change: adjusting how the HttpResponseNotAllowed object is returned in an async context. The affected functionality (http_method_not_allowed) is fairly isolated. For someone already familiar with Django's class-based views, this is a targeted fix, involving a minor modification (around one function), and does not require deep architectural changes. Some quick testing is needed to verify the fix, but overall this is a modest coding task that should take less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues or blockers are present in this sample. The scenario is concrete and reproducible. The core technical challenge is clearly specified, and the kind of code being changed is typical for open source contribution benchmarks. The only slight caveat is that familiarity with async context in Django helps, but it remains a reasonable targeted engineering task. The sample is suitable for coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16429": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well specified. It clearly describes a TypeError occurring when calling timesince() with USE_TZ=True and a >1 month interval, due to mixing timezone-aware and naive datetimes. It provides concrete steps to reproduce, including enabling timezone support, creating appropriate datetime objects, and invoking the function. It also points to a relevant code section and offers an explanation of why the bug happens (pivot loses tzinfo) with a suggested direction for the fix. The function and file in question (django/utils/timesince.py, timesince) are explicitly referenced. An experienced engineer would have all the information required to attempt a solution, as well as understand the criteria for a successful fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is isolated and clearly defined. The engineer needs to ensure that the 'pivot' datetime object in timesince() carries the timezone information (tzinfo) from the input. This is a small change, mostly involving updating the construction of the pivot datetime object. The manipulation with aware/naive datetimes is standard Python/Django fare, and with the reproduction steps detailed, it would not require significant time to find and patch the affected line. The changes necessary to resolve the bug are contained within a single function and are conceptually straightforward, hence a relatively quick fix for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are present. The issue is focused, well-scoped, and relevant as a benchmark sample for coding ability. Reproduction does not require any information not specified in the issue text.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16454": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clearly states the problem: Django's management command subparsers do not retain the error formatting customizations of their parent parsers, resulting in stack traces rather than user-friendly error messages. The issue traces the cause to CommandParser.add_subparsers not propagating certain arguments to subparsers, and gives concrete examples of the undesired and desired behaviors. Furthermore, it indicates the desired solution: ensure that subparsers receive the necessary arguments to preserve Django's error formatting. A developer familiar with Django's management command infrastructure would know where to look (django/core/management/base.py and related parser usage), and what a successful solution looks like (subparser errors formatted in the intended user-facing way, not raw stack traces).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The main challenge is understanding Django's management command parser structure and ensuring that subparsers created by add_subparsers inherit the relevant formatting behavior. This likely requires reading and modifying how CommandParser's add_subparsers works, and knowledge of Python's argparse module. The code change is conceptually localized and not very large (as shown by the patch), but requires precise reasoning about argument propagation. For an engineer familiar with Django internals and Python argparse, fixing this would take 15-60 minutes, including testing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue is a good fit for the benchmark: the problem is clearly scoped, the expected user-facing outcome is defined, and an experienced Python/Django engineer could attempt a meaningful fix. Providing the downstream test cases ensures the solution matches behavioral expectations. There are no exceptional blockers with respect to using this issue in the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16485": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes exactly what input produces the error (\\\"0.00\\\" and Decimal(\\\"0.00\\\") as input to floatformat with 0 precision), what the expected output should be (\\\"0\\\"), and what the current (faulty) behavior is (raises ValueError: minimum allowed precision is one). The explanation offers a concrete, reproducible scenario and specifies the expected result for this case. Thus, an experienced engineer would understand both what to reproduce, what is wrong, and how to recognize a correct fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small bug fix that requires inspecting and adjusting a conditional in the floatformat filter implementation. The issue, once understood, does not require significant codebase changes or extensive new logic: just a simple fix to the logic controlling allowed precisions. However, a bit of thought is needed to ensure the change doesn't introduce side-effects, and some familiarity with formatting/representation of numeric types is important. It is more than trivial, but would typically be solved in 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues that would prevent this sample from being useful in an evaluation context. The issue is self-contained, specific, and the test patch provided validates the fix. No external domain knowledge is required beyond familiarity with numeric formatting and Python.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16569": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. It provides a clear description of when the bug occurs, the relevant attributes (self.can_delete, self.can_delete_extra), and under what conditions (when index is None). There's a detailed explanation of where in the code this happens (line 493 in django.forms.formsets), what the error is (TypeError due to comparison of None and int), and even suggests the corrected logic. The example for reproducing the issue is concrete, mentioning exactly how to set up the test and what line triggers the bug. The required change is also unambiguous. This would make it straightforward for an engineer to understand what's wrong and what needs to be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a small but non-trivial fix. While the root cause is isolated to a specific conditional and the required change is clear, it requires understanding the context in which the method is called and the implications of the indices in formsets in Django. The fix itself is a straightforward improvement of a condition, so it should take an experienced engineer 15 minutes to 1 hour to fully analyze, test, and implement, assuming they're somewhat familiar with Django's forms framework.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The issue is clear, well-scoped, and has a specific technical context. It's suitable for evaluating coding ability in a benchmark setting: successful resolution requires code reasoning, bug identification, and safe modification for a widely used codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16667": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It provides a concrete example of a Django form using SelectDateWidget, instructions on how to reproduce the crash (by supplying an excessively large year parameter), and details the root cause: an unhandled OverflowError when parsing large integer inputs for date construction. It even identifies the failing line of code (date_value = datetime.date(int(y), int(m), int(d))) and the exception type. The expected output (no crash; instead valid error handling) is strongly implied. This provides an experienced engineer with enough information to both locate and resolve the issue without requiring further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires handling an OverflowError during the parsing of form inputs in Django's SelectDateWidget. The solution is localized to catching the exception at the correct point and returning an indication of invalid input (consistent with other validation errors). The core logic change is small and straightforward for someone who understands Django's forms framework; it just involves modifying exception handling to cover this new case. Familiarity with Django's testing and form validation might be useful but not strictly required to address the code bug. Writing or updating tests would also be a small and direct task, similar to existing validation logic. Overall, this is not completely trivial, but should take well under an hour for an experienced engineer once they are familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The use case and failure mode are well-isolated, the requirements for correct handling are clear, and the scenario (crashing on large input) is realistic for benchmark assessment of input validation robustness.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16950": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides specific Django model and admin definitions, a step-by-step reproduction scenario, and describes the exact runtime error (integrity error on the UUID field when saving an inline formset). Although the reporter does not dive directly into implementation specifics or root cause in Django internals, the scenario is clear, and an engineer could reliably recreate the bug using the provided models, admin setup, and reproduction steps. There is, however, some ambiguity about how Django's form/model machinery should ideally behave in this edge case\u2014precisely how UUIDFields with defaults should interact with related inlines\u2014leaving some room for interpretation when crafting the correct fix. An experienced engineer will have to deduce both the expected behavior and the best way to achieve it. Hence, this is best rated as '1': the issue has blanks to fill, but a sensible interpretation is possible, and a meaningful solution can be implemented.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue spans Django's form model internals and requires understanding how inline formsets interact with fields like UUIDField with default values. Solving it involves identifying the problematic code path, determining correct handling of auto-generated keys, and ensuring no regressions in related functionality. This requires a solid understanding of Django admin internals, model and form interaction, and possibly writing/adjusting tests to confirm the fix. Although the code change may not be huge, the reasoning required and familiarity with core Django model/formset mechanisms make this a substantive (1-4 hours) task for even experienced engineers.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues prevent this issue from being used as an evaluation sample. The specification is adequate for someone with experience in Django to reproduce and fix the bug, and the observed difficulty is in line with typical, nontrivial framework bugfixing. The reproduction steps and affected code are concrete, and a valid PR could be constructed from this description alone.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-20859": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and provides a clear description of the bug and what is expected as a solution. Specifically, it states that invoking the legend method on a SubFigure does not work and results in a type error. The reproduction steps are clearly described: create a Figure, extract a SubFigure, plot a line with a label inside axes of that SubFigure, and try to add a legend to the SubFigure. The expected outcome (that this should work and display a legend) is explicitly described. Furthermore, there is a strong hint provided for the source of the bug (type check in legend.py needing to use FigureBase instead of just Figure), and a specific code location is even cited. There is enough detail for a developer to understand what is broken and what behavior is desired, making the issue well-specified.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires identifying where the type checking is too strict and adjusting it to allow SubFigure (by using the base class FigureBase). The provided line reference in the issue makes locating the code fast. Because this is a narrowly-scoped change in one file/class (legend.py), and the fix is a one-line modification, it's straightforward for an experienced developer after initial familiarization. Writing or adjusting a test to check the behavior is a small additional task, fitting well within a 15m-1h timeframe.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The issue is specific, addresses a concrete behavior, and has a clearly defined expected outcome. The test patch clearly verifies that the fix works as intended. This is a strong candidate for benchmark use.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-22719": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a reasonably clear summary of the bug (confusing deprecation warning when plotting empty data with category units), concrete reproduction steps (setting up a categorical axis and plotting empty data), expected and actual outcomes, and additional context about recent changes to unit conversion APIs. The problem is well scoped and an experienced engineer can infer that the solution should suppress or refine the warning in the empty-data case. However, there are some blanks: it's not fully explicit about the precise desired behavior (should empty artists be created without warnings, or should they inform the user differently?), requiring the developer to make a sensible interpretation based on Python/Matplotlib conventions. There is some ambiguity about the correct user-facing API behavior, but the issue is still actionable without further clarification. Hence, this fits level 1 on the well-specified scale.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding of how unit converters work in Matplotlib and tracing how empty data triggers warnings in the plotting logic, but the required code changes are relatively minimal: the solution is essentially an additional guard on when to issue a warning (only warn if input is non-empty and numeric), and a similar check/info log elsewhere. Familiarity with the codebase and API evolution helps, but the main task is inserting and testing this logic, which for an engineer familiar with Matplotlib would take well under an hour. Therefore, I rate it as a 15 min - 1 hour change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues to note. The sample is suitable for the coding benchmark\u2014there's a clear bug, a replicable case, and a straightforward code/test fix. The domain is specialized but not esoteric, and the issue is relevant for evaluating practical debugging and reasoning skills.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and detailed. It includes a concrete reproduction procedure specifying step-by-step how to trigger the bug (using rc_context and creating a figure, then comparing the Gcf registry before and after get_backend()), as well as an explicit summary of the actual vs. expected behavior. The undesirable outcome is precisely described: figures created under rc_context disappear from the Gcf registry unexpectedly after get_backend() is called. The underlying mechanism is hinted at (interaction between rc_context and backend settings), so an experienced engineer has sufficient guidance on what must be preserved (figures should not be removed when querying the backend). The consequences of the bug are also linked to a concrete use-case (plt.close(fig2) not working). Thus, it is clear what constitutes a successful solution: after calling get_backend(), Gcf should retain figures created under rc_context. The relevant objects and compositions (rc_context, rcParams, backend, Gcf, get_backend()) are standard Matplotlib code areas, so discovery is tractable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug requires tracing the interaction between rc_context (which manages rcParams and should temporarily override settings) and Gcf (the figure manager), as well as understanding backend switching and how backends are managed in rcParams. Although the root cause (specifically excluding the backend from rc_context reset) takes some familiarity with Matplotlib's configuration system, once identified, the patch is quite compact (removing 'backend' from the rcParams snapshot used by the context manager). Making and validating the fix is conceptually straightforward, and the precise reproduction steps help isolate the offending code paths quickly. Therefore, it should be solvable by a Matplotlib contributor familiar with rcParams logic within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns. The issue is well-specified, reproducible, and maps directly to a concrete coding fix. The issue is not underspecified or ambiguous, and the requirement for a successful solution is clear from the issue description.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23476": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It describes clearly a reproducible bug: when unpickling a Matplotlib figure on an M1 Mac, the figure's dpi doubles every time, leading to an overflow. The user gives exact instructions for reproduction, details about the environment (OS, Matplotlib version, backend, Python version), clarifies the scope (seems to happen only on M1 Macs), and provides both the observed behavior and the expected behavior in the form of repeated dpi values. There is no ambiguity in what is considered a successful fix: unpickling a figure should not change its dpi. Any engineer could confidently determine when the issue is fixed using only this description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"I assess the difficulty as '15 min - 1 hour.' The problem requires some understanding of Matplotlib figure serialization, specifically how dpi (and probably device pixel ratio) is handled during pickling/unpickling. A developer would need to trace how dpi is stored and restored through the pickling process, identify where the duplication occurs, and introduce code to preserve the intended dpi. This is not an entirely trivial typo but is generally a small, isolated fix in the relevant serialization logic. The actual patch is indeed just a couple of lines, but it does require knowing where to make the change and reasoning about why the value is being doubled.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues prevent this issue/PR pair from being included in the benchmark. The test change directly captures the regression and would serve as a good check. The only note is that it is Mac/M1-specific, but this does not limit its value for a general coding benchmark, as the logic can be reasoned about cross-platform.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24026": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is decently well-specified. It clearly states the current behavior (stackplot changing the Axes color cycle and failing with a specific ValueError), the desired behavior (stackplot should not change the Axes cycler and should use specified colors via aliases), and the practical use case. The reproduction steps give an idea of the sequence of plotting commands and what is expected at each. However, there are still some blanks: it doesn't name specific files or functions (though these are discoverable by an experienced engineer familiar with matplotlib), nor does it specify exactly how to alter stackplot. But, assuming access to the codebase and knowledge of matplotlib, an experienced engineer could sensibly interpret and address the issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core fix consists of stopping stackplot from setting the property cycle on the axes, and instead using a cycle of the colors specifically provided. This is a neat targeted fix to a function in a single file, requiring some understanding of how matplotlib's color cycles and the stackplot function work, but it does not involve major refactoring, breaking up, or understanding of complex domain logic. For someone with matplotlib experience, it should take 15 minutes to 1 hour: sufficient to read, verify the current logic, update the color selection, and add/adjust related tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are noticed for using this sample. The issue targets a specific unwanted side effect and is well-aligned with typical matplotlib use cases. The gold PR and test together confirm the intended scope. While the issue doesn't explicitly mention method and file names, it is realistic and sufficient for an experienced contributor.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24149": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is quite well-specified: it states precisely what the old (3.6.0) behavior was, what the erroneous behavior is in 3.6.1, and provides concrete code snippets that cause the issue. It gives detailed reproduction steps, expected vs actual outcomes, and even pinpoints the cause to a change in handling of NaN values for x positions in bars. There is some mild ambiguity in the expected handling of all-NaN bar data in general (e.g., should other NaN configurations behave similarly?), but as written, it is clear what behavior the user expects to be restored. A well-informed engineer could move forward successfully. The issue only leaves a small blank in terms of understanding whether this adjustment should be generalized beyond this specific case, but there is a reasonable and sensible interpretation of the requirement.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"After a bit of investigation, it is apparent which code paths need to be examined: input validation in ax.bar, especially around finite value searches in x/y. The solution is a small logic adjustment to handle StopIteration by falling back to the first element. This is not a trivial typo fix, but also doesn't require hours of deep investigation: a familiar engineer will likely take 15-60 minutes, including writing and validating a new test. The main challenge lies in identifying the right exception pathway and ensuring correct handling.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are present. The setup is clean: the test illustrates the fix, and the core requirement can be inferred from the context and the code/test diffs. The only possible note is that the proposed behavior (falling back to the first element) is a bit of a workaround rather than a clear \\\"correct by design\\\" fix, but it is demanded by the regression and historical contract. This is appropriate for real-world bugfixing.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24970": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it describes a specific regression after upgrading to NumPy 1.24 (deprecation warnings are emitted), provides clear steps to reproduce (import specific modules, pass empty uint8 array to colormap), gives the observed warnings, and states the expected outcome (no warnings). The context specifies operating system, Python, Matplotlib, and backend versions, minimizing environment ambiguity. Even though the exact deprecation messages are not copied, the behavior to avoid\u2014warnings when using empty uint8 arrays with colormaps\u2014is concrete and reproducible. It is clear what counts as a successful fix: no deprecation warnings in this scenario.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change mostly involves understanding why the deprecation warnings are triggered (the way data type conversions and clipping are handled internally), and placing np.errstate around the right casting operations. This requires reading through the relevant colormap/colornormalization code, checking how and where arrays are cast, and possibly writing or updating a test. For an engineer familiar with NumPy/Python, this is a localized and common warning-suppression/code-path adjustment task likely to take less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no significant outstanding issues: The issue is clearly described, has a specific and narrow reproduction path, and the fix can be tested in isolation using the steps and test case provided. The environment is specified closely, reducing risk of hidden dependencies or ambiguous behavior.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25311": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The problem is clearly described: pickling a matplotlib figure with a draggable legend fails due to an unpicklable FigureCanvasQTAgg object. The reproduction steps are unambiguous, including the requirements (Python version, Matplotlib version, steps to create the problematic figure, enabling draggable legend, and attempting to pickle). The expected outcome is explicit: pickle.dumps should succeed without error. The actual outcome (the TypeError and its likely cause) is also described. The engineer has all the information needed to reproduce and understand the root cause, and a clear goal for what a correct solution should achieve (being able to pickle the figure, regardless of GUI canvas backend). No external info or clarification is strictly required to attempt a targeted fix. No ambiguity about required scope or expected behavior.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a 15 min - 1 hour type bug fix for an experienced engineer familiar with the codebase. It requires reading the figure pickling logic and understanding why a reference to the canvas is unpicklable, then refactoring the code to ensure only picklable components are stored/referenced. The fix requires decent familiarity with properties vs. attributes in Python, and knowledge of the Matplotlib event/callback/artist system, but the scope is contained to a single class and is conceptually straightforward (avoid storing non-picklable references). No complex algorithmic work or large-scale refactoring is needed.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The reproduction and requirements are clear, and the problem is relevant for a developer-facing benchmark. The test changes provide a concrete passing scenario. There are no external dependencies or vague business logic issues here.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25332": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It provides a clear summary of the bug: after calling align_labels() (used to align y-axis labels in Matplotlib subplots), trying to pickle (serialize) the figure object fails with a type error, because an internal object related to the alignment is not serializable. The reproduction steps are precise, with expected code structure (create subplots, plot data, label axes, call align_labels(), attempt pickling). The actual outcome (type error on pickling) and expected behavior (successful pickling) are clearly articulated. The information supplied about the environment (Matplotlib version, OS) helps scope the problem, and although no code snippet is directly provided, the description is detailed enough for an engineer familiar with Matplotlib to reproduce and understand the issue. The description makes it clear what a successful solution should accomplish: allow figures that have had align_labels() called to be successfully pickled.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The primary challenge is identifying and making the internal structures created or used by align_labels() picklable, possibly by implementing __getstate__ and __setstate__ or similar mechanisms. This requires understanding how alignment is tracked internally (e.g., via Grouper and use of weak references) and ensuring correct (de-)serialization to avoid breaking label alignment or introducing memory issues. For an engineer familiar with Python's pickle mechanism and somewhat with Matplotlib's internals, this is a moderate bug fix rather than an extensive overhaul; it is not trivial, but it is localized and well-scoped once diagnosed. It is unlikely to require more than 1 hour, but is more involved than simple code changes, thus level 1.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a strong candidate for a benchmark coding task: the requirements are clear, the problem is well-defined and testable, and the expected outcome is objectively measurable (pickling should succeed). Although no explicit code snippet is given in the issue, the steps to reproduce are described unambiguously for anyone familiar with Matplotlib. The only minor omission is lack of explicit code in the report, but this is counterbalanced by the clearly described steps and context.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25479": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well specified. The reporter provides a concrete example, explains the problem (the mismatch between registered colormap names and the colormap object's own internal name causing unexpected behavior when using pyplot), and indicates the expected behavior (after registering a colormap under a new name, it should be accessible by that name regardless of its internal 'name' attribute). However, the reporter refers to internal implementation details (\\\"double internal name lookup tables\\\") and presents the problem via an example rather than as a concrete specification. There isn't a highly prescriptive step-by-step request, but the issue as written gives a clear problem and a sensible path to a solution for a familiar engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch shows a change to the registration function to ensure that the registered name and the colormap's internal name match, and a minor change to equality comparison logic. This is a small code change, though it requires understanding of how colormaps are registered, stored, and looked up in matplotlib. For someone familiar with the codebase, the issue should take between 15 minutes and 1 hour: it is not completely trivial, but does not require substantial research or editing many files.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major reasons this sample should not be used for evaluating coding ability. The issue is clear, the bug is reproducible, and the solution is localized and testable. The concept relies on understanding how matplotlib's colormaps work, but the sample is meaningful for a coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-26291": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified: it clearly describes the context (using 'inset_axes' from mpl_toolkits.axes_grid1.inset_locator), the method of reproduction (setting up a figure with subplots and adding a small inset box), and the outcome (attribute error when drawing because renderer is missing). The expected behavior is also stated (should see an inset axes appear as in the documentation). However, there are minor blanks: the exact code used, the specific location where the error occurs, and explicit mention of the call environment are omitted and must be inferred. Nevertheless, an experienced engineer can make a sensible interpretation and work towards a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding how the renderer is handled internally for inset axes, but the symptoms and resolution are fairly direct once familiar with the basics of how matplotlib assembles and renders figures. The code change indicates adding a guard/default for a None renderer. This would require pinpointing where and why the renderer object is missing and fixing it -- a bit of thought and code tracing, but not a major rewrite. Likely solvable in less than an hour by someone experienced with this codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent with this sample. The bug and reproduction are clear enough for experienced developers to create tests and a corresponding fix. The test patch supports automated verification, so this fits well within the benchmarking setup.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-1724": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is relatively well-specified: it describes a reproducible scenario (constructing a request with method 'POST' as a unicode string instead of a byte string, sending a file, and encountering a UnicodeDecodeError in Python 2.7.2). It also provides code context (sessions.py, line 313) and a likely cause (use of .upper() on the unicode method). However, it is not 100% explicit about whether the solution should be in the form of accepting unicode method names everywhere, or always converting them internally, or raising an error earlier. The most sensible solution is to ensure the method is always a string (not unicode) internally, but one could imagine alternate interpretations. Still, a competent engineer could reasonably infer what is required. Therefore, I rate this as a '1' \u2014 some blanks to fill in, but a sensible solution can be implemented.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires understanding where in the code the unicode method type is problematic and ensuring it is properly converted to a suitable string type before further processing. For someone familiar with the requests codebase, this is a relatively minor change\u2014finding the spot in code, converting the method to the correct str type, and validating with a simple test. There is some thought required to ensure this does not impact other parts of the code, but the change is quite localized. Thus, 15 minutes to an hour is a reasonable expectation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blocking issues. The example in the issue is specific and actionable, and the proposed fix can be tested easily (as reflected in the provided test patch). The environment (Python 2.7.2) is slightly dated, but this does not affect the sample's suitability for evaluating coding ability in the described scenario.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-5414": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly states the precise error case (GET request to 'http://.example.com' raises UnicodeError), describes the current and expected behavior (should raise InvalidURL with an appropriate error message), and situates this in the context of the codebase (hints at specific code areas handling related errors). There are reproduction steps, expected vs. actual result, and even references to specific behavior that should be changed. All relevant context is provided for a developer to know what needs to be changed for a successful solution, and the error handling pattern is already established for similar situations in the codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively small change within an established error-handling flow. The engineer needs to locate where the URL is being processed and ensure that a hostname starting with a dot leads to InvalidURL rather than a UnicodeError. The inclusion of details like a stack trace, specific line references, and similar prior cases means most time would be spent confirming implementation details and running tests. For an experienced engineer familiar with the Requests codebase, this could be solved in 15 minutes to 1 hour, mostly consisting of updating error handling and writing/adjusting tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present. The sample is a good test of coding and debugging ability for robust error handling.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3151": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it gives a minimal complete verifiable example (MCVE) outlining the exact steps to reproduce the problem, including how to create the relevant xarray Dataset objects and the intended use of xr.combine_by_coords. The expected outcome is clearly stated\u2014combining the datasets should succeed and not raise an error\u2014and the problem description makes it clear both what error occurs and why this is incorrect (it conflicts with the documentation's claim about non-varying coordinates being ignored). Precise details of input structure, sequence, and desired behavior for the fix are given. An experienced engineer would readily understand what behavior must change and how to demonstrate success.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change requires a small adjustment to which dimensions are checked for monotonicity in combine_by_coords. It is a targeted fix in a specific code path, involves some inspection of the dataset structure, and requires updating or adding a test. The location to edit is clearly indicated by the problem description and error. The overall scope is a straightforward logic correction and is neither a large nor a trivial change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no unaddressed issues. The MCVE and expected output are clear, and this sample is highly suitable for evaluating coding ability relating to reasoning about dataset manipulations and code logic.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3677": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The problem, the steps to reproduce, and the expected behavior are described clearly. The issue describes an inconsistency between the behavior of the global merge() function (which correctly merges both Dataset and DataArray objects into a single Dataset) and the Dataset.merge() method (which fails when passed a DataArray). The expected result is clear: both methods of merging should support merging a DataArray so that the Dataset adds the DataArray as a new variable. The root cause is identified: Dataset.merge() expects mapping-like objects, but DataArray does not conform to that. This gives clear guidance for the required fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual change required is quite minimal (a one-line fix to convert the DataArray to a Dataset before proceeding), but it does require a small amount of thought to recognize and handle the type check. An experienced engineer, with some time to explore the codebase and understand how merge() is implemented and used, could solve this in under an hour. Writing or updating tests is also straightforward.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues that would make this sample unsuitable for a coding benchmark. The scope of the problem and the resolution are well-contained. The test case is natural. The issue does not require knowledge beyond the codebase and the general expectations of Dataset/DataArray API consistency.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4094": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear minimal example, explaining the expected roundtrip behavior for stacking and unstacking single-dimension variables within an xarray Dataset. It specifies the steps to reproduce the bug and the undesired error seen. However, it does not spell out the precise change needed in the code or clarify what the 'working roundtrip' output should look like in all cases\u2014in particular, for more complex data structures or edge cases. Nonetheless, for a competent engineer familiar with xarray, it should be clear that the bug is in round-tripping 'to_stacked_array' then 'to_unstacked_dataset' with single-dimension variables, and that the expectation is for this to work as for multi-dimensional variables. Therefore, while not maximally detailed, the issue is concrete and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change to fix this issue is localized and relatively simple (changing the 'sel' call in 'to_unstacked_dataset' to use 'drop=True'), as seen in the patch. The minimal code change required, together with the clear MCVE provided, would allow an experienced developer to test and iterate quickly. While some codebase familiarity is needed, the scope of the fix is small and does not require deep architectural changes or large-scale refactoring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues identified. The test changes are also clear and provide a concrete pass/fail scenario, which aligns well with benchmark evaluation purposes. The issue could be improved slightly by specifying the precise expected dataset output format, but it is still sufficiently clear for coding and evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4695": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite clear about the problem: naming a dimension 'method' in xarray causes .loc selection to throw an error, because the dimension name collides with a keyword argument in the .sel selection logic. The reporter provides a minimal reproduction (step-by-step), specifies the input, the action, and the observed and expected behaviors. The error is described precisely (including the type of error). Details about the environment are included but are not necessary for understanding how to fix the problem. An experienced engineer can reasonably reconstruct the scenario and understand what a successful fix must achieve (allow a dimension to be called 'method' and still permit label-based selection via .loc without errors).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix only required a minor change in how the .loc method delegates selection: passing the key as a positional argument instead of as keyword arguments. The problem is isolated to disambiguating between dimension names and keyword arguments. Given a few minutes' familiarity with the relevant selection logic, a developer could locate and implement the change quickly. The patch is small and localized (single-line change), so this is a 15 min - 1 hour difficulty task for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional problems or concerns. The issue is clear and technical, and the fix is concrete. This is a high-quality benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6599": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well specified. The user describes the unexpected behavior (wrong results and swapped dimensions when using np.timedelta64 coordinates in xarray's polyval), and notes explicitly that this behavior differs between versions. They provide a minimal, complete, verifiable example explaining how to reproduce the issue, and specify the expected/correct and actual/wrong outputs in terms of dimension/order and values. The environment information is complete, and no outside context is required\u2014the issue can be understood and fixed using only the information provided in the issue text. The expected correct result is behavioral consistency between stable and dev versions. The function and entrypoints affected (xr.polyval, computation related to DataArray coordinates) are also clearly inferable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a small patch-level bug fix. The solution only involves a relatively simple change: ensuring that np.timedelta64 (\\\"m\\\" dtype kind) coordinates are converted to float before polynomial evaluation. The fix is a conditional addition to a utility function, requiring correct type checking and the use of the astype(float) call. The underlying root cause may take a bit of thought to identify (why dimensions are swapped and values are off), but once recognized from the MVCE and description, it is not a large refactor or major redesign. For an engineer familiar with NumPy dtypes and xarray, 15 minutes to 1 hour is reasonable for diagnosis and patching.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The only possible consideration is that the difference between expected and actual outputs is described in broad terms and not with actual sample output arrays, but the MVCE and expected/actual behavior are otherwise clear enough. This sample is appropriate for coding benchmark purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6721": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a clear explanation of the problem: when accessing the 'chunks' attribute on a zarr-backed xarray object, all array data is loaded into memory, rather than only inspecting the metadata. The expected behavior is specified (should inspect only 'encoding' on DataArrays, not load data). Although the user does not provide a minimal code example, the descriptive context and environment details are sufficient for an experienced engineer to reproduce and reason about the bug. There are some implementation details to decide (e.g., exactly how to avoid loading data), but the required change is clear from the intent stated. Therefore, it is mostly well-specified, with minor blanks to fill.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug pertains to mistaking access to the wrong attribute when checking for chunks, causing unnecessary data loading. Fixing this involves a small targeted code change, as evidenced by the gold patch (just switching from v.data to v._data) and the addition of a simple regression test. The problem is well-scoped and does not require major architecture or deep domain-specific research, though some familiarity with xarray and zarr is helpful. This would likely take an experienced engineer 15 minutes to 1 hour, covering the time to understand, implement, and test the solution.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is suitable for coding ability benchmarks. The environment details in the issue are verbose but do not distract from the required fix. The absence of a minimal code example does not meaningfully impede solution for experienced engineers, given the detailed behavioral description.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6386": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear. It states a specific problem: when using the short '-v' (verbose) flag with Pylint, the tool incorrectly prompts for an argument, whereas the long '--verbose' flag does not. The description includes a minimal reproduction, expected behavior, and relevant output. An experienced engineer would know that success means making '-v' work exactly the same as '--verbose', without requiring an argument, and likely updating command-line argument parsing logic and help text. No ambiguity exists about the root problem, reproduction, or expected fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires making a small but precise change in the command-line argument parsing logic so that the short '-v' flag works identically to '--verbose', and does not expect an argument. Though changes are needed in a few areas (definition of the argument and how it is handled in the code), this is a standard fix involving the command-line interface code and tests, and should take less than an hour for someone familiar with the framework, as demonstrated by the concise gold patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The test case is straightforward, targeting the new logic and confirming the fix.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6903": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is quite well-specified: it states that running pylint in a Kubernetes Pod with --jobs=0 can cause a crash due to the CPU count calculation returning zero (as a result of particular cgroup file contents), and it describes the exact cause: the fraction calculated (2/1024) is cast to an int, resulting in 0, which is invalid for multiprocessing. It also clearly states the expected behavior: pylint should not crash if a zero CPU count is detected\u2014the number should never become zero. A possible solution is even proposed (\\\"or 1\\\" at the end of the line). The only minor ambiguity is whether similar logic should apply at a different location (line 55), but overall, a sensible fix is achievable with the info provided.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding a CPU calculation, why returning zero is a problem, and amending the code so that zero CPUs never get passed to the process pool (using at least one as a fallback). The actual code change is simple (a check and assignment), but it requires a bit of familiarity with how multiprocessing expects its input and a basic understanding of how CPU cgroup stats might interact. An experienced engineer should be able to implement and test this in less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The problem scenario is reproducible and the intended solution is clear. The expected behavior and the 'why' are specified, and example input data is given. The ambiguity about another line in the code is minor and does not prevent a meaningful solution.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-8898": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly states that the 'bad-names-rgxs' configuration option in pylint, when given a regular expression containing commas (such as those used in regex quantifiers like '{1,3}'), is incorrectly split on commas, corrupting the intended pattern. It explicitly describes the steps necessary to reproduce the problem and observes how pylint crashes when such a configuration is used. The desired behavior is also clear: any valid regex pattern (even those containing commas) should be allowed, or at minimum there should be an escaping mechanism. The provided configuration, reproduction steps, error observed, and expected outcome are clear enough for an experienced engineer to understand and address the issue, even without further discussion or clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires creating logic to parse a comma-separated list of regular expressions without splitting inside comma-containing quantifiers. This involves a bit of care to avoid breaking regex patterns while parsing, but the change is local, fairly contained, and does not require large-scale architectural changes. The solution impacts several files (utility, argument parsing, and tests), but the logic itself is not complicated and can be accomplished within an hour by an experienced engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being used to evaluate coding ability. The original issue is clear, the solution is meaningful, and the associated test coverage verifies that the problem is fixed and provides a clear means of evaluating correctness. The task is a realistic example of what engineers must handle in codebases that parse config values with potentially nested syntactic content.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10051": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It concisely explains the problem: after calling caplog.clear(), caplog.get_records() still returns old log records, while caplog.records has been cleared. It provides a clear reproduction case, describes both the expected and actual behaviors, and covers the usage context (pytest caplog fixture). An engineer would understand what is wrong (stale records held after clear), what is expected (both should be cleared), and under which conditions. The issue also carries enough detail that success criteria are unambiguous: after clear(), both get_records() and records are empty. No further outside clarification is needed to attempt a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, targeted bug fix in a well-scoped feature (pytest's logging fixture). The fix as shown involves bridging the behavioral disconnect between caplog.clear() and internal handler state; it is straightforward once the relationship between caplog and its handler is understood. The patch is concise and the test changes are minor. An engineer familiar with the codebase should be able to reason about this and implement it within an hour, including verifying the test. Slight familiarization may be needed, but no substantial refactoring or research is required.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues are present. The issue is self-contained and the description enables direct mapping of problem to requirements. The coding and testing aspects directly assess the ability to understand, implement, and verify the correct behavior in pytest tooling code, making this a suitable and meaningful benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10081": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It clearly states the behavior observed: When running pytest with --pdb, the tearDown() method of unittest.TestCase classes decorated with @unittest.skip at the class level is unexpectedly executed, triggering errors. It explains the expected correct behavior (neither setUp nor tearDown should be called when the test/class is skipped), and it gives the environment (Python, pytest, OS versions) and a precise minimal reproducible example. It explicitly compares current incorrect behavior to the correct behavior in a normal run without --pdb. There is little ambiguity about what needs to be fixed\u2014ensure setUp/tearDown are not invoked for skipped classes even with --pdb. The filename for the example is supplied, and all logical steps are described. Any experienced engineer with access to the codebase could work from this issue description alone without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is rated as 1 (15 min - 1 hour). The issue is localized (as seen in the gold patch) to a small section of logic in the test runner where it checks if tearDown should be called. The main task is to add an additional condition to properly consider both function-level and class-level skips, which requires some understanding of how skipping is tracked and the role of the usepdb option. The code change is short and limited to one file, though some investigation is needed to identify the right place to insert the condition and to verify the logic. Adding an appropriate test also requires minor extension of an existing test. This is not entirely trivial but is clearly in the realm of a small change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are identified. The sample is well suited for evaluating coding ability, with a clear specification, a reproducible minimal test case, and a self-contained scope. There are no external dependencies or ambiguities introduced by the issue description, and the necessary context is present in the text. The gold and test patches further confirm that the problem is solvable as described.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5262": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It provides a clear description of the bug: the _pytest.capture.EncodedFile advertises a binary mode ('b' in mode), but only accepts text, which causes an exception when a tool (e.g., youtube-dl) inspects the mode and tries to write bytes. The desired and expected operating behavior is described (should work with str and not falsely advertise 'b'), a concrete reproduction method is provided (including required dependencies, how to write the test, and expected/actual output), and the affected class and property ('mode' of EncodedFile) are described by context. For an engineer familiarizing with this codebase, it is clear that the solution would involve changing the handling of the mode property in EncodedFile so as to not include 'b', matching its real capabilities, resolving the TypeError and removing the implication that the stream is binary. There is little ambiguity about what problem needs fixing and how to test whether the problem is resolved.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires an understanding of the EncodedFile object in pytest's internals and familiarity with how file-like objects advertise their mode. The code fix is quite small (as seen in the gold patch, just override the mode property to remove 'b'), but it does require some consideration about the broader impact of this change, and ensuring it is correct and compatible with similar usages. For an experienced engineer, this would typically be a 15-60 minute fix (closer to the lower end if already familiar with the relevant code).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent with this sample. The specification, reproduction steps, and desired outcome are all clear, and the required change is modular and well-bounded. The test adds a check for the absence of 'b' in sys.stdout.mode under the test harness, which is sufficient to check for regressions.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5631": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified, with a clear technical explanation of the problem: a ValueError is raised due to ambiguous truth evaluation when a patch decorator is used with a numpy array as the replacement value. The reporter identifies the line of code responsible for the bug and directly highlights the logic problem (using `p.new in sentinels`, which yields ambiguity with numpy-like objects). However, the description makes some assumptions about the reader\u2019s familiarity with pytest\u2019s internals, how the patch mechanism works, and the specific role of the sentinels in question. Details such as exact file or function names are missing, but there is enough context for an experienced developer who is familiar with pytest or mocking frameworks to find the problematic logic, reproduce the error, and implement a fix. Thus, there are some blanks to fill, but there is a sensible interpretation and clear direction for the solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change is localized: it requires understanding why equality comparison for numpy arrays or numpy-like objects causes problems when used directly in an 'in' check, and adjusting this logic to use identity comparison instead. This involves a small but careful rewrite in a single utility function, and understanding the expected behavior. The actual code change is only a few lines, but care must be taken to ensure correctness and not create regressions. Writing or editing targeted tests is straightforward once the cause is understood. For an experienced engineer, this task would likely take between 15 minutes and 1 hour, factoring in the time to diagnose, implement, and test the correction.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues were identified. The sample is a good test of understanding mocking, identity vs. equality checks in Python (especially with numpy), and cross-version compatibility. It also is self-contained enough that it would make a good coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is quite well-specified: the expected (and current) behavior is described in detail for both regular pytest runs and pytest-xdist runs, describing the scenario where exception chains are not preserved in serialized reports in xdist usage. The user gives step-by-step instructions and explicit test function ideas for reproducing the bug. However, the explicit requirement for \\\"chained exceptions need to be serialized/deserialized properly across processes in xdist\\\" is implied, not stated as a formal requirement. There are some blanks to fill in (such as exact method locations), but any experienced engineer could reasonably figure out what to do based solely on the main issue text and the code/test context.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The solution is non-trivial: it requires understanding pytest internals, notably report serialization/deserialization, as well as the structure of chained exceptions and the requirement for compatibility between normal and xdist runs. The patch involves changes across the reporting and test code, new or refactored helpers, handling intricacies of various report objects, and extending/deserializing custom exception representations. All told, while not a multi-day project, it likely takes several hours for a new contributor to reason about the appropriate cross-process report serialization and robustly implement and test the solution.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major concerns. The issue is technical, well-scoped, and the provided material (including a test plan) is concrete. There is sufficient context for reproducing and verifying the fix, and the problem domain\u2014exception serialization with chaining under a distributed pytest environment\u2014is clear and testable. It is suitable for evaluating coding ability in a rigorous way.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5809": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly states which code path is affected (the --pastebin option in pytest, specifically src/_pytest/pastebin.py), describes the request being made to bpaste.net, and pinpoints the parameter that causes the problem (lexer=python3). It provides the observable problematic behavior (400 Bad Request error when submitting certain outputs), includes a concrete example, and explains not only what goes wrong but also what the correct behavior should be (use lexer=text), and the rationale for this change. An experienced engineer could confidently implement a fix from this description without needing further clarification.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a trivial change: it involves adjusting a single parameter ('lexer') from 'python3' to 'text' in the parameters for a POST request. The location in the code is clear, and there is no need for deeper architectural understanding or consideration of side effects. For an experienced engineer, this change (including any necessary tests) would likely take less than 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The problem is straightforward, localized, and suitable for use as a benchmark coding sample. It involves string substitution, not complex logic.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5840": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text provides detailed reproduction steps (specific pytest versions, uninstalling/installing, directory to invoke, marker to use) and a clear description of the observed problem (path casing lowered causes conftest.py not to be found due to case sensitivity/casing mismatch on Windows). It also specifies the expected behavior ('pytest preserves the original path casing and successfully discovers the configuration file'). However, it does not specify the precise area of the code or method responsible, which leaves some blanks for the implementer. Still, an experienced engineer should reasonably deduce what needs to be changed: the code should use case-preserving (or case-sensitive) operations for path handling, especially under Windows. Overall, there's a clear sensible interpretation of what a fix would involve, but some investigation is required.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix spans several logic changes in path handling within pytest's internals, likely requiring an understanding of both how pytest discovers conftest.py modules and how Python's (and py.path's/pathlib's) path resolutions work, especially regarding filesystem case-sensitivity. It requires investigation into the codebase, updating multiple functions (potentially across more than one file), and ensuring backward compatibility for both Windows and non-Windows platforms. The test patch also shows the addition of new tests to cover upper-case/lower-case issues. For an experienced engineer, this would likely take 1-4 hours: it's more involved than a simple fix but does not amount to a complete refactoring or highly esoteric issue.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being used. The problem is contained, the tests are relevant, and the requirement is clear once the description is read carefully. The only minor caveat is that some domain familiarity with path handling in Python and Windows file systems is helpful, but this aligns with typical expectations for such a benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6197": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is detailed and provides a concrete minimal reproduction. It specifies the regression between pytest 5.2.2 and 5.2.3: under 5.2.3, __init__.py files are being imported unnecessarily, causing the test run to fail if they have side effects (as demonstrated by using a failing assertion). The expectation (that only test files are collected and run, not arbitrary __init__.py) is clear. The only slightly underspecified aspect is where in the codebase the patch needs to be made, and exactly how to stop the unwanted collection, but the problem and the success criterion (py.test should not collect or import non-test __init__.py files in packages unless directly appropriate) is explicit. An experienced engineer can reasonably infer what a correct solution requires.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires understanding how pytest's collection and import mechanism works, tracking down the regression between two versions, and updating test collection logic and/or file filtering behavior. This would involve reading through parts of the test collection codebase, identifying where __init__.py are being imported in 5.2.3 but not in 5.2.2, and implementing the correct guard for collection/import. This requires familiarity with pytest internals; it is not a trivial change, but it's not a multi-day project either. It involves careful reasoning and some nontrivial testing/editing of code and test files across the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The scenario is reproducible in the issue, and the requirements are understandable. The only minor gap is guidance on possible edge cases or platform specifics, but these are not blocking for the benchmark use case. The issue is appropriate for evaluating code debugging, modification, and regression-fixing ability.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6202": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It clearly describes both the symptoms of the bug (incorrect display of test names with parameter values that start with '..[') and the exact location in the code ('return s.replace(\\\".[\\\", \\\"[\\\")' in python.py) that is responsible. The minimal reproducing example is described and the expectation for behavior is unambiguous. The issue even details a proposed fix and points to exact lines in the code. An engineer would know exactly which file and method to address. No significant ambiguities remain.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is small (a one-line change, removing a specific 'replace'), and is in a well-isolated part of the codebase. However, understanding the context (why that replacement was there at all) and being sure that removal is the correct action requires spending some time to confirm that no other cases are affected. Therefore, this is a small change that a thoughtful engineer could accomplish in 15-60 minutes, assuming moderate familiarity with pytest's code structure.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major concerns. The issue is clear, focused, and the solution is straightforward. The only minor concern is the question raised by the OP about the original purpose of the line being changed, but verifying this is part of the engineer's due diligence and does not affect benchmark suitability.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7205": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite well-specified. It reports a reproducible problem (BytesWarning is raised when using pytest with --setup-show, the -bb flag, and a test parameterized by bytes on Python 3.8.2). The expected behavior is clearly described: pytest should print the setup step and show the bytes parameter in a readable form, not raise warnings. The cause is identified as an unsafe conversion to string (str()), and an explicit suggestion is given to use saferepr or similar instead. This makes it clear what needs to be changed and how the solution should behave, so an experienced engineer can directly proceed to fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires finding the code responsible for displaying parameters during setup, understanding why str(bytes) triggers BytesWarning, and applying a safe string representation (like saferepr). The diff shows only a minor code change is required. While a bit of investigation might be necessary to identify the correct code path, this is a small-scope problem with a targeted fix, likely to take 15-60 minutes for someone experienced with pytest's structure.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The setup and verification are clear, and the sample is appropriate for evaluating engineering ability: it tests reading comprehension, codebase navigation, an understanding of Python's str/repr mechanics, and safe string conversion. The test file demonstrates the failure mode and validates the fix.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7236": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clearly describes the problem: when running pytest with the --pdb option, unittest.TestCase's tearDown method is executed even for skipped tests, which is unexpected and inconsistent behavior compared to earlier pytest versions and standard test runs. The issue provides a minimal reproducible example and explains both the expected and actual outcomes. The context (Python and pytest versions) is included, so it is clear what constitutes a correct solution: ensure that setUp and tearDown do not execute when tests are skipped, even when --pdb is used. The relevant class and methods (unittest.TestCase, setUp, tearDown, pytest skip decorator, --pdb) are explicitly mentioned. An engineer familiar with pytest internals would know where to look and what the successful solution would be.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is a small but thoughtful change. The engineer needs to understand when and how pytest calls setUp and tearDown in conjunction with skipping and the --pdb flag, but the core code change is limited in scope and concept: guarding teardown to not run for skipped tests, and modifying related logic to use a helper function. It likely requires reading a few functions and checking the pytest/unittest integration, but the overall change is a straightforward conditional and a small utility. Estimated time is 15-60 minutes for an engineer familiar with testing frameworks or pytest internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The sample is suitable for coding ability evaluation: the requirements are clear, there's a reasonable and focused technical fix, and the problem is self-contained. The only minor caveat is that deep knowledge of pytest internals may help, but the solution itself remains accessible to strong engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7324": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly specific regarding the environmental circumstances (debug-build Python 3.8+, interactive mode), the action (creating an AST expression from a literal False and compiling it), and the unexpected outcome (interpreter aborts with an internal compiler assertion failure). The scope of the problem is technical and clear: a particular code path triggers a crash inappropriately. It is also clear that the desired behavior is to return a code object or a controlled exception, not a crash. However, there are some blanks to fill in regarding which exact filenames or functions are at play in the pytest codebase, and the title (\\\"Pytest crashes the interpreter on debug build for 3.8+\\\") could cause ambiguity as to whether the fix is expected within pytest, or to contribute a fix upstream. The issue includes a reference to a related Python bug report but does not specify the solution strategy. Overall, this is a 1: some blanks to fill in, but sensible interpretation is possible.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves creating a prefix for certain identifiers while converting to Python AST and appropriately stripping it in the matcher, which is localized to a small part of the codebase related to mark expressions and their translation to ASTs. The patch touches a few lines in one core source file and in the related tests. The conceptual challenge is small: identifying the source of the identifier clash and adding a prefix. The actual editing is straightforward and should be manageable within 15 minutes to one hour for an experienced engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues are evident. The case focuses on a technical crash triggered by specific interactions between identifier parsing in pytest and Python language constants in debug builds post-3.8. While some background understanding of AST construction and token parsing is assumed, the issue as stated is fair for a benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7490": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides clear instructions for reproducing the undesired change in behavior between pytest versions 5.x and 6.x. It describes both the expected (5.x) and actual (6.x) test outcomes when dynamically applying an xfail marker to a test using the request.node.add_marker mechanism. The sample includes enough details (py.test version, OS, how to write a test to trigger the bug, exact command-line options to use, etc.) to know what the faulty and correct behaviors are. An experienced developer familiar with pytest's source can deduce that the cause is related to dynamic marker evaluation timing and can identify relevant entry points (such as the runtest hooks) for investigation and a fix. This makes the requirements for a successful solution clear and testable.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"While the scope of the code changes is not massive and limited to parts of pytest's runner and skipping logic, resolving this issue requires a good understanding of pytest's test collection, marker handling, and execution hooks. The engineer has to reason about when dynamic changes to a test's node (like adding an xfail) are processed, and ensure consistent, backward-compatible behavior between dynamic and static xfail applications. There is some nontrivial logic to ensure xfail evaluation occurs at the correct time relative to test execution, which may require editing multiple locations and verifying changes with appropriate tests. It is more complex than a trivial bugfix and would likely involve 1-4 hours of work for a well-familiarized engineer, especially because of the need to think through edge cases and test coverage.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are present. The problem is reproducible, scoped, and the success condition is clear. Sufficient context about the code, test, and expected result is included in the issue text. This is a good sample for evaluating coding ability, requiring both debugging skills and understanding of advanced pytest internals.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7521": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear description of the observed regression: capfd.readouterr() in pytest 6.0.0rc1 is converting carriage returns ('\\\\r') to newlines ('\\\\n'), whereas in pytest 5, carriage returns were preserved. The steps to reproduce, the expected behavior, and what actually occurs are described in reasonable detail, citing both a minimal demonstration and the motivating real-world case. However, it does not specify exactly where in the codebase the fix should be applied or the exact mechanism for fixing, so some investigation is required. An experienced engineer can understand what a correct outcome looks like, but some minor interpretation and exploration of the codebase is still needed to connect the issue to the correct point of intervention.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires some understanding of the pytest codebase related to capturing output, but the code fix itself only involves a minor change (adding an argument to the open() invocation to preserve newline characters). Once the root cause is understood from the description and minor confirmation is performed, the implementation is straightforward and unlikely to take more than an hour for a familiar engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue appears to be well-formed for use as a coding benchmark. The only subtlety is that it is regression-related and requires awareness of differences between Python versions or the relevant behaviour of file handling with respect to newlines, but no special blockers are apparent.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and provides sufficient context for an experienced engineer to produce a clear solution. The problem is that the RidgeClassifierCV class does not accept a store_cv_values parameter, despite the documentation suggesting it should (mirroring the RidgeCV class). The repro steps are clear: instantiate RidgeClassifierCV with store_cv_values=True and it fails. The expected behavior is explicit: classifier should train successfully and include an attribute with cross-validation predictions. The actual error message is quoted. There is even supporting evidence from the documentation, confirming this is not a usage issue, but a missing feature or bug. The target class and parameter are named, as well as the relevant attribute to be populated. The context (versions, OS) is not essential, but included. No significant ambiguity remains for implementing a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small code change: adding the missing store_cv_values parameter to RidgeClassifierCV, ensuring it is passed to the relevant superclass and that its effect is as described in the documentation. The solution mostly involves modifying the RidgeClassifierCV class' __init__ method and verifying that the attribute is correctly set/propagated. Some understanding of the base class (RidgeCV) is required, but the code changes affect just a couple of lines plus associated docstrings and tests. This is a straightforward fix once the class structure is understood, appropriate for a 15 min - 1 hour task for an experienced contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues appear present. The issue is clear, self-contained, and appropriate as a benchmark question. It covers new parameter addition, propagation, and interfacing with an existing feature, which is a representative engineering task. The test patch confirms the expected behavior and that testing can be applied automatically after solution attempts.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10908": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes inconsistent behavior between CountVectorizer when using a user-supplied vocabulary and when not, specifically focusing on the get_feature_names method raising a NotFittedError even if a vocabulary is provided. It details the current behavior, the steps to reproduce, and what behavior the reporter expects: get_feature_names should not raise NotFittedError if a vocabulary was supplied (i.e., if fixed_vocabulary_ is True), even before fitting or transforming. Key functions, methods, and classes (get_feature_names in CountVectorizer) are clearly identified. An experienced engineer would understand exactly what a successful solution must achieve.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding the initialization and state management of CountVectorizer, specifically with regard to handling a custom vocabulary. The fix involves ensuring vocabulary_ is set if a vocabulary was supplied, and modifying get_feature_names accordingly\u2014a small code change but requires some understanding of the class's data flow. The code change is localized and relatively simple, unlikely to exceed an hour for an engineer familiar with the repository.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be other major issues. The description is sufficient, and relevant test cases are straightforward to infer/generate from the report. The request and success criteria are clear, and the issue is appropriate for benchmarking coding ability in this context.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-12585": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and provides a clear reproduction case and expected behavior. The text describes the error in the clone function when an estimator has a parameter set to an estimator class rather than an instance. The issue explains how to reproduce the bug (by creating a StandardScaler estimator with a parameter set to the StandardScaler class and cloning it), and describes what currently happens (TypeError when get_params is called on a class). It also suggests a specific fix in base.py, line 51, by altering the handling condition to account for types. The requirement is clear: clone should not fail on estimator parameters that are types, and a possible concrete code change is identified. An experienced developer could independently construct a valid and meaningful fix from this description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small and well-isolated change in the codebase. The logic of the clone function is simple enough, and the bug is localized to a specific code path. Understanding the problem, verifying it with a small test, and implementing the precise fix as suggested would likely take 15 min to 1 hour, depending on familiarity with the codebase and any need to run tests or adapt related code. No deep or substantial code redesign is required, and there is a suggested solution provided.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no significant additional issues or reasons this sample should not be used for evaluating coding ability. The scenario is unusual but legitimate, and the issue text plus the code context is sufficient for a meaningful exercise.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13135": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified: it describes a concrete error scenario that can be reproduced, including the exact feature and options used (KBinsDiscretizer, strategy='kmeans', ordinal encoding, 5 bins), a specific dataset, and the observed versus expected behavior. The requirement for a fix is also explicit: transformation should not throw an exception due to unsorted bin edges, i.e., the discretizer should work without error for the described use case. The failure mode (unsorted bin_edges leading to np.digitize raising) is clearly stated and the context is sufficient for someone familiar with scikit-learn to identify the relevant code areas and logic that need adjustment. There are no significant ambiguities in what constitutes a successful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix, as revealed by the patch, requires sorting the cluster centers before calculating the bin edges, which is a small and localized change. While it requires understanding the flow of KBinsDiscretizer's kmeans strategy (and potentially tracing the data transformation steps), it does not require substantial refactoring or complex debugging beyond pinpointing where to sort. For an engineer familiar with the codebase, this should be a matter of minutes to an hour, as it only involves adjusting a single line and ensuring kmeans output is always sorted before use.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue is a solid benchmark for evaluating coding skill related to machine learning library bug fixing. The steps to reproduce are clear and the required code change is meaningful, not trivial, but the problem scope is narrow and does not introduce external ambiguity.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13142": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it describes a clear bug in the scikit-learn GaussianMixture API where the fit_predict and predict methods yield inconsistent results when n_init > 1. The description provides (1) exact reproduction steps with sample data, (2) precise expected and actual outcomes (cluster labels should match, but do not), and (3) affected methods/classes. The engineer is told which class and parameters to investigate, what behavior is incorrect, and what correct behavior should look like. No ambiguity about outcome or intent remains.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding the fit_predict implementation and ensuring it is consistent with a fit + predict flow, particularly when n_init > 1. The solution involves ensuring fit_predict returns consistent assignments and likely fixing an off-by-one or re-computation bug after multiple initializations. This is a targeted change in a single method once the logic is understood, requiring some investigation but not a major rewrite or broad refactor. For an engineer familiar with scikit-learn patterns, this should take 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues apparent. The issue is focused, concrete, and testable with automated checks. All relevant context (expected vs actual outcomes, usage pattern, environment) is provided. Suitable for coding benchmark purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13328": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It provides a reproducible example (generate a regression dataset, create boolean feature matrix X_bool = X > 0, fit HuberRegressor on it, and observe the failure), a detailed description of the expected vs. actual result (should accept boolean arrays and implicitly convert to floats, as similar estimators do), and even points to the nature of the error (unary minus on boolean triggers TypeError in NumPy). The requirement is concrete: make HuberRegressor work with boolean input features in line with existing scikit-learn estimators like LinearRegression. No ambiguity remains about what a successful solution should achieve.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix primarily involves ensuring type conversion of boolean arrays to float before the fit process proceeds, similar to related estimators. For someone familiar with the scikit-learn codebase, this would be a small change (updating the usage of check_X_y to enforce float dtype), verifying no downstream code is broken, and adding a simple regression test. The exact location of the necessary change is straightforward to locate based on the error description and the behavior of comparable classes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be any blockers, ambiguities, or dependencies outside of what is already detailed in the issue description and required for the bug fix. The issue is suitable for benchmarking coding ability in the described setup.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13779": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, providing a concrete bug (AttributeError when an estimator is set to None in VotingClassifier or VotingRegressor with sample_weight) and a minimal reproduction strategy (using iris dataset, removing estimator, and refitting). The expected change is specified: the code should detect and properly skip or handle estimators set to None so that fitting succeeds with sample weights. The failure mode is identified and the problematic code path is strongly implied (iteration over self.estimators and not checking for None). No critical ambiguities exist, making the requirements for the fix quite clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change involves adding a check within a loop over estimators in the fit method to skip over estimators set to None. This is a small, targeted fix that requires understanding the estimator iteration and sample_weight handing code. Given the clarity of the issue, the solution will likely be an extra guard clause and minimal testing, taking an experienced engineer about 15-30 minutes to implement and test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The issue is clearly described, the change is localized, and an appropriate test scenario is specified. The bug is reproducible from information in the issue.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14053": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is quite well-specified. It describes a reproducible bug in export_text when using a decision tree with only one feature. Steps to reproduce are given in detail, including how to prepare the data (the iris dataset, isolating a single feature), how to train the model, and how to call the function in question. The expected and actual outcome are both described: the function is expected to return a text representation of the tree, but instead raises an IndexError. The environment information is detailed, but not necessary for solving the issue. The main requirement is clear: fix export_text so it properly handles single-feature trees, returning correct output instead of an error. No significant ambiguity remains about what needs to be fixed or how success will be determined.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small bugfix that requires a bit of investigation into how export_text handles cases where there is only one feature. The actual patch shows that it is just a matter of properly handling tree_.feature values that might refer to undefined features (i.e., _tree.TREE_UNDEFINED), which is a localized change and does not require deep restructuring or understanding of the whole codebase. The engineer needs to add a conditional to avoid an IndexError, so most time is spent reading and understanding a small part of the code. For someone familiar with the codebase, this should take 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent for using this issue/PR in a benchmarking setting. The issue is specific, has a clear pass/fail criterion, and the test patch addresses the regression. The dataset and code examples are standard for a Python data science environment.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14087": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. It clearly describes the exception ('IndexError'), the context (using LogisticRegressionCV with refit=False), and the specific model configurations (solver, tolerance, folds, data generation process, etc). It also explains the error's location in the underlying procedure (averaging of coefficient paths) and what the expected vs. actual results are. This gives a clear definition of what constitutes a successful solution: the cross-validation should run and return the correct parameters without any indexing errors. Additionally, the included test code and configuration details would make it straightforward for an engineer to reproduce and address the issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The underlying fix appears to be a small change to conditional logic within the fit method and checks a couple branches for edge cases. While this requires understanding the cross-validation path in sklearn's logistic regression, the problem is isolated and the context is well-described, so an experienced engineer could find and resolve this within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues. The bug, context, and desired behavior are clear, and an engineer could be objectively assessed by using the issue text and the provided test. The data generation, relevant class and method, and triggering configuration are all sufficiently described.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14629": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It provides a clear description of the error that occurs when using cross_val_predict(method='predict_proba') with MultiOutputClassifier, explains the suspected source of the bug (use of estimator.classes_ instead of mo_clf.estimators_[i].classes_), and gives reproducible steps using synthetic data and specific estimators. The expected and actual behavior is also explained. The references to specific attributes (classes_) and the class involved (MultiOutputClassifier) are clear, allowing an engineer to reproduce and fix the issue without additional clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires a good understanding of how MultiOutputClassifier works and how cross_val_predict interacts with it. The patch itself involves adding a classes_ attribute to MultiOutputClassifier to be consistent with what cross_val_predict expects. This is not a trivial one-line change, but it also does not require a major rewrite\u2014it's a focused fix in a single class, with corresponding tests added. For someone familiar with scikit-learn's conventions and design patterns, the solution could be implemented in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The sample is fully appropriate for evaluating coding ability on this type of bug, since it involves knowledge of object-oriented design, familiarity with machine learning libraries, and correctly designing and running tests for complex estimators.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14710": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It provides a precise explanation of the bug: during early stopping in HistGradientBoostingClassifier, 'y_true' is passed as integer codes, while 'y_pred' remains as original class labels (strings), leading to a type comparison error. The steps to reproduce, including the exact configuration and synthetic data, are given in detail, making the bug easily reproducible. The expected behavior (no exception), actual results (type comparison error), and a specific suggested code location/patch offer even more clarity. Any competent engineer familiar with scikit-learn could proceed to solve it purely from the issue text.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is limited in scope: it requires understanding how class labels are encoded internally and ensuring correct mapping before scoring. While this requires a bit of investigation into how y_true is processed versus y_pred and the structure of HistGradientBoostingClassifier, the core change is relatively small and localized. Thus, it falls in the 15 min to 1 hour bucket for an experienced engineer familiar with the scikit-learn codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No disqualifying issues are present. The issue statement is sufficiently clear, and the test ensures the fix works as intended. This sample is suitable for use in a coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14894": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is highly specific and clear. It identifies a concrete bug (\u201cZeroDivisionError in _sparse_fit for SVM with empty support_vectors_\u201d) and describes exactly what triggers the failure (fitting an SVR with particular parameters and a sparse matrix input which results in no support vectors). It gives precise reproduction steps, expected results (no error and self.dual_coef_ = sp.csr_matrix([])), and observed results. All relevant context (function names, class, and critical parameter settings) are present. An engineer with access to the code (specifically sklearn/svm/base.py, and the _sparse_fit method) could start investigating and implement a fix based on these details. There are no major ambiguities about what constitutes a correct fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This bug fix fits squarely in the 15 min - 1 hour category for an experienced software engineer. The logic for initializing dual_coef_ needs to handle the edge case when the number of support vectors is zero. The code change is small and localized, requiring only the addition of a guard clause to prevent division by zero or invalid construction of the CSR matrix. However, determining the correct fix still requires a brief investigation of the flow, understanding why the edge case occurs, and verifying that the patch doesn't break other functionality. The test addendum is trivial.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The problem is well-isolated, there are clear instructions for reproducing the bug, and the expected behavior is explicit. This sample is very suitable for evaluating coding ability on real-world bugfixes involving numerical edge cases and array/matrix logic.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25747": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides detailed reproduction steps, expected and actual outcomes, and sufficient context to understand the problem. It specifies that a custom transformer, when grouped by date and aggregated, works in NumPy mode but fails in pandas output mode within a FeatureUnion. It describes the root cause (misalignment of DataFrame indices due to aggregation) and specifies the error message. However, it does not specify the exact location of the code to be changed and does not tie directly to a specific class or function (e.g., _wrap_in_pandas_container in sklearn). Thus, while a skilled engineer can make a sensible interpretation and locate the source with minor inference, there are still some blanks to fill regarding the precise code locus and expected API behavior.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires identifying where scikit-learn attempts to reattach the index to an already-aggregated DataFrame, modifying this logic so that indexes are not overwritten improperly, and confirming this with new or updated tests. The source code involved is somewhat deep in sklearn internals, but the change itself is small and localized, requiring a bit of code reading and understanding of set_output and pandas DataFrame handling. For an engineer familiar with sklearn, this is a straightforward fix that should take less than an hour, possibly longer to identify the best spot to patch if they aren't already familiar with the internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues stand out. The problem is fairly scoped, the requirements are clear, and reproducibility is ensured by the provided reproduction steps. It is suitable as a benchmark sample for evaluating coding ability involving bug fixes and DataFrame handling in sklearn-like environments.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25931": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes the unexpected behavior (a warning about missing feature names when using IsolationForest with a non-default contamination parameter on a DataFrame) and explains both the context (DataFrame input, setting contamination, calling fit) and the expected vs actual results. The likely root cause is suggested, and reproducible steps are provided, including sample data and specific parameter settings. Temporary environment and library versions are also provided. There is no ambiguity in what constitutes resolving the bug: ensuring that fitting IsolationForest with a DataFrame and contamination != 'auto' does not emit the feature names warning. References to code or function names are present to guide implementers. Therefore, the issue specification is clear and sufficient for an engineer to attempt a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires familiarity with the scikit-learn codebase, specifically how IsolationForest processes data and manages feature name validation when calculating the offset_ parameter during fit. However, the solution itself involves only a minor but thoughtful code change (calling _score_samples instead of score_samples to avoid extra validation and feature name loss), and the impact is narrow in scope. Locating the issue, understanding the validation mechanism, and writing the fix and a regression test could reasonably be accomplished within an hour for someone already familiar with scikit-learn's conventions.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present. The issue is concrete, reproducible, minimally scoped, and the expected behavior is clear. It is suitable for evaluating coding ability regarding Python, data validation, and scikit-learn internals. No external dependencies or ambiguous context.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25973": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is extremely well-specified. The problem is clearly described, including precise usage steps to reproduce the bug, an outline of the data preparation, a specific model and API involved (SequentialFeatureSelector with cv parameter), and the exact error encountered (IndexError raised when passing an iterable of splits to cv). The difference between expected and actual behaviors is explicit and well-motivated by reference to the scikit-learn convention for similar parameters in other classes, providing a clear and standard baseline for what correctness means. An experienced developer can directly focus on resolving this discrepancy.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual code change required is quite small (a few lines in a single function), and the root cause is very focused: the code previously didn't process iterable/generator cross-validation splits and needed to consistently standardize the cv parameter using check_cv, as is typical in scikit-learn. While it may require some investigation to understand where cv is handled and to ensure proper integration, a developer familiar with the codebase would be able to identify and implement the necessary fix fairly quickly. Writing a straightforward regression test is also easy given the steps in the issue.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified. The issue is focused, coherent, likely to be valuable in a benchmark, and the problem/solution boundary is well-contained. The test changes directly address the scenario discussed in the issue, confirming suitability for evaluating coding ability related to handling scikit-learn CV conventions.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10323": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and clearly describes the problem and the expected behavior. The description details both the process to reproduce the problem and the expected result. Reproduction: A user tries to include a section from an XML file in their documentation using the literalinclude directive with the prepend option. When they do so, any text added by prepend loses its leading whitespace, causing misaligned code examples. An attempted workaround using dedent causes a warning and build failure. The expected behavior is unambiguously requested: the leading whitespace in code examples, including that introduced by prepend/append, should be preserved. The issue also specifically points out that dedent should only affect the included file, not prepend/append content. There is enough contextual detail about the test environment, Sphinx version, and attempted solutions. A developer familiar with the codebase would be able to understand what is required for a successful fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue likely requires 15 min to 1 hour to analyze and fix for an experienced engineer. The underlying mechanism that applies dedent and prepend/append needs to be reviewed and adjusted so that dedent does not affect prepend/append content. The problem is conceptually simple, though the exact spot in the code needs some investigation. Test case updates are non-trivial but not extensive. The actual code change in the provided patch is small and localized, indicating the issue does not require a major rewrite or deep architectural changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues observed. The issue has clear reproduction and expected outcome instructions and can be used to evaluate coding ability in a benchmark context. The steps and requirements are sufficiently clear and do not depend on information outside the issue text. The problem is not too trivial, nor is it highly esoteric or reliant on hard-to-discover context.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10673": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text is quite clear about the problem: users want to include genindex, modindex, and search in their toctree without warnings about nonexistent documents. The example specifies the expected reStructuredText directive to work and not produce errors, and references the exact page names at issue. The problem is well contextualized, the required outcome is explicit, and there is enough context for an engineer familiar with Sphinx to proceed directly (modifying toctree logic so special documents like genindex, modindex, and search are treated appropriately for inclusion in user toctrees). No filenames, function names, or codebase internals are mentioned, but the user story is detailed enough for a meaningful attempt at a solution without ambiguity.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding Sphinx's internal handling of toctrees and special pages (genindex, modindex, and search). The engineer will need to trace how these 'virtual' index pages are normally generated and excluded, and update the environment and toctree logic so they are recognized and can be linked from a user toctree without triggering warnings. This touches multiple components and is not a trivial patch; it requires careful modification to parsing and output, potential adaption of environment data structures, and proper updating of tests. These characteristics, and the number of files touched in the actual patch, suggest a difficulty level of 2 (1-4 hours for an experienced engineer).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The issue statement is sufficiently self-contained. The original patch includes relevant test additions, showing the problem is testable as stated in the issue and in a way compatible with a coding benchmark scenario. There are no privacy concerns, dependency on external data, or ambiguous requirements that would make this sample unsuitable. The only note is that understanding some Sphinx specifics is implied, but a benchmark-taker familiar with Sphinx should not require more context than what is provided.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7440": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well specified: it clearly states that building the documentation fails due to a duplicate glossary term 'mysql' with different cases, e.g., 'mysql' and 'MySQL' being treated as duplicates erroneously. The steps to reproduce are sufficient, and it describes the error that occurs (build fails). However, the expectation is only obliquely stated: the reporter writes \\\"MySQL != mysql term right ?\\\", which suggests that the system should treat these as distinct terms rather than as duplicates. There is some ambiguity, especially as to whether the system should be fully case-sensitive for all glossary terms and what the broader implications of that are, which prevents this from being a 0. However, there is a clear and sensible interpretation of what is required: the glossary should treat different-case terms as separate, and building the docs should not fail if 'mysql' and 'MySQL' are both present.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires understanding how Sphinx handles glossary term registration and case sensitivity. Once that is understood, the actual code change is relatively minor (removing or adjusting the lowercasing of the term during registration and xref configuration). The change itself is small, but it does require accurate identification of where case normalization occurs and a bit of care to ensure backwards compatibility. An experienced engineer should be able to analyze and implement the solution within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are apparent. The issue is suitable for inclusion: it is solvable based on the provided information, and the test patch demonstrates that an automated test can check for regression. The expectation and interaction are clear enough for benchmarking coding ability, particularly for engineers familiar with documentation tooling like Sphinx.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7462": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes the bug (an IndexError when handling an empty tuple type annotation in Sphinx), how to reproduce it (create a function with an empty tuple return type annotation, use Sphinx autodoc, run the build), the expected behavior (docs are built, type annotation renders correctly), and the environment in which the bug occurs. The provided example and expected behavior, along with the specification of the context (Sphinx autodoc), are sufficient for an engineer to understand both the bug and what a correct solution should achieve. Specific files or functions are not mentioned, but the details allow the engineer to find and fix the relevant code without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This bug is relatively contained: it involves handling an edge case (empty tuple annotation) in Sphinx's Python domain code. The fix is a minor adjustment to handle situations where the tuple is empty, rather than always assuming there is at least one element. It\u2019s a small change, focused on error handling and the rendering of type annotations. Locating the problematic code may require a little investigation, but does not require a structural rewrite or deep domain knowledge. The total effort\u2014understanding, finding the relevant location, coding, and testing\u2014would likely be under one hour for an experienced dev familiarized with Sphinx.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The issue is clear, concise, and self-contained without ambiguity. The sample is appropriate for use in evaluating coding ability, especially for testing handling of edge cases and defensive programming. It does not require extra context from issue discussions or external links; everything required is outlined in the main issue text.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7985": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well specified. The main point is clear: 'linkcheck' currently does not check local (internal) links in Sphinx documentation, and this would be useful. The procedure describes how to reproduce the oversight (by adding a local link and running linkcheck), and the desired result is that the linkchecker should also check these. However, the description leaves some implementation details ambiguous, such as exactly what qualifies as an \\\"internal\\\" or \\\"local\\\" link (relative paths, anchors, etc.) and the preferred reporting style. The examples given are sufficient to infer that a relative file like 'conf.py' should be checked for existence, but the boundaries of what to check may need to be guessed. That said, an experienced engineer could reasonably interpret and implement the required solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively small change to the linkcheck builder. The core task is to add logic to check the existence of local files referenced by internal links, which does require understanding how external vs. internal links are currently handled. It requires modifying some control flow and writing additional code to perform the existence check, but it does not require deep changes to the overall structure. The time to understand how and where to add this (and to write tests) puts it in the 15 min - 1 hour range for an experienced developer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers or missing pieces in this sample for evaluation. An engineer could be expected to make a reasonable implementation from the description. The only ambiguity is in the details of exactly which kinds of links should be checked, but the codebase conventions and the method for link collection would likely clarify this during implementation. Test cases provided in the PR explicitly cover expected behaviors.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8269": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well specified. It clearly states the current behavior of the 'linkcheck' command when linkcheck_anchors is enabled and describes the incorrect reporting of anchor not found even when the HTTP request itself fails (e.g., 404 or 500 error). The steps to reproduce are explicit (create a Sphinx project, add a specific link, run linkcheck, and observe the reporting). The expected behavior is also unambiguously described: error codes from the server should be reported as HTTP errors, matching how linkcheck behaves when anchor checking is disabled. No ambiguity remains in terms of what the successful fix should produce, and there are specific examples of both the actual and expected output.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue involves inspecting handling of HTTP responses in the 'linkcheck' builder and ensuring that HTTP error responses are correctly surfaced and reported, even when anchor checking is enabled. The core fix is adding a call to response.raise_for_status() so exceptions are immediately raised on HTTP errors, rather than only checking for anchors. An experienced engineer familiar with Sphinx's codebase could locate the relevant code path and add this check in under an hour, including validation with the appropriate tests. It's not a trivial change, but it's also not complex or requiring wide-ranging changes or extensive research.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues detected. The sample provides clear reproduction instructions, input, expected and actual output, and the required change fits cleanly into the existing workflow. This makes it well suited for use as a benchmark for coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, detailed, and includes step-by-step reproduction instructions. It defines precisely what the current buggy behavior is (ambiguous cross-reference warnings when building docs with Sphinx for :type: and :rtype: fields), gives a concrete example scenario (two classes with the same name in different modules, resolving names in different module contexts), and clearly lays out the desired expected behavior (that unqualified names in the context of a module should resolve locally, with no ambiguity/warnings). An experienced engineer would know exactly what to look for and what the correct behavior needs to be. No critical information is missing, and successful resolution is well-defined.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Addressing this bug requires understanding of how Sphinx processes implicit cross-references in Python docfields, analysis of how module context is handled when resolving names, and making changes likely in core parts of the Sphinx build process (including patching the code for cross-reference construction). It requires writing and running tests, verifying resolution in several contexts, and may need careful adjustment to avoid breaking existing behavior. The problem crosses at least two main files, involves correctly updating references, and ensuring compatibility with other parts of Sphinx. These factors make it a moderate challenge, likely requiring over an hour to understand, fix, and validate but unlikely to occupy a full day. Thus, 1-4 hours is appropriate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The scenario is self-contained, the behavior is reproducible within tests, and the expected outcome is unambiguous. The example, while domain-specific, is clear for engineers familiar with Sphinx or related Python doc generation tools.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-9711": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is clearly described, including details on what is wrong (string comparison of extension versions), how to reproduce the bug, what is expected (correct semantic version comparison, e.g., 0.10 > 0.6), and a concrete use case with specific package names and versions. The expected behavior is unambiguous: version requirements for Sphinx extensions should be checked using proper version number semantics, not raw string comparison. The engineer is given all necessary details to understand what is required for a successful solution and what constitutes correct behavior.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires identifying where version comparison is performed and replacing string comparison with proper semantic version comparison. This can be addressed by using a library like 'packaging.version.Version'. While the change requires understanding of where in the codebase this operation happens and familiarity with version comparison best practices, the alteration itself is not complex and affects just a small piece of logic. A thorough engineer with some experience in package management and Python should be able to implement and test a fix in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no blockers or major omissions in the sample. The reproduction steps are helpful, but even if not running the code, an engineer experienced with version comparison issues will fully understand the core problem and required fix. The issue is self-contained and suitable as a benchmark task.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13372": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete bug: a UnboundLocalError is raised when calling numerical evaluation (.evalf) on an unevaluated multiplication with Max(0, y) as the leading argument. It clearly points to an initialization problem of an internal variable based on the order of multiplication arguments, and suggests that the elif clauses that define 'reprec' and 'imprec' should have an 'else: raise NotImplementedError'. However, the report does not specify exactly which files or functions this relates to, and does not provide specific code locations. Still, the behavior and the cause are described well enough that an experienced engineer familiar with symbolic computation libraries and this codebase could make a sensible attempt to reproduce, investigate, and fix the problem. There is some expectation that the engineer fills in small gaps and explores the relevant code paths themselves.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is localized (a specific UnboundLocalError due to uninitialized variables in .evalf) and the solution amounts to adding an explicit else branch. This is a straightforward code change (less than ten lines), but it requires some inspection of code logic and a small bit of investigation to confirm the correct location for the fix. Given familiarity with the codebase, it should be solvable in 15-60 minutes by an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is focused, relatively self-contained, and has a clear test case. No other major issues exist that would prevent this from being a useful coding benchmark sample. The context and hints about the solution strategy (add an else: branch raising NotImplementedError) may make the task slightly less open-ended, but such hints are common in real code review tasks. Overall, this is a reasonable test of debugging and small-scope reasoning.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13480": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It identifies a precise bug: when substituting integer values into coth(log(tan(x))) in SymPy, a NameError is raised due to a reference to an undefined internal name ('cotm', which should be 'cothm'). The expected outcome is clearly stated as receiving a numerical value after substitution, not an exception. Steps to reproduce are provided, and the scope is clear: fix the erroneous reference so numerical evaluation works. For an engineer, this is sufficient to locate and resolve the bug within the appropriate SymPy module (likely hyperbolic functions or coth implementation). No ambiguities remain around the required outcome or the nature of the error.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change that requires a bit of thought. While identifying the source of the undefined name requires familiarity with the codebase, the core problem is a typo/reference error. Locating the bug and applying the fix should take less than an hour for an engineer familiar with the relevant SymPy internals. The patch is a single-line change, which supports this difficulty assessment.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The problem is well-described, has clear reproduction steps, and is easily validated by testing. The sample is suitable to evaluate coding/debugging ability in this domain.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13877": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is quite well-specified. The exact steps to reproduce are given: constructing a specific n x n symbolic matrix and computing its determinant for successive values of n, showing that for small n the answer is meaningful but at n=5 and above a NaN or error emerges. The expected behavior is explicitly stated: that the determinant should always yield a meaningful symbolic result rather than NaN. The report also hints at the likely cause (Bareiss algorithm is only valid for integer matrices). Therefore, it is clear that the code handling symbolic determinants should be fixed so that it works for this case and does not produce NaNs or raise errors. The bug is reproducible, and the output to expect is described. A code-savvy engineer would know what to fix or guard for in the determinant computation to fix this.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires substantive debugging and likely some research. Understanding why a symbolic matrix determinant computation fails for certain symbolic entries, investigating the Bareiss algorithm, and understanding (or amending) pivot handling for zero-checks in the symbolic case is non-trivial. While the fix is not huge in code volume, it involves context-specific symbolic mathematics expertise, test writing, and careful logic. This is not a trivial or 'simple logic' fix, and may take between 1 and 4 hours for an engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be any major issues that would make this sample unsuitable for the benchmark. The problem is self-contained, has a clear entry and exit (matrix determinant with symbolic entries should not return NaN), and the fix is not trivial but achievable. The PR and related test focus specifically on this aspect without needing broader context. The only note is that familiarity with matrix algorithms and symbolic math libraries is assumed, but this is appropriate for the domain.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-16792": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is highly detailed and well-specified. It clearly states when and how the problem appears (wrapping expressions with unused array arguments in the cython backend), gives minimal and expanded code examples, provides the specific observed error, and diagnoses the cause with examination of the generated code signature. It additionally explains the practical use case and specifies the expected correct behavior: the generated function should accept the unused array argument as an array (double pointer), even when not appearing in the expression, rather than as a scalar. There is enough direct evidence and explanation for an experienced engineer to work out the intended resolution solely from the main issue text. No ambiguity about requirements or expected results remains.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small but meaningful change in the code generation logic to ensure array arguments are handled correctly even when unused. The bug is localized: the failure is due to not properly setting the 'dimensions' metadata for unused array arguments. The code change involves understanding the argument generation logic and ensuring that the dimensions are attached regardless of usage in the expression. With detailed reproduction and diagnosis provided, an experienced engineer familiar with codegen or C/Python interop can identify the fix and implement it in under an hour. The patch itself is concise, modifying a helper and a code path. Testing involves replicating the minimal example, which is straightforward. Thus, this problem is moderately simple; it requires investigation, but not a large-scale rewrite or significant architectural changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue, while specific to code generation with Cython and unused array arguments, is self-contained and testable. The use-case for unused arguments is justified, and the expected output is unambiguous. The example, diagnosis, and solution context make it a strong test of coding/debugging ability, with an objective pass/fail criterion from testable output. No hidden dependencies or unclear requirements are present. Using this for benchmarking coding ability is appropriate.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17139": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete way to reproduce the bug (using SymPy, create a symbolic x, try to simplify cos(x)**I), describes the expected behavior (a simplified form of the expression), and specifies the actual error (TypeError due to invalid comparison of complex with zero). However, there are some blanks: it does not specify if a particular form of simplification is desired, only that the current behavior is invalid. Nevertheless, it is reasonably well-specified for an engineer familiar with symbolic computation and SymPy; the main goal is clear\u2014to handle the case where the exponent is complex and avoid invalid comparisons. Required changes should prevent the TypeError. Some minor interpretation is needed, but overall it is suitable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a bug fix involving a specific case in the simplification logic. The solution is to add a type-check or property-check to avoid comparing a complex exponent. Once the problem is located, the code change is small (a couple of guard lines), but some familiarity with the code structure is needed to identify where this check should go. The work required to identify the failing code path and implement the fix is manageable in less than an hour for someone familiar with the codebase. The fix doesn't require rewriting major sections or deep research.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no significant reasons this sample should not be included. The test cases are clear and focus on the identified issue, and the fix is straightforward. The issue is useful for benchmarking basic debugging and code-reading skills related to symbolic math software.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17318": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite well-specified. It describes a concrete function (sqrtdenest) and a very specific input expression that causes an IndexError. The expected behaviors are clearly spelled out: if the function can't denest the input, it should simply return it unchanged, rather than erroring out. The underlying failure mode (IndexError from trying to access a tuple out of range) is called out explicitly, and the specific input that causes the error is given both in natural language and as explicit code. The expected output after the fix is also shown via REPL output. This makes it very straightforward for an engineer to reproduce, localize, and understand both the bug and the desired change. No additional clarification would be needed as to what needs to be fixed, and the required change is clearly limited to suppressing the error and producing the correct fallback value.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires identifying why an IndexError occurs in sqrtdenest, making the internal logic robust to this case (typically by adding preventive checks), and ensuring that the function returns the unchanged expression when the input cannot be handled. The patch shows that the fix was made by augmenting the checks for valid surd inputs. This is not a trivial typo or assertion addition, but it is also not a major rewrite; it should be solvable in under an hour by an experienced engineer after understanding the error path and the necessary check. The affected code is localized and the addition of extra tests is straightforward.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues remain. The error scenario and desired behavior are clearly described, and the solution scope is well-contained. This sample is suitable for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17630": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear explanation of the bug, including how to reproduce it: multiplying a BlockMatrix that contains ZeroMatrix blocks works as expected for one multiplication but fails after repeated multiplications. The source of the error is also described (ZeroMatrix blocks turning into scalar zeros, causing an AttributeError). The user provides relevant observations from their debugging session and gives information about the affected SymPy version and Python version. While the precise root cause in the codebase is not specified, an experienced engineer would have enough information to meaningfully attempt a fix by ensuring that zero blocks remain ZeroMatrix objects. There are minor blanks to fill, such as exactly where and how the conversion occurs, but this is standard for a bug report of this complexity, so I rate this as a '1'.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue itself describes a reasonably focused bug that likely only occurs in a particular edge case within the matrix multiplication logic of SymPy's BlockMatrix class. The patch confirms this: just a few lines of change in one file (matexpr.py), all regarding how matrix addition logic is handled post-multiplication. Tests are added to confirm the expected matrix block behavior. The nature of the bug does require understanding how block matrix multiplication and zero-fill behavior are implemented, but it does not appear to require major rewrites or deep research\u2014just careful process tracing and a targeted fix. Thus, this is a '15 min - 1 hour' fix for an experienced developer, possibly trending toward the longer side due to the need to understand codebase nuances.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional major concerns. The test coverage reflects precisely the issue described, and the issue text, while technical, gives enough reproduction steps for an experienced engineer. The patch is focused, and the problem is localized. This is a typical example of a moderately challenging but well-bounded bug-fix issue.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17655": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. The user describes a specific operation with SymPy's geometry Point objects: multiplying a point by a scalar (on the left and right), and then adding the result to another point. The problem is detailed: multiplying a point by a scalar (point * scalar) works as expected, but scalar * point raises an error. The user provides a concrete example (points at (0,0) and (1,1), scalar 2.0), and the expectation is that both forms of multiplication should work and yield the same result. This makes the requirements very clear: implementing or fixing the __rmul__ method for Point so that scalar multiplication is commutative. There are no ambiguities or missing details relevant to solving the problem for an engineer familiar with Python operator overloading and the SymPy codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small and targeted change in behavior: implementing or correcting the __rmul__ method on the Point class so that left multiplication by a scalar yields the same result as right multiplication. For an engineer familiar with Python's dunder (magic) methods and the structure of the SymPy Point class, this should be a straightforward task. Locating the class and adding or modifying the method requires minimal code changes and little to no research. The main complexity is ensuring that the fix does not introduce side effects and is consistent with other usage of Point and number multiplications.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The expected behavior, underlying cause, and desired outcome are all clearly outlined. The issue tests for both 2D and 3D points, providing explicit test cases, and there are no ambiguities or external dependencies. This is a good benchmark issue for evaluating core Python/operator overloading and understanding of unit testing.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-18211": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the current problem: the solveset function is raising a NotImplementedError when it cannot solve an equation, but should return a ConditionSet instead. The expected output is shown, and there's an explicit example with n*cos(n) - 3*sin(n) = 0. The task requires modifying the code so that, when a NotImplementedError is raised (because the solver can't solve the equation), a ConditionSet is returned instead, describing the set of real solutions. This provides enough detail for an experienced software engineer to implement a solution without needing clarification. Relevant object/class names (solveset, ConditionSet), and context (SymPy, equations, sets of real solutions) are sufficiently clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires intercepting the NotImplementedError in solveset (or related code) and returning a ConditionSet instead. The patch confirms that this was a small change involving adding a try-except block to capture the error and returning ConditionSet as appropriate. There is no substantial rewriting or research required, just a minor but non-trivial modification to error handling for a clear, specific case. Implementing and testing the change should take 15 minutes to 1 hour for an experienced engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being used to evaluate coding ability. The issue is clear, bounded, and the expected change is meaningful from both code and API perspectives. The presence of test cases in the patch helps confirm correct resolution.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20428": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it describes the observed wrong behavior (the polynomial clears denominators but still holds invalid internal state), the expected correct behavior (representation should be truly zero both in print and internal checks), and a concrete example (through code and object representations) for reproduction. It explains the consequence of not fixing (downstream methods fail or error), and hints about the internal data structure involved (DMP representation). However, there are minor blanks\u2014such as the precise definition of 'internal zero', and the full extent of special cases for EX domain coefficients\u2014but these do not prevent a sensible attempt at fixing the core bug. The engineer is told what function is wrong, what outputs to look for, and provides enough observable differences to guide correction.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding polynomial representation internals and SymPy's Poly/EX domain implementation, but the root cause is clearly described (lingering zero in DMP representation), and the solution is a relatively localized logic change. The fix is small (override a boolean check method), and the tests required are straightforward. With familiarity with the codebase, an experienced engineer should need less than an hour to solve and properly test the issue.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The bug is concrete, reproducible, and relevant to the functionality under test. The only slight caveat is that some domain-specific knowledge (about polynomial representations and EX domains in SymPy) is assumed, but that is fair in context for benchmarking advanced coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20438": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well specified: it describes a series of concrete operations involving FiniteSets and ProductSets, and points out exactly where the API's behavior deviates from expected results (e.g., is_subset returns None instead of boolean, and simplify crashes due to missing 'equals' method). Clear steps to reproduce are provided, as well as what an expected result would be (e.g., b should be a subset of c so is_subset should be True). However, some details about exactly what the 'expected output' should be in all edge cases, or how general the fix should be, are not spelled out; the expected behavioral contract of these set operations must be inferred by the engineer. Still, a sensible technical interpretation is possible. No filenames or explicit function names are given, but the objects and operations involved (e.g., FiniteSet, ProductSet, is_subset, simplify) are clear from the context.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding multiple internal set types (FiniteSet, ProductSet), their interaction (including with general Set objects), and the logic for set comparison/subset behavior. The solution touches multiple files and involves working with SymPy's multiple-dispatch mechanisms as well as extending or fixing class methods appropriately. Although the patch is not excessively large, the changes are non-trivial and not just a matter of adding or fixing a line or two. A new handler for ProductSet inside is_subset_sets is added, and special handling is done for simplify/equality in Relational handling. It would likely take a few hours for an engineer unfamiliar with the codebase to figure these requirements out, thoroughly test, and implement them.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is quite mathematical and assumes familiarity with SymPy's set operations, but there is nothing invalid about using this issue for benchmark purposes. The only slight challenge is that an engineer not familiar with mathematical set theory or symbolic computation libraries might find it difficult, but the problem itself and the requirements are otherwise suitable. No further problems with ambiguity or suitability identified.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20590": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. The user describes a regression in the behavior of the Symbol class between version 1.6.2 and 1.7 about the presence of the __dict__ attribute, which suggests an unintended change due to missing __slots__ in a parent class. The intent is clear: restore the previous behavior so that Symbol objects do not have a __dict__ (in line with __slots__). However, the exact class hierarchy and filenames are not mentioned \u2014 the standard solution would be to ensure __slots__=() is set in the relevant parent/mixin class. This leaves some room for interpretation, but is fairly direct for someone familiar with Python\u2019s __slots__ mechanism.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is straightforward for an engineer familiar with Python\u2019s class internals and __slots__. It requires identifying the relevant class(es) (likely Printable or a parent of Symbol), adding __slots__=() if missing, and adding tests. While the root cause analysis requires a bit of thought, the actual code and test changes are minor\u2014thus, it should take between 15 minutes and 1 hour to solve.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent: the problem is well-contained, and both the desired regression and direction for the fix are described. The only minor ambiguity is in the specific class, but any reasonable solution meeting the description would be acceptable. The patch confirms the expectation of the solution.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-21379": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well specified. The reporter provides clear information on how to reproduce the bug: which symbols to define, the form of the expression (using exp, sinh, Piecewise, division by z), what substitutions are being performed (1 -> 1.0), and at what point the error occurs. The interaction with Sympy's cache and the difference between using real vs non-real symbols are described. The expected and unexpected outcomes are stated, and contextual reasoning is given. However, the root cause is not clear from the issue text itself: the description hypothesizes about ordering of assumptions but does not definitively pin down the cause. Nonetheless, for an engineer familiar with SymPy, there is enough detail to interpret the issue and work toward a fix, although some blanks (e.g., exact location of buggy code, some specifics about assumptions propagation) must be filled in via investigation. Thus, it's a 1: some blanks but a sensible, concrete bug report, and clear artifacts to check (no PolynomialError in this workflow).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required is localized and essentially involves catching an exception in the gcd computation for modular expressions inside Sympy, as evidenced by the gold patch. The main intellectual effort relates to reproducing the issue, following the description and test, recognizing the root cause, and implementing a typical try/except pattern to handle PolynomialError. For an experienced engineer familiar with Sympy, this is neither trivial nor highly involved\u2014it's a small fix requiring a bit of investigation but is unlikely to take more than an hour once the reproduction is confirmed.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is appropriate for evaluating debugging and narrow bug-fixing skills. The only potential concern is that reproducing the error may require understanding of Sympy's caching and symbol declaration nuances, but this is clearly detailed in the issue. There are no external blockers or ambiguities severe enough to disqualify the use of this issue in the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-22714": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It provides a clear description of the bug (unexpected error when constructing Point2D with evaluation disabled), a minimal reproducible example, and explicitly explains both the expected and actual behavior. The commands to reproduce the problem and see the correct behavior are directly included. This is sufficient for an experienced engineer to both reproduce the failure and understand what behavior is required: Point2D should work with integer coordinates regardless of the evaluation context as long as the input contains no imaginary numbers. No additional clarification about the impacted code, function, or context is required to attempt a meaningful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a straightforward logic bug fix, most likely involving a small change in how inputs are validated in the Point2D constructor or related method. The issue text gives an exact reproduction, and the expected fix is local to a clear area. For an engineer familiar with SymPy, this would most likely take under 1 hour including test writing, investigation, and fix, so fits well into the 15 min - 1 hour bucket.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The setup is sound: the issue text is self-contained and clear about desired behavior, the bug and correct state are easy to verify, and the tests are amenable to being used as a correctness check. Sample is suitable for coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23824": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly explains the observed bug in kahane_simplify(): the order of leading, uncontracted gamma matrices is reversed when there is a contracted pair at the end of the product. Concrete examples of both correct and incorrect behavior are provided (\u03b3^\u03bc \u03b3_\u03bc \u03b3^\u03c1 \u03b3^\u03c3 yields correct output, \u03b3^\u03c1 \u03b3^\u03c3 \u03b3^\u03bc \u03b3_\u03bc yields reversed output), including the mathematical expectation and the observed vs. correct outputs. The explanation goes further by describing the likely cause of the bug (\u201cthe insertion loop is just backward\u201d) and where in the function the problematic manipulation occurs. The purpose, expected outcome, and required behavior are all clear, making this a well-specified issue that an experienced engineer could confidently address.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is well-localized (\u201cleading matrices are removed at the beginning of the function and then inserted at the start ... insertion loop is backward\u201d) and the mathematical expectations are precise. An engineer familiar with the codebase could use the clear reproduction instructions and examples to verify the problem and test the fix. The actual code fix is small (as observed in the patch: changing an insertion loop to a direct concatenation), but does require careful understanding of how the indices are handled in the function. Therefore, it\u2019s not purely trivial, but is a 15-60 minute task for someone with some domain familiarity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues present. The issue is tightly scoped, the domain knowledge required is manageable for a developer familiar with symbolic or physics-related code, and the description supplies all needed details. The minimal patch size corresponds well to the issue description. The test modifications further clarify expected behavior, removing ambiguity.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23950": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it describes a concrete buggy behavior (Contains.as_set returns itself instead of an actual set), gives a clear expected outcome (should return a set object, like the second argument to Contains), explains why the current behavior is wrong, and ties it to a failure mode (AttributeError due to lack of as_relational). However, while the main requirement is clear, some implementation details (e.g., what to do if the second argument to Contains is not itself a set) are not fully detailed. Still, any reasonable, experienced contributor could make sensible progress and interpret the requirement to mean: 'as_set' should return the set being tested against. There\u2019s a small blank to fill in but the intent is highly actionable.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This fix requires changing the as_set method to return the set argument (the second argument) of the Contains object. This is a trivial, single-line change in the code, and updating the corresponding test to assert the expected behavior. An experienced engineer, after familiarizing themselves with the relevant code, would likely implement and test this in under 15 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no blockers in using this issue as a benchmark for coding ability. The problem is clear and the implementation requires both an understanding of the bug and a simple code change. No additional context is required for the core logic of the task.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24066": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a concrete reproduction scenario involving SI._collect_factor_and_dimension(), specific units, and the failure mode (exception when dealing with exponentials whose exponent should be dimensionless). It explains the difference in current behavior versus expected behavior and references explicit constructs (like an expression for time/(ohm*farad), adding exp of that to 100, etc). While some familiarity with the codebase, physics units, and how dimension analysis is handled internally is needed, an experienced engineer with knowledge of SymPy's units system should be able to understand and address the problem as described. There is only a minor blank to fill: the exact nature of how the code currently checks dimensions, but the problem is sufficiently constrained by the domain and usage scenario described.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch for the fix is small in code footprint\u2014a relatively minimal logical modification in the handling of function arguments' dimensions in _collect_factor_and_dimension(). The problem, while nontrivial and requiring familiarity with concepts of dimensional analysis and SymPy codebase, should not take more than an hour for an experienced engineer to analyze and resolve. The required change is a targeted adjustment to handle dimensionless exponents correctly, not a full rewrite or broad architectural change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is a good candidate for a coding benchmark, as it requires attention to detail and reasonable understanding of both the scientific domain and code architecture. The test change further clarifies the exact scenario to be fixed, making this suitable for evaluation. The scenario is also self-contained and doesn't require ambiguous interpretation or access to external discussion.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24443": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: It describes a clear bug in the _check_homomorphism and _image functions related specifically to PermutationGroups in SymPy. It provides repro steps: constructing the dihedral group of order 6, mapping generators to themselves, and getting a ValueError unexpectedly. It indicates that the issue is in the handling of generator inverses. The explanation is detailed enough that an experienced engineer familiar with group theory and the codebase could know what to check and how to proceed, though it leaves some implementation interpretation (e.g., exactly how to simplify the logic) to the solver.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires a good understanding of group theory, SymPy's combinatorics module, and careful tracing of group generator/inverse handling. The fix necessitates significant modification of logic, not just a one-line change, and careful handling to ensure correctness across group types, impacting a core function for homomorphisms. Verifying correctness involves building or modifying nontrivial test cases. The time to understand, design, and implement this is likely 1-4 hours for an engineer new to the code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major blockers\u2014issue is suitable for benchmark use. It involves a tricky but well-localized algorithm/data structure bug. The only subtlety is the requirement for background in abstract algebra, which may bias the sample toward those with mathematical training, but the repro steps and specifics mitigate this concern somewhat.\",\"q2_5_confidence\":5}"
    }
]