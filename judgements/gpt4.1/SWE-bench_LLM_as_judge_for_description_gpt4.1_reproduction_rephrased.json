[
    {
        "astropy__astropy-13033": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It describes exactly what the current exception is, how it is misleading, what the expected message should communicate, and provides a clear set of steps to reproduce. The code context is summarized correctly around handling required columns for a TimeSeries, and the needed fix is to improve the error messaging when required columns are removed. The information given is sufficient for an engineer to know what is wrong and what change is desired in the code.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is rated as 1 (15 min - 1 hour). The engineer needs to modify the logic for error message formatting regarding required columns in the TimeSeries class, ensuring that both single and multiple required columns are communicated clearly when missing. While some thought is needed to generalize the error message, it is a small and localized change that does not require substantial refactoring, affecting mainly one method and updating the corresponding test cases. The code changes are concise, but care is required to properly format the error message and handle different data types in the error context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The sample is clear, well-scoped, and appropriate for evaluating an engineer's ability to interpret an error report, generalize error handling, and update both code and tests accordingly. The context and required code changes are clear from the issue description alone.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-13977": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified: it describes a concrete behavior in astropy.units.Quantity.__array_ufunc__ when interacting with duck types such as DuckArray, specifically when units are different and the operation triggers an error instead of returning NotImplemented. The description gives a motivating example and clear reasoning, and refers to NumPy's documented expectations. It also illustrates the undesired error and describes the relationship to duck typing mechanics and operator overloading in Python. However, the 'what should happen' is stated (return NotImplemented to let __radd__ be called), but there is some ambiguity as to edge cases, exact expectations under all possible input types, or full integration in astropy (e.g., tests show custom duck types that may or may not be expected to fully mirror Quantity). Some blanks need filling, but there is a clear, sensible interpretation possible.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires understanding intricate parts of Python's binary operator mechanics, NumPy's __array_ufunc__ protocol, and astropy's type system/inheritance, plus careful editing of exception handling to return NotImplemented instead of raising errors (but only in the correct places). It also requires good judgment about when to raise and when to return NotImplemented to not break existing functionality. Testing must cover subtle cases (as the test patch shows). This is not a trivial bugfix and likely involves at least one function rewrite plus test addition/modification across multiple files. Familiarization and coding would likely take 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues stand out. The issue is technical but scoped, the requirements are testable, and the context is reconstructable without need for discussion. The proposed sample is suitable for benchmarking experienced developers on operator overloading and Python/NumPy integration issues.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14096": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, specific, and reproducible. It gives all necessary context: a subclass of SkyCoord is defined with a property that accesses a non-existent attribute, and the problem is explicitly stated\u2014the AttributeError message misleadingly refers to the property rather than the missing attribute. The error traceback is included, making it easy to understand what is happening and what should be improved. The expected outcome (the error message should refer to the real missing attribute, not the property) is unambiguous. An engineer with access to the codebase could work directly from this information to identify and resolve the problem.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding Python's data model, specifically how __getattr__ and properties interact in attribute access. The fix involves a small but carefully considered change in how missing attributes are handled in SkyCoord. An experienced engineer should be able to locate the relevant method (__getattr__), grasp the issue based on the report, and implement the fix in a relatively short period of time (15 minutes to an hour). The solution involves swapping a custom error raise to a call to __getattribute__, leveraging standard Python behavior. No extensive code rewriting is required, but some knowledge of Python internals is necessary.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is well-suited for the benchmark. The issue and expected behavior are clearly described, the fix is non-trivial but focused, and the scenario is realistic for software engineering tasks. No other major issues are present that would disqualify the use of this sample.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14182": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified. The user describes the exact input table, the function being used (`QTable.write` with format='ascii.rst'), the working analogous functionality in 'ascii.fixed_width' mode, and clearly states the desired behavior: support for multi-row headers (via `header_rows`) when writing in 'ascii.rst' format, just as is already supported in another output format. The issue includes a detailed error traceback showing that the argument is not currently supported, making it clear support should be implemented. While it does not specify the full expected output or implementation details (e.g., how multiple header rows should be formatted specifically for RST), there is a clear and reasonable analogy with the other format's behavior that can be used to guide development. Thus, there is a little ambiguity about the presentation in RST but a sensible, consistent interpretation is obvious, so I rate this a 1.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This feature requires understanding the table writing API and RST file format, updating the RST output logic to handle an arbitrary number of header rows, and integrating this with the existing serialization logic. The engineer needs to ensure both writing and reading with custom headers work as for other formats, including updating documentation and tests. Multiple files and classes must be edited (see the gold patch), and off-by-one errors or test failures are possible in header rendering. This is a moderate task\u2014more than a trivial patch, but not a multi-day effort, so I rate it as taking 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is relevant, concrete, code-oriented, and enables meaningful evaluation of engineering ability. The missing details in the issue are minor and sensibly filled in by analogy with existing fixed-width support. It has associated tests that make automated checking feasible. The table and header structure is clear from the context provided. No external factors (e.g., data access, external APIs) would prevent this from being a suitable benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14309": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue includes a precise and straightforward reproduction scenario using a single function call to 'identify_format', specifying all arguments needed and giving the exact input that triggers the bug. The resulting exception is shown in full, including the stack trace and the precise error message (IndexError: tuple index out of range). There is also a clear description of the context (HENDRICS cron tests failing with devdeps), the relevant code location (identify_format from astropy.io.registry), and remarks about a recent change in behavior after a particular commit. The problem is easily traceable to a specific function call with provided parameters, so an engineer has all necessary detail.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is clearly localized and boils down to fixing robust argument handling in a specific function. The underlying issue is a missing argument check before accessing args[0], and the patch reflects a minimal, localized change to the relevant function. While it requires some understanding of the code structure and intent, the fix itself is straightforward and does not require restructuring or deep investigation. Most experienced engineers would be able to solve this in less than an hour after reading the issue.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue and test patch together provide a clear reproduction and verification pathway (a regression test is added). There are no hidden requirements or ambiguities. The sample is a good candidate for evaluating coding ability in a targeted way.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14365": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides a clear problem statement: the ASCII QDP table reader in astropy assumes QDP commands are upper case, whereas the QDP format itself is case-insensitive. It clearly states what goes wrong (case sensitivity causes a failure), what is expected to happen for a specific example file (\\\"read serr 1 2\\\" should be accepted as a command), and provides explicit reproduction and desired behavior steps. The problem is local to QDP command parsing in astropy.io.ascii.qdp. All necessary context to attempt a solution is provided. Filename and code context (Table.read with format='ascii.qdp') is specified, along with example input and expected result.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial fix: recognizing QDP commands case insensitively, likely requiring changes to how regular expressions are compiled or how command strings are compared. The bulk of the work involves identifying where the code is enforcing case sensitivity and updating it, as well as understanding the command parsing logic. Familiarizing with command parsing in ASCII QDP will take a bit of time, but the implementation is likely a straightforward refactor or the addition of a regex flag. The potential for subtle bugs exists, but overall it's a targeted code change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major obstacles are apparent. The issue is clearly described, reproducible, and the requirements are well-bounded. The main challenge is correctly identifying all sources of case sensitivity in the QDP parser. Provided tests and reproduction steps help verify a successful solution. Acceptable for coding evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14508": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a specific problem with the string representation of floats in io.fits.Card, clearly explaining the impact (truncation of comments, inability to create certain valid FITS cards). It gives a concrete, reproducible example including inputs, current behavior, and expected behavior. The description is focused on the formatting/representation of floating point values and explicitly mentions which class and even method is at fault (io.fits.Card, the float string representation part). Overall, it is sufficiently detailed to allow an experienced engineer to attempt a resolution without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is well-localized (float formatting in io.fits.Card) and mostly requires adjusting how floats are converted to strings to respect constraints. The example in the issue provides enough detail to guide the implementation. While the engineer will need to review relevant FITS formatting rules and test that the string fits the necessary card length, it is a targeted change in a single function/file and should not require broad codebase changes. Estimated time is 15 min to 1 hour for someone familiarizing with astropy's FITS handling.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain\u2014sample is clear, relevant, and adheres to the assumptions of the benchmark setup. The issue is fully reproducible with supplied details. The primary challenge is the logic of formatting floats to balance precision and reserved card space, but the requirements are clear.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14995": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is quite well specified. It details the exact context (arithmetic with NDDataRef objects in astropy v5.3, mask propagation with handle_mask=np.bitwise_or), describes the expected and erroneous behaviors, and gives an explicit reproduction path (with pseudo-code for creating masked/unmasked NDDataRef objects and performing multiplication). While the issue author admits not knowing every detail internally, the failure is described (TypeError due to None and int in bitwise_or), and the expected outcome is clear: when one operand is unmasked, just propagate the mask from the masked operand, replicating v5.2 behavior. There is slight room for interpretation in the exact copying logic (\\\"Or whatever was done in v5.2\\\"), but it\u2019s not vague. A knowledgeable engineer could make a well-reasoned fix equal to the one in the gold patch.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change essentially involves adjusting conditional logic for how masks are propagated in arithmetic operations for NDDataRef. The patch is small (a few lines of code) and limited to a single private method, with a corresponding direct test suite addition. Understanding the code path involves a bit of reasoning about mask logic, but once the relevant function is located, the fix is straightforward. It is not trivial, but it is a focused and self-contained bugfix, likely solvable in under an hour for someone familiar with astropy/nddata internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further major issues are detected. The reproduction steps are clear and the change is self-contained. The scenario is realistic and appropriate for a benchmark evaluating code reasoning and bug fixing.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7336": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and includes a summary of the problem, how to reproduce it, the observed error message, details about the environment, a workaround, and even a suggestion for the fix. The affected decorator, expected function signature, and context are all given. For an engineer familiar with the codebase, there is minimal ambiguity: the bug is that @quantity_input decorator does not handle constructors (or functions) with a return type annotation of None, leading to an AttributeError. The fix is evidently to check for the None annotation and skip further processing, which is within the issue scope. The requirements for a successful solution are clearly communicated, and there is enough information to write meaningful tests as well.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires a small code change: adding an additional condition to handle cases where the return annotation is None. This is a targeted fix that doesn't require extensive code rewrites, only a focused edit to the decorator logic and relevant tests to cover this case. The difficulty is slightly above trivial because it requires understanding Python signatures and handling special cases, but would not take substantial amounts of time for a proficient engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers for using this sample in a coding benchmark. The issue is fully self-contained and provides both the context and expected behavior. All necessary information is in the issue and can be solved by someone who only has access to this text and the codebase. No hidden assumptions or missing information affect its usefulness as a benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7606": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly describes the observed problematic behavior (TypeError raised when comparing a unit with None), how to reproduce it (creation of a unit x with a specific API call and silent parse mode, then running 'x == None'), what should happen instead (the result should be False), and even provides the exact stack trace seen during the error. The involved class is likely related to 'Unit' in astropy.units.core, and the issue is with the __eq__ method's handling of None as an argument. With this information, an engineer can identify what needs to be changed and how success can be judged (no TypeError and correct comparison result).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a targeted fix to the equality method (__eq__) of a class, ensuring it properly handles None (and possibly other inappropriate types) without raising unexpected exceptions. The patch indicates the fix is only a few lines, mainly improving error handling and making use of NotImplemented to let Python handle cross-type equality. Writing and running relevant tests is also straightforward once the cause is identified. Thus, the change is relatively simple but requires careful understanding of Python's comparison protocol and custom class behavior.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The issue is tightly scoped and technically precise, with a clearly defined desired behavior and a reproducible minimal example. It is a solid candidate for inclusion in a benchmark evaluating coding ability related to Python class methods and data model protocols.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7671": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and detailed. It clearly points out that after a particular PR, calling minversion('numpy', '1.14dev') raises a TypeError due to a known bug in LooseVersion, specifically when comparing versions with 'dev' or similar non-integer parts. The issue gives both a concrete example (with traceback), explains which call is problematic, what component is failing (LooseVersion), refers to how comparison works with different version strings, and even suggests that pkg_resources.parse_version would work as intended. All these details provide enough context for an engineer to understand the nature of the bug, what kind of input triggers it, the type of fix likely required (handling version strings like 'dev'), and how success can be checked (no exception and correct comparison result).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires a bit of thought, as the engineer must understand the problem with LooseVersion and why certain version string formats cause a TypeError. The engineer needs to identify how to preprocess the version string or otherwise change the comparison logic to avoid the error. However, it is limited to a small edit in an existing function and does not require extensive changes across the codebase. Additionally, the example in the issue description as well as knowledge of PEP 440 and the workings of LooseVersion make the change focused and implementable in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers or issues with this sample. The problem is reproducible as described, and the example tracebacks and context make the requirements for a successful solution clear. The test file changes corroborate that an actionable, automated check is possible. This issue is suitable for evaluating coding ability under the specified setup.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-10554": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a reasonably detailed account of the bug encountered when unioning querysets with ordering in Django. It describes the specific queryset operations performed, the expected and actual outcomes, and provides both actual and expected query results as well as the raised error (including the stack trace). The explanation references model and method names (Dimension, values_list, order_by), which should be sufficient for an engineer experienced with Django ORM to reproduce and understand the problem. However, it does not spell out the exact desired behavior in all edge cases, and some details (such as minimal reproduction steps, the specific model definitions, or desired fix approach) would require interpretation. Still, there is a sensible interpretation that can be made from the context and information given, hence the rating of 1.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"While the core of the fix (allowing ORDER BY to work correctly on unioned querysets, even with derived querysets and ordering cleared) is somewhat localized, it requires a solid understanding of the internals of Django's queryset compilation, the SQL backend, and how order_by interacts with select columns in both the ORM and generated SQL. The patch involves understanding and editing key, complex parts of the Django SQL compiler and query construction, and requires adding new methods and handling for alias-less columns. This likely would take between 1 to 4 hours, as it involves more than a small change, editing multiple files and understanding both ORM and database backend behavior. It also necessitates writing or amending substantive tests to ensure edge cases are handled. This level of analysis and change is not a trivial bugfix nor a minor feature, but also not likely to take many hours or require research beyond the codebase, so level 2 is appropriate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is strong as a coding benchmark, since it requires deep understanding of the framework and careful attention to ORM/SQL translation details, as well as the ability to write effective tests. There are no disqualifying flaws such as vague requirements or external dependencies. The only minor challenge is the presumed familiarity with Django's higher-level ORM and internal implementation details, but this is suitable for an advanced engineering benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11087": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides strong context, a clear reproduction scenario, actual traceback details, and a high-level diagnosis of the core problem: Django's .delete() unnecessarily fetches all fields (including bad-unicode data) when only a subset of fields is needed for cascade deletion. The description gives line numbers where the error occurs and outlines why only referenced fields should be loaded. However, it does leave room for interpretation in exactly how the ORM should be modified or what edge cases to watch for. The exact structure of the fix is not explicitly stated, and some technical knowledge/tradeoff decisions (e.g., around signal listeners and field deferring) must be made by the implementer, rather than prescribed. As such, it's not entirely \\\"mechanical\\\" to translate to code\u2014it requires judgment, but it is reasonable to form a sensible implementation plan from the information given.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This task requires in-depth understanding of Django ORM internals, how model deletion and related field selection works, as well as handling signal listeners and possible side effects. The patch is nontrivial: it introduces new helper functions, conditionally adjusts queryset field selection, and accounts for interactions with signal listeners and select_related optimizations. Multiple files/functions are touched, and careful reasoning is needed to avoid breaking ORM functionality. For an experienced Django/core Python dev, this would take a few hours including code changes and necessary test expansions. It is clearly harder than a simple bug fix but not at the level of an architectural rewrite or highly esoteric bug.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues detected. The sample is appropriate for benchmarking coding ability and reasoning about ORM/backend behavior. The problem and the expected class of solution are clear, and key context for exploration is provided. Only moderate interpretive judgment is required, which is appropriate for this type of benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11265": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete example involving Django's FilteredRelation annotation, specifying the problem: using 'exclude' with an annotated FilteredRelation raises a FieldError rather than producing the expected empty result. It mentions the relevant test file (django/tests/filtered_relation/tests.py), and gives details about the annotation name (book_alice), the query used, and the error message received. It additionally speculates about the faulty function (split_exclude), and describes the new query lacking extra data. However, while a Django-proficient engineer would have enough context to make a meaningful attempt at a solution, some knowledge of the internals of Django ORM and how annotations and filtered relations are handled is assumed rather than fully explicit in the issue text. The expected behavior is implied rather than explicitly stated, though it is reasonably inferrable from the failing test/code snippets and description. Thus, there are some blanks to fill in, but it is generally well-specified for an experienced engineer familiar with this part of the codebase.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this issue requires understanding how QuerySet annotations, FilteredRelation, and exclusions interact in Django's ORM internals\u2014especially how filter/exclude operations translate to SQL and handle query state (such as the propagation of filtered relations in subqueries). The fix involves propagating filtered relations to new Query objects within the split_exclude function and altering logic in trim_start. The solution is not trivial (not simply one or two lines); it involves carefully understanding and editing important ORM internals so as not to break unrelated functionality. While not a major refactor or requiring changes to many files, it is definitely more complex than a quick one-off fix and would likely take 1-4 hours for an experienced engineer to fully diagnose, implement, and test, especially given the risk of subtle issues in this part of Django.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major outstanding issues. The sample appears fully suitable for a coding ability benchmark: it is clear enough for an experienced engineer to make a meaningful attempt, and the solution/test patches map directly onto the described problem. The issue specifically references the impacted methods and the type of test that fails, as well as the nature of the desired behavior and the received error.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly identifies the root cause of the problem: Django incorrectly qualifies field names when generating SQL CHECK constraints if the constraint expression involves a combination of OR and AND, causing migrations to fail on SQLite. The example provided in the description is detailed and concrete, outlining the model structure, constraint definition, and the exact SQL generated. The actual failure message is included, pinpointing how the schema fails due to qualified column names not being found, and the description also explains what the correct output SQL should be. An engineer with access to the codebase would know to focus on how constraints and their SQL are generated (most likely in SQL rendering/construction logic for CheckConstraints) and what needs to change to meet the requirements. Thus, there is little ambiguity about what needs fixing or how correctness should be judged.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix likely requires a non-trivial understanding of Django's migration operations, queryset construction, and SQL rendering logic. It is not a superficial bug\u2014one will need to trace how Django's ORM translates a complex Q expression into SQL, and why certain branches use qualified names and others do not. The solution might involve changes to how columns are represented (e.g., SimpleCol vs Col) or how qualification is applied conditionally. The test changes suggest touching core query/migration logic and writing an appropriate regression test for the database backend. While not a full rewrite or multi-file architectural change, the problem requires tracing subtle code paths, so it may easily take between 1 to 4 hours even for an experienced Django developer. It is not a quick one-line bugfix, but also not a major cross-cutting change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional issues with using this sample. The issue is specific, reproducible, and assesses both code understanding and the subtlety of database migrations and SQL generation. All information needed for a meaningful code challenge is present in the issue statement.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11532": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It describes a clear and reproducible bug: when the system hostname or an email domain uses non-ASCII/unicode characters and the mail encoding is non-unicode (for example, iso-8859-1), Django erroneously attempts to encode headers (like Message-ID) directly, resulting in UnicodeEncodeError. The issue includes example input, affected files (django/core/mail/message.py, django/core/mail/utils.py), an explanation of what should happen (domains should be punycode-encoded before use in headers), and the traceback verifying the error. The expected fix is stated: convert non-ASCII domains to punycode, suggesting where the fix should go. This makes the requirements for a successful solution clear to an experienced engineer working from the text alone.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The required fix is non-trivial: it involves carefully locating all places in Django's mail system where domain names are placed in headers and ensuring that punycode encoding is applied consistently. It touches multiple files (mail/message.py, mail/utils.py, core/validators.py, utils/encoding.py, utils/html.py), requiring both an understanding of email standards and Django's internals. While the desired change itself (apply punycode encoding) is not algorithmically hard, ensuring correctness across the codebase, modifying the relevant utility functions, considering edge cases, and updating (and creating) tests would take 1-4 hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues: the sample is suitable for evaluating coding ability. The scenario is well-motivated, specific, and tests a candidate's ability to read, reason about, and robustly update code across a moderately complex module with real-world constraints.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. The author gives a detailed description of the problem, including steps to reproduce, specific models and configuration, the precise nature of the regression (difference between Django 2.0.7 and 2.2.1), the relevant method (_check_list_display_item), and a table listing the different logical cases and expected/actual outcomes. Furthermore, a candidate/fix implementation is described directly in the text, so any competent engineer familiar with Django\u2019s admin checks subsystem would know exactly what needs to be fixed and could find the relevant file (django/contrib/admin/checks.py). The requirements for a successful solution are clear and unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue involves understanding model field resolution within Django\u2019s admin validation logic and the nuances introduced by descriptor fields and attribute access. However, the bulk of the work is careful logic reordering and improved exception handling in a single admin check function. With detailed reproduction info and a high-level algorithm included, implementing the fix would likely take 15-60 min for an experienced Django contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present. The specification is clear, the relation to bugfixes and previous regressions is well-motivated, and corresponding tests (for edge cases) can be written directly from the problem statement. The only minor challenge is understanding the nuances of Django descriptors, but this is reasonable for the expected audience.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11734": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified. It clearly describes the bug involving the use of OuterRef in exclude() or ~Q(), specifically in the context of Django queryset logic involving subqueries and field references across related models. The error message, expected behavior, and steps to reproduce (tests involving Number and Item models and their tags__category_id relationships) are described, making it clear what is broken and what the expected outcome should be. However, while it is quite clear for a developer familiar with Django ORM internals, someone less familiar with the codebase or this particular area of functionality might need to consult the source or tests to fill in some gaps about the specific mechanics of OuterRef and exclude. The high-level requirement (fix handling of OuterRef in exclude/negated queries) is clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a solid understanding of Django's ORM internals, especially how query construction, subqueries, and references between query parts are handled. The patch affects several files in core ORM logic, and the fix itself involves understanding when and how to properly resolve OuterRef expressions. The engineer would need to both identify the cause of the error (improper use or handling of OuterRef in certain lookups) and design a solution so that correct model references are maintained in these edge cases. Writing a suitable regression test also requires reproducing the issue precisely. For an experienced Django contributor, this is definitely more than a minor bug: it involves reasoning about and modifying non-trivial code paths, albeit not rewriting a whole subsystem. Total work would fit in a few hours (1-4h).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is specific and technical, but reasonably well-contained, and the tests and patch validate a clear fix. It is suitable for evaluating strong coding/debugging ability within the context of a complex codebase like Django.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11740": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified in that the reporter describes a concrete sequence of steps: (1) an initial model definition with a UUIDField, (2) a change to that field to be a ForeignKey to another app's model, (3) running makemigrations and expecting a dependency in the generated migration. The error raised (ValueError: Related model ... cannot be resolved) is included, and the expected behavior (automatic dependency on testapp2 in the migration) is clear. However, there are some minor blanks left: while the model and field names are mostly given, the exact code structure is not included, and the broader context around how Django migration dependencies work (and if this is truly a 'bug' or a user misunderstanding) requires domain knowledge. Nevertheless, an experienced Django developer can make a sensible interpretation and attempt a fix geared toward migration dependency handling on ForeignKey changes. Thus, it lands at level 1: some blanks to fill, but a clear, reasonable interpretation is possible.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding the Django migration autodetector and how it infers dependencies when fields change, particularly involving ForeignKeys across apps. Fixing it involves editing nontrivial logic in django/db/migrations/autodetector.py and adjusting/factoring how dependencies are computed when a field is altered from a plain field to a ForeignKey. It also requires running and extending the migration generator's tests to ensure the dependency is correctly recognized and the migration is valid. This would take 1-4 hours for an experienced Django contributor familiar with the migration system, as it touches core migration logic and requires careful reasoning about dependency graphs and backwards compatibility.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is acceptable for evaluating coding ability. It tests knowledge of Django's migration system and dependency inference logic, and is sufficiently well-scoped and independent to allow for a meaningful programming task and automated testing. No major concerns beyond those already discussed. It is a suitable benchmark task for experienced Python/Django engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11815": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: the problem is clearly described, giving a specific and realistic example involving Django migrations, Python enum members (whose values can be marked for translation), and the error that arises if the migration serializes the translated value instead of the enum name. It also provides the expected correct code behavior/output side-by-side with the problematic behavior. Any experienced software engineer familiar with Django and Python enums could directly implement and test a solution based solely on the information provided. No clarification is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch changes a single serializer class to use enum.name instead of enum.value when serializing Enum objects in migrations. The test changes span a number of assertion lines but mostly reflect expected migration output. Fully understanding the patch requires knowing how Django migrations are serialized (and how Enums work in Python), which may be non-trivial for a newcomer, but for an experienced Django contributor this should be a focused <1 hour task: understand the bug, update the serializer, and adjust/extend tests. It does not require a large refactor or cross-file logic.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The only caveat is that some prior understanding of Django migrations is helpful to avoid unnecessary time spent reading context, but the problem is clear, and the solution is testable.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11964": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it provides a clear description of the unexpected behavior (the __str__ of the field returns the enum label, not the value), expected outcome (str(field) should return the enum's value as a string in all scenarios), precise reproduction steps, sample test structure, and even the real-world relevance for external API integration. It describes a specific failure and the test that fails, and suggests the direction of the fix\u2014ensuring str() on a Choices (TextChoices/IntegerChoices) field returns the value string, not the enum member label.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a modest change: defining or overriding the __str__ method of the Choices class to return self.value. This is a relatively common pattern fix in Python and Enum usage, and an experienced engineer with some familiarity with Django's codebase should be able to locate the source and implement the change quickly. Some consideration is needed to ensure the fix does not introduce regressions, but overall the core implementation is simple.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is a strong candidate for evaluating coding ability: the requirements are clear, the context is provided, and the fix is a well-scoped code+test change. The main challenge is understanding Python enums and Django field behavior; the risk of misunderstanding is very low. No other major issues exist with this sample for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12155": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear about both the nature of the bug and its solution. It explains exactly where the problem occurs (admindoc's handling of docstrings where the first line is not empty), provides a concrete error message, pinpoints the problematic code (the line calculating indentation in trim_docstring), and even suggests a concrete fix (skipping the first line when calculating indent). All necessary details are provided for an experienced engineer to resolve the issue without ambiguity. The references to specific code and the inclusion of an example error make the requirements explicit and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves understanding how the trim_docstring function works, how docstrings are used in admindoc, and altering a line to skip the first line in indentation calculation, or substituting the function entirely. The scope is limited to a small number of functions and files. Although it requires familiarity with parsing docstrings and possibly with the Python standard library, it is not a large or complex fix, and should be achievable within 15 minutes to 1 hour for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is well-suited for use in coding benchmarks as it tests reading comprehension, pinpointing bugs, making practical code changes, and understanding Python docstring conventions. There are no apparent blockers or major weaknesses in the sample specification or difficulty.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12262": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly explains what is going wrong: custom template tags in Django raise TemplateSyntaxError when defined with keyword-only arguments with defaults, and describes the symptoms in detail, giving concrete examples (the 'hello' and 'hi' tags and their invocations, including error messages expected versus actual). It even mentions which versions are affected, and that inclusion tags share the problem. An engineer could directly identify where in the source this is likely to occur and what logic needs updating. No crucial ambiguities exist -- the text gives enough context about expected versus actual behavior for a solution attempt without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is not a trivial fix, as it requires understanding the argument parsing code of Django's template tag system and updating type checks to properly handle keyword-only arguments with defaults. However, the scope of the change is relatively small and localized: the patch modifies just one check in one function (parse_bits). The engineer must map the observed errors to the handling logic and write appropriate tests. All these steps would likely take between 15 minutes and 1 hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are present. The issue describes the scenario well and the problem is a logic bug rather than an underspecified or process issue. It is suitable for judging coding ability as it tests understanding of function argument handling and template internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12325": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete report of an unexpected behavior in Django models' handling of OneToOneField with parent_link, specifically when multiple such fields are present. It describes a minimal repro (base Document model, Picking subclass with two OneToOneField links, order affecting error), names the exception raised (django.core.exceptions.ImproperlyConfigured), and asks for ordering not to affect the logic since parent_link=True is explicit. However, it doesn't provide exact code for the minimal example or the precise expected behavior in all edge cases. The main intent and the bug are clear, and an experienced engineer could make a sensible interpretation and construct an appropriate fix with this information, but will have to fill in some blanks. Thus, I rate this as '1: some blanks to fill in, but there is a sensible interpretation.'\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires understanding Django's model inheritance logic, the metaclass machinery responsible for processing fields (especially OneToOneField), and how parent_link is processed. With a few hours' familiarity, an engineer would need to study the order of field processing and ensure that parent_link=True is properly respected regardless of field order. The fix itself requires careful changes across core Django files (notably base.py and options.py), plus corresponding tests. This goes beyond a small bugfix but is not a massive rewrite. Thus, this fits best into '2: 1-4 hours'.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues beyond some lack of explicitness in the minimal example. The problem is clearly described, and fixes/tests provided make it reproducible and testable as a benchmark coding task. The sample is suitable for evaluating coding skill in the context of core framework logic.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12663": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified for an experienced Django contributor. It describes the regression, provides relevant background on the behavior that used to work, and details steps to reproduce using Django ORM models and a combination of Subquery, annotations, and SimpleLazyObject. It shows the traceback, allowing clear location of the failure in Django's internals (output_field or get_prep_value using a SimpleLazyObject, leading to a TypeError). There are a few blanks for a resolver to fill in: one must decide on the appropriate way to restore or enable comparison with a SimpleLazyObject, and the issue doesn't specify if there are other side effects. However, it is quite clear what needs to be fixed (restore the comparison/filtering ability between annotated subquery and SimpleLazyObject as previously worked).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This requires a solid understanding of Django's ORM internals, especially annotation handling, QuerySet filtering, and how Django fields prepare values for queries. The solution involves modifying or patching part of SQL query construction to gracefully handle SimpleLazyObject (possibly via unwrapping or extracting the underlying value). This is not a simple 1-line fix and requires working knowledge of the call chain. One needs to write code in at least one core file and update or add relevant tests. Allowing time for reproducing, diagnosing, implementing, and testing a robust fix, I estimate 1\u20134 hours for an experienced Django contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues: this issue and its corresponding test provide a meaningful benchmark. The issue is directly mappable to an engineering task, the bug is well-captured, and the fix is testable. The only minor ambiguity is that the core solution requires some judgment on what is the best way to support SimpleLazyObject in this context, but the expectation is clear from the given context and traceback.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12708": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is relatively well-specified in that it provides a clear, reproducible scenario: in Django 1.11.10, when a model initially has both index_together and unique_together on the same set of fields, removing the index_together and running a migration causes a crash. The error, file, and method are specified (django/db/backends/base/schema.py, _delete_composed_index). Additionally, the expected behavior is articulated: deleting index_together should work without error and without affecting unique_together. However, some ambiguity remains around the precise expectations regarding migration coherence and around support for the new Options.indexes feature, and how moving to it should interact with migrations. The core issue, the crash when removing index_together while unique_together remains, is clear enough to make a meaningful attempt at a solution, but an engineer must interpret some details (e.g., what 'coherence' means).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding Django's schema migration system, how index_together and unique_together are implemented and interact, as well as the logic around constraint deletion. The engineer must identify where Django is failing to distinguish between deleting an index (index_together) versus a unique constraint (unique_together) when both exist on the same fields. This involves careful analysis and editing of schema editor methods and possibly adjusting tests. While the patch itself is small, analyzing, verifying correctness, and ensuring no regressions would likely take 1-4 hours for someone familiarizing themselves with Django migrations.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues detected; the sample sufficiently checks the ability to reason about database schema changes and constraint interactions. It is suitable for coding benchmarks, though knowledge of Django's migration internals is required.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12754": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified, describing both the problem encountered (FieldError due to field clash when migrating a field onto a new subclass in the same migration step) and precise reproduction steps: starting with a model with the field, moving the field to a subclass, running makemigrations and migrate, and noting the error. The issue also suggests a remedy (the order of operations between removing the field and creating the subclass), and what an ideal solution would be (autodetector should choose the correct order). However, it does not specify exactly how the migration logic should be implemented, nor does it reference specific code locations. Still, there's a clear and sensible expectation for what must be changed and the desired outcome. Details about testing and precise integration into the codebase would need to be inferred from the context. Therefore, it's not perfect, but it is good enough for a meaningful and concrete engineering effort.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a solid understanding of Django's migration system, specifically the migration autodetector, and how model inheritance and field migrations are handled internally. The engineer must reason through the sequence of migrations, identify why the error occurs, and ensure the migration order is correctly handled so that field removal precedes inherited model creation. This likely involves careful adjustments to how dependencies are determined in the migration planning logic. While not a trivial fix, it is contained to a specific subsystem and would require both debugging and targeted code modification (along with adequate testing). It might not require rewriting a large amount of code, but the domain complexity, debugging, and correct handling of edge cases could easily take 1-4 hours for an experienced contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues noted. The scenario is replicable, the failure mode is clear, and the desired solution is concrete. The test patch also shows there is a clear way to validate the fix via automated tests. This makes the sample suitable for evaluating coding ability within this context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12858": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well specified for an engineer familiar with Django internals and the error message. It describes the specific models.E015 error, what field lookup is being flagged ('supply__product__parent__isnull'), the underlying model relationships, the observed behavior in the Django shell, and refers to a change in Django (#29408) as a potential cause. The context is clear enough to understand that the developer expects ordering by a lookup like 'isnull' to be recognized as valid, but currently it's not. However, the issue just implies what behavior is expected (ordering by lookups should be possible) instead of making explicit requirements for validation logic or backward compatibility, and doesn't discuss what a proper check/fix would be for the field resolution logic. It requires some interpretation, but an experienced engineer can sensibly infer that a fix should allow ordering by a lookup-only (not transform) path.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized and small, involving changes to the validation logic in the Django ORM code. Once the underlying reason for models.E015 being raised is understood, the engineer only needs to adjust field resolution to consider lookups, not just transforms, in one place. Writing or updating relevant tests would take some care but be straightforward for someone with Django experience. Understanding the chain of indirection (nested foreign keys, lookups vs transforms) does require some understanding of Django internals, but the required modification itself is a one- or two-line tweak.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue is a good example of a bug that is specific to complex ORM validation and has a concrete resolution. The sample is suitable for benchmarking, especially for candidates with framework internals experience. Main caution: the initial issue text does not overtly spell out exactly what the validation logic is supposed to accommodate, but the expectation is strongly inferred from context and described model relationships.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13012": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete example and a clear problem statement: constant expressions wrapped in ExpressionWrapper objects are wrongly included in the GROUP BY clause resulting in Postgres errors. The reporter gives a comparison to a working case (using Value(3) directly) and the error message is quoted. The function and usage context are explained enough to infer that 'ExpressionWrapper' should not cause constants to be grouped by. While the low-level fix direction requires familiarity with Django expressions, it is clear that the bug relates to SQL generation involving constants and GROUP BY clauses. Overall, the issue leaves nearly all details clear, though the implementation specifics (exact method/class to touch) are up to the engineer, making this a 1 (almost fully specified, with a little interpretation required).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change is straightforward for an engineer with experience in Django's ORM internals: it's a small override of a single method on ExpressionWrapper that delegates group-by columns selection to its child expression. The problem is well-localized; the test patch shows only a small addition verifying get_group_by_cols. The debugging of SQL output and identification of ExpressionWrapper as the problem is already done. Familiarity with Django expression APIs (e.g., get_group_by_cols) is needed, but the change is succinct\u2014likely implementable in under an hour including test and validation, though more time may be needed for someone new to this codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is suitable for coding ability evaluation: the issue is clear and admits an unambiguous solution. The necessary background can be obtained from Django's documentation and reasonable codebase inspection. The required fix is focused on a specific class/method, is not spread across many files, and does not depend on external systems. There are no blocker ambiguities or structural problems with the sample.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13028": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description includes a detailed account of the observed error when using Django's ORM with a particular model field labeled 'filterable'. The exact traceback and code context (model definitions and the query triggering the error) are provided. The behavior (NotSupportedError being raised) is clearly described, and an experiment (renaming 'filterable' to 'filterable_test' fixes the error) is mentioned. However, the user also says: 'This should be documented or fix.' This leaves open whether the solution should be code or documentation, though the context points toward a code fix. The engineer would need to interpret what the correct behavior is\u2014should having a field named 'filterable' be allowed or handled differently? Nevertheless, there is a sensible interpretation for a code fix, especially after examining the provided traceback and code. Thus, the issue is reasonably well-specified but leaves some ambiguity (level 1).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required fix involves understanding a specific error path in Django's ORM (the check_filterable logic). The traceback and error message are explicit, helping an engineer to locate the faulty code. The solution only involves a minor code modification (adding an additional condition to a method) and some targeted test changes. While it requires familiarity with Django's internals, the breadth of the change is small and would generally be a sub-hour task for an experienced engineer with some codebase familiarity, placing it at level 1 (15 min - 1 hour).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The problem is reproducible from the given description, and the engineer has all the necessary context to resolve and test it. The ambiguity around whether the solution should be code or documentation could be noted, but it doesn't preclude use in the benchmark. The requirement is sensible for coding evaluation. The models, error, and queries are all described.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13112": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete context and a reproducible error scenario: a Django project with an INSTALLED_APPS entry using a mixed-case app name, which raises a ValueError during migration as of Django 3.1b1 due to a mismatch in lazy references for related models (ForeignKey). The expected behavior and what is broken are described clearly. The exact error is provided, along with relevant app, model, and field names. However, the issue could be slightly ambiguous in that it does not explicitly state what the expected fix should be (e.g., whether to handle case normalization at migration or model reference time), so an engineer must infer that Django should support mixed-case app labels and resolve references correctly. The scenario is sensible and reconstructable, making it suitable for a meaningful attempt at a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The underlying problem is limited in scope: the ForeignKey deconstruction logic must handle mixed-case app names when constructing lazy references for model relations during migrations. It requires a careful examination of how model references are parsed and normalized in Django but only entails a small code change in one function. However, knowledge of Django's migrations system and the context is beneficial; the logic is more involved than a trivial patch, but not a major refactor. A focused engineer familiar with Django internals should be able to identify and patch the bug in under an hour. Writing a regression test would also be straightforward based on the symptoms and supplied scenario.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues preventing this sample from being included in a coding benchmark. The bug and solution are self-contained. The only potential note is the expectation of some knowledge of Django's specific migration references, which should be stated for participants, but otherwise, the sample is suitable.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description presents a real, concrete problem in Django ORM usage: subtracting two DateTimeFields and annotating with the result as a duration, but receiving a FieldError about mixed types and an output_field requirement. The author clearly describes the model fields, the query they want to write, and the error message returned, which provides enough context to infer the insufficient behavior and what must be patched (i.e., allowing temporal subtraction without an explicit ExpressionWrapper). However, the description does not specify what changes are needed in the codebase, what versions are affected, nor does it provide example code or mention environments with/without native duration support (relevant for cross-DBMS compatibility)\u2014there are some knowledge gaps that require developer judgement. Still, it is reasonably well-scoped with a clear and sensible target outcome.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The required fix involves non-trivial changes in Django's database expressions internals, specifically about how annotation expressions involving temporal subtraction (DateTimeField, DateField, TimeField minus another such field) are handled. The solution requires understanding Django's complex ORM internals, especially how SQL compilation, expression wrappers, and automatic type resolution work. Multiple test cases (across date, time, datetime, and subqueries) require updates, and the patch makes changes to nontrivial logic in core files. This level of work\u2014diagnosing, implementing, and validating a correct fix\u2014would take an experienced engineer 1-4 hours, as it goes well beyond a superficial or one-line fix and requires thoughtful, codebase-wide consistency.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blocking issues for benchmark use. The issue is sufficiently scoped, and the final patch plus tests cover meaningful core logic alterations. The sample would assess understanding of Django ORM internals, temporal data types, and annotation/aggregation logic. The only minor caveat: domain expertise in Django ORM is assumed for fair evaluation. Otherwise, this is a strong benchmark candidate.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13297": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly clear: there's a regression/change in behavior in Django 3.1, where TemplateView.get_context_data()'s kwargs now returns values wrapped in a SimpleLazyObject, which is not compatible with certain usages (like passing these values directly to get_object_or_404, which expects a str and not a lazy object). The crash and its cause are well described, including the reproduction scenario (passing offer_slug from kwargs to get_object_or_404) and what works (explicit conversion to str). However, there are a couple of blanks: the description relies on the reader understanding what SimpleLazyObject is and how it interacts with the Django context; it also does not mention exactly where in the code this wrapping is happening (as it is in the internals of TemplateView). Still, the core problem and a sensible solution are evident: ensure context data passed via kwargs are of basic types (esp. str) so downstream code (like get_object_or_404) works as expected. Therefore, it is not perfectly specified, but as an experienced engineer it's quite sensible to interpret and solve this.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small bug fix that requires understanding the Django TemplateView machinery and kwargs/context passing. The actual code change is simple (change how wrapping happens from SimpleLazyObject to properly using lazy()), but finding the exact place to fix it requires some inspection of the base class. For someone familiar with Django's codebase, this can be solved in 15-60 minutes, especially given the concrete reproduction steps. There is little to refactor, and only a single function/area is affected (the context preparation).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue is relevant, the code and test patches directly map to the described problem, and there is a testable before/after difference. There is no reliance on external links or prior discussion. The only minor complication is that it involves Django internals, which might require setup/familiarization, but this is within the intended scope. This sample is well-suited for benchmark use.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13406": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified. It precisely describes the context (Django querysets, pickling, values()/values_list(), annotate, internal state issues after unpickling), includes exact steps to reproduce the bug, expected and actual output type differences, and pastes a traceback to demonstrate the failure. It refers to the purpose (GROUP BY via annotate+values), the model (Toy), and how the query behaves differently after pickling/unpickling. However, it lacks a precise expected patch or interface change; some behavior around \\\"fresh queryset\\\" and \\\"replacing its query\\\" might require an engineer to read Django source to fully understand both the cause and characteristics of a robust fix. The core requirements are clear: after pickling/unpickling a query with values()/values_list(), evaluating the queryset should yield dicts (not model instances) as originally. There's little ambiguity, but some room to interpret implementation-level details or edge cases.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires a good understanding of Django ORM's internal query and queryset implementation, specifically how iterables are selected (ValuesIterable vs. ModelIterable) and how pickling/unpickling interacts with them. The fix itself is short but identifying exactly where to set the _iterable_class requires reading through Django's query.py and understanding its state machinery. The engineer would need to reproduce the issue using a model and values+annotate query, investigate the internal state post-unpickling, reason through why dicts are replaced by model instances, and patch where the iterable class is set on reassignment. This process includes causing/debugging a non-trivial ORM issue, so it's more than just a quick patch but less than a full rewrite, likely taking 1-4 hours even for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The sample contains a well-isolated, reproducible bug, and the required depth of knowledge is appropriate for a benchmark. The problem is practical, and implementers can see what success looks like (dict output, no exceptions).\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13449": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it clearly explains the scenario that triggers the error (using Django's Lag window function with a DecimalField and SQLite), provides explicit code examples for reproduction, presents the exact error message (including the generated SQL), and describes the functional difference compared to a similar case with FloatField that works. The user even hypothesizes as to the cause (the placement of the CAST in the generated SQL) and offers a workaround via setting output_field explicitly. The requirements for a successful fix are clear: correct handling of Lag with DecimalField on SQLite such that the error no longer occurs, consistent with the workaround. No ambiguity remains on what needs to be fixed or how success can be determined.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue likely requires a strong understanding of Django ORM's window/annotation system, SQL generation intricacies, and how Django maps field types for different backends. The solution involves modifying logic in the ORM to ensure correct SQL generation for window expressions with DecimalFields, as well as understanding casting behavior in SQLite. This isn't a trivial fix\u2014it requires tracing ORM expression compilation, understanding backend-specific quirks, and updating or extending the relevant logic. There are also related tests to update/add. For an experienced Django contributor, this would take 1-4 hours: not quite a major refactor but certainly more involved than a small patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major disqualifying issues. The sample tests a relevant problem in SQL generation involving field types and demonstrates both test and code modification. It requires the engineer to understand both the ORM and backend-specific SQL details, making it a meaningful evaluation of coding and debugging ability. The test patch provides clear verification of correctness.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13568": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well specified, though some interpretation is necessary. The description explains the current behavior of Django's system checks regarding USERNAME_FIELD and uniqueness, the motivation for the request (to allow uniqueness to be enforced by UniqueConstraint instead of unique=True), and describes the edge case where this matters (avoiding extra implicit indexes on PostgresSQL). It indicates exactly what is not working and what a successful fix should accomplish: the system check should recognize UniqueConstraint in Meta.constraints and not raise auth.E003 when uniqueness is enforced in that way. The engineer will need to refer to Django's Model._meta API and system checks, but the intent and functional requirement are reasonably clear. No file names or lines are given, but the text is direct about the behavior and the fields involved.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a focused issue affecting a single place in the code (the Django auth system checks for user models). The required solution involves modifying a system check to recognize uniqueness constraints not just through unique=True, but also through UniqueConstraint on the username field. This requires some familiarity with Django's ORM meta API but does not require major refactoring or touching many files. For an experienced Django engineer, this should take less than an hour, mostly spent verifying nuanced ORM behaviors and writing tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no significant additional issues with the sample. The issue is self-contained, technically detailed, and can be evaluated with tests. It is a suitable benchmark for coding ability involving framework comprehension and extending checks. One minor point is that knowledge of Django ORM and system checks is assumed, but this is reasonable for experienced engineers or candidates for such a benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13807": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very detailed and clearly specifies the problem, the conditions in which it happens, and both the root cause and relevant code context. It states that using a Django model/table name which is an SQL reserved keyword (like 'order') causes loaddata to fail on SQLite due to unquoted identifiers in PRAGMA calls. The file and function where the bug is located are named, with code snippets pinpointing exactly where quoting is missing. Even the concrete SQL output and error trace are included. An experienced engineer would clearly understand what's required: ensure table names in PRAGMA calls are properly quoted, using Django's quoting utilities.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution requires a code change to use proper quoting for potentially reserved table names in SQLite PRAGMA calls. The problem and its location are well-identified, and Django's quoting function (self.ops.quote_name) is available and documented. After understanding the context, the fix is fairly straightforward string interpolation and test adjustment. The only challenge might be verifying impacts in related code or ensuring no regressions, but the core fix is modest.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue is well-specified, the cause and target code are clear, and test changes to verify the fix are feasible. Good candidate for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13810": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it provides a clear context (interaction between Django middleware, ASGI, and django-debug-toolbar), describes symptoms, points to the suspected code location (django/core/handlers/base.py), and proposes a possible cause (handler variable being incorrectly overwritten/skipped leading to incompatibility with async/sync middlewares). However, it does leave some details up to interpretation, such as the exact intended behavior regarding async support and what is expected after MiddlewareNotUsed is raised. The reporter gives enough context to make a sensible attempt at a fix, but someone unfamiliar with the intricacies of Django's ASGI middleware loading may have to fill in a few blanks.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue sits between small change and substantial effort: while the code modification is relatively concise, understanding the interaction between synchronous and asynchronous middleware in Django, the consequences of mistakenly setting a handler, and being confident in not breaking other middleware flows all require good familiarity with Django\u2019s middleware loading process and ASGI internals. Additionally, designing a test that reliably captures the bug and verifies its fix increases complexity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is suitable for benchmarking coding ability, particularly for someone experienced with Django middleware or framework-level async/sync abstractions. No other blocking issues are apparent.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14017": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it provides both an example of existing (working) behavior and the problematic (TypeError-raising) behavior, including code snippets and the traceback. It explains that combining Q() and Exists() using the & and | operators is not symmetric, and that an attempt to do so in one order results in a TypeError. The expected requirement \u2013 that these operators should be commutative for Q and Exists objects \u2013 is clearly specified. It also gives a hypothesis for the bug (missing __rand__), but this is not strictly necessary to understand what needs fixing. The required change is clear: it must be possible to combine Q() and Exists() in either order using & or | without error, matching the working direction. All needed context is provided for an experienced engineer familiar with Django's ORM internals.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix itself is not trivial\u2014it requires understanding how Q objects and conditional expressions interact in the Django ORM layer. However, the change is focused and likely involves editing a small set of methods responsible for combining Q and Exists, as indicated in the example traceback. An experienced engineer familiar with Django would not need to make extensive edits or research, but must ensure cross-type handling is correct. The gold patch further confirms this as a targeted change (a few lines updated in one method). This is likely a 15 min - 1 hour task for someone already familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is clear, and the solution/fix can be tested and verified through unit tests. The description and examples provided are sufficient for the benchmark purpose, especially for engineers with experience in Django or similar ORMs. No blockers or ambiguities to using this as a coding benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14140": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly describes the problem: calling deconstruct on a Q object with a single child works (returns kwargs), but if the single child is not a (key, value) tuple (for example, wrapping an Exists object in Q), it crashes with a TypeError. The stack trace is provided, as well as precise code snippets exhibiting the failure. Additionally, the recommended solution is discussed (remove the special case and always use args, a change which is justified by likely lack of user reliance on undocumented behavior). This is concrete, points precisely to the place and class involved (Q and its deconstruct method), and there is a sensible interpretation of what is required: remove the special-case logic so that all single-child Q objects deconstruct to args.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change is localized to the deconstruct method of Q, and involves removing special-case handling and updating it to treat single-child Q objects consistently. The engineer needs to understand the class's current logic, realize why the failure happens for non-tuple children, and generalize the code path. The patch is small and only requires edits in a single function, with perhaps a small need to understand the implications for test coverage. This is a relatively straightforward fix requiring some understanding of how Q, deconstruct, and boolean expressions work, but does not require substantial changes or advanced research.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue presents a concrete bug, points directly to the location of the problem, and describes reproducible steps. The description does not rely on external discussion or undocumented internals beyond the behavior cited, so it is suitable as a coding benchmark. There is no evidence of additional major issues, and the presented task is in line with typical bugfixes in mid-to-large Python codebases.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14238": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and provides all necessary context for a knowledgeable Django developer. It states that when setting DEFAULT_AUTO_FIELD to a custom subclass of BigAutoField or SmallAutoField, Django fails with a ValueError because the check for whether the field is a subclass of AutoField does not recognize custom subclasses. The stack trace, setting example, and the relevant class names all point towards the required area of investigation. The reporter even suggests a specific fix (altering AutoFieldMeta.__subclasscheck__). Thus, it is quite clear what the problem is, under what conditions it occurs, and a good sense of how to fix it.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the issue is subtle (involving details of Python's metaclasses and Django's internal model machinery), the code change itself is straightforward once the problem is understood. The engineer needs to adjust a metaclass method to properly handle issubclass checks for custom subclasses, and add/extend related tests. This is not a trivial one-line fix, but it's clearly bounded and should be achievable in under an hour for someone familiar with Django internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues with using this as a benchmark sample. The description is clear, scoped, and includes enough details for an engineer to tackle the problem meaningfully. It assumes basic familiarity with Django's field and model systems, but that is appropriate for the intended audience.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14351": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified for a Django expert: it identifies a concrete error resulting from the behavior of Q object filters with OR conditions, provides code snippets to reproduce the bug, explains which fields are being wrongly selected (all fields rather than just the PK), describes the error as a subquery returning too many columns, and suggests that limiting to PK works. However, the issue does not fully specify what the correct behavior for all similar cases should be, nor does it indicate where in the codebase the change should be made. Some knowledge of Django ORM internals is assumed, and there is some room for interpretation regarding a robust or generalizable fix. Thus, it's between 'well specified' and 'some blanks to fill in,' but overall there is a sensible interpretation for a fix based on the description and repro steps.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this issue requires understanding how Django ORM composes subqueries for related objects, and debugging why 'Q' objects with 'OR' conditions are causing all columns to be selected instead of just the PK. The fix involves modifying how the ORM constructs such queries, likely touching non-trivial and sensitive internals. While the actual code change might be a few lines, identifying the source, writing a fix, and testing its correctness is definitely not trivial and could readily take 1\u20134 hours, especially as it requires in-depth inspection, regression testing, and likely some reading of lower-level ORM query compilation code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major blockers. The issue has a concrete coded reproduction and the root cause is identifiable from the problem text. While it requires specialized Django ORM knowledge, it is an appropriate benchmark for experienced engineers with that background.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14493": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well-specified\u2014it clearly describes a circumstance under which Django's ManifestStaticFilesStorage will crash (when max_post_process_passes is set to 0). The error (UnboundLocalError: local variable 'substitutions' referenced before assignment) and its cause (the variable only being set in a loop that isn't always entered) are explicitly stated, and the stack trace is included for developer context. The only minor ambiguity is that the issue references external links and a secondary motivation, but for the main bug (the crash and how to fix it), the requirements are clear enough to implement a fix. Filling in the blanks (how \\\"not producing invalid CSS\\\" ties to this change more broadly) is not strictly necessary to solve the immediate problem, but there is a sensible interpretation and path-to-fix based on the provided stack and description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is relatively simple and involves ensuring that 'substitutions' is always initialized, even if the for loop is not entered. This is a common pattern and the engineer can identify the problem and solution quickly from the stack trace and the explanation. Inspecting the code at the indicated location would make the bug and fix straightforward once the context is understood. The conceptual leap (initializing a variable outside a loop based on code inspection and the error message) is small for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is suitable for evaluating debugging and patching skills, as it requires diagnosis from a stack trace and targeted code adjustment. The reliance on framework conventions means some Django familiarity is helpful, but the issue itself is self-contained for an experienced engineer. The external links provide extra context but are not required to resolve the core bug, which is a variable initialization issue.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14580": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear and reproducible setup: using Django 3.2.4, it asks the reader to define a custom TextField subclass, an abstract base model, a simple mixin class, and a concrete model inheriting from these, using the custom field as a primary key. It then gives a specific command (makemigrations), clearly describes the observed bug (missing 'from django.db import models' import in the generated migration file), and shows the exact traceback/error. The expected behavior ('Django generates a migration file that is valid Python') is explicitly stated. The issue also hints at a possible location of the bug (django.db.migrations.writer), but does not require thorough investigation to other modules. Any experienced engineer would understand what successful resolution means: ensuring generated migrations have all required imports such that Python can resolve all used names (like models.Model).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The scope of the fix is limited: it requires identifying where migration files are generated and ensuring the correct import is included. The problem is localized and concrete, affecting how the bases tuple is serialized. The solution, as shown by the gold patch, involves modifying a special case in the serialization logic to ensure an additional import is written when models.Model is serialized. This kind of focused code generation fix is straightforward for someone familiar with Django's migrations and would likely take less than an hour after codebase orientation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The only caveat is that some minimal code familiarity is needed to identify where the serializer logic lives, but the issue is very direct and reproducible. The test file is clearly modified to check exactly for serialization of models.Model, tightly tying the fix to a testable outcome.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14672": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and gives clear directions for a successful solution. It identifies a missing call to make_hashable on self.through_fields in the ManyToManyRel class within Django's codebase. The problem is explained in the context of Django models and the identity/hashability machinery, and a stack trace is provided showing the TypeError ('unhashable type: list') that occurs. The description also gives a minimal reproducible example, walking through how to trigger the bug, and then succinctly states: 'Solution: Add missing make_hashable call on self.through_fields in ManyToManyRel.' This makes it clear both what the bug is and how to solve it. References to class names and variables are precise (e.g., ManyToManyRel, through_fields, make_hashable).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires familiarity with the codebase, specifically with Django's ORM and the reverse-related field machinery. However, the issue and the fix are both tightly scoped: once the relevant class (ManyToManyRel) and property (identity) are located, the solution consists of applying make_hashable to self.through_fields. There is only a single location to change, and the nature of the change is straightforward. An experienced developer could likely implement, test, and validate this fix in 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues with this sample. The issue, context, proposed solution, and reproducible example are all provided in detail, and the problem is of a type suitable for coding benchmarks. The PR and test changes directly address the symptom and confirm the defect is resolved. This is a good benchmark test for evaluating ability to reason about code structure, hashability, and codebase navigation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is generally well-specified, though not perfectly explicit. The reporter describes the usage of method_decorator on a class method, an expected behavior (logger decorator should see __name__ etc.), and provides the error message indicating functools.partial object does not have '__name__'. It is fairly clear that the desired fix is to ensure that whatever object is produced by method_decorator preserves the attributes like __name__ and __module__, as expected by typical Python decorators/inspectors. However, the description does not detail the full context (such as all expected behaviors or edge cases), but the goal is sensible and well inferable: wrapping the partial return with functools.wraps to preserve the original function's metadata. No ambiguity about the main problem, but details like which module, how exactly to apply, or other expectations for edge cases are not spelled out.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual fix is small and primarily involves ensuring that the result of partial(method.__get__(...)) is wrapped with functools.wraps to preserve function metadata. The patch reveals this involves a one-line change. However, understanding why functools.partial strips function metadata when used like this, and realizing that a simple wraps(...) call restores expected behavior, requires moderate familiarity with Python decorators and descriptor protocol. For an experienced engineer, it would likely take 15-45 minutes to go from reading the issue to deploying and testing the fix, after gaining context on how method_decorator internally works.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No blockers or issues beyond what is discussed above. The sample is valid for benchmarking: it tests understanding of decorators, functools.partial, and method-wrapping, which is core Python knowledge. It does not depend on external discussion or information that is not present. The main challenge is reasoning about function metadata when using functools.partial and decorators; otherwise, both debugging and solution implementation are clear. The unit test change also illustrates the requirement in practice.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15104": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified, with a clear reproduction scenario and a pointer to the code line causing the problem. The author explains the context (running Django tests in verbose mode leading to a KeyError), provides a traceback that points exactly to where the error is happening (del deconstruction[2]['to']), and details the circumstances (a ForeignKey subclass that removes its 'to' argument from deconstruct). The tester even describes a minimal reproducing example and a simple fix ('del deconstruction[2]['to']' to 'deconstruction[2].pop('to', None)'). An experienced engineer could produce a correct solution from this information\u2014specifically, to change the deletion of a dict key to a safe pop, likely in autodetector.py. Test coverage for the bug is also explained in the minimal reproduction description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change that requires understanding why the KeyError occurs and implementing a safer method for removing a dict key ('pop' instead of 'del'). The root cause is directly described and localized in the traceback and reproduction steps. The solution, while simple in code, must be chosen with some care (ensuring correct behavior even if 'to' is not present) but does not require substantial rewriting or cross-file work. Most of the effort would be spent on understanding the context and verifying (through testing) that the change solves the issue and doesn't introduce regressions.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues that prevent this sample from being used for coding ability evaluation. The problem is easy to understand, not overly esoteric, and requires both bug tracing and correct application of a Python dict manipulation fix. The only subtlety is understanding how the ForeignKey field interacts with Django's migration autodetector, but the description and test case provide full context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is well-specified in terms of context, steps to reproduce, expected behavior, and a suggested direction for the fix. The user provides versions, a minimal reproducible example, a traceback, a reasoned root cause analysis, and a proposal for how to address the alias collision issue in Django's ORM QuerySet combine logic. However, there are some blanks to fill in as the developer must fully deduce how to generalize the solution (the description suggests but does not fully formalize what the fix should be) and ensure that any change won't break other ORM behaviors. Still, an experienced engineer can sensibly interpret and understand what is required from this information.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This task requires solid understanding of Django's ORM internals, including query aliasing and how QuerySets are combined. The engineer must investigate and validate the bug, design a fix for non-commutative QuerySet OR logic (ensuring alias maps do not conflict), edit core logic in the SQL query construction, and write/modify tests to check for regressions. This involves modifying non-trivial code, understanding complex interactions, and careful testing to ensure backwards compatibility; hence, it is not a quick fix, but could be completed within a few hours by an experienced developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The sample provides a solid and realistic challenge relevant for evaluating coding ability, especially for deep framework comprehension and debugging skills. The tests are also adequate for validating the correctness of the solution.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15277": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well specified: it describes the current and problematic behavior when creating a Value('test') and calling its _resolve_output_field method in Django. The precise origin of the bug (CharField.__init__ append a MaxLengthValidator even when max_length is None) is described, and a clear, concrete code snippet is proposed as a fix. The reporter provides reproduction instructions and details on how and why the bug occurs, including example code, stack trace, and fine-grained performance measurements. There is also a comparison with precedent in another field (BinaryField). Overall, the requirements for a successful solution\u2014changing CharField.__init__ to only add a MaxLengthValidator when max_length is not None\u2014are unambiguous. Anyone familiar with the Django codebase can implement the requested change based on this issue text alone.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Although the code change itself is very simple (an if statement in CharField.__init__), and the logic is clear, resolving this would require a brief investigation to confirm that omitting MaxLengthValidator when max_length is None does not introduce downstream issues and to understand that this matches behavior in BinaryField. Given the clarity of the issue and the provided precedent, it would generally take an experienced developer 15 minutes to 1 hour to review, implement, and test the change (assuming general familiarity with Django and model fields).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues. The issue is highly usable for evaluating coding ability: it is specific, self-contained, provides context, reproduction steps, and a straightforward place for changes. The test patch provided directly checks the resolution of the bug.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15280": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is clearly described in terms of the model relationships, fields, queries performed, and the exact behavior and assertion failures encountered. It outlines the reproduction steps, including model setup (User and Profile, fields used, types of relationships), and specifies query expectations and the problem (deferred fields handling across nested prefetches). The core of the bug is well explained. The only minor ambiguity is that it assumes the engineer has enough knowledge of Django internals and the specific mechanisms of prefetch_related, which is reasonable for this context. There is a sensible interpretation for fixing the issue: ensuring correct handling of deferred fields with nested prefetches, so that no additional/unexpected queries occur. The overall scope and expectation for a correct fix are clear, although the implementation details are left to the engineer.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires a good understanding of Django's ORM internals, especially how prefetch_related, deferred fields, and object caches work. The actual patch is small, but diagnosing the bug, understanding exactly where the deferred fields are not set up correctly, tracing it through prefetch logic, and writing an effective test require more than simple code edits. Reasoning about object caching, queryset construction, and the underlying metaprogramming/Django glue logic makes it substantially more difficult than a trivial fix. This would likely take 1-4 hours for an experienced engineer familiar with Django's ORM codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is substantive and would be a good benchmark for evaluating coding ability at a moderate-to-high level of difficulty, particularly for ORM or framework-level competency. It also provides enough context for one to craft reasonable expectations and tests based on the description.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15315": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It explains the buggy behavior: Field.__hash__ changes its value after a field is assigned to a model class, causing issues such as failing to find the field used as a dict key. The problem is described in concrete terms: after assigning the field to a model, the hash changes, which breaks dictionary lookups. The relevant class and method (Field.__hash__) are specified, and there is a strong hint towards the desired solution (revert the hash change made in a particular commit). The overall objective\u2014making Field.__hash__ immutable with respect to model assignment\u2014is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue does not require major architectural changes or a deep rewrite. Once the Field.__hash__ method is located, updating it to use only stable attributes (such as creation_counter) is a straightforward code change. The primary difficulty lies in determining the correct attributes to include in the hash to ensure it remains stable and unique, which requires some thought and minor investigation, but should not take longer than an hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues identified. The issue provides sufficient context, the target class and method are clear, and the expected change is well-motivated and testable. The test patch also directly validates the fix.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15375": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides detailed reproduction steps, including the Django and Python versions, the sequence of queryset operations (annotate with F('id'), aggregate with Sum('id', default=0)), and the observed error (including SQL and error message). While the underlying cause is not explicitly stated, and the user assumes familiarity with Django ORM's aggregation mechanics, it is clear from context what is broken (annotate() + aggregate(Sum with default arg) breaks). The expected result is also very clear (should not crash, should compute the sum as with regular Sum or with Coalesce). An experienced Django developer would find this well-specified, but someone less familiar with ORM internals may need to fill in minor gaps. Overall, it's clear what needs to be fixed: the ORM should support the default argument with aggregate after annotate without crash.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix interacts with advanced parts of the Django ORM\u2014expression resolution for aggregates, and making sure default arguments propagate correctly through the annotation/aggregation SQL composition. The solution involves a precise adjustment (carrying over the is_summary attribute on a Coalesce wrapper), but to reach that point, a solid understanding of ORM expression trees and query planning is required. Although the diff is small, understanding the flow and why the SQL is malformed is non-trivial and will require >1 hour for most engineers unfamiliar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues spotted. The issue is well-specified and touches an advanced but isolated feature in ORM. The test patch is isolated and clearly checks the fix. The coding task is representative of challenging, real-world framework development, though it does assume strong Django familiarity.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15380": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly specific: it explains the action performed (renaming a model and a field in a Django app), the method of reproduction, and provides the complete traceback, pointing to a KeyError in 'autodetector.py' when running 'makemigrations' after renaming both a model and one of its fields. The intent\u2014that the migration autodetector shouldn't crash when renaming a model and a field at the same time\u2014is clear. However, there could be ambiguity in how such renames are intended to be processed together, and how the migration system should reconcile multiple simultaneous renames. Nevertheless, an experienced engineer can make reasonable assumptions and fix the crash as described. Hence, I rate this as 1: there are some blanks to fill in, but the requirements for a successful solution are fairly clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual code fix involves a targeted one-line change in the autodetector to properly reference the model's new name in the lookup, and the associated test addition. Understanding the traceback and following the code logic requires some familiarity with Django's migration system, but once the bug is identified, the change is straightforward and small. It may take some thought to root-cause the exact bug and produce an appropriate fix and test, but an experienced engineer familiar with Django should be able to perform this within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is suitable since the scenario and requirements are clear enough for an engineer to attempt a solution without clarification; the crash is easily reproducible and localizable based on the stack trace. No further major issues appear in terms of ambiguous description, external dependency, or lack of clarity.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15503": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well-specified and provides a concrete context: when using Django's JSONField has_key (and related) lookups with numeric keys, queries behave differently across database backends (works on PostgreSQL, fails on SQLite/MySQL/Oracle). The problem is illustrated with a clear minimal example and a failing test output is included. However, the explanation is narrowly focused on 'has_key' with SQLite, while the initial summary includes several lookups ('has_key', 'has_keys', 'has_any_keys') and multiple DB backends. The required solution is clear in spirit: ensure numerical JSON keys are handled consistently like string keys. There are some blanks to fill in, such as the exact mechanism underlying the failure and how to ensure parity across all affected lookups and backends, but the crux of what must be fixed is sensible and clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a good understanding of how Django constructs SQL for JSON lookups on different backends and specifically, how the ORM's path compilation logic interprets numeric keys\u2014which may get confused with array indices in some DBs. The solution involved subclassing and adjusting path compilation so integers used as object keys are not interpreted as array indices, and the change spans several internal classes to ensure all relevant paths (including 'has_keys' and usage in isnull lookups) are handled. The engineer would need to read current ORM code, reason about path handling, possibly check several database backends, and write/adjust comprehensive tests. This is more than a 'small tweak' and requires reading into the codebase and editing in several places. The changes and test coverage indicate this is a moderate engineering task (1-4 hours) rather than a quick fix or massive rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no showstopper issues with the suitability of this sample for evaluating coding ability. The issue is clear in terms of expected functionality, and the provided minimal, reproducible example makes it possible for an engineer to proceed without further clarification. The scope could be clarified slightly regarding coverage for all listed backends/lookups, but this is not a blocker for evaluation purposes. The required solution exercises understanding of database abstractions and JSON semantics, making it a strong fit for a benchmark of this type.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15525": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a detailed, reproducible example including the models, fixture data, exact commands run, the observed stack trace, and a summary of the expected behavior. The problem is that loading data with natural keys and foreign key dependencies into a non-default Django database fails\u2014the description makes clear this occurs when using loaddata with the --database option, and that Books with foreign-keyed Authors cannot be loaded even though the data is structured properly and works for the default database. The example includes the relevant model code, the failing command, the error stack trace, and all context needed to identify both the source of the bug and the expected outcome. An experienced Django engineer could use only this information to devise a PR that correctly resolves the problem.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding Django's serialization and deserialization, particularly how natural keys and multiple databases interact. The solution involves identifying where the database context is incorrectly set or missing and ensuring the correct database is used for object lookup during fixture loading. The patch is small, but arriving at it requires familiarity with the ORM's internals, how natural keys work, and Django command/fixture loading. This goes beyond a trivial change and may require tracing through several layers of code (serializers, managers, model methods). For a Django-savvy engineer, the expected time to analyze, test, and implement the change (and confirm with tests) is likely 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The description is detailed, and testability is ensured through fixtures and clear setup. The only (minor) caveat is that the use of custom managers and natural keys may pose a steeper learning curve for those without Django experience, but for the target audience this should not be disqualifying.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15695": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is generally well-specified: it describes the context (RenameIndex crashing when an unnamed index is moved backward/forward in migrations), the observed bug (crash with ProgrammingError due to attempted duplicate index creation), and a concrete example (migration with index 'new_pony_test_idx'). It also points to the expected outcome (RenameIndex should restore the old auto-generated name when moving backwards, and re-applying should not crash). However, some details require interpretation, such as the exact definition of \\\"unnamed index for unique_together\\\" and whether the problem occurs solely on PostgreSQL or generally. A reasonably experienced Django developer can infer the requirements and reproduce the failure using the provided scenario, though minor blanks remain around some background Django migration mechanisms.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires understanding how migration operations work in Django, particularly the RenameIndex operation and how it handles the naming and recreation of indexes during forwards and backwards migrations. The solution itself is a relatively small code change (essentially a guard clause preventing a noop from attempting to recreate an already existing index). While the root cause must be diagnosed, most of the work is understanding the issue context, repro, and writing/adjusting a clear conditional. For an experienced developer familiar with Django migrations, this will likely take 15-60 minutes, especially since the test scenario is described.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are noted. This is a realistic engineering task: the issue is reproduceable from the information given, and the scope is clear once the initial background is understood. The required knowledge is mostly limited to Django's migration framework and basic SQL concepts about indexes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15732": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear problem statement: it's about an inability to drop a unique_together constraint on a single field (the primary key), resulting in PostgreSQL maintaining two unique constraints on the same column. The steps to reproduce are described, and the unexpected outcome is specified. However, it doesn't specify the desired behavioral change in absolute terms (should the migration be able to distinguish and safely drop the correct constraint?), nor does it point to exact locations in the codebase. Still, the scenario is clear and actionable for an experienced Django developer, and the testing and code patches show that the intent was to improve constraint handling logic to properly drop only the correct uniqueness constraint.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires a good understanding of Django's schema editor, database migrations, and how constraints are named and tracked (including database-specific behaviors like PostgreSQL's ability to have multiple unique constraints on the same column). Implementing this requires not just a small code change but adding/refactoring functions to identify and remove only the correct constraints, as well as writing/adjusting adequate tests for these edge cases. This level of system interaction, plus the need to avoid regressions, likely takes between 1 to 4 hours for a competent engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues found. The problem is clear, tied to a specific feature, and the provided test changes demonstrate how resolution can be validated programmatically. The sample is suitable for benchmarking coding ability for an engineer familiar with Django and database constraints.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15814": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and clear. The description lays out the model structure, the query that causes the crash, the environment, and includes a full traceback. Most importantly, the reporter describes the precise buggy behavior (the ValueError: 'id' is not in list), pinpoints the problematic line in the codebase, and offers a minimal working fix (replacing 'opts = cur_model._meta' with 'opts = cur_model._meta.concrete_model._meta') that resolves the issue. An experienced engineer would have all the necessary context to reproduce, understand, and fix the issue as described, making this a highly instructive, reproducible bug report.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding Django's ORM, specifically how proxy models work and interact with deferred fields, as well as select_related and only querysets. However, the fix is ultimately a small code change related to using the correct model meta attribute, with the bug and requisite context provided. The actual change is single-line, though determining why the fix is correct does require reviewing Django ORM internals. This fits the 15min-1h bracket for an engineer already familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is strong for benchmark purposes: it is highly reproducible, gives enough detail to diagnose and fix the problem, and the required patch is small but requires understanding of Django internals. There are no significant blockers or caveats to using this as a benchmark item.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15930": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite detailed and well-specified. It explains the use case (annotating users with a boolean using a Case expression with ~Q(pk__in=[])), the precise error encountered (ProgrammingError, including the problematic SQL), and the expected behavior (all rows should be annotated as True). It also gives context on why this construction matters in real application code. An experienced Django developer with access to the codebase would understand what needs to be fixed (i.e., ensure Case expressions with always-true conditions render valid SQL), and the test case is clear from the description. No significant ambiguity is present regarding the requirements.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the underlying issue is subtle and requires understanding how Django ORM translates query expressions into SQL, the scope of the change is small (special handling for always-true conditions in CASE expressions). For an engineer familiar with Django's query expressions, it would take some consideration and inspection of the codebase, but writing the fix would not require a substantial code rewrite or modification of multiple modules. The solution primarily involves editing the way SQL is generated for particular cases, which is a localized fix. Thus, it's likely to fall within the 15 min\u20141 hour range for a competent engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The description is self-contained, avoids outside references, and the problem is clearly linked to a specific codepath within the ORM. The benchmark would exercise both understanding of Django ORM internals and general Python/SQL reasoning, making it a good coding test.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15957": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well specified: it clearly states a concrete use case (prefetching a limited number of related objects using slices, assigned to a specific attribute via a Prefetch object) and the resulting error (\\\"Cannot filter a query once a slice has been taken\\\"). It also provides rationale for why this is a common/important use case. However, it leaves some blanks about exactly how Django prefetch/queryset internals should be modified to support this, and does not prescribe a particular implementation. An experienced Django developer would have to interpret exactly how to enable sliced prefetching safely and efficiently without changing intended semantics. But the required outcome\u2014that using sliced querysets in Prefetch should work and return only that slice per related instance\u2014is clear. Thus, there is some room for interpretation, but there is a sensible and actionable problem definition.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding of Django's ORM, particularly how prefetch_related and Prefetch objects interact with sliced querysets. The solution involves non-trivial changes in the internals of how Django handles prefetching and slicing, potentially rewriting or adding logic to properly process sliced querysets and ordering with respect to related objects. This is not a simple patch: it requires a solid grasp of advanced Django internals and careful changes to avoid performance regressions or behavior changes. The related test cases also show that multiple relationship types need to be supported. An experienced Django engineer could complete this in a few hours, but they need to read/understand complicated code paths and test accordingly. Thus, '1\u20134 hours; substantially rewriting a function or editing multiple files' is accurate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues observed. The issue is sufficiently linked to the code (fields/Prefetch API), has a testable outcome, and is appropriate for evaluating ability to navigate a mature codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a detailed account of the structure and relationships between the three Django apps (fonte, variavel, fonte_variavel) and their model fields. It identifies the AttributeError encountered during migrations, includes the traceback, and explains the setup (using ManyToMany with a through model across apps). However, it leaves some minor gaps: it doesn't specify the Django version, if any recent changes were made to Django or the project, or the precise expectation of how through models across apps should be handled in migrations. Still, for a Django-experienced engineer, the core problem (error occurs when ManyToMany through model is defined across different apps and referenced as string) and the working alternative (single models.py) are clear, as is the required high-level solution (fix the error in migrations so that the ManyToMany through model works across apps). Thus, the issue is well-specified enough for a sensible solution, but not as precise as the best examples.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This problem goes beyond a simple tweak. It involves understanding Django's migration/autodetector internals and how model references (especially for ManyToMany with a through model across apps) are resolved, and why referencing the through model as a string leads to an AttributeError. The fix may be concise but requires deep knowledge of Django's migration framework, debugging the error, and devising a solution that resolves cross-app model reference issues. Additionally, the tests show the necessity to expand test coverage, so some comfort editing or writing tests is expected. For an experienced Django engineer, this would likely be a 1-4 hour task, factoring in time to reproduce, analyze, implement, and test the fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The problem is clearly scoped within Django model migrations. As long as the engineer is familiar with Django's model and migration framework, the sample is appropriate for evaluating coding ability. Enough context is given and the error is reproducible from the setup provided.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16032": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well specified and provides a clear step-by-step procedure to reproduce the bug, including exact QuerySet operations to perform and the expected result (specifically the exact error message and the expected names to be returned). While it does not reference specific Django internal code, field, or method names, it provides enough detail for someone familiar with Django ORM internals to understand where to look and how the logic is supposed to work. There are some blanks to fill regarding how QuerySet.select fields interact with annotate and alias, but the main requirements\u2014fixing the erroneous behavior and reproducing the test scenario\u2014are well laid out. The targeted classes are discoverable from the context (Publisher, Book).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a deep understanding of Django's ORM internals, specifically how QuerySet's annotate, alias, and related lookup (__in) interact at the SQL generation level. The fix itself, as revealed by the patch, is fairly localized but understanding why the fix is needed and verifying its correctness involves tracing how the QuerySets are built and how subqueries select fields. This likely involves reading and reasoning across several parts of the ORM's SQL/query-building machinery, which is nontrivial. Thus, I rate this as a 1-4 hour issue for an experienced engineer with ORM knowledge.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is suitable for inclusion as a coding benchmark. The scenario is realistic and the issue is clear enough for a knowledgeable engineer to attempt without external context. The expected behavior and minimal reproducible code are given, but a basic familiarity with Django ORM and its internals is required. There are no hidden blockers or external dependencies referenced; all information needed to attempt the fix is provided in the issue text.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16136": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It provides a concrete error message, context on when and how the error occurs (in a Django 4.1.1 project with a class-based view that only implements an async 'post' method), and a step-by-step reproduction guide. The expected behavior (returning 405 Method Not Allowed rather than raising a TypeError) is clear. It precisely identifies both the failure and its expected handling. An experienced engineer would easily understand the context and purpose of the fix without ambiguity about requirements, locations of change, or desired outcome.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the issue requires understanding Django's request handling and the distinction between sync and async code, the core fix is localized to how HttpResponseNotAllowed is handled in async views. The code change is straightforward: ensure that an awaitable is returned when needed. For a Django-fluent engineer, this is a small, thoughtful code edit and minor test addition, fitting into the 15 min - 1 hour range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues that would preclude this sample from being used in the benchmark. The problem is clear, not affected by missing context, and the solution scope is concrete. The engineer is not required to have domain expertise beyond common Python and async principles, and the scope of affected code is minimal. The setup is realistic and practical for coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16429": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It clearly identifies the function affected (timesince), the setting required for the bug to occur (USE_TZ=True), how to reproduce the issue (subtracting 31 days from timezone.now() and calling timesince), the exact error message produced (TypeError: can't subtract offset-naive and offset-aware datetimes), and the suspected cause in code (creation of a pivot datetime object without respect for tzinfo). The minimal example, expected result, and context are all provided, with precise references to the relevant code section and how to fix it. This provides more than enough information for an experienced developer to understand and solve the problem without any ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the fix itself is a one-line change (adding tzinfo to a datetime constructor), identifying exactly where the issue lies (pivot creation in timesince) does require understanding the time zone handling in Django and the context in which offset-aware/naive datetimes can cause issues. The issue description's clarity and the code reference make it accessible but require some attention to detail. For a developer familiar with Django and its timezone system, this is likely to take between 15 minutes and an hour, including verifying the fix and running tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this benchmark sample. The scenario is fully reproducible with the information in the issue, the fix is clear, and proper tests can be formulated from the given description. It is a suitable case for evaluating coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16454": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, specific, and provides concrete examples that illustrate both the expected and problematic behaviors when using Django management command subparsers. It specifies that the error formatting provided by CommandParser is not carried over to subparsers created via add_subparsers(). It provides an example of both the correct output and the undesired stack trace, and identifies the root cause (missing arguments to subparsers). The description proposes a high-level approach to correcting the problem (ensure that relevant arguments are copied through to constructed subparsers). An experienced engineer familiar with Django would have a clear understanding of what constitutes a successful solution: subparsers must have consistent error formatting with the main parser, preventing CLI stack traces in favor of usage messages. No additional clarification is needed to begin work on a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a small code change involving `add_subparsers` in Django's `CommandParser` to ensure that error formatting arguments are propagated to subparsers. This likely involves overriding or extending an existing method, and understanding how Django integrates with argparse. The fix is isolated (as seen in the gold patch), primarily requiring knowledge of the code flow around parser creation, and can be completed within an hour by a developer acquainted with Django's management commands. The example and root cause provided further reduce the investigative load.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is quite crisp, the solution is verifiable through tests, and aligns well with benchmark requirements. The clarity and focus on a typical developer-facing usability case make this a strong candidate for benchmarking coding ability, particularly in understanding and extending framework internals. The only caveat might be general familiarity required with how Django wraps argparse for those unfamiliar, but this is expected in a benchmark scenario.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16485": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is clearly described: when the floatformat filter is used in Django with either the string \\\"0.00\\\" or Decimal(\\\"0.00\\\") and a precision argument of 0, it crashes with a ValueError. The error is shown, and both the inputs that cause the crash and the undesired behavior are given. Any engineer familiar with the codebase would know to investigate the floatformat function to ensure it handles the case when the precision is 0 without raising an exception. There's a clear input, a clear error, and therefore a well-specified goal: prevent the crash and return the expected formatted value ('0' for 0.00 with precision 0).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue involves understanding how the floatformat filter works and making a minor modification so that it correctly handles '0.00' with precision 0. The patch illustrates that the fix is a simple conditional tweak and not a substantial rewrite. An experienced developer would be able to track down where the ValueError is coming from, understand the logic, and implement the fix relatively quickly\u2014likely within 15-60 minutes including test updates.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The test and code changes are straightforward and localized, and the original issue is clear.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16569": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is highly specific. It names the class/method involved (Formsets\u2019 add_fields()), describes the triggering conditions (self.can_delete == True and self.can_delete_extra == False, and index is None), provides a concrete exception message (TypeError: '<' not supported between instances of 'NoneType' and 'int'), gives an example of when the error occurs (accessing empty_form after specific configuration), and points to the relevant line in the code (line 493 of django/forms/formsets.py). Furthermore, it proposes a sensible fix (add an 'index is not None' check before comparison) and includes minimal reproducible steps. An engineer familiar with Django and the codebase would have all the information required to create a targeted PR that addresses the issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix consists of understanding a conditional statement and ensuring it safely handles NoneType values. While it does require understanding the code flow, the error described is clear, the fix is straightforward, and only requires a minor change to the conditional in one place. The problem is slightly non-trivial as you need to understand why index can be None and how the logic is affected, but it would take most experienced engineers less than an hour to implement and test the fix, given the clarity of the report.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues\u2014this sample is suitable for the benchmark. The issue and fix relate to a concrete bug, the description is clear, and test coverage for the regression is possible. It targets a nuanced edge case without relying on any information outside the provided description.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16667": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes precisely how to reproduce the crash using the SelectDateWidget with excessively large integer values for the year (or any date component), explains the reason for the error (unhandled OverflowError in datetime.date(int(y), int(m), int(d)) when user input is too large), and pinpoints the relevant area of code (SelectDateWidget.value_from_datadict function in widgets.py). The expected outcome is implicit and standard: user-supplied date input should not cause server errors, and invalid/big numbers should be gracefully handled as invalid data rather than causing crashes. This gives all information required for a successful solution, such as where to make changes and what scenario must be prevented.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires a straightforward try/except guard around the code where the date is constructed from user input in SelectDateWidget.value_from_datadict, catching OverflowError and handling it as invalid input. The problem is clear, concise, and well-localized, requiring minimal interaction with the rest of the codebase. The fix is not entirely trivial because it does require knowledge of Django Forms internals and awareness of Python's OverflowError, but it's a standard defensive coding pattern likely resolvable in under an hour by a competent engineer with minimal familiarity with Django's widget code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional issues with the sample. The bug is precise, easily reproduced, and the solution/test interaction is standard for this type of coding benchmark. The test update is clear and directly linked to the described bug. This is a strong candidate for benchmark use.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16950": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified, giving a concrete example (Django 3.1.3, Python 3.8.5, the relevant models and their fields, and the admin inline being used). The problem is described as an exception that occurs when adding a SubThing inline while creating/editing a Thing, with the value of \\\"id\\\" set to null. The exact stack trace is linked but not included in the issue, which limits direct debugging. However, the core behavior and rough conditions are reproducible with the model and admin setup described. An experienced engineer could reasonably recreate the scenario and observe the bad behavior, though some time may be spent recreating the setup. Further, the expectation for \\\"correct\\\" behavior is implied\u2014to not trigger an exception and to treat UUIDField and its default correctly in this scenario\u2014but the full expected behavior of Django in UUIDField inlines is not thoroughly stated, requiring some assumption. So, there is still a sensible interpretation but some blanks to fill about precise requirements and what the user expects as \\\"fixed\\\" behavior.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this issue requires understanding Django's admin, inline model forms, field defaults, and how primary/foreign key assignment works under the hood. The code change, while relatively small, demands tracing formset and model logic and possibly writing or adjusting tests to capture nuanced behavior around auto-generated keys and how UUID defaults are handled. Identifying the precise point where the wrong assignment occurs isn't trivial and involves reading and debugging Django internals, potentially across multiple files. Reproducing the minimal example alone might take some effort, and writing high-quality tests for edge cases (as seen in the patch) could take significant time.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues\u2014the sample fulfills the required structure for benchmarking: reproducible bug, clear if slightly under-specified context, meaningful code and test changes. The link out to a stack trace is unfortunate, but the problem is described in enough detail that the test and patch confirm the core scenario.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-20859": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly explains the bug: calling legend() on a SubFigure raises a TypeError, whereas it is expected to work and display a legend (just like it does for Figure or Axes). The reproduction steps are concrete, and a precise code snippet is provided to elicit the failure. The traceback pinpoints where in the code the problem arises, and the reporter even suggests exactly where (a type check around L437 of legend.py) the problem lies, hinting at replacing 'Figure' with 'FigureBase'. The expected behavior is stated\u2014calling legend() on SubFigure should work and show the legend for the axes. There are no ambiguities in requirements for a successful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small but non-trivial modification: a type check needs to be updated in the relevant code section to accept SubFigure (which likely inherits from FigureBase) as a valid parent for Legend, not just Figure. Locating the right place to make the change only takes a bit of reading (helped by the traceback and location hint), and the actual code change is minimal (swap a class in an isinstance check). Minimal new logic is required, but an understanding of the library's object model is necessary. Testing the change is straightforward because a reproducible test case is given.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected. The issue is clear, testable, and the patch and test changes directly correspond to the stated problem and requirements. This makes it suitable for the benchmark scenario described. The only note is that the hint about the fix being a type check change somewhat trivializes the problem, but the requirement is about specification and solvability, not the difficulty of the fix itself.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-22719": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is quite clear. It provides a minimal reproducible example, details about the expected and actual outcomes, and some investigation into the cause (catching exceptions too broadly, issues with handling empty data structures with unit converters). The reporter also speculates about the root cause and references code paths in the stack trace. However, there is some ambiguity in the precise desired behavior: Should plotting empty data always work silently, or is it only about suppressing the deprecation warning? The user outlines two possible expected outcomes, but doesn't strictly demand which to implement. There's also reference to changes in the Matplotlib API that could be interpreted in multiple ways. On balance, an experienced engineer could sensibly decide what to implement, but there are minor blanks to fill in regarding the precise preferred behavior.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix required adjusting the conditional checks in a couple of functions to special-case empty data structures, preventing unnecessary warnings or errors. The code change is small, but determining exactly where to make this change and the correct logic requires a bit of thought and familiarity with NumPy/array-like types and how Matplotlib unit handling works. However, no substantial algorithmic change or deep redesign is needed\u2014just some concise fix after understanding the control flow.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are noted. The issue provides a clear reproducible scenario and there is an associated test. The example is directly relevant to evaluating an engineer's ability to diagnose and fix subtle but concrete library issues.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear summary of the problem, a precise sequence of steps to reproduce (using code), the actual and expected outcomes, and sufficient context (including affected function/class names like get_backend(), rc_context, and plt._pylab_helpers.Gcf). It is explicit about what should NOT happen: calling get_backend() must not clear figures from Gcf if those figures were created in an rc_context. An experienced engineer would be able to understand what constitutes a successful resolution: fix the clearing of figures in this edge case. The presence of detailed reproduction steps and assertion makes the requirements clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue itself is well-bounded and focuses on the preservation of figure references when using rc_context and get_backend. The underlying problem relates to how rc_context handles the backend parameter specifically, which is subtle, but the solution is localized\u2014modification to avoid resetting or deleting 'backend' when rc_context exits. This requires an understanding of how rcParams and backend state management works in matplotlib, but should not require major code changes or wide-ranging rewrites. It is likely achievable in 15-60 minutes for an engineer familiar with the matplotlib codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major blockers for using this issue in a benchmark. The patch and the test give confidence that the issue is tractable as a coding benchmark, and there is only a small risk that some matplotlib-specific idioms may slow initial progress for the unfamiliar, but the core problem and solution scope are clear.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23476": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and includes clearly articulated sections: a bug summary, code for reproduction, actual and expected outcomes, environment information, and setup details. The reproducible code instructions are clear, and the expectations (that dpi should remain constant on reload/unpickle) are explicit. It is apparent from the description that the bug is about the 'dpi' value doubling on every unpickle on an M1 Mac and that a correct solution should ensure that unpickling a figure does not alter its dpi unexpectedly. An experienced engineer would have no trouble knowing what the correct behavior should be and how to validate their solution. The only minor blank is the root cause, but the problem and its correct resolution behavior are specified without doubt.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"I chose a difficulty level of 1 (15 min - 1 hour) because the issue is well-scoped\u2014a single well-defined bug with clear instructions, and the solution is localized. Determining the cause may require some inspection of pickling/unpickling logic and looking into how dpi or device pixel ratio is stored/restored. Once identified, the fix\u2014modifying how dpi is persisted or restored\u2014is likely small and only needs careful attention, not extensive rewriting.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The description, reproduction steps, and expected behavior are unambiguous, making this an excellent sample for evaluating coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24026": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: the user describes the expected behavior (color cycler references like 'C0', 'C1', etc., should work consistently across plotting functions, including stackplot) and the current behavior (stackplot throws a ValueError if given a 'C'-style color reference). The example illustrates exactly how the error manifests, and provides an explicit use case and an error traceback. This is enough for an experienced engineer to reproduce the issue and understand what needs to be fixed: stackplot should accept and correctly process color arguments of the form 'C0', etc., just like other plotting primitives. This provides a clear and bounded scope for a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is rated as 1: 15 min - 1 hour. The change is localized to a single plotting function. The bulk of the work is in understanding how the color cycler is used in the context of stackplot and why it doesn't accept the 'C' color syntax. Once that's clear (which the traceback and issue description help with), the code change just updates how colors are handled inside stackplot. The fix involves replacing the use of set_prop_cycle with a cyclical color iterator. The engineer would need to understand the usage of itertools.cycle, how the property cycler interacts with color references, and update method calls accordingly, but this is not a deep architectural change nor a multi-file rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The sample gives a clear context and an expected behavior, with a reproducible error. It fits well for benchmarking coding ability related to familiar libraries (matplotlib), error traceback interpretation, and fixing small but precise functionality bugs. The test patch also clearly demonstrates the bug is eliminated and shows the correct usage.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24149": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly states the context and specifics of the bug: calling ax.bar with only NaN data for x and height causes an exception in matplotlib 3.6.1, whereas it worked as expected in 3.6.0. The way to reproduce the bug is specified with details about the code to use, what fails, and the expected (prior) behavior. There is a traceback, and the exact location of failure in the code is also easy to map from the stacktrace. The relevant function appears to be _convert_dx in axes/_axes.py, and the necessity is to handle cases where no finite values are present. The problem is fully reproducible and the expected behavior is stated: the call should not raise, and should produce a BarCollection. Given this, a developer can meaningfully attempt a fix informed by the example and the expectation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires knowledge of numpy, matplotlib internals (particularly how ax.bar and its dependencies handle data arrays), and Python exception handling. The actual patch is relatively small, entailing catching a StopIteration when attempting to find a finite element, and falling back to the first element if there are none. The involved reasoning is not trivial but also not complex, and the change mainly adds two small exception handling blocks to an internal function. This should take an experienced engineer less than an hour to analyze, determine where the failure occurs, and apply and test the fix, assuming some familiarity with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be any other major issues with this sample for evaluating coding ability. The description, expected and actual outcome, and context are very clear and actionable. The problem is reproducible, and the requirements for a correct fix are known (matching the previous behavior and not raising an exception for all-nan input).\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24970": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear summary of the problem: deprecation warnings in NumPy 1.24 when handling colormap indexing in matplotlib with certain input types. It includes a concrete code example that triggers the bug via a specific function call, along with the actual warning messages from the code. The desired outcome is explicit (\\\"No warnings\\\"). Relevant context such as OS, Python, and package versions is given, but even without these, the issue is reproducible and the expected behavior is unambiguous. The engineer is told exactly what constitutes a successful solution: the code-path that produces these warnings should, after the fix, succeed silently. Since the affected code is pinpointed by the warnings (colors.py:730+), it's straightforward for an experienced contributor to locate and address the problem. This is well-specified for a benchmark.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue can be understood and fixed by an experienced engineer in 15 minutes to 1 hour. The warning message and code sample make it clear what usage pattern is broken, and the warning points to the exact lines in the code where the problem arises. The engineer needs to adjust how the conversion to integer occurs to avoid the deprecated behavior (i.e., not convert out-of-bounds Python integers directly to unsigned integer arrays). This involves understanding dtype handling and knowing how to cast or clip appropriately, but does not require a rewrite or architectural changes\u2014just a one or two-line targeted fix within an existing function, plus adding tests to ensure the warnings are gone and behavior is correct. The level of investigation is modest and the fix is neat and limited in scope.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is clear, has a well-defined scope, and is suitable for evaluating coding ability with respect to debugging, dtype awareness, and backward compatibility with evolving libraries like NumPy. The only very minor note is that knowledge of NumPy deprecation practices is helpful, but the sample remains appropriate for the benchmark task.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25311": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description here is quite well-specified. It gives a clear bug summary, describes how to reproduce the bug\u2014including the specific sequence of using matplotlib to create a figure, add a draggable legend, and then attempt to pickle the figure. It enumerates the input data, the code actions, and the expected error. The expected outcome is also directly stated: 'The figure should be pickled successfully without raising any errors.' The underlying cause (related to the presence of unpicklable references within objects when the draggable legend is used) is demonstrated by the error message. There is little room for misinterpretation of what the successful solution should achieve: the engineer needs to ensure that figures with draggable legends (and by implication, annotations) can be serialized with pickle without error. Any missing context about the relevant classes and internals is expected to be filled by someone familiar with the codebase. Thus, this is a clear and well-specified issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue is relatively contained. It requires understanding why the 'FigureCanvasQTAgg' (or in general, the canvas) is being pickled when it shouldn\u2019t be, and then changing the object structure or how references to the canvas are stored to prevent this. The actual fix (making the canvas a property rather than a direct attribute) is small (one or two lines), but it does require a bit of familiarity with how object serialization works and how matplotlib manages references between figures, legends, and canvases. An experienced developer would likely spend most of the time confirming the source of the issue and testing, not writing code. The presence of a precise test case speeds this up considerably. Thus, 15 min - 1 hour is reasonable.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues stand out. The sample is clean, the scope is clear, and the success metric is unambiguous (successful pickling of a figure with a draggable legend). The environment and versions are specified. The scenario is realistic and representative of real-world bugfixing tasks in a mature library. The only possible complexity is in understanding how pickling interacts with GUI backends, but this is an expected part of the task.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25332": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-structured. It provides a concise bug summary, precise reproduction steps (including code, plotting details, and the method calls used), the specific error encountered during pickling, and what the expected behavior is (successful pickling of the figure). The context provided (specific Matplotlib version, OS, etc.) makes it clear that the reporter expects align_labels should not break pickling of figure objects. All necessary information to understand and attempt a fix is present and clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Although the details of the bug require some inspection of how weak references are handled during pickling, the reproduction is straightforward, the cause (failure to pickle due to a weakref object) is well-indicated by the traceback, and the fix is localized to implementing proper state handling for weak references in a single class. This requires some knowledge about Python pickling and weakref mechanics, but it\u2019s a contained fix that should take less than an hour for an experienced engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is suitable for use. The only minor challenge is that full understanding requires some familiarity with Python's pickle and weakref modules, but these are common for developers working on serialization or library code. No major blockers for benchmark use.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25479": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is quite well described: it provides clear steps to reproduce, an explicit explanation of when things break (registering a colormap under a key name different from its internal name and trying to set the colormap via pyplot), and diagnostic output. It also analyzes the root cause (matplotlib maintains two separate lookup tables, one for the internal colormap name and another for the registered key). The desired/expected behavior is implied: registering under a name should make the colormap usable under that name, even if its internal name differs. The text includes all technical concepts, function names (e.g., register, set_cmap, get_cmap), and context for creating and registering colormaps. There remains a small blank about whether the solution should always synchronize colormap names or just on registration, but the direction is clear and the steps to reproduce and success criteria are concrete. This merits a rating of 1, as a sensible and well-motivated fix is clearly achievable from the issue text.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue, while requiring understanding of how colormaps are registered and used in matplotlib, only requires changes to a small amount of code, primarily relating to the registration logic and possibly colormap equality. The root cause and expectations are clearly described in the issue, and the code patch is quite concise, mostly involving updating the name attribute on registration. An experienced engineer familiar with matplotlib's architecture would solve this in less than an hour, most of which might go into validating assumptions or running the relevant tests rather than writing code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present; the issue is reproducible, actionable, and the required fixes are within scope for an individual engineer. There is a clear link between the described problem and the part of the API to be changed. The only minor ambiguity is whether the fix should be more general, but the sample as provided is perfectly sound for coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-26291": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The reporter provides a minimal summary, a clear code reproduction scenario, a full error traceback, and expected/actual behavior. By following the instructions, an experienced engineer can reproduce the problem using the provided code, Python, and Matplotlib versions. The core issue is an AttributeError when calling inset_axes. The traceback indicates that a 'NoneType' object has no attribute '_get_renderer', which occurs during the figure rendering. The required behavior (inset axes working as per the demo) and the specific circumstances under which the bug occurs (using Matplotlib 3.7.2, rendering with tight layout in a Jupyter lab environment) are described clearly. No critical information is missing for debugging and addressing the bug in mpl_toolkits.axes_grid1.inset_locator.inset_axes.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This bug is self-contained and would not take long for an experienced developer: it requires tracing through the call stack, understanding when renderer can be None, and defensively initializing it if needed. The patch is only a few lines in a single method and does not require extensive refactoring or changing multiple files outside of the test. However, a bit of thought is needed to ensure the renderer is initialized correctly, and the engineer should verify via reproducing/fixing the bug with a test. Hence, the time is typical for a small bug fix (15 min - 1 hour).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other significant issues. The bug is specific, reproducible, has a clear expectation, and is suited for evaluating the ability to reason about a stack trace and provide a targeted fix in a scientific Python codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-1724": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly describes the error that occurs (UnicodeDecodeError when passing a unicode string as the HTTP method in Python 2.7.2 using requests), provides exact code to reproduce the bug, the expected outcome (should work the same as when passing a standard string), details the error traceback, and even hints at the root cause (unicode method infecting the header, sessions.py:313 with req.method = method.upper()). The problem scope is clear enough for any engineer to provide a fix without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small change: converting the method parameter to a string type before using it, ensuring compatibility with the rest of the Python 2 codebase. While a bit of thought is needed to figure out the best place for the conversion and to ensure all usages are covered, it does not require editing multiple files or substantial rewriting. Thus, an experienced engineer should solve it in 15 min to 1 hour after familiarizing themselves with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue provides a clear reproduction scenario, and the solution is straightforward. The sample is appropriate for benchmarking coding ability involving unicode/str type handling in Python 2.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-5414": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"This issue is well-specified and clearly describes both what is wrong and what needs to be fixed. The core bug is that when trying to fetch a URL such as 'http://.example.com', a UnicodeError is raised rather than the intended InvalidURL exception. The expected behavior is spelled out, including the exception type and message. There are reproduction steps, the actual vs. expected results are explicit, and the context (library versions and OS info) is provided. Any experienced engineer familiar with the codebase would know to look for URL validation in the handling code, identify where InvalidURL should be raised, and cover this with an appropriate test. The desired error message and exception are unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is small but non-trivial: it requires identifying where an unintended UnicodeError is raised and converting this into an InvalidURL exception with a specific message instead. The main thought is tracking where IDNA/hostname validation occurs, catching the right error, and properly translating it. The code change is minor (essentially catching a UnicodeError and raising InvalidURL), and adding the relevant tests, but it does require a modest understanding of the code path and error handling in the requests library. Overall, it would likely be handled in 15-60 minutes by someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being used to evaluate coding ability. The issue is well-formulated, contains clear expected behavior, and simple reproduction steps. It also describes actual vs. expected errors explicitly. No required domain knowledge beyond basic Python and HTTP libraries. The only minor caveat is that the exact error message is specified, so candidates should pay attention to this detail.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3151": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It contains a concise summary of the observed behavior when using xarray's combine_by_coords with non-monotonic but identical coordinates, a code sample that reproducibly triggers the error, and a pointer to expected behavior based on the API documentation. The problem is entirely self-contained within a specific function (combine_by_coords), and the scenario is described in a way that an engineer can precisely understand what is breaking and what needs to be different: non-monotonic but identical-by-value coordinates should not cause an error if they are bystander (i.e., non-concatenation) dimensions. All of the necessary context (inputs, error, intended behavior, and reference to documented API) is given. No further clarification is strictly needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core of the issue is a modest logic error in how combine_by_coords checks monotonicity across dimensions. The resolution requires understanding the purpose of the monotonicity check, distinguishing between coordinates that are concatenation dimensions and bystander dimensions, and modifying the logic to only enforce monotonicity for the relevant axes. This requires some familiarity with xarray's internal data model and process for combining data, and careful reasoning about axes and dataset shapes, but the fix is targeted and does not require extensive rewrites. For an experienced coder familiar with xarray, this is a 15-60 minute change including writing a test, given that the core logic is updated in one spot and both cause and effect are tightly scoped.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues. The sample is appropriate for benchmarking. The issue is understandable and well-scoped, and both input and expected output are precisely specified via code. The link between the observed error and the required logic change is clear.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3677": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly states that merging a DataArray into a Dataset via the Dataset.merge() method fails with a specific error, while the top-level merge() function succeeds. The error traceback is provided, indicating the cause is an AttributeError because the code expects something with an `.items()` method, but receives a DataArray. A minimal example is described (Dataset with variable 'a', DataArray named 'b'), and the expected outcome is clear: after merging, both variables should be present in the combined Dataset, as is achieved by the top-level function. This provides a direct, unambiguous guide for a solution: ensure that Dataset.merge(DataArray) works the same as merge([Dataset, DataArray]).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a straightforward bug fix requiring knowledge of how Dataset.merge and the top-level merge function work. The solution is a small change (one line) to ensure that, within Dataset.merge, a DataArray argument is converted to a Dataset as the top-level merge does. The change is easy to implement once the correct spot is identified. Understanding, coding, and verifying the fix would take 15-60 minutes for someone familiar with the codebase and xarray's merge semantics.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues with using this sample for evaluating coding ability. The specification is clear, the change is well-bounded, and the issue is of a type (API consistency and method robustness) that is common in real-world projects.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4094": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It provides a minimal reproducible example (MCVE) of the code that triggers the bug, describes the expected versus actual behavior, and supplies details about the error encountered (including the specific Exception and message). The issue clearly states that stacking and unstacking should be a lossless round-trip where the dataset before stacking matches the one after unstacking. The context (xarray, DataArray, Dataset, stacking/unstacking functions and their expected behavior) is fully expressed. An engineer working only from this issue description and codebase access would know precisely what constitutes a correct solution and how to verify it. There is no evident ambiguity or missing requirement.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized and requires understanding of how variable selection and dimension handling work in the to_unstacked_dataset method. The solution consists of a small adjustment to how selection is performed inside a loop\u2014changing an argument to ensure dimensions are appropriately dropped. An experienced engineer familiar with this codebase (or with some familiarity with NumPy/xarray indexing) could read the MCVE, reproduce the error, comprehend the source of the shape/dimension problem, and produce a solution within 15 minutes to 1 hour. It is not a trivial search-and-replace and requires a bit of thought about the proper API usage, but the surface area affected is small and the logic is isolated.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected. The sample is directly relevant and tests practical debugging abilities, including reading minimal examples, interpreting errors, and applying a minimal targeted fix. All necessary contextual information is present for the benchmark setup.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4695": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The reproduction section gives a concrete example, including explicit code for creating two DataArrays with specific dimension names. The difference in observed and expected behavior between the two arrays is clearly described, including both the code to reproduce the error and the expected results. The problem description explains what goes wrong and gives details about the misleading error message, pointing toward a likely cause. No substantial ambiguities remain: an engineer would understand what user behavior is intended to be supported, what fails, and why\u2014making the requirements for a fix clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial change. The engineer must trace how dimension names are propagated and how the .loc method interacts with keyword arguments, and ensure no name collision occurs between dimension names and sel parameters. The code change is small (a single function line), but identifying the root cause requires some understanding of how selection operates in the codebase. For an engineer familiarizing themselves with xarray, this could reasonably be solved in under 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a good candidate for benchmarking: the bug is clearly reproducible, well-isolated, and the scope is limited to a single feature and case. The stack trace and version output are helpful but not essential for understanding or fixing the bug. There are no sensitive or ambiguous elements.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6599": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"This issue is well-specified. The reporter clearly describes a regression: when using `polyval` with timedelta64 coordinates, the function yields incorrect (massively inflated) results in the latest unreleased version of xarray, whereas it worked as expected in version 2022.3.0. The reproduction steps are explicit, describing exactly how the array and its coordinates are prepared, the expected outputs, and the observed erroneous behavior. Expectations are clear: the stable and latest `polyval` should yield identical/correct outputs, with sample output magnitudes provided for both the correct and erroneous cases. There is also an environment specification. An experienced engineer would have enough information to identify, reproduce, and test the bug, and to determine what must be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively straightforward bug fix involving proper handling of timedelta64 dtypes, which were previously mishandled after a recent code change. The primary task is to ensure timedelta64 coordinates are appropriately converted to numeric types for the polynomial evaluation. The fix (casting timedeltas to float within a helper function) is specific and localized. An engineer familiar with the xarray codebase would spend most of their time pinning down the problematic function and adding the type handling, followed by ensuring tests cover the timedelta case. This would involve less than an hour of focused work. There is no need to design new algorithms or refactor large portions of code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The problem is well-scoped, with clear expected behavior and explicit reproduction steps. The issue is suitable for benchmarking coding ability for a Python data library.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6721": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is quite thorough in describing both the experienced and expected behavior: accessing ds.chunks on a Zarr-backed xarray dataset ends up loading the entire array into memory when it should only inspect metadata about chunk sizes. Although there is no MCVE, the included error traceback, user expectations, and environment are detailed. The reference to the encoding attribute and relevant methods (like Variable.data and Dataset.chunks) help clarify what should and should not happen. However, without a concrete small reproducing code sample, some assumptions may need to be made about test setup, but it's sensible for an engineer to confidently identify and solve the problem as described.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is isolated to over-eager data loading on a specific property access (ds.chunks). The core change is a targeted fix (a single line change: using v._data instead of v.data to avoid triggering array loading). An experienced engineer familiar with xarray would likely identify this quickly after inspecting the code path described in the traceback, and could implement and test a solution in under an hour. Writing or updating a test is straightforward given the description and cause.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers that make this unsuitable. The absence of a minimal runnable example is a minor drawback, but the traceback and description are sufficient for someone with access to the codebase and test infrastructure. The regression test in the PR also clarifies expected behavior.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6386": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes a problem with the 'verbose' command-line option in Pylint: the short version ('-v') is currently implemented differently from the long version ('--verbose'). When using '-v', the program erroneously expects an argument and fails, while '--verbose' works as intended without requiring an argument. The reporter provides the exact error message, a minimal reproduction case, what the current versus expected behavior is, and the relevant Pylint version. An experienced engineer can understand exactly what is required: make '-v' behave like '--verbose', i.e., it should not expect a value and should enable verbose mode. There is minimal ambiguity here, and the solution is straightforward.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a small change that requires a bit of thought, as it involves reviewing the existing command-line argument parsing in Pylint and ensuring that the '-v' flag is properly mapped to the same behavior as '--verbose', i.e., an action-based flag that does not expect an argument. The patch affects several files, but the changes themselves are incremental and conceptually simple. The hardest part is tracing how arguments are added and handled, but the lines touched and the nature of the code fall into the realm of a small fix that would likely take 15-60 minutes for an engineer experienced with argument parsers and the codebase. The associated test update is also straightforward.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The problem is well-scoped and clearly articulated, and the resolution can be objectively tested via the test suite, which includes a regression test for the specific bug described.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6903": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well specified: it explains the problem (pylint crashing with --jobs=0 in Kubernetes due to incorrectly computed worker count from cgroup files), includes reproduction steps, relevant output, the code location affected (pylint.run._query_cpu), and what a successful solution should do ('computed worker count should never be zero', suggesting a fallback like max(computed,1)). Enough context and constraints are given for an experienced engineer to directly address and resolve the issue without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small code change involving careful handling of a specific edge case in the function computing CPU counts. The engineer must interpret cgroup settings, understand why the calculation might yield zero, and add logic to handle it sensibly\u2014likely inserting a guard clause or fallback value. The fix is local and does not require major refactoring, but one needs to reason about platform specifics, so it takes a bit more than a trivial fix but should not take more than an hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The problem, expected behavior, and context are clear enough that the issue is suitable for coding benchmark purposes. The described test also accurately checks correct handling of the edge case, verifying that the crash is prevented.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-8898": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It explains that the 'bad-name-rgxs' option in pylint, when set with a regex containing a comma (such as '(foo{1,3})'), is incorrectly split on the comma, leading to parsing errors and a crash. The reproduction steps are concrete, including the affected config, the command to run, and the resulting traceback. The expected behavior is also described: valid regex patterns (including those with commas) should be supported, or at least a way to escape commas should be available. The overall context and requirement are specific and unambiguous, so it is clear what a successful solution should address.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is rated as '1: 15 min - 1 hour.' The problem, while requiring attention to detail, is conceptually straightforward for an experienced engineer. It involves altering how comma-separated regex patterns are interpreted so that commas within regex quantifier brackets are not treated as delimiters. The change is small and localized, as evidenced by both the gold patch and required tests. The engineer needs to implement custom parsing logic for these option strings, but neither the breadth of changes nor the problem complexity is high, provided the engineer is familiar with Python and general parsing and regex handling.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no reasons to exclude this sample from coding benchmarks. The test patch included checks for both correctness in splitting and error reporting, and the gold patch demonstrates clean, targeted changes. The sample fairly evaluates the candidate's ability to read, interpret, and implement requirements related to string parsing and regex, which are widely applicable tasks.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10051": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the problem: after calling caplog.clear(), caplog.get_records() no longer tracks new records and does not get cleared, causing inconsistency between caplog.get_records() and caplog.records. The issue further provides a reproducible example within a pytest test, and it points to relevant segments of code (e.g. caplog.records vs caplog.get_records and the difference in handling by caplog.clear()). This is sufficient information for an experienced engineer to know what is broken and what needs to be changed to make the caplog.clear() operation result in the expected behavior: clearing both records and get_records(). The expected end state is that, after clear(), caplog.get_records() returns an empty list and receives subsequent records as expected.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change requires understanding how the logging capture system in pytest works, how caplog.get_records() relates to caplog.records, and updating the clearing logic so that both are consistent and do not diverge after clear(). While it requires reading and understanding the code flow in a moderately complex area, the actual changes to fix the issue are relatively small. The gold patch simply moves the clearing logic such that both records and the handler's state are reset appropriately. This is a classic example of a small but non-trivial bug fix in application logic, likely solvable in less than an hour for a familiar engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The issue is well scoped, contains all necessary information for a solution, and is suitable for evaluation of coding ability. The provided test also checks the expected behavior after the fix, aligning with the scenario described in the issue.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10081": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It details the bug (unittest.TestCase.tearDown being executed for classes marked with @unittest.skip when running pytest with --pdb), the conditions under which it happens, a minimal reproducible example (including code snippets), observed vs. expected behavior, as well as the specific Python, OS, and pytest versions used. The relationship to a previous issue (#7215) is also specified. There is minimal ambiguity about what needs to be fixed: tearDown (and setUp) should not be called for test classes that are skipped via @unittest.skip even when --pdb is used. This is a clear, actionable problem statement for an experienced engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is '15 min - 1 hour'. The bug is non-trivial but narrowly scoped: it requires understanding of how skips are handled by pytest in conjunction with unittest.TestCase and pdb. The engineer needs to investigate where and why tearDown is being called and ensure correct skipping logic. However, the required fix is localized (one conditional check in the codebase), and the expected change is small (the patch is only a few lines). The provided test case makes it straightforward to verify the fix. Thus, while some care and thought are needed, it is not a substantial or multi-file rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no blockers or other issues. The problem is clear, the solution is verifiable through tests, and the scenario is realistic for benchmarking coding ability. The Python version, pytest version, and minimal reproduction instructions all remove ambiguity. This is a suitable sample.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5262": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It outlines the bug (incorrect .mode property with a 'b' in it for _pytest.capture.EncodedFile), the consequences (TypeError due to mismatched expectations between bytes and str), the concrete context (using youtube-dl with pytest's output capture), the exact exception and stack trace, versions of the relevant packages, and a minimal reproducible example. The affected class is clear, and the nature of the required change (the .mode property should not contain 'b', as the file-like object expects str not bytes) is explicitly described. An engineer familiar with pytest could reasonably deduce the change required and where in the codebase to look. The sample is ideal for a benchmark, as it requires a code change with a clear before-and-after behavioral difference that is independently testable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial code change. The engineer must understand the interaction between _pytest.capture.EncodedFile and its underlying buffer, recognize how the .mode attribute is used by clients (like youtube-dl), and ensure that returning a mode without 'b' is correct and does not introduce regressions. It should take 15 minutes to an hour for someone with moderate familiarity with the codebase and typical file-like object conventions in Python, especially since the stack trace and context are very clear. The fix itself, as shown in the gold patch, is a short property override that removes 'b' from the mode string, with a targeted new test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The issue is clear, has a well-defined expected behavior, and includes ample context (exact error, test scenario, environment details, etc). The solution is sufficiently involved that it tests code understanding and not just superficial knowledge. The test change confirms that the behavioral change can be verified mechanically. This is a strong benchmark candidate for evaluating coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5631": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: the user describes a regression/failure when using unittest.mock.patch with a numpy array as the 'new' value in a decorator. The exact decorator and usage are provided, and the failure traceback is included, clearly showing that the issue arises in _pytest/compat.py in the num_mock_patch_args function. The root cause is explained: 'p.new in sentinels' fails for numpy arrays because numpy arrays cannot be compared for truth with 'in'. The user even identifies the offending commit, and the minimal reproduction (using a numpy-like object as the 'new' value for a patch) is straightforward. The requirements for a successful solution are clear: pytest should be able to collect and run this type of test without error, just as it did in a previous version (3.1.3). The correct behavior is restoration of compatibility with numpy arrays (and likely other objects with similar behavior), avoiding the ValueError.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change is localized\u2014the problem is specifically in the comparison used within num_mock_patch_args. The solution replaces an equality check that can trigger errors with an identity check, which is both faster and robust for sentinels. An experienced developer would need to trace through the traceback, understand the breakage introduced by the comparison logic, and update the problematic line to use 'is' semantics. This requires some Python introspection and knowledge of numpy and mocking, but the problem is clear and the change involves a small code area.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is well-suited for a benchmark of coding ability. It combines debugging from a traceback, understanding Python object semantics and special cases (like numpy truth handling), and making a targeted fix. The issue relies on having a basic understanding of unittest.mock.patch and numpy, but this is within the expected skills for most Python engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified but assumes familiarity with how pytest handles exception chaining and serialization, and the relationship between xdist and core pytest report serialization. The problem is illustrated with minimal but concrete examples, contrasting output with vs. without xdist, and specifying that chained exceptions are not fully serialized when run through xdist. However, it doesn't specify exactly what internal representation or data structures must change\u2014this will require the engineer to examine how pytest serializes test reports and where exception chain information is lost, but there is a sensible interpretation and clear goal: make serialization/deserialization include chained exceptions. The filename(s) involved are suggested through references to pytest internals.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This change required substantial understanding of pytest test report internals and how both serialization (to JSON, etc) and chained exception representation works. The fix required refactoring existing (de)serialization logic, adding new functions to handle the ExceptionChainRepr structure, and updating test infrastructure\u2014several files are involved. This is a medium-complexity, nontrivial change that would likely take a few hours for an experienced engineer to work out, understand the issue, design a robust fix, and validate with proper tests. It is more involved than a quick fix or light patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues not already discussed. The problem is well-contained in test report serialization and is meaningful as a coding benchmark; it requires both code comprehension and nontrivial implementation. All needed context (including before/after behavior) is in the issue and the associated patches.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5809": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and clear. It identifies a specific bug: using the 'lexer=python3' parameter when posting to bpaste.net causes HTTP 400 errors for some types of content, and changing 'lexer' to 'text' resolves the issue. The file and the relevant function in the code are unambiguously identified ('src/_pytest/pastebin.py', lines handling the 'lexer' parameter), and a clear reproduction scenario is provided. The rationale for the change is explained: pytest output isn't strictly Python code. No extraneous interpretation or guesswork is required beyond making this parameter switch.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is straightforward: substitute the value of a POST parameter in an API request. An engineer needs to trace the use of the --pastebin flag to the relevant function, and update one line (change 'lexer' from 'python3' to 'text'). Familiarizing oneself with the project conventions and ensuring legacy behavior doesn't break (such as for 'expiry') might take a short amount of additional time, but overall this is a simple, targeted fix requiring limited research. The hardest part is confirming that the output is indeed arbitrary text and not Python, which is covered well in the issue text.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues observed. The issue is concise, technical, and self-contained. The expected behavioral change and the reasoning behind it are both clear. No ambiguity or external dependencies exist that would hinder a competent engineer from implementing and testing the change as described in the issue.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5840": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides key details: the problem only occurs after upgrading pytest from 5.1.1 to 5.1.2, and appears to be related to the handling of path casing on Windows, specifically when importing conftest.py files. It contains an expected outcome (pytest enumerating tests without error) and the actual error (ImportError, ModuleNotFoundError: No module named 'python', with path casing differences evident). While the full technical root cause is not spelled out, there is a clear, sensible interpretation: recent changes in pytest's handling of file paths/casing cause an import failure on case-insensitive filesystems like Windows, which did not occur previously. An experienced engineer can reasonably deduce the requirements for a fix based on this information, even if some blanks about internal implementation details must be filled in. Thus, it is not perfectly specified, but is sufficient for meaningful work.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue is not trivial: it involves debugging changes to how paths are resolved and compared when loading conftest.py files on Windows, requires awareness of platform-specific filesystem intricacies (case sensitivity), and ensuring future compatibility. The fix involved removing a custom path normalization function and relying on pathlib.Path.resolve() for proper behavior, as well as updating multiple call sites and associated tests. Understanding the subtleties of path handling, identifying the right abstraction to use, and ensuring edge cases are covered can take a few hours, especially for engineers new to the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is sufficiently specified for the context, and there are clear acceptance criteria: path casing issues on Windows causing ImportErrors when collecting tests should be resolved, and tests confirm this. The main caveat is the engineer must be familiar with or able to quickly ramp up on py.path and pathlib quirks. Otherwise, the issue is appropriate for evaluating coding ability relating to filesystem/path handling across OSes.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6197": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified. It describes a regression in pytest 5.2.3 where __init__.py files are being collected/imported when they shouldn't be, resulting in assertion errors in certain cases. The steps to reproduce are clear, including how to create the failing condition and how previous behavior (in pytest 5.2.2) was different. However, it leaves some minor blanks for the engineer to interpret, such as the exact root cause in the codebase or whether any particular mechanism of package/module collection should be altered. But overall, an experienced engineer could interpret what a successful fix entails: restoring the behavior so that unnecessary __init__.py files are not collected/imported. Specific filenames, like foobar/__init__.py, and tools, like tox.ini and pytest, are referenced clearly.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires understanding pytest's collection/loader mechanisms and why behavior changed between versions, tracking how __init__.py files are identified and dealt with, and then modifying code to avoid unwanted collection of such files. The patch modifies the core logic related to test collection (involving multiple properties, functions, and classes inside src/_pytest/python.py), and the tests are both updated and newly added. Although the scale of changes is moderate, it definitely requires a solid understanding of the pytest architecture and could take 1-4 hours for an experienced engineer to correctly identify both the bug and the suitable changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues. The scenario and the technical requirement are sufficiently clear for a benchmark coding problem. The scope of the fix is tangible and measurable via the associated test changes. No external dependencies or blockers are evident from the issue description alone.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6202": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and provides all necessary information for an experienced engineer to make a meaningful attempt at a solution. The problem is described through a clear reproduction case (parameterizing a test with a string '..[' causes improper output formatting in pytest), and the expected vs actual behavior is illustrated. The issue follows through tracing to the precise problematic line in the codebase and then suggests a plausible fix (removing a .replace operation in a function). Although the original purpose of the problematic code line is unclear, the issue directly identifies what is broken, how to reproduce it, the relevant files and functions, and a recommended solution path, making it very actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires tracing the bug from user-visible output through several layers of code to a specific line, which is made easier by the detailed traceback in the issue description. The fix is simple\u2014removing a string replacement\u2014but understanding why it is acceptable to do so (or whether it might cause regressions) might require briefly reviewing code context and existing tests. For an engineer familiarizing themselves with the codebase, this is a small and focused change with low code churn, so the estimated time is 15 min - 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue, fix, and test patch are all clear and straightforward. There are no hidden requirements or dependencies that would diminish its suitability for coding ability benchmarks. The issue is self-contained, easily reproducible, and tests are appropriately modified to catch regressions. I consider this a good benchmark candidate.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7205": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The reporter clearly describes the environment (Python 3.8.2, pytest 5.4.1), the usage context (parametrized fixture with a bytes value, pytest run with --setup-show and byte-warnings promoted to errors), the precise behaviour (BytesWarning during fixture setup), a concrete error traceback (with references to src/_pytest/setuponly.py and the offending line), and an explicit suggestion for a fix (should use saferepr rather than implicitly str()'ing potential bytes in output). The success criterion is also clear: running the described test setup with these options should not produce a BytesWarning. File and function references further clarify where the fix should likely be made. An engineer should have all reliable information needed for a focused solution attempt.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a small bug fix: the problem is clearly localized (output formatting in src/_pytest/setuponly.py, _show_fixture_action). The code to change is identified in the traceback, and the suggested fix (saferepr for fixturedef.cached_param) is straightforward. While familiarity with pytest's codebase may be helpful, the required change is small, affecting only how a parameter is displayed, and test case requirements are precise. This fits in the '15 min - 1 hour' range, mainly due to navigating the code and verifying the fix but not involving substantial rewriting or cross-file interactions.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major concerns for benchmarking coding ability with this sample. The issue is clear, the success criteria are directly verifiable through a test, and both the code context and minimal reproduction are well-articulated. The required background (pytest, Python, bytes/str formatting) is appropriate for a strong intermediate engineer. No ambiguity, external dependencies, or unclear requirements exist.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7236": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and it is clear what unsuccessful and successful behaviors look like. The issue provides a concrete minimal code example using unittest.TestCase with both setUp and tearDown referring to an undefined variable, and marking the test with @unittest.skip. The reporter provides exact output from running pytest normally and with the --pdb flag, highlighting the problematic call to tearDown on skipped tests with --pdb, and points out specifically that this behavior changed between pytest 5.4.1 and 5.4.2, indicating a regression. The desired behavior (tearDown should not be called on skipped tests, even with --pdb) is unambiguously described and justified via observed difference versus previous pytest versions, making the specification strong enough for an engineer to know what code change is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a nontrivial but targeted change. An engineer needs to understand the interaction between pytest's handling of unittest.TestCase skipped tests, setUp/tearDown, and the --pdb option. The requirement is clear (disable tearDown for skipped tests when using --pdb), but finding the right place in the codebase (pytest's unittest plugin) and ensuring correct integration may require some careful reading and localized refactoring, as well as adding or updating tests to confirm the fix. This is above a trivial fix but would reasonably take less than an hour for someone familiar with the codebase, so it's a '15 min - 1 hour' task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The problem is self-contained, with sufficient example and diagnosis. The interaction with --pdb and skips is subtler than average, but does not require external context. The engineer would have adequate information from the issue description alone, and the solution scope fits typical code challenge settings as the behavior and expected change are clear. There is no ambiguity in test or implementation intent.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7324": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is somewhat well-specified but does require some inference. It clearly identifies that invoking 'Expression.compile' with the string 'False' on Python 3.8+ debug builds causes an interpreter crash, related to an internal Python assertion. While the expected high-level outcome is obvious (don't crash when given these keywords), it does not describe the intended behavior in precise functional terms. For instance, does 'False' (or 'True', 'None') need to be explicitly disallowed, or should they be handled specifically? The related Python bug is referenced, but its contents are not included, so only the information in the text can be used. Nevertheless, a sensible engineer would interpret this as needing to change Pytest's handling of 'False', 'True', and 'None' so as to avoid hitting the core interpreter assertion. Together with knowledge of the codebase, this is enough for a reasonable attempt at a fix. The mapping to which file and function are to be patched is inferable for an experienced engineer via the mention of 'Expression.compile'.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue involves addressing an interpreter-crashing assertion when compiling certain strings as expressions in Pytest for Python 3.8+ (debug builds), specifically for 'False', 'True', and 'None'. The patch indicates that the solution is to treat these keywords specially by prefixing their identifiers, and to adjust a matcher accordingly. This change is localized to a small number of functions in one file, and involves straightforward string manipulation and understanding the AST construction. For an engineer familiar with the codebase, this would generally take between 15 minutes and 1 hour as it requires some investigation, modification, and validation, but does not entail a large refactor or highly complex logic.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample for evaluating coding ability. The problem is specific, the required change is sufficiently non-trivial to test engineering skills, and the tests provided in the PR patch directly verify that the change addresses the bug. The problem involves understanding language internals and safe code modification practices, making it a suitable benchmark candidate.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7490": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clear. It provides a concrete code example that illustrates the expected behavior under pytest 5.x and the incorrect behavior introduced in pytest 6.0.0rc0. The description explains the dynamics of adding an xfail marker at runtime via request.node.add_marker(pytest.mark.xfail(...)), and observes that while this previously resulted in an XFAIL outcome for a failing test, it now produces a failure instead. The repro steps are minimal and direct, describing which file, which fixture, and the key code change. The explanation, observed versions, and command lines to run are all included and explicit. The required successful solution is to restore the old behavior so that dynamically adding xfail at runtime marks the test as xfailed, not failed. No external context or code navigation is needed to understand what a fix is supposed to achieve.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires a good understanding of pytest's test collection and execution lifecycle, marker application, and hooks. Fixing this means carefully locating where dynamically added markers are picked up (potentially in lifecycle hooks), ensuring dynamic xfail is recognized at the correct test execution phase, and confirming that the bug is actually due to a change in how/when markers are assessed. It involves editing core test running logic, likely touching more than a trivial function, and interacting with pytest's internal item store. The fix is more than a one-liner and demands a close reading of test lifecycles, thorough testing, and verifying edge cases. This would likely take 1-4 hours for an experienced contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues are present. The issue is clear, directly reproducible, has an unambiguous expected outcome, and is testable via automated tests. The user-oriented repro steps map directly to what would need to be addressed in implementation and covered in a regression test. No external dependencies or ambiguous requirements remain.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7521": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is extremely well-specified. The reporter gives a precise test scenario that demonstrates a regression in handling carriage return (\\\\r) characters in pytest 6.0.0rc1's capture feature, supported by concrete failing assertions and minimal reproducible code samples. The expectation is clear: captured output should preserve \\\\r characters and not convert them to \\\\n. The relevant test names, lines of code, and behaviors are detailed, making it crystal clear what a correct fix should accomplish. There is no significant ambiguity in the specification or desired outcome.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is relatively localized (how the capture subsystem handles line endings in stdout/stderr). The tests and the minimal reproducible example point directly to the faulty behavior and what a fix should yield. For an experienced engineer familiar with pytest's codebase, it would likely involve <1 hour of investigation and patching -- mainly confirming where \\\\r gets converted, and updating the relevant open/write calls or buffer settings to preserve newlines as expected. The required change is not trivial, but also not complex; it's targeted with clear test coverage. \",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other notable issues with this sample. The test coverage provided in the issue and the clarity of the failure make it suitable for evaluating coding ability in this benchmark. The scenario is realistic and technical, but not ambiguous or esoteric.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, well-structured, and specifies the exact problem: RidgeClassifierCV does not accept the store_cv_values parameter, although its documentation and related attributes indicate it should. Steps to reproduce are specific, and the expected and actual behaviors are directly compared. The requested solution (add store_cv_values parameter support to RidgeClassifierCV, as in RidgeCV) is explicit. References to class names are unambiguous, and the intended API change is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a typical example of a moderate API bugfix/feature addition in a mature codebase. The solution requires understanding the constructor of RidgeClassifierCV, propagating a parameter, updating documentation, and verifying/correcting the attribute logic, but the change is conceptually straightforward and only touches a few areas. For an experienced engineer familiar with the codebase, this should take between 15 minutes and 1 hour, including testing and verifying that the attribute and behaviors work as documented.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major omissions or blockers in this issue that would prevent its use for coding ability evaluation. The problem, context, and success criteria are very clear. No extra context is required, and no external documentation references are strictly needed, as all relevant behaviors and expectations are explained in the issue text.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10908": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clear about the required behavior. It gives concrete examples, specifically contrasting how CountVectorizer behaves when a vocabulary is provided versus when it is fit with data. The core requirement is straightforward: \\\"get_feature_names\\\" should not raise NotFittedError when a custom vocabulary was supplied during initialization, even if fit was not called. The description provides both the context and expected change in behavior. An experienced engineer would be able to understand both the problem and the expectations for a successful solution without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a small code change in the get_feature_names method to handle the case where a custom vocabulary is supplied. The patch indicates that this just involved checking or initializing the internal vocabulary\u2014no significant refactoring of logic was needed. An experienced developer familiar with scikit-learn\u2019s conventions could identify and implement this change (as well as write a relevant test) in less than an hour, though it is not entirely trivial because it involves correct handling of the vocabulary initialization semantics.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would preclude the use of this sample for evaluating coding ability. The requirements are clear, and the solution cleanly scoped to a single function/area of the codebase. The tests reflect the described scenario and verify the correct behavior.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-12585": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly describes the current problem: the clone() function in sklearn.base fails when an estimator's parameter is itself an estimator class rather than an instance. The error message and a reproducible example are provided, detailing how to trigger the issue. The expected behavior (clone should work and not error in this scenario) and actual behavior (a TypeError with a specific traceback) are explicit. The proposed code solution is suggested directly in the issue, and enough context is supplied (e.g., mention of clone in sklearn.base.py at line 51) for an experienced engineer to understand both the nature of the bug and the method of fixing it. No substantial ambiguities are present about the requirements, so a meaningful and correct solution can be implemented.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is small but requires understanding how sklearn's clone() works and under which scenarios parameters can be types rather than instances. The code change involves a minor logical condition update in the clone function (one line in base.py). However, it does require a deliberate thought process to decide whether it's safe to treat classes this way, and to ensure this does not introduce side effects elsewhere. Understanding the error, root cause, and verifying the fix makes it slightly more than a trivial task, but it should comfortably fit within an hour for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The problem, desired behavior, and failure scenario are clear. Sufficient technical context is provided, and the requirements are explicit and testable. The proposed solution is straightforward and safe for benchmarking coding ability. Test coverage can be reasonably added or inferred.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13135": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. It provides a concrete description of the problem, including which class is affected (KBinsDiscretizer), under what conditions it fails (strategy='kmeans', more bins), a reproducible example with specific input data, the error message, and what the expected behavior is (no error is thrown). An engineer can work directly from this description without needing further clarification, as all relevant contextual details, versions, and traceback are included. The nature of the failure (unsorted bin_edges for np.digitize) is also clearly identified.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This should be a 15 min - 1 hour fix for an experienced contributor familiar with the scikit-learn codebase. The root cause is clearly described (unsorted bin edges from kmeans), and the fix is conceptually simple (sorting the centers before using them to construct bin edges). The change only requires modifying a minor aspect of an existing function, rather than a substantial refactor or multi-file edit. Some care is required to ensure side effects are avoided, but the technical challenge is local and straightforward.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The issue is a perfect benchmark sample: clear, focused bug with precise repro and well-bounded required fix. The added tests clearly demonstrate both problem and solution. No ambiguity in requirements.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13142": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well specified and concrete. It describes an unexpected inconsistency between predictions from fit_predict and predict when using GaussianMixture with n_init > 1. The steps to reproduce are clear, with exact usage: create a random dataset, fit a model, compare outputs. The expected and actual output is shown, including useful debugging output and the relevant portion of the error message. Anyone familiar with scikit-learn and this class can immediately understand what's going wrong and what the correct behavior should look like (outputs of fit_predict and predict should be equal regardless of n_init). There is no ambiguity in the instructions.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix, as shown in the gold patch, is quite localized and conceptually straightforward for someone familiar with scikit-learn's GaussianMixture code. It involves ensuring a final E-step is done in fit_predict, similar to fit+predict, to guarantee consistency. This requires understanding the fitting and prediction flow, but is not a substantial rewrite or architectural change. Most of the time would be spent confirming intended behavior and tracing through the class methods, not writing code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is an excellent benchmark sample. The issue is precisely stated, easy for someone with appropriate background to act on, and there are no hidden requirements or external dependencies. The test scenario is also simple and clearly validates the desired behavior. No hidden gotchas or deal-breakers for the setup.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13328": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"This issue is well-specified. The reporter clearly describes the exact context (HuberRegressor from sklearn), the steps necessary to reproduce the error (creating a boolean-valued feature matrix from make_regression, and fitting the regressor), the actual vs expected behavior, and provides a minimal reproducible error message with traceback. The issue includes information about corresponding behavior in similar estimators (LinearRegression works as expected) and specifies that boolean feature matrices should be accepted by converting to float internally. All required information to implement, test, and verify the solution is present, and the patch/test diffs confirm the intended scope and solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem itself is clear and narrow in scope, focusing only on HuberRegressor's acceptance of boolean feature matrices, which should be converted to float types. The code change necessary is minimal: adjusting the dtype argument for check_X_y, which is a targeted fix. Understanding what to change does require knowledge of sklearn's input validation utilities and conventions, but it is a standard pattern; no deep algorithmic changes or significant debugging are required. For an engineer familiar with sklearn patterns, this would require some thought and codebase search, but should take less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no significant issues with using this sample for benchmarking coding ability. The problem requires reasoning about data types and working within sklearn's estimator validation patterns, which are realistic tasks for a contributor. The problem is self-contained and the provided test covers the fixed scenario.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13779": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides clear guidance on what is going wrong. It describes the expected vs. actual behavior, provides instructions on how to reproduce the problem (using Iris dataset, VotingClassifier, setting an estimator to None, and passing sample_weight to fit), and includes a clear error message. The failure context and expected behavior are specified: calling fit should skip None estimators and complete successfully, but currently it raises an AttributeError. This gives enough detail to identify the cause and the area of code to be fixed (handling sample_weight and skipping None estimators in VotingClassifier/VotingRegressor). An experienced engineer would be able to create a correct fix with this guidance.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is relatively straightforward for someone familiar with the codebase. It involves adding a simple check to skip estimators that are None when iterating through the estimators during fit when sample_weights are provided. The problem is easy to understand, the problematic behavior is easy to reproduce, and the code change required is minor (conditional check and continue in a for-loop), and limited to a single function. It took only a few lines of code in the actual patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues were identified. The sample is suitable for benchmarking coding ability: it is well-scoped, has a direct link from description to code change, and is easily testable.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14053": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and includes a minimal reproducible example. It specifies a function, export_text, that throws an IndexError when passing a single-feature DecisionTreeClassifier with feature_names provided. The reproduction steps are detailed enough that an experienced engineer would be able to replicate the error, understand its cause (indexing into feature_names with an out-of-bounds index), and create a PR to resolve it. There is no ambiguity over what needs to be fixed: export_text should handle trees with a single feature and not raise IndexError in that case.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves understanding how export_text works with feature_names and why it's indexing out of range for single-feature models. It likely requires a small guard or logic adjustment to handle undefined feature indices. An experienced engineer could identify and solve this within 15-60 minutes, as the scope of the change is limited to at most one or two functions and does not entail refactoring or substantial new logic.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue is well-suited for benchmark evaluation. The bug is clear, oriented on code correctness, and reproducible from the description. Only minor note: the version/environment section has unrelated error messages about Fortran that can be ignored\u2014they are not related to the bug in question. The testable outcome (no IndexError for single-feature trees) is unambiguous.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14087": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It provides a clear description of the error, along with explicit steps to reproduce the problem, including all parameters, random seeds, and hyperparameters. The observed and expected behaviors are described precisely. The error traceback and software versions are given. For an engineer familiar with scikit-learn, it is clear what a successful solution should achieve: prevent the IndexError and ensure that LogisticRegressionCV with refit=False works as expected in these settings. No essential information appears missing and the requirements are concrete.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This bug requires a solid understanding of the code flow in LogisticRegressionCV and understanding the conditions for refit, penalty types, and test coverage. The fix itself\u2014properly handling attributes based on penalty\u2014involves a few logical branches and not a major code rewrite. Locating the root cause, proposing the logical conditional, and updating attribute assignment is a small but nontrivial change for someone familiar with the code. Implementing and testing would fit comfortably in 15-60 minutes for an experienced engineer. The change affects a localized portion of the code and does not require redesigns or major refactoring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue and solution are both well-bounded. The main potential source of difficulty is navigating scikit-learn's codebase if one is unfamiliar, but the task itself is clear and self-contained, relying on modifying a few conditionals based on already well-documented behavior.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14629": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly states both the conditions that trigger the bug (using cross_val_predict with method='predict_proba' together with MultiOutputClassifier) and what goes wrong (AttributeError: 'MultiOutputClassifier' object has no attribute 'classes_'). It identifies a likely cause (the use of estimator.classes_ instead of mo_clf.estimators_[i].classes_), and provides a reproducible scenario (synthetic multilabel dataset, wrapping LinearDiscriminantAnalysis in MultiOutputClassifier, etc.). It also describes the expected result (NumPy array with probability estimates for each label) and the actual result (an error). No important information appears to be missing for an engineer familiar with scikit-learn internals.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is somewhat subtle, as it involves the interface and expected attributes of a scikit-learn meta-estimator (MultiOutputClassifier) and the downstream way cross_val_predict expects to interact with supplied estimators. However, with familiarity with scikit-learn and its class structures, the fix is relatively localized: adding or modifying the classes_ attribute to ensure conformity. The required work involves understanding the expected API, tracing through the class hierarchy, and adding a small amount of code. Likely <1 hour for a familiar or even moderately new contributor after understanding the context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues with this sample. The bug report is clear, the reproduction path is solid, expected/actual output is explicit, and the fix does not involve external dependencies. The setting is realistic for assessing coding/debugging ability on a software library such as scikit-learn.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14710": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite clear and detailed. It explains the specific context: using HistGradientBoostingClassifier with string targets and early stopping turned on. The problem is described in both general terms and with detailed steps to reproduce: create a synthetic dataset with string labels, fit the model, and observe the TypeError due to dtype mismatch between y_true and y_pred during scoring. The stack trace shows exactly where things break, and the expected behavior is unambiguous (the classifier should train to completion without errors). There is also a high-level suggestion for resolution, indicating that y_true needs to be encoded whenever the score is computed. The relevant code location is hinted at. Someone familiar with the codebase would know where to look and what needs to be modified.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is conceptually straightforward: ensure that the labels passed for scoring are consistently encoded and comparable, so that scoring does not mix integer and string labels. The code is localized to one relevant method, and while a bit of code reading is required to figure out where y_true should be encoded, this is a small but non-trivial change. It requires understanding the flow of early stopping and scoring in the codebase, but the main clue is provided. For an experienced engineer who is familiar with scikit-learn, this would likely take between 15 minutes to 1 hour, including testing. It is not a trivial one-line fix but it is not large or esoteric either.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be any major issues that would prevent this sample from being used in a benchmark. The issue is reproducible, the patch is self-contained, and the corresponding test covers the fix and regression. All the necessary context is present in the issue text. It is a realistic and suitable task for assessing coding ability in a machine learning codebase context.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14894": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It provides a clear sequence of steps to reproduce the problem, including details on the use of an SVR model, data setup, and the difference in behavior between dense and sparse matrix inputs. The description states the precise expected behavior (no exception should be raised and dual_coef_ should be an empty sparse matrix), the actual behavior (ZeroDivisionError), and includes a relevant code traceback. All necessary context about the module involved (sklearn.svm, _sparse_fit), the attribute affected (support_vectors_), and expected outcome is included. An engineer could directly determine what needs to be fixed based on the given information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires localizing the error within the _sparse_fit method, understanding how the dual_coef_ attribute is constructed for sparse data, and adding a check for the case with no support vectors. It is a focused code change confined to a specific function, and although it requires some understanding of sparse matrices and scikit-learn SVM internals, it is ultimately a small change to handle an edge case. An experienced engineer familiar with the codebase should be able to implement this within 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blocking issues are present. The issue is clear, testable, and corresponds to a meaningful bug fix and regression test. This sample is suitable for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25747": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well specified. It provides a minimal reproducible example: a DataFrame construction, a custom transformer, the conditions under which the bug is triggered (FeatureUnion with pandas output), and the difference in behavior between 'default' and 'pandas' modes. The exact error traceback is included so it's clear what exception is occurring and what causes the failure. The expectation ('No error is thrown when using pandas transform output') is explicitly stated. Any experienced engineer familiar with sklearn and pandas would have all the information needed to replicate, analyze, and attempt a fix for this bug based solely on the provided issue text and the described steps.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix required a small but non-trivial change to a core utility function, involving an if-condition guarding the assignment of a DataFrame index. The description makes the root cause relatively clear (index handling mismatch due to transform_output set to 'pandas'), so an experienced engineer, after familiarizing themself with _wrap_in_pandas_container, could resolve this in 15 to 60 minutes. The main skill needed is careful reasoning about DataFrame handling and FeatureUnion behaviors.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are apparent. The bug is clear and self-contained, and the test patch demonstrates the appropriate non-regression checks\u2014all making it a suitable benchmark for coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25931": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It contains a clear description of the error, including when it is triggered (fitting IsolationForest with a pandas DataFrame and contamination != 'auto'), the exact warning message, rationale for why it is unexpected, the likely cause (internals of IsolationForest), and version info. It also provides a complete reproduction script, expected/actual results, and context on why the warning is being triggered. This is enough for an engineer familiar with scikit-learn's codebase to identify and address the bug.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix primarily involves adjusting internal method usage to avoid input validation that strips feature names, and introducing a private method for scoring without repeated validation. This requires a bit of investigation to understand why the warning arises and care to preserve the API contract and validation consistency, but does not involve major refactoring, new APIs, or deep algorithmic work. For an experienced engineer familiar with scikit-learn, this is a small-to-medium change that should take between 15 minutes and 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The sample is suitable for inclusion as a benchmark. The test modification is clear, targeted, and checks only for the undesired warning. The description and expected outcome are precise; the codebase context is typical for a widely used library.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well specified: it describes a specific problem encountered when using the SequentialFeatureSelector class in scikit-learn with a custom cv argument (an iterable of splits from LeaveOneGroupOut), and provides clear steps to reproduce the error. The error trace is included, and the desired behavior is specified (should run without error). However, there is minor ambiguity in whether the expected solution is just to support all iterables of splits in general or specifically those coming from cross-validation generators, and what constraints (if any) on the \\\"cv\\\" argument must be enforced or documented. The general expectation, though, is clear and sensible for an engineer to implement.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively small change\u2014what is needed is for the SequentialFeatureSelector to handle cv passed as an iterable of splits, aligning its behavior with other scikit-learn components. This involves minor refactoring (using sklearn's check_cv), and updating tests. The gold patch is concise and targets two methods. Given familiarity with the codebase, this change likely falls within the 15min-1hr window, although some investigation is required to ensure consistency and appropriate use of check_cv.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues detected. The issue is usefully reproducible, and the required test and functional change are well-aligned with the description. No API ambiguity or domain-area knowledge beyond basic sklearn patterns is needed.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10323": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well specified: it details the undesired behavior observed when using the Sphinx literalinclude directive with the prepend (and optionally dedent) options, describes precisely what is wrong (indentation of the XML is not preserved, and dedent option leads to warnings and build failure), presents both a reproduction case and the expected behavior. However, there are some blanks to fill in about the nuanced intended behavior when these options interact (especially regarding whitespace handling), and the engineer is left to infer the required changes to the processing order for the filters. Overall, though, a motivated engineer could proceed and meaningfully resolve the issue without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change is relatively contained: reordering how filters are applied within the code handling the literalinclude directive, so that dedent is applied before prepend and append. The patch shows that the main task is to update the filter order in a single function. While care must be taken to ensure no new bugs are introduced, it only requires a bit of reasoning about filter order, an understanding of the Sphinx extension's code, and updating or adding a relevant test. The complexity is in reasoning about string manipulation and the interaction of options, but not in changing many files or algorithms.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue description is sufficient for benchmark purposes. It covers the technical problem and expected outcome, and the scenario is reproducible. The patch and test are straightforward. The issue is self-contained and is not reliant on special external dependencies.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10673": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and makes clear what the problem is and what a successful solution should look like. It identifies that referencing 'genindex', 'modindex', and 'search' in a toctree currently produces errors about non-existing documents, and describes the expected behavior: users should be able to include these auto-generated pages in a toctree without warnings. There's a concrete example of the syntax, the exact warning messages, and an explicit feature request to allow these special documents to be referencable in the toctree. An engineer understands both what the failure mode is (warnings on 'nonexisting documents') and what the desired outcome is (the directive should succeed without warnings).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the problem is clearly described, solving it involves working across several files that handle toctree logic, parsing, environment adaptation, and label resolution, as well as ensuring that the generated/indexed documents are recognized as valid by the system. There are several moving parts (handling both the document registry, globbing, and document resolution), and the fix needs precision to avoid causing regressions. The code change is moderate in size (spanning multiple files), requires understanding of Sphinx's internals, and updates core logic that interacts with toctree references and document resolution.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues that would prevent this sample from being used in the benchmark. The specification is clear and the testing patch verifies the fix with sufficient coverage, especially by asserting the tree structure and entries of toctree. The issue, while referencing external sources, does not require reading them to understand the requirement. The example warnings and directives are precise.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7440": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It gives a clear error message related to the Sphinx documentation build: a duplicate glossary term with different cases (\\\"mysql\\\" vs. \\\"MySQL\\\"). The user describes how to reproduce the issue (cloning the repo, running Sphinx), and their expectation (that \\\"MySQL\\\" and \\\"mysql\\\" should be considered distinct glossary entries). There is also an explicit filename and line reference (doc/glossary.rst) that clarifies where the problem occurs. Taken together, it is unambiguous that resolving this issue requires making Sphinx treat glossary terms as case-sensitive, so that entries named \\\"mysql\\\" and \\\"MySQL\\\" do not conflict. No extra context or clarification is needed to implement a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is rated as 1 (15 min - 1 hour). The required code change is small (removing or altering a lowercasing operation to allow glossary term case sensitivity in Sphinx). The main challenge is identifying where glossary term uniqueness is enforced and ensuring changes do not break related functionality, but this is localized work. Adjusting associated tests is also straightforward. While it requires awareness of Sphinx's glossary handling, it does not demand substantial refactoring or domain knowledge.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected. The issue provides a sufficient amount of information, is not ambiguous, and the associated patches show the problem can be tightly scoped. It is suitable as a benchmark sample for evaluating coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7462": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The problem is clearly described: when using a specific type annotation (Tuple[()]) in a Python function and generating documentation using Sphinx, an IndexError is raised. The expected behavior is also well described: the documentation should generate without errors, rendering the function with the empty-tuple annotation. The environment (Sphinx, Python), reproduction steps, and an exact code sample are provided. The source of the error is identified in the stacktrace, giving a strong clue where to start investigating in the codebase. There is little ambiguity for an engineer: the required outcome is that such empty-tuple annotations should be supported and not cause a crash during doc generation. The only sensible interpretation is to fix the Sphinx code so that the case of empty tuple annotations is handled without error.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is best rated as a 1: 15 min - 1 hour difficulty. Although it requires understanding Sphinx's internals regarding how type annotations are parsed and rendered, the error is traceable to a particular chunk of code (from the error message in the stacktrace and from the user's description). The code changes needed are small and localized\u2014primarily dealing with empty elements in parsed tuples\u2014yet require careful updating of the relevant function(s) and some thought about edge cases. This would likely take an experienced engineer less than an hour, including creating or updating a relevant test case.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues preventing this issue from being used as an evaluation sample. The environment and reproduction steps are reproducible, and the test changes are focused and verifiable. The issue is neither vague nor dependent on external information beyond what is given in the sample.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7985": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clear about the required functionality. It provides a direct statement of the current limitation: Sphinx's linkcheck builder does not check internal (local) links in documentation files, only external links. The steps to reproduce are provided, including example `index.rst` content and the observed result. The expected behavior is also directly specified: both broken external URLs and missing internal targets should be flagged. There is enough detail (including exact error logs and environment) to understand both what currently happens and what the desired solution should accomplish. There is no ambiguity about what needs to be changed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively contained extension to the existing linkcheck logic. For an experienced engineer familiar with Sphinx, it requires reading/modifying a function to add checks for local links (files and internal targets). The main challenge is understanding how URIs and references are parsed, and then implementing checks similar to external link checking but for internal targets. The amount of new code is moderate, and the test additions are straightforward. While it involves making sure the linkcheck handles more cases, the scope and requirements are clear, and the work does not span multiple subsystems or large refactors.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no outstanding issues that would prevent this sample from being used in a coding benchmark. The problem is clearly described, the expected outcome is concrete, and there is an appropriate test update that reflects the requested feature. The scenario is typical for real-world bugfixes/feature requests in open source Python projects.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8269": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. The problem is described concretely: the linkcheck command misreports HTTP error status codes (e.g., 404, 500) as \\\"Anchor not found\\\" when checking anchors with linkcheck_anchors=True. The user illustrates this with an exact reproduction process, showing how to trigger the bug using a fresh Sphinx project and a specific broken link, and provides both actual and expected output. There is no ambiguity about what is broken (incorrect error reporting) or what correct behavior is expected (report HTTP status errors instead). The environment is also specified. Anyone familiar with Sphinx and its link checker should know what and where to fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a straightforward bug fix requiring some inspection of linkcheck's handling of HTTP requests and error conditions when anchors are checked. The solution (calling response.raise_for_status()) is simple once the relevant place is found. The changes are small and localized to a single code path. While understanding the full flow for first-time contributors could take some time, for an experienced engineer familiar with the codebase it should be a 15-60 minute bug fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The issue is clear, easy to reproduce, and tests can verify the correct behavior. The only challenge may be the need to understand Sphinx's link checking internals, but the problem and requirements are clearly conveyed.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description gives a clear description of the unexpected warnings and how to reproduce the problem, specifying exactly how Sphinx currently handles cross-references created by ``:type:`` and ``:rtype:`` fields as opposed to explicit xref roles. The steps to reproduce describe the context, specific object/module/class names, and expected behavior, including explicit details of the warning messages and which references are resolving incorrectly. The expected behavior is unambiguously described: no warnings, and that unqualified type names in a particular module context should resolve specifically to ``mod.submod.A``. An experienced software engineer would have a clear understanding of what needs to be fixed and what would count as a successful solution (fix the lookup so that it respects the current module context, eliminating the ambiguous xref warning and resolving to the correct class).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"While the scope of the change is relatively well-bounded, the developer must understand Sphinx's somewhat complex internal structure for cross-referencing, docfields, and context resolution. The fix itself requires digging into multiple components to ensure field information is being passed with the correct context and attached to the cross-reference nodes, updating both the Python domain cross-reference logic and the docfield transformation logic. The engineer needs to trace the flow of module/class context and how xrefs are generated and resolved, which is non-trivial. Together, this would likely take between 1 and 4 hours for an experienced engineer familiarizing themselves with Sphinx internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues with using this sample for coding benchmarks. The problem is well-scoped, doesn't depend on ambiguous external context, and successful solutions can be objectively verified via tests and warning absence. The only technical challenge is the need to navigate Sphinx internals, but instructions and expectations are clear.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-9711": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The description clearly explains the problem: version strings are being compared lexicographically (as strings) rather than numerically (as semantic versions), which leads to incorrect version checking when verifying extension requirements (e.g., '0.10' being considered less than '0.6'). The reproduction steps are precise, including which versions of sphinx-gallery trigger the bug, and the expected behavior is also very clear: version 0.10.0 should be accepted if 0.6 is the specified minimum. The file(s) and functionality involved (needs_extensions check, related to sphinx extensions) are easily inferred from context. Any experienced engineer familiar with Python packaging and Sphinx would be able to identify that proper version comparison (possibly using packaging.version.Version or similar) is required. The requirements for a successful solution are clear and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change that requires some thought but does not entail complex changes across multiple files. The fix is to adjust the version comparison logic from string comparison to proper version comparison (likely by using the 'packaging' module or similar tools). The context is well-contained, the reproduction and expected behaviors are clear, and a solution\u2014while possibly requiring brief investigation into how/where version comparisons are done\u2014should comfortably fit into a 15-min to 1-hour window for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is solid for evaluating coding ability around bug fixing, comparison logic, and understanding of packaging/versioning concepts. No major issues noted; documentation, reproducibility, and testability are clear. Provides opportunity to demonstrate both understanding the problem and implementing a correct, robust fix.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13372": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear reproduction scenario (calling evalf on a Mul with Max(0, y) in different argument orders) and provides the actual traceback and the error caused (UnboundLocalError in evalf due to unassigned 'reprec'). The user even offers a suggested fix (adding 'else: raise NotImplementedError' after the elif clauses for reprec and imprec). However, the user does not deeply explain why the order matters or detail further context about the expected vs. actual behavior. Still, it's clear how to replicate and what bug is hit, with enough information for a sensible engineer to produce an appropriate bug fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While this involves understanding the evalf function and a somewhat subtle argument order problem, the core fix is minimal: ensuring that variables ('reprec', 'imprec') are always defined or an appropriate exception is raised. The patch confirms that only a couple of lines are added. An engineer familiar with SymPy and Python would be able to identify where to place the 'else: raise NotImplementedError' logic fairly quickly. The problem requires some brief investigation but is not highly complex.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The issue provides a sufficient reproduction, a traceback, and a hint towards a solution. The expected result in the test is clear, and the fix scope is limited. It serves as a suitable benchmark sample for coding ability evaluation. No additional blockers are apparent.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13480": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly states the scenario under which the bug occurs: evaluating coth(log(tan(x))) for integer values (substituting various integers for x) in SymPy raises a NameError, specifically that 'cotm' is not defined. The minimal code to reproduce is essentially present in the description. The file and function are implicitly indicated by the included Python error trace. It is clear that a NameError needs to be fixed when evaluating this symbolic expression, and what triggers the error. The description also provides concrete problematic input values. While not explicitly requesting a particular output or fix, it is very clear that the fix should address the missing variable/call and prevent crashing on these inputs. This is sufficiently detailed and leaves little ambiguity about what a successful solution would be: substitution on these expressions should not cause the reported error and should produce the mathematically correct result. This is well-specified by typical standards for bugfixes in open-source code.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual issue is caused by a typo/misnaming of a variable\u2014'cotm' should be 'cothm'\u2014resulting in a NameError in a straightforward code path. The fix, as shown, is a one-line change. The problem is well-localized, and the error trace gives the precise location of the issue. For someone familiar with the Python language and with a basic understanding of SymPy's codebase, this would be a 15 min to 1 hour fix, including verifying the bug, locating the wrong identifier, and writing or updating the relevant test. The conceptual issue is trivial, but reproducing and verifying the fix could take some minutes. It is not just a two-minute assertion but may require one build/test-run cycle, so level 1 is most appropriate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other issues that would prevent this sample from being used to evaluate coding ability. The sample provides a concrete bug, clear reproduction steps, and an unambiguous fix\u2014excellent for benchmarking typical bugfix skill. The only note is that the fix is straightforward and not algorithmically complex, so it is suitable for simple debugging tasks rather than for assessing deep algorithmic skill.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13877": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified for an experienced developer familiar with symbolic computation and the SymPy library. It gives a precise account of how matrices with symbolic entries cause the determinant function to sometimes raise an 'Invalid NaN comparison' error for specific inputs, complete with a reproducible example (f(n) for various n) and traceback. It also raises a plausible hypothesis about the Bareiss algorithm being unsuitable for non-integer (i.e., symbolic) matrices, focusing the search for a fix. A sensible interpretation is to adjust determinant computation so that it works robustly for matrices with symbolic entries, but some implementation details are implicit\u2014that the fault arises from inappropriate zero-checking, and that a robust method for symbolic zero-checking is needed. Thus, the issue requires filling some blanks but is actionable without additional outside information.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this issue demands an understanding of symbolic matrix operations and the specifics of the Bareiss algorithm as implemented in SymPy. The fix itself involves creating a new helper function for improved zero-detection of symbolic matrix entries, updating core matrix computation routines to use it, and verifying that determinant calculations are correct for a broad class of inputs (possibly requiring test additions). This likely requires reading, understanding, and modifying non-trivial parts of the linear algebra subsystem, as well as updating or adding nontrivial tests that probe edge-cases. For a moderately complex core algorithm like matrix determinant with symbolic math, and given the potential for subtle bugs, the effort fits in the 1-4 hour range for a skilled developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The issue is well-suited for evaluating problem-solving and code-reading skills in a technical benchmark: it requires reproducing a bug, interpreting algorithm flow, refactoring at least one core method, and validating correctness with non-obvious edge cases. All necessary information is present or can be reasonably inferred by a capable engineer.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-16792": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly explains the problem scenario: using the cython backend for autowrap in Sympy, when a function has array arguments that do not appear in the final expression, produces generated C code with an incorrect function signature. The example provided (using a MatrixSymbol x and an expr not dependent on x) is explicit and easy to reproduce. The user included both the expected behavior (the function should accept a pointer for x even if unused in the calculation) and the incorrect output and code snippet. The scope is clear (fix code generation so array arguments retain their type in the signature regardless of usage in expr), and no details are left ambiguous. Anyone familiar with the codebase should know which files and classes to look at (e.g., codegen.py, autowrap, function signatures).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding how function arguments are analyzed for code generation with the cython backend, particularly how array-type arguments are handled when not referenced in the final expression. The fix involves identifying where argument types are determined and ensuring correct type metadata is used even for unused array arguments. For an engineer familiar with Sympy and its code generation subsystem, it is a focused and straightforward task that can be completed within 15 minutes to 1 hour. The main complexity is auditing how codegen assigns argument signatures, not implementing a new feature or rewriting major components.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no substantial missing details or reasons this issue would be unsuitable for a coding benchmark. External context is unnecessary: the provided information is reproducible and detailed. Good test coverage is present in the patch, showing input and exact expected output.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17139": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear context: the user calls simplify(cos(x)**I) and receives a traceback indicating a TypeError due to an invalid comparison of complex numbers in sympy/simplify/fu.py. The specific error is traced down to an if-statement that tries to compare rv.exp < 0 when rv.exp may be complex. While the user doesn't explicitly say what behavior is desired (e.g., should it simply return the input, or something else), it is reasonable for a developer to deduce from the traceback and context that the code is attempting an invalid comparison and should instead check for realness before making such comparisons. The example and error are concrete and reproducible, and point to a clear bug. However, there is some minor ambiguity in what the \\\"best\\\" result is for such inputs (e.g., in mathematical terms), as the user only expects a simplified expression, not the specific result. Therefore, this is best rated as a 1: some blanks, but a sensible interpretation is possible and the required change is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem involves tracking down an exception in a stack trace and fixing a comparison involving complex numbers in a special-case simplification function. The required fix (adding a check for the reality of the exponent before making a comparison) is easily inferred from the traceback and domain-specific knowledge, and the actual code change is quite minimal (a guard clause with is_real). However, it does require some codebase familiarity to locate the relevant logic and understand why the error happens, making it non-trivial. The expected time to solution for an experienced developer is between 15 minutes and 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are obvious. The issue is focused, reproducible, and the patch/tests demonstrate that it fits a benchmark-style evaluation of coding ability related to bug localization and guard clause addition for edge cases. This is a good example for evaluating software engineering skills relevant to code correctness and defensive programming in symbolic computation.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17318": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and clear about what is required: calling sqrtdenest on certain expressions (such as (3 - sqrt(2)*sqrt(4 + 3*I) + 3*I)/2) causes an IndexError, due to assumptions in _sqrt_match and split_surds. The correct behavior should be that if an expression cannot be denested, it is returned unchanged, not throwing. The issue even gives a concrete input, the error stack trace, and the desired post-fix result, both in terms of output & expected non-crashing behavior. It also explicitly specifies that the exception should be replaced by unchanged output.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue likely requires focused analysis and a clear fix: the main work is identifying why the IndexError occurs, updating the relevant logic in _sqrt_match (and possibly split_surds, _split_gcd) to handle when the tuple is empty, and ensuring sqrtdenest returns unchanged when a denest isn't possible. The code changes required should be around a few targeted conditions and some extra logic. For an experienced developer with some knowledge of the codebase, it would likely take 15min to 1 hour, counting time to run the tests and verify the edge cases.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues. The sample is well-defined, and the requirements are clear: the core is a runtime fix for non-denestable expressions in sqrtdenest, and ensuring they are handled gracefully, as well as not raising IndexError. The test update is reasonable and verifies a range of such scenarios. Suitable for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17630": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is quite well-specified: it gives concrete code examples for constructing the problematic objects (BlockMatrix containing ZeroMatrix blocks), exact commands to reproduce the problem, the observed output (exception and its traceback), and analysis of the cause (that Zero is being inserted instead of ZeroMatrix). However, the user is unsure of the internal details of the implementation, and while they point to the probable bug (scalars like 0 are used in a block matrix where ZeroMatrix is needed), the issue does not specify exactly what part of the code should be changed, nor exactly how to fix the internal representation\u2014leaving some design/implementation choices to the engineer. Nevertheless, a sensible, clear interpretation of the problem and required behavior is reasonably evident, and an experienced engineer could confidently attempt a solution with the given information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This bug is exposed by multiplication and collapse of BlockMatrices containing ZeroMatrix blocks, and the exception trace clearly indicates a mismatch in expected block types. The fix involves making sure that mathematical operations on block matrices promote zeros to the correct matrix-shaped zero (ZeroMatrix), rather than accidentally inserting the scalar 0 (Zero), particularly within internal matrix expression functions. While understanding the control flow in the relevant SymPy matrix expression manipulation routines takes some time, the actual code change is small and localized (as seen in the patch). Writing robust tests is also not difficult, as the problem is easily reproducible and test cases are already quite concrete in the issue text. An experienced contributor familiar with SymPy's structure could resolve this in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues preventing this sample from being used to evaluate engineering ability. The issue description allows for clear testing via provided examples, and the problem is straightforward to replicate. While some knowledge of symbolic matrix manipulation and the codebase is useful, the task is a good fit for a benchmark evaluating practical debugging and targeted repair.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17655": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very clearly specified. The user describes the exact behavior observed (multiplying a Point by a scalar works in one order but not the other), provides a minimal reproducible example (code snippets), the traceback of the error, and the expected behavior (that both orders should yield the same result). The function/class involved is clear (geometry.Point in sympy), and the issue is localized to arithmetic with Point and scalars. The desired fix is unambiguous: commutative multiplication between Point and scalars should be supported. This is a canonical example of clear, actionable bug report.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires a small change: implementing __rmul__ for the Point class to support reversed operand multiplication. This is a common Python protocol fix for such issues. Understanding the bug is straightforward given the error message and the code, and making the change requires familiarity with Python dunder methods. Creating/adjusting tests accordingly is also simple. For an experienced engineer, this would take 15-45 minutes including test changes and verification.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The problem is isolated, the fix is small and tractable, and the test requirements are clear. This is a good candidate for evaluating coding ability, especially for understanding Python data model and operator overloading.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-18211": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides all the necessary information for an experienced engineer to attempt a fix. It explicitly states what function is misbehaving (solveset), the current incorrect behavior (raises NotImplementedError), and the expected correct behavior (should return a ConditionSet representing all real solutions of the equation). There is a clear example of the problem (n\u22c5cos(n) \u2212 3\u22c5sin(n) = 0), and the expected form of the fix is described, namely that ConditionSet should be returned where appropriate. No external contextual knowledge is required, and the problem occurs in a well-established library (Sympy), making it straightforward for someone familiar with the codebase to navigate to the relevant code and tests (e.g., as_set(), solveset, ConditionSet).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue would likely take 15 minutes to an hour for an experienced software engineer familiar with SymPy. The change only requires catching the NotImplementedError and returning a ConditionSet, without the need for significant changes or deep investigation. The engineer would need to locate where NotImplementedError is raised, handle it accordingly, and update/add tests. The number of lines changed is relatively small and the logic is clear; it is not especially difficult but does require understanding of how SymPy manages equation solution sets.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues with using this sample in the coding benchmark. The issue is clear, self-contained, and requires a meaningful code change that can be validated with a straightforward test. This is a good representation of a small but nontrivial engineering fix.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20428": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it describes a clear bug in how zero polynomials are represented internally after clear_denoms() in SymPy. It provides a concrete example, describes the erroneous and expected behaviors, includes traceback details, and even examines the internal representations involved (DMP([EX(0)], EX, None) vs DMP([], EX, None)). The affected methods and phenomena (is_zero, conversion to expression, terms_gcd(), primitive()) are specified. Someone familiar with the codebase could infer that the problem is in the representation/handling of zero polynomials after clear_denoms, and deduce that zero values must be normalized so all code expects and handles them correctly. Overall, the issue's 'what' and 'why' are clear, and it is sufficient for making a targeted fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a moderately simple bug: The underlying problem is isolated to handling of zero values in representations of polynomials in the 'ExpressionDomain'. Understanding precisely why DMP([EX(0)], ...) is not recognized as a true zero in contrast to DMP([], ...) requires looking at internal representations, but the fix itself is small\u2014updating the boolean evaluation to use 'is_zero' on the underlying SymPy Expr rather than a direct equality check. The change is a single-line update and some verifying in tests, amounting to a typical small bugfix in a mature codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues. The problem is well-scoped, and the PR and test changes align directly with the described bug. The input polynomial is sufficiently specific for benchmarking coding abilities relevant to symbolic computation libraries like SymPy. The issue is not ambiguous, and evaluating the fix is straightforward with proper tests.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20438": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-described and provides clear guidance about the symptoms: it explains an example where is_subset returns wrong results, describes the sets involved, references relevant code paths, and points to an AttributeError occurring in simplify. The ambiguity is minimal: while the exact desired return values for all is_subset cases are not spelled out, it is clear that the goal is to fix is_subset for ProductSet and FiniteSet such that subset and equality relations between such sets are handled as expected, as well as to address the AttributeError in simplify for cases when non-Expr arguments are involved in Equality. An experienced engineer can reasonably infer what a successful solution must achieve, but some implementation details (e.g., how to best check subset-ness for ProductSet vs FiniteSet) are left for the implementor to figure out.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The patch affects several files, introduces new overloads and logic for subset checks between ProductSet and FiniteSet, and handles edge cases for simplifying Equality objects. The test changes also reflect that subtle behaviors between various Set subclasses need to be considered. While the code changes do not impact a huge number of files, understanding the internal set dispatch logic, the possible shapes of sets in SymPy, and guaranteeing correct behavior for all relevant combinations adds complexity. A new function to handle is_subset(ProductSet, FiniteSet) must be written, and changes to relational simplifications require familiarity with sympy's internals. For an experienced engineer familiar with the codebase, this would likely take a few hours, possibly less if they already understand the dispatch mechanics and set structures in SymPy, but it is not a trivial one-function fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blockers are evident. The issue is precise enough for this benchmark, the fix is nontrivial, and the test coverage appears solid. The only thing to note is that a basic familiarity with mathematical set theory and SymPy's Set class is needed, but that is expected for this type of issue.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20590": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well-specified. It clearly identifies a regression in the behavior of sympy.Symbol: in version 1.6.2 instances had no __dict__, but in 1.7 they suddenly do. The reporter even points out likely cause: the removal of __slots__ in a superclass. The desired outcome is also implied (restore the previous behavior so Symbol does not have a __dict__). However, the issue doesn't explicitly state what the correct/best fix is, or whether there are side effects or backwards compatibility concerns. Still, someone familiar with Python and the SymPy codebase could sensibly interpret what is required and produce a correct fix. Thus, it is between fully clear and requiring some reasonable interpretation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively small and targeted change. After understanding the regression about __dict__ on Symbol objects, an experienced developer could identify the likely cause (change in __slots__ in a superclass or mixin), confirm by inspecting class inheritance, and add __slots__ = () accordingly. Implementing and testing this fix requires some knowledge of __slots__ and class design in Python, but shouldn't require more than an hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present. The problem, context, and expected state are sufficiently described to evaluate candidate solutions using the provided test. It is a reasonable sample for coding ability benchmarks, especially with the associated regression test.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-21379": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified. It provides a minimal working example (MWE), a detailed reproduction case, specifics about sympy version, the sequence of steps to trigger the bug, and even what elements cause the bug to appear/disappear. Though the user is not sure about the root cause, there is enough information for a competent developer to reproduce and investigate \u2013 namely, that substituting {1: 1.0} in a certain expression after symbols are declared as real causes a PolynomialError, but does not when assumptions are omitted. The successful solution involves handling PolynomialError in relevant code. However, the issue is not as clear-cut about exactly how to fix underlying logic (should the substitution always succeed? Should a specific error be suppressed?), requiring some interpretive reasoning. All key information needed to produce a fix and test is present, but not in the format of an explicit, actionable request for specific logic: the engineer would need to decide on the best way to avoid the error, so I rate this as 'some blanks to fill in' (1).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is moderately easy to fix for an experienced developer familiar with SymPy. The required code change is contained within a small section (modifying how gcd is called and handling PolynomialError), which is a local fix and does not involve major redesign or large-scale surgery. While the investigation might require some debugging (to understand why the error is triggered), the implementation is a small addition of a try/except block and a test, which should take 15 minutes to an hour, mainly depending on familiarity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further major concerns. The issue offers a reproducible example, specifies the version, and is tightly scoped. It is suitable for benchmarking coding ability, as it requires understanding error propagation, exception handling, and writing a targeted test case, but does not depend on ambiguous project context. The only minor challenge is understanding the subtlety of type/assumption handling in SymPy, but all necessary details are present.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-22714": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The reporter provides a detailed reproduction scenario (using SymPy's sp.S('Point2D(Integer(1),Integer(2))') inside a with sp.evaluate(False): block) and contrasts it with situations that work correctly. The error message and a traceback are included, pointing specifically to the place in the code (sympy/geometry/point.py) where the error originates ('Imaginary coordinates are not permitted.'). While the precise line numbers are not provided, the context is sufficient for an experienced engineer to identify the problematic logic. The expected and current behaviors are both clear: running under evaluate(False) should not crash (should work as in the other invocations), so the fix should address this discrepancy. Thus, the requirements for a successful solution (no crash, Point2D is created as expected) are explicit and unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required is small and targeted. Although understanding the logic flow for coordinate evaluation under SymPy's context management requires familiarity, the issue is localized, and the traceback provides a clear pointer to point.py where ValueError is being raised. The overall fix is a one-line change adjusting the condition that tests for imaginary coordinates, changing im(a) to im(a).is_zero is False. This requires some knowledge of SymPy's types but would be resolvable within 15 min to 1 hour for an engineer with some familiarity with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues remain. The test and patch both clearly validate and fix the reported bug. The sample is suitable for benchmarking coding ability: it is normal Python, a self-contained defect-fix, and does not depend on unstated context or external systems. The error condition and correct behavior are well articulated; reproducibility via the steps in the issue description is straightforward.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23824": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text is well-specified and provides a clear description of the bug in kahane_simplify(). It describes exactly what the correct and incorrect outputs are in response to given gamma matrix input expressions. The required change is precisely the order in which leading uncontracted gamma matrices are reinserted into the output. The example with \u03b3^\u03bc \u03b3_\u03bc \u03b3^\u03c1 \u03b3^\u03c3 and the output 4*GammaMatrix(rho)*GammaMatrix(sigma), versus the erroneous output when the contracted pair is moved, removes ambiguity. The function and file to change (kahane_simplify in sympy/physics/hep/gamma_matrices.py) are named. The root cause and a suggested fix are even discussed; while the engineer does not need this level of detail, it reduces ambiguity and makes the requirements clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires following a clear logical mapping between the input and output expressions, and adjusting how uncontracted gamma matrices are reinserted in kahane_simplify. The solution is a localized and concise edit (a small loop or line), and the examples and output provided allow for quick verification. While the mathematics of gamma matrices could be unfamiliar, the bug itself and its correction once familiarized with the code are straightforward. The test updates suggest boundary checking and verification, but do not require rewriting large portions of the codebase, nor significant architectural change, so it likely takes 15 minutes to 1 hour for someone familiar with the code and the physics context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional major problems with this benchmark sample. The issue is precise, involves tangible and testable input/output, and can be both implemented and verified solely using the information presented.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23950": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes the specific behavior of the Contains.as_set() method returning the same Contains object instead of a set, explicitly identifies the problem (Contains is a Boolean, not a set, so this is incorrect), and provides a concrete example, complete with traceback, showing why this is a problem in practice (causing an AttributeError in Piecewise). The engineer understands that Contains.as_set() should return the set argument (i.e., .args[1]) of the Contains object. This gives all necessary context for a clear and direct fix.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a straightforward change requiring only a modification to the as_set method of the Contains class to return the set argument instead of the Contains object itself or raising NotImplemented. There is very little logic to implement, no tricky edge cases, and relevant context (method name, example) is provided. For someone with codebase familiarity, it's a <15 minute fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue is self-contained, the requirements are explicit, and there are clear test cases to verify the solution. The sample is suitable for coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24066": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified, describing a defect in SI._collect_factor_and_dimension's handling of exponentials with dimensionless exponents. The steps to reproduce are clear: import relevant Sympy entities, build a unit expression that should be dimensionless, and observe the error when plugging it into an exponential. The desired outcome is that SI._collect_factor_and_dimension recognizes the expression as dimensionless (Dimension(1)), instead of raising a ValueError or returning an incorrect dimension. However, there are some assumptions left for the developer, such as the exact internal functioning of _collect_factor_and_dimension and how SI units should interact with mathematical operations. The precise expectation (the function should treat dimensionless exponentials correctly) is explicit enough to allow a competent engineer to attempt a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted bug fix that requires understanding how the units system represents dimensions and handles mathematical operations (like exponentials) internally. Modifying one function and updating/adding a test is involved, as the patch shows. For someone who is familiar with Sympy or can quickly familiarize themselves with its units code, this is a moderate-level task that should take 15 minutes to an hour, mainly involving careful reasoning about units propagating through function calls.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues are evident. The sample is technical but concrete, with clear instructions for reproduction and an explicit bug to fix in a named function. External context or links are not required to interpret the problem. The only minor challenge is that some familiarity with dimension systems and the code structure is needed, but this is typical for such tasks and acceptable for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24443": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear problem reproduction path: using SymPy's combinatorics, construct the dihedral group of degree 3, and use homomorphism with its generators, yielding an unexpected ValueError. It identifies the suspected source (the _image function in homomorphisms.py, with an issue distinguishing inverted generators), and offers some diagnosis (\\\"the 'in gens' test fails\\\"). While it does not state exactly how to fix it, it points both to affected code and the high-level bug, and suggests a desired outcome (valid homomorphism object should be returned). An experienced engineer would reasonably interpret this and propose a fix. However, it is not fully explicit about every edge case or precise output (e.g., if there's expected behavior for other types of groups or about simplification). Thus, it fits level 1: some gaps, but a sensible solution is possible.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix required substantial rewriting of a core helper function (_check_homomorphism and its _image inner function). The problem required understanding both group theory concepts and the code's handling of permutations and generators (and their inverses). Implementing and validating a correct, general fix involved more than a trivial change, including ensuring correctness across group types and adjusting internal mapping logic. The patch also shows significant changes to code structure. For an experienced software engineer familiar with SymPy, this would likely take between 1 to 4 hours to fully understand, design, and test a robust fix, including test update work.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues preventing use in benchmark. The issue provides enough technical detail and is reproducible by following the specified steps. While it lacks ultimate specificity (e.g., for groups beyond PermutationGroup or non-trivial images), it is in line with challenging but clear real-world bug reports suitable for evaluating coding ability.\",\"q2_5_confidence\":5}"
    }
]