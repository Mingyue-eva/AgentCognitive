[
    {
        "astropy__astropy-13033": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly clear: when a required column is missing from a TimeSeries object, the exception message is misleading and does not correctly inform the user which required columns are missing. The expected and actual behaviors are both described, with an example of the confusing exception. However, the text could be more explicit about how multiple required columns should be represented in the exception (e.g., exact phrasing or formatting). Still, it is a reasonable interpretation that the exception should enumerate all required columns that are missing or expected. The file and function involved (TimeSeries/_check_required_columns or related validation/exception handling) are implicit but reasonably deducible to someone familiar with the codebase. Thus, there are some blanks to fill in, but the task is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change requires modifying or adjusting exception handling in the function that checks required columns in TimeSeries. It is a small fix to improve the clarity of an exception message and handle the case where multiple required columns exist. It would require understanding the column requirements and adjusting the string output logic, as well as updating tests. For an experienced engineer familiar with the codebase, this falls within a small change that might require a bit of thought but not substantial rewriting or structural changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues that would disqualify this sample for use in the benchmark. The user intent and needed fix are clear from the issue text, and the required code and test changes are well-contained. The only minor uncertainty may be around precise output formatting, but not enough to cause problems for benchmarking code ability.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-13977": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well specified: the user presents a concrete incompatibility between astropy.units.Quantity.__array_ufunc__ and duck-typed objects implementing similar interfaces, highlighting an error where a ValueError is thrown instead of returning NotImplemented. The functional area (Quantity.__array_ufunc__) and the types of operations/inputs involved are clear from the description and the stack trace. The user states the expected behavior (return NotImplemented for incompatible types/units to allow reflected operations), and references numpy's documented conventions. However, the description leaves some minor implementation details open, such as exactly when \\\"incompatible\\\" should result in NotImplemented, and what error types should be intercepted, but it is reasonable for an experienced engineer to fill these blanks. Overall, it's not fully deterministic but a clear sensible solution path exists.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The required change is moderately complex: the engineer must understand the details of numpy's ufunc machinery, duck typing, and how exceptions should be handled in the context of operator overloading in Python. The patch involves modifying a significant method (__array_ufunc__), making sure to properly intercept and redirect certain error cases, and ensuring compatibility with a variety of input types. Additional test cases also require writing or adjusting. For someone experienced with numpy, ufuncs, and Python dunder methods, this still requires a few hours of careful thought and regression testing, but is not a full-day task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues apparent with using this sample for the benchmark. The problem is self-contained (focused on a specific class and method), the motivation is clear, and the expected behavior is supported by both the description and the referenced numpy conventions. Full reproduction and testing are possible with the provided information and the changes are within the scope for an experienced engineer.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14096": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes a problem when subclassing SkyCoord and adding custom properties: if a property attempts to access a non-existent attribute, the resulting AttributeError mentions the property itself rather than the missing attribute. The expected behavior is clearly implicit: the error message should refer to the missing attribute (e.g., 'random_attr'), not to the property ('prop'). The example traceback provided clarifies the current (incorrect) behavior, and a concrete use case and expected outcome are described. There is enough detail for an experienced engineer to understand what constitutes a successful resolution: fixing the underlying error reporting so that it accurately names the missing attribute in exceptions.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but not entirely trivial change. It involves understanding how attribute access (__getattr__ and __getattribute__) is managed in the SkyCoord class, and then updating the fallback path to raise a standard AttributeError for the missing attribute, not for the property itself. This requires some knowledge of Python's data model and a short dive into the code, but it's localized to a specific method and needs only a minor change. The patch is very small (changing basically one line), so for someone with experience in Python and the library, this would likely take between 15 minutes and 1 hour (including time to understand context and run tests).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues were identified. The problem is clear, testable, and the solution is fully within the information given. There is a natural, objective way to verify correctness via the error message and this is enforced with a regression test. The sample is suitable for use in a coding benchmark evaluating ability to read and interpret error reporting issues in Python properties and __getattr__ implementations.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14182": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well specified. It explains the desired new feature (support for header_rows in RestructuredText output when writing tables via astropy's ascii.rst format). It explicitly shows a code example that fails, including the exact error (TypeError: unexpected keyword argument 'header_rows'), and describes clearly what is expected to work. There is enough context, including desired example output and the format in which the new functionality should be supported (paralleling other fixed-width formats). The main challenge is engineering the new functionality, but the 'what' is clear: enable 'header_rows' as a keyword argument for writing tables with format=\\\"ascii.rst\\\". There are no significant ambiguities.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This change is more than a trivial or minor edit. It requires understanding the serialization/deserialization classes for astropy.io.ascii.rst, altering constructor signatures to accept the new keyword, making changes to how headers are written, and updating (or introducing) logic regarding how headers are placed and indexed in the output. The engineer must ensure that both reading and writing support the new header_rows behavior, handle the interaction with existing code paths, and update or add appropriate tests. The actual patch modifies class constructors/methods and tests, demonstrating a need for intermediate knowledge of the codebase and attention to table serialization logic. This level of refactoring and feature addition reasonably falls in the 1-4 hours (level 2) category.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The problem is well-scoped, has a clearly described target behavior, and is accompanied by a motivating example. It fits well as a benchmark for coding ability, requiring both understanding of class design and the ability to alter/control output structure. Testability is strong given the provided example and reproducible nature of the feature.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14309": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue report is moderately well specified, as it includes a traceback, concrete error message (IndexError: tuple index out of range), and identifies the line and function at fault (is_fits in connect.py). The description provides background on how a code change led to the regression, and copy-pastes the relevant error with stack trace, so it is clear where the problem occurs and the scenario in which it is triggered. It is implied that a recent change caused is_fits to access args[0] even when no arguments are present, leading to the exception. However, it is not explicitly detailed what the expected behavior should be if the file is not a FITS or filepath is just a string without FITS extension. Nevertheless, an experienced engineer can make a sensible interpretation (do not raise an error, return False or something similar, as in previous versions) and attempt a fix. The root cause and affected function are clear but the exact required fix is open to some interpretation, hence score 1.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue is relatively contained \u2013 it involves tracing a regression induced by a recent commit, reading the stack trace, understanding control flow in a small section of code, and correcting the logic to avoid accessing empty args. The code change ultimately adds an early return, preventing IndexError. Understanding and implementing the right guard clause may need understanding the logic of is_fits and how identify_format works, but is unlikely to take more than 1 hour for an engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"While the problem scenario could have been more explicit about the desired output (should return False/None in this case), the error, context, and stack make it practical for benchmarking. No major issues with suitability. The test patch demonstrates the scenario to guard against, which supports using this sample.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14365": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. It clearly describes the current problem: the ascii.qdp reader assumes that commands in a QDP file must be uppercase, but QDP files themselves are not case sensitive and allow both lower/upper case. The issue gives a concrete example of a command ('read serr 1 2') that causes a failure, and shows the error and traceback produced. The problem is clearly that the code is not accepting lower-case commands where it should. The description also gives a good sense of the expected behavior: it should accept QDP commands regardless of their case sensitivity. There are no ambiguities in what needs to be changed for successful resolution. The affected package (astropy.io.ascii.qdp) is made clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small change to command parsing to make it case-insensitive, and possibly handling upper/lower case for sentinel values. The core fix is likely to involve updating regular expression(s) and comparison(s) to be case-insensitive. For an engineer familiar with the codebase, this will likely take between 15 minutes and 1 hour, since verifying the fix and making sure all commands are handled consistently requires a bit of thought, but not extensive research or code rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The issue is suitable for evaluating coding ability: clear specification, meaningful but not trivial code change, and measurable using tests. The supplied version numbers also help remove any ambiguity about environment.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14508": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified and provides all the details necessary for a developer to attempt a meaningful fix. It describes the problematic behavior (overly verbose string formatting for floats in io.fits.Card that causes comment truncation), isolates where in the code the issue arises (the _format_float function in astropy/io/fits/card.py), gives a concrete example (0.009125 being rendered as 0.009124999999999999), and even suggests a plausible approach to fixing the problem (prefer str(value) unless necessary to format). There is also a clear statement of the desired outcome (being able to create any valid FITS card that is allowable in the FITS standard) and the impact (comments being truncated unnecessarily). This level of detail is sufficient for an experienced engineer to implement and verify a solution without outside clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This task requires understanding the formatting of floats in the context of FITS headers and making precise string formatting changes to adhere to a character limit (20 characters). While the issue points to the relevant function and provides specific examples, some care is needed to ensure edge cases are handled and that the formatting does not introduce new issues. The fix is essentially a targeted refactor of an existing formatting method\u2014modest in code footprint, but requiring attention to detail. A typical experienced engineer could likely address this in 15-60 minutes, including writing relevant tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The sample is appropriate for use in a coding benchmark, as the requirements are clear and the implementation is focused and self-contained within a particular function. There is no hidden complexity or missing context that would prevent a fair and meaningful assessment of coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14995": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified. It describes a regression between v5.2 and v5.3 of astropy's NDDataRef when performing arithmetic involving masks, specifically when one operand has a mask and one does not. The user describes the error (TypeError: unsupported operand type(s) for |: 'int' and 'NoneType'), explains the context (arithmetic using handle_mask=np.bitwise_or), and shares what should happen (mask should be copied if only one operand has mask). However, while it is clear what is broken and what behavior is expected, the report leaves a little ambiguity around certain details, such as whether there are edge-cases, and the specifics of 'mask propagation' behavior in v5.2; you would need to review the code or documentation for precise handling. Nonetheless, there is enough information for a sensible engineer to attempt a correct solution, especially with access to the codebase and tests.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core of the change involves adding or correcting a small check to handle cases where one operand has a mask and one does not, ensuring a mask is not treated as None in bitwise operations. The change itself is a couple of lines modifying dispatch/logic in a relevant method, and writing/adjusting appropriate tests. For someone familiar with the codebase, this should take less than an hour: some time to understand the context, reproduce, patch, and test. It's a small targeted fix with clear requirements, not demanding architectural changes or deep investigation across files.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major outstanding problems affecting suitability for a benchmark. The issue's domain is somewhat niche (mask propagation in array arithmetic), but the problem description is appropriate and testable in the benchmark setting. The expected fix is clear and well-bounded.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7336": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue clearly describes both the unexpected behavior and the correct behavior. It identifies precisely the scenario (using the units.quantity_input decorator on a constructor that uses typing hints, specifically with a return annotation of None) and details the observed exception, including the full traceback, indicating where the AttributeError occurs. The workaround describes removing the return type hint (None), which works but at the cost of static type checking. The possible fix is clearly stated: the decorator should explicitly check for None and omit the unit check if so. This gives both the expected symptoms and a concrete suggestion for correcting the issue, making it well-specified for an engineer to know what code change would constitute a successful fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution requires a targeted change to the decorator logic\u2014specifically, ensuring that when the function's return annotation is None, the unit check/conversion is skipped. The patch confirms this, adding just a small condition to the code. The engineer needs to understand Python's annotation and decorator machinery, but does not need to substantially refactor code or introduce new tests beyond adjusting the logic for this edge case. The change itself is simple and localized, but it does require a careful read of how type hints and decorators interact. Implementing and validating the change (unit tests may need slight adjustment or extension) should fit comfortably within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"All relevant information for implementing and verifying the fix is provided in the issue text: the code context, the stack trace pointing to the error location, and the high-level reasoning for the change. The affected function and decorator's file are referenced; an engineer would be able to implement the change and test it without difficulty.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7606": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes a TypeError that is raised when comparing a Unit object with None using '==', where the expected behavior is that the comparison should return False. The issue provides a code snippet that results in the error, points to the relevant method (__eq__ in astropy/units/core.py), and gives the traceback. This, together with the error context, makes it clear what change is needed: comparison with None should not raise, but instead behave as a normal (potentially unequal) comparison. An experienced engineer would be able to identify where to address the problem and what outcome is expected.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves modifying the __eq__ method(s) to gracefully handle the case where 'other' is None (or not a valid Unit), rather than letting an exception bubble up. The solution is straightforward and localized to a specific method, requiring some familiarity with Python dunder methods and the error handling patterns, but does not require rewriting major logic or touching many files. This would likely take between 15 minutes and an hour for an experienced developer familiarizing themselves with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues detected. The bug is directly reproducible, the context and expected behavior are clear, and the solution does not require codebase-specific tribal knowledge beyond standard Python and code inspection. The sample is suitable for a benchmark coding task, as it tests error handling in equality comparisons.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7671": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a stack trace, affected usage (calling minversion with e.g. '1.14dev'), the problematic context (LooseVersion raising a TypeError due to mixing ints and strings in version comparison), and the likely cause (LooseVersion bug). Although the root cause is not fully specified (e.g., exactly what minversion should accept, or if only certain version formats are problematic), it gives enough detail for an experienced engineer to understand the failure and track where and why it happens. The expectation is to make minversion robust to such version string issues. The issue includes an example failure, so a sensible engineer could reproduce and fix it, but some minor decisions (e.g., how strict to be with accepted version string formats) may need to be filled in.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is well-scoped to handling version strings that include development suffixes with minversion such that it does not raise a TypeError. The required code change is minor but requires a bit of investigation (regex to filter version string, awareness of PEP440). Implementing a solution and writing/adjusting relevant tests would likely take 15-60 minutes for a familiar engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There does not appear to be any other major reason this issue sample is unsuitable for coding benchmarks. The problem is both real-world and clear, and the code/test change is illustrative of common defensive programming work.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-10554": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a clear traceback and a description of the error: 'ORDER BY position 4 is not in select list' occurring during a queryset union with ordering in Django. The example traceback points to a precise internal failure in Django's ORM. The issue suggests that evaluating the queryset instead of chaining results works as expected. However, there is some ambiguity in terms of an explicit minimal reproduction (no precise code or models shown in the issue), and the expected fix must be inferred from the error context. An experienced engineer would sensibly deduce that the problem involves making sure columns for ordering are present in the SELECT list when creating SQL for unioned querysets with order_by. However, the lack of a minimal code sample adds a slight burden to fully grasping all edge cases. Thus, it is not fully specified but clear enough for a meaningful attempt at a fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue deals with the Django ORM's SQL generation process for unioned querysets with order_by clauses. Fixing this requires understanding how Django constructs SQL, particularly with respect to select lists and orderings in combined queries. Reviewing the provided patch, the solution involves modifying core compilation logic and adding new utility methods, suggesting non-trivial architectural understanding. Additionally, the fix affects both the compiler and the query classes with new logic. This goes beyond a quick-change but doesn't reach full architectural overhaul, so 1-4 hours is an appropriate estimate for an experienced engineer familiar with this area.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues were identified. The test patch provides adequate coverage, and the issue scope is appropriate for a realistic coding benchmark sample, despite the initial lack of a full minimal code reproduction in the issue text.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11087": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides detailed context, including a full stack trace, the specific Django version and Python migration context, a reproduction scenario, problematic SQL queries, and an analysis of the underlying causes (unvalidated unicode data and unnecessary field selection in DELETE cascades). The desired improvement (optimize .delete() to select only required fields, avoiding the unicode decode error and improving performance) is clearly articulated. However, there are minor blanks to fill in: the engineer must interpret precisely how to identify 'referenced' fields, and reason about the intersection of deletion signals and field selection, but the requirements are clear enough to attempt a targeted fix without external clarifications.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix requires in-depth familiarity with Django's ORM internals, specifically deletion cascade logic and query optimization. The engineer must refactor part of the deletion collector logic to ensure only required fields are selected, and must handle the edge case of signal listeners, while avoiding regressions. The patch size (affecting collector logic and adding nuanced test cases) and the need to understand interactions across several Django ORM concepts elevate the difficulty above a simple bugfix. For a competent engineer familiar with Django's codebase, this would likely take 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue and its context are rich, specific, and technically deep, making it a strong candidate for a benchmark of coding ability within advanced Python or Django topics. There are no major blockers for its use. The only minor note is that the bug's root cause is partially data-corruption/historical (bad Unicode writes in Py2), but the test explicitly checks the new code's correct behavior, so the sample is valid.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11265": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear minimal example of the problem, including a traceback and where the code fails, centered around using 'exclude' on a queryset with an annotated FilteredRelation in Django. It identifies the relevant function (split_exclude) as potentially responsible, and gives a concrete error message. However, while the root cause is hinted at, it isn't precisely specified what must be changed in the codebase to resolve the issue; the reader must infer the full context by understanding how Django querysets and FilteredRelation annotations interact. An experienced Django developer can reasonably interpret the required fix, but there remains some room for interpretation as to the best design and implementation. Thus, the issue is not perfectly specified, but does provide enough actionable detail for a meaningful solution attempt.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires in-depth understanding of Django's ORM internals, specifically how filtered relations and query cloning operate during query construction and evaluation. The solution involves editing non-trivial core code and ensuring that filtered relations state is preserved across subqueries. Identifying the right state to clone and its implications on query correctness could take significant time, testing, and careful thought. Additionally, the bug is subtle and may interact with related mechanisms (such as join trimming) elsewhere in the codebase, requiring broad understanding and non-trivial validation. Writing and validating tests is also necessary, as in the associated PR. While the direct patch appears concise, developing and validating it for a robust fix would likely take hours for someone new to this subsystem.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue provides a clear problem statement with a concrete traceback and enough details for an experienced Django contributor to make meaningful progress. The only slight concern is that it presumes familiarity with core ORM concepts, which may not be universal, but for the purposes of a coding benchmark targeting knowledgeable Python/Django developers, it is reasonable.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11299": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified for an engineer familiar with Django and its ORM or migration system. It clearly identifies a bug relating to how Django generates check constraints in SQL when both OR and AND clauses are combined. The root cause is described: the use of fully qualified field names (table.field) causes constraint creation to fail when tables are renamed, specifically due to the mismatch in the expected schema during table swaps (SQLite is mentioned as an example). It also hints at the difference between the use of Col and SimpleCol inside the codebase, guiding the implementer to look at how expressions for constraints are generated. However, the description leaves some blanks open, such as whether this issue exists on all databases or just SQLite, and the specific scope of the fix (e.g., which code paths and classes are affected\u2014though 'Col' and 'SimpleCol' provide a strong clue). Nonetheless, an experienced engineer can make a sensible interpretation and find the affected area within Django's migrations and constraints logic. Thus, this fits \\\"1: Some blanks, but a sensible interpretation is possible.\\\"\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires a solid understanding of Django's internals, particularly how check constraints are constructed and compiled, and the differences between \\\"Col\\\" and \\\"SimpleCol\\\" usages in query construction. One must track down where the different representations are used, understand why fully qualified names are generated differently based on clause combination, and refactor logic to unify the representation with respect to how the SQL schema expects it. Writing or updating relevant tests to ensure the issues are fixed adds to the scope. This is not a trivial patch or a simple logic bug; it involves tracing through several layers of abstraction and is likely to take between 1-4 hours for an experienced contributor. Thus, level 2 is appropriate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues stand out; the bug is tied to functionality and can be meaningfully checked with the provided tests. The issue relates to a nuanced ORM/SQL codepath involving constraint creation, so it's challenging but suitable for a benchmark measuring coding ability in a complex framework like Django.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11532": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is quite clear about the problem: sending emails in Django fails when the hostname (used as the domain part in Message-ID and other headers) contains non-ASCII (unicode) characters, and the email encoding is set to a non-unicode encoding such as iso-8859-1. There is a specific test reference and even a traceback provided, alongside a concrete suggestion for the fix: convert domain names to punycode before use. Although the description assumes some familiarity with email encoding, Django\u2019s mail subsystem, and the Punycode standard, an experienced engineer should be able to accurately identify what needs to be changed and where. There are some blanks to fill in, such as exactly which locations need punycode conversion (for all headers, only Message-ID, etc.), but the required solution and its goal are sensibly interpretable.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This task requires updates to multiple files (utilities, validators, email, and possibly more) in the Django codebase, as well as understanding Django\u2019s mail send logic and where domains are used in headers and addresses. The change is not trivial and involves careful application of domain encoding at the right locations, not just a single-line fix. Proper handling is crucial in several places, verified by multiple code patches in the gold PR. The problem is subtle due to the intersection of unicode, email RFCs, and Django\u2019s abstractions; integrating and testing the fix would reasonably take at least 1-4 hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is representative of a real-world encoding bug with a clear fix suggestion and is suitable for coding ability benchmarks involving diagnosis and targeted bugfixes. The only challenge for the participant is familiarity with email encoding and punycode, but both are standard for experienced engineers working on frameworks like Django.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is highly detailed and precise about both the context and the broken behavior. It names the function to change (_check_list_display_item), explains the intended vs. actual behavior, and even details the logic table the function must address. It also walks through a code sample of the desired logic and error handling. The core problem\u2014when and how E108 and E109 get triggered in admin list_display checks, particularly with fields accessible only via instance\u2014is clearly described. An experienced engineer familiar with Django would have everything needed to implement and verify a correct solution using just this issue text, as it specifies the problematic commit, what regressions it caused, and the exact conditions for success. No ambiguities remain in what defines a 'successful' solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual code change is relatively localized\u2014rewriting a function's logic and slightly altering its error handling flow. Understanding and implementing the fix isn't trivial, as it requires following non-obvious control flow, exception handling, and type checks, and being careful with Django's model meta programming. For an experienced Django developer, this would likely take 15-60 minutes, including reading the checks code, writing and running tests, and possibly some debugging, but doesn't require major refactoring or cross-file changes. The problem is described thoroughly enough to focus quickly on the right place for changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is an excellent candidate for a benchmark: it is clear, technically rich but not obscure, and well supported with reasoning, context, and even guidance for the main logic. The presence of a supportive test case in the PR also closes any gaps. No blockers or major caveats identified.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11734": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is quite vague: it only states that an error arises when using OuterRef in exclude() or ~Q() as it \\\"uses wrong model.\\\" No details are given about what type of QuerySet triggers this, on what model or field, nor about the context of intended use. The error message helps narrow down that the problem is related to how outer queries are referenced, but it requires significant deduction and knowledge of Django's ORM internals. There is no concrete example or reproduction steps. A knowledgeable engineer could make inferences and attempt a fix as in the provided patch, but there is clear ambiguity and room for multiple interpretations.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The required changes involve multiple files and demand a solid understanding of Django's ORM, query compilation, and how OuterRef objects are handled internally. Arriving at a correct fix would likely require significant investigation, debugging, and experimentation, especially due to the lack of a concrete example in the original issue description. Thus, it's more than a simple or minor change and would likely take 1-4 hours for an experienced engineer to solve.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is somewhat ambiguously worded, but the test patch and the gold patch demonstrate it was possible to create a meaningful and effective fix. The lack of specificity in the issue text, however, could make it less ideal for benchmarking unless the focus is on engineers' ability to interpret and investigate poorly-specified bug reports. There do not appear to be any other disqualifying factors.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-11740": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly detailed: the reporter describes the migration scenario (changing a model field from UUIDField to ForeignKey in Django 2.2), the expectation (a dependency should be automatically created for the related app in the generated migration), and the observed problem (the migration does not add the dependency and triggers a runtime error about an unresolved related model). However, it lacks explicit details such as concrete model and app names, code snippets of the models/migrations, or exactly what should appear in the migration file. There is also uncertainty from the reporter, seeking clarification if this is a bug rather than describing it as a definite bug. Nonetheless, an experienced Django developer could fill in these details logically and understand what behavior is being reported and what would constitute a fix. This is why I rate it as '1' (some blanks to fill in, but a clear interpretation is possible).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although this is not extremely complex\u2014likely not requiring days of work\u2014solving it necessitates a good understanding of Django's migration autogeneration machinery, how dependencies between migrations are determined, and correctly extending the functionality so that changes from a non-relational field (like UUIDField) to a ForeignKey correctly introduces app dependencies. The fix involves edits to the migration autodetector and likely requires creation and/or adjustment of tests to ensure the correct dependencies are now added. This is a nontrivial change to a core Django component, requiring careful reasoning about potential regressions and correctness in migration dependency logic. So, this would typically require 1-4 hours, especially including building and running proper tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent from the provided description. The sample is suitable, as the issue is rooted in a standard Django use case, relates directly to correct migration dependency generation, and does not appear to rely on external information or context not present in the description.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11815": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a clear description of the current and undesired behavior: Django migrations serialize Enum default values using the Enum value, not the name, which breaks if the value is a translated string that changes at runtime. The error and its root cause are explained. The desired direction (use the Enum name instead of value) is stated, and a code context (migrations and CharField defaults) is provided. However, some blanks remain: the specific class/method to be altered is implicit and some details of implementation require the engineer to reference the Django migration codebase and serialization logic. Still, the sensible and most direct interpretation is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial change: the code must be adjusted so that migrations serialize Enum instances using their name rather than their value, especially when used in model fields and defaults. The engineer must locate where Enum serialization is handled in Django migrations, understand the relevant methods/classes, and update how the string representation is formed. The conceptual leap is minor, but understanding the correct place to make the change and verifying correctness via tests could take 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be other issues that would invalidate this benchmark sample. The problem is fairly self-contained, and the issue text provides enough reasoning. The solution is not trivial, but not overly complex either, making it a reasonable test of practical coding and reasoning skills related to Django and Python Enums.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11964": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the undesired behavior: when an instance of a Django model has a field using IntegerChoices or TextChoices, calling str() on the field returns the enum member instead of its value (e.g., 'MyChoice.FIRST_CHOICE' instead of 'first'). The repro is explicit, a failing test is given, and the required correct behavior is mentioned (i.e., str() should produce the value, not the enum member). The context makes clear that the fix should ensure __str__ on those choices returns the .value of the enum.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Implementing a __str__ method on the Choices base class is a small change that requires a modest understanding of how enum/Choices work in Django, but the problem and correct solution are both straightforward. An experienced engineer familiar with Django's codebase could implement and test this within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major additional issues. The sample provides sufficient context, a concrete test, and the fix is clear. The problem is isolated and does not require changes across multiple files or components. The sample is suitable for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12155": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It describes a concrete problem with admindoc's handling of docstrings when the first line is not empty, referencing a specific error from docutils. The relevant problematic code is given (the calculation of indent in trim_docstring), and a specific solution is proposed (skip the first line when calculating indentation). The code snippet is clear, as is the reason for the change. Any experienced engineer familiar with Python and the context would be able to implement a solution based solely on this description. Function and variable names are referenced, making the required change very explicit.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires familiarization with the trim_docstring function and the context in which docstrings are processed and passed to docutils in Django's admindocs. However, the code change itself is minor and straightforward\u2014changing a generator expression to skip the first line. The main time investment would be understanding/testing the implications and making sure tests pass. Solution scope is limited to one function, with minor auxiliary edits.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no outstanding issues that would prevent this sample from being suitable for benchmarking. The problem, expected fix, and context are all described in sufficient detail. The only caveat is that knowledge of Django internals will help, but the sample remains approachable for experienced Python engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified. It identifies a concrete bug: custom Django template tags that use keyword-only arguments with defaults raise TemplateSyntaxError when supplied, and the error messages when a keyword is supplied more than once are not accurate. It includes observed error messages and references both simple tags and inclusion tags. However, it doesn't give code samples or point to concrete locations in the codebase, so there is some ambiguity about exact reproduction steps, and it leaves minor interpretation to the developer, for example on whether all existing error messages should change or just specific ones. However, for an experienced Django developer familiar with custom tags, the requirements are clear enough for a meaningful attempt at a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a straightforward bug fix. Once the developer understands the issue\u2014how keyword-only args with defaults cause unexpected TemplateSyntaxError and undesired error messages\u2014it is relatively clear where to start looking (e.g., parsing of template tag arguments in Django). The fix may be a one or two-line tweak in the argument handling logic, with some new unit tests or corrections to test cases. This fits in the 15 min\u20131 hour category for someone familiar with Django's template tags.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues present. The problem is well-scoped: it only impacts argument parsing for template tags and producing the correct error messages. The description is detailed enough for a solid engineering evaluation, especially since the impact on both simple and inclusion tags is noted. No external dependencies or complex architectural decisions are involved.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12325": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete example of a Django model inheritance scenario where there are multiple OneToOneField references from a child class to a parent class, and highlights a confusing error message that arises depending on the order of field declarations. The description identifies that the order of field declarations affects whether django.core.exceptions.ImproperlyConfigured is raised, even when parent_link is set, and raises the question whether this ordering should matter given the explicit marker. The report is specific about the problematic model code and the error encountered. However, the issue could be clearer about the expected or intended behavior, and while it points out that ordering 'should not matter,' it leaves it open-ended as to whether the current error should be triggered differently, removed, or if some implementation detail is simply wrong. Thus, while an experienced engineer could reasonably infer the direction for a fix, there are minor blanks about exact desired behavior, making this a '1', not a '0'.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The resolution involved changes in two core Django modules (django/db/models/base.py and django/db/models/options.py) to alter how OneToOneField parent links are processed and validated, as well as changes in the test suite to cover these scenarios. Understanding and editing this area requires familiarity with Django's ORM and metaclass model setup, and the bug relates to subtle interactions in model inheritance machinery. Implementing, testing, and validating these changes would likely take 1-4 hours for an experienced developer after understanding the problem. The required patch is nontrivial, affecting the field resolution and raising logic in Django\u2019s internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues appear present. The only caveat is that the issue assumes the reader is relatively conversant with Django model internals and the semantics of parent_link, but this is not a blocker since it's meant for experienced engineers. The test cases are sufficiently self-explanatory and provide regression coverage. The example in the issue is precise, even if the sentence structure is a bit colloquial. This should be suitable for benchmarking within the target audience.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12663": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description describes a regression: using a SimpleLazyObject with a nested subquery annotation in Django now fails, whereas it previously worked. The issue provides a stack trace illustrating the error, including the final TypeError and its context, which is reasonably informative about the problematic code path. However, it lacks a precise, minimal reproducible example or explicit high-level requirements for what 'support' for SimpleLazyObject should entail (e.g., whether it should always be accepted, how it should be handled in get_prep_value, etc.). Still, the typical engineer familiar with Django ORM can reasonably infer the expected behavior: restoring compatibility with SimpleLazyObject in annotated querysets so the regression is fixed. There are minor blanks (exact user expectations, maybe edge cases), but the goal is interpretable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual solution is a small, localized change to how an output field is resolved in the ORM's SQL query logic, specifically restoring/fixing attribute access in a property to handle the structure as it arrives when SimpleLazyObject is involved in subquery annotations. This fix is not trivial\u2014one must trace through the code (with the help of the stack trace), understand why the regression occurs, and know how Django field resolution works. However, it does not require massive refactoring, changing many files, or introducing new abstractions: it is a surgical, thoughtful fix that could be completed in 15 min to 1 hour by an experienced Django developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further blocking issues. The issue is suitable for the benchmark, as it illustrates regression diagnosis and a targeted fix within an advanced Python/Django codebase. Anyone evaluating coding ability via this sample should be sure to assess Django ORM and testing familiarity, but the use case matches the benchmark's goals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12708": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides the context that a migration in Django crashes when trying to delete an index_together on fields that already have a unique_together constraint, due to both constraints existing and causing a ValueError. It cites a specific Django version and the line of code that fails, giving a concrete stack trace. The user also splits their report into two actionable items:\\n1) Deleting index_together should work independently or be coherent with unique_together.\\n2) Refactoring index declaration shouldn't trigger unnecessary index recreation.\\n\\nWhile the description is fairly technical and does not exhaustively specify the expected behavior for all edge cases, it is clear enough for a knowledgeable engineer to reproduce, understand, and fix the core problem: index deletion fails due to multiple constraints (index and unique) on the same fields. The desired relationship between index_together and unique_together is implied, and the references to migration operations and code file are concrete. However, there is some room for interpretation (e.g., the ideal behavior when refactoring declaration), so it's not a perfect specification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue sits between a small change and a moderate refactor because although the code fix itself is relatively small, it requires a decent understanding of Django migrations, database schema management, and how constraints work within the schema editor. The engineer must reason about the interaction between different types of constraints (index, unique) and make sure that deleting one constraint does not inadvertently remove the other. The fix impacts migration operations, so careful consideration and testing are required. Additionally, updating or adding tests to verify the correct behavior is non-trivial given the subtleties of constraints on complex schemas.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue, while technical and requiring framework-specific knowledge, is an appropriate challenge for a coding benchmark assessing a mid-level or senior engineer familiar with Django.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12754": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a clear error message and describes the problematic scenario: migrating a field from a base model onto a new subclass in Django causes a FieldError due to duplicate field names. It does not, however, specify exactly what the correct migration or behavior should be in more depth, or which file(s) to focus on in core Django. There is a sensible interpretation of the fix (ensure migrations are ordered so the field is first removed from the base before being added to the subclass), but some context and blanks must be filled in\u2014such as how Django's migration engine should handle the dependency ordering and model state. Thus, the issue is not as detailed as it could be but is sufficiently specified for someone familiar with Django migrations to attempt a meaningful solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this issue requires a deep understanding of Django's migration engine, including how model states are tracked and how migrations are generated and ordered. The fix involves carefully modifying logic in the migration autodetector to produce correctly ordered migration operations when fields are moved between base and subclass models. Although not a massive change, it is non-trivial and likely to take between 1 and 4 hours for an experienced engineer who is familiarizing themselves with the migration system, as it requires reasoning about edge cases and potentially subtle migration dependencies.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present for using this issue as a benchmark. The scenario is realistic and the core challenge is suitable for evaluating an engineer's understanding and problem-solving ability regarding Django migrations and model inheritance.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12858": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete error raised by Django's system checks, specifically models.E015, indicating that the 'ordering' Meta option in a model refers either to a nonexistent field, related field, or lookup. It then gives an example (supply__product__parent__isnull) that triggers this error. The relationships (Stock.supply, Supply.product, Product.parent) are described, offering enough information to see that 'isnull' should be interpreted as a lookup. It references a recent change that may have introduced the regression. However, it does not explain in detail how lookups vs. transforms are validated in the codebase or the desired/expected behavior\u2014so the engineer must infer the correct logic (that lookups as well as transforms should be allowed for Meta.ordering). Thus, some assumptions are needed about the solution, but an experienced engineer can make a sensible, actionable interpretation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue can be solved with a local change to the system-check logic: modifying the code that checks for valid 'ordering' arguments to permit lookups as well as transforms. This is a small conceptual extension to the previous logic. The patch itself is concise. However, it may require some investigation into Django's ORM field/lookup/transform validation logic and the relevant function (e.g., _check_ordering) to confidently implement and test the change. An experienced Django maintainer should be able to make the change, write/adjust tests, and verify the fix within 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with the sample. The problem and expected solution are well-rooted in Django SystemCheck framework, and the error message plus model structure make the reproduction straightforward. Test case changes are tightly connected.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13012": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly explains a specific bug: when using ExpressionWrapper with a constant expression (e.g., Value(3)), Django's ORM generates a SQL query with the constant in the GROUP BY clause, leading to a PostgreSQL error. The exception message is quoted, providing direct evidence of the problem. However, the issue is somewhat under-specified in terms of where and how the fix should be applied. It requires familiarity with how the ORM compiles expressions (e.g., where get_group_by_cols is relevant). Nevertheless, for someone familiar with Django\u2019s internals, the bug and desired behavior are clear: constant expressions should not be included in GROUP BY. So, while there are some blanks to fill in (e.g., precisely where to implement the fix), the requirements are sensible and actionable for an experienced engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is rated as a small change that requires some thought. The actual code change is quite targeted (a method addition/override), but it does require understanding how expressions propagate through Django\u2019s ORM, where GROUP BY columns get computed, and why constant expressions should be omitted. Familiarization with the expression system is required, but the scope remains within a single class (ExpressionWrapper) and related testing. Thus, an experienced Django developer could address this in 15-60 minutes after understanding the problem.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The example is representative of a real-world bug involving the Django ORM, requires both understanding and code changes, and is testable via provided test code. The only slight ambiguity is the implicit assumption that the reader knows how ExpressionWrapper and Value interact, but this is acceptable for an experienced engineer.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13028": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides enough information for an experienced engineer to reproduce and understand the error: there is an interaction in Django 3.0.7 where having a BooleanField named 'filterable' on a model seems to cause NotSupportedError in queryset filtering, and changing the field's name fixes the bug. The issue includes a code snippet of the models involved and the full error traceback. However, the description does not prescribe exactly what the correct behavior should be or how Django internals should handle custom fields named 'filterable'. There's a sensible interpretation: Django shouldn't treat a field named 'filterable' as the built-in attribute, so the solution is either documenting this or making the ORM more robust. An engineer can attempt a meaningful fix based on that. But the text leaves some ambiguity about whether the change should be code or documentation, so it's not perfectly specified but is workable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Once the root cause is understood (a naming collision where a user-defined field named 'filterable' accidentally interferes with Django's ORM internals), the code change needed is conceptually minor: ensure Django checks for the attribute only on appropriate objects, not any with a 'filterable' attribute. The patch is very small; solving it mainly requires tracing the code path and understanding the background. For an experienced Django engineer, this would likely fall within a small change needing a bit of thought (15 min - 1 hour).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major additional issues with using this sample. It exercises knowledge of Django internals and a subtle model-ORM interaction, which could be educational for a benchmark. One could mention that the model refers to outside types (Produit, Plateforme) but for the benchmark setting, this can be assumed resolvable in context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13112": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides a concrete bug scenario: makemigrations crashes if a ForeignKey references an app label with mixed casing that does not match the lowercased app label in INSTALLED_APPS. Example model code, settings.py, and apps.py are provided to illustrate the source of the error. The stack trace and exception message directly identify the problematic field and reference style. The reader can deduce that the core problem is inconsistent/mixed-case handling of app labels when resolving model references. All necessary context is included for an experienced engineer to reproduce, diagnose, and devise a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a targeted, non-trivial change to how string references to models are processed, specifically in relation to their case sensitivity and normalization. The solution involves understanding how Django reconstructs model references in migrations, which, for an experienced engineer familiar with Django's ORM, should take between 15 minutes to an hour to fix - it's not just a one-line change, but also not a complex multi-file or architectural rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The description covers the problematic code and expected behavior. The only mild ambiguity relates to broader edge cases (e.g., non-ASCII or further mixed-case testing), but for the provided scenario, it is suitable for coding evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description identifies a specific problem: performing temporal subtraction (such as subtracting two DateTime fields) fails in Django unless an ExpressionWrapper is applied because of mixed types in expressions. The error message is clearly cited, and the request to \\\"make temporal subtraction work without ExpressionWrapper\\\" provides a sensible, actionable requirement. However, it is somewhat terse and assumes familiarity with Django's expression system; it doesn't enumerate affected code locations, test scenarios, or give much context. An experienced Django contributor can reasonably infer what needs to be done: modify query expressions so that temporal subtraction supports DateTimeField and DurationField without needing ExpressionWrapper. Still, a few open questions (e.g., do any backwards compatibility or side effect concerns exist?) remain. Thus, it is sensibly specified but not exhaustively so.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix involves nontrivial internal logic changes in Django's expressions system. It requires careful handling of type inference, branching based on connection and field types, updating core internal logic, and adding/adjusting tests. The solution modifies and extends the logic of a core method (as_sql, resolve_expression), and affects several tests, indicating a reasonable scope of affected code. This would likely take 1-4 hours for a competent Django developer familiarizing themselves with the codebase, testing, validating on multiple database backends, and ensuring compatibility. It is not a trivial nor a superficial change but doesn't rise to the level of a major subsystem rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues apparent. The issue is sufficiently well-specified for an engineer experienced with Django, the test coverage is good, and the scope is appropriate for an evaluation task\u2014provided test and code diffs are given. No blockers found for use in a coding ability benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13297": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified, though not fully explicit. The description clearly identifies a crash occurring when filtering in TemplateView.get_context_data() due to kwargs containing values of type SimpleLazyObject, which are not being properly converted, leading to errors with get_object_or_404 and SQLite parameter binding. There is explicit mention of the type misbehavior and a workaround (explicit conversion to string). However, the description could be improved by being more direct about the intended solution (ensuring kwargs are converted to the correct type before usage in contexts like filtering or lookups) and the expected type. Still, an experienced engineer can sensibly interpret the intent and required outcome: to avoid passing unresolved SimpleLazyObject instances to code that requires concrete types, especially for use as DB query parameters.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small code change (~1-2 lines), but requires an understanding of Django internals, specifically how SimpleLazyObject is used in context data and its implications in template rendering and database interaction. The fix is not completely trivial, as it needs a bit of investigation to realize where coercion or lazy evaluation should occur, but once the spot is found, the change itself is straightforward. Reading the error and reproducing the type/context issue would take some time, but an experienced Django engineer can resolve this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major concerns. The test patch shows a clear, testable regression and added validation for the use case, and the description is sufficient to deduce what is wrong and what should change. The error only appears in certain template or context uses, but that's a normal level of context for this kind of bug report for a major framework. This is a good benchmark candidate for coding ability relevant to framework bugfixes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13406": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear, reproducible scenario\u2014serializing and deserializing a Django queryset with values()/values_list() (and annotate)\u2014and describes the observed behavior (incorrect type returned after unpickling, AttributeError on .id). The stacktrace is included, allowing an engineer to reason about where the deserialization and iteration go wrong. However, while the problem is well-described, the exact expected internal Django behavior (e.g., how iterable class assignments are meant to persist through pickling) is not fully stated. An engineer familiar with Django internals would reasonably infer the expected fix (ensure values()/values_list() queries are restored properly on unpickling), but there's some room for interpretation about exactly where to apply the fix. Thus, it qualifies as 'some blanks to fill but a sensible interpretation' (level 1).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Although the underlying internals of Django's queryset evaluation and pickling might require some inspection, the problem is ultimately a mismatch between the state of the query and the iterable class after unpickling a query object. The patch is a concise change\u2014setting _iterable_class appropriately when reassigning the query\u2014which would likely take an experienced Django engineer 15-60 minutes to trace the cause, write a fix, and add tests. It's not a trivial one-liner, but it doesn't require deep multi-file refactoring or a complete rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The issue is clearly reproducible, and the description plus patch show that the solution doesn't require external context or heavy reverse engineering. The test cases are sufficient and capture the required coverage for the fix.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13449": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified: it clearly states that the use of the Lag() function with a DecimalField on SQLite fails with a SQL syntax error due to improper placement of the CAST() statement in the SQL generated by Django's ORM. It describes what happens (including a SQL example and the error), gives a plausible diagnosis (the placement of CAST()), and provides an example of a workaround using output_field=FloatField. The scope of the fix is fairly clear: the ORM should generate a CAST around the entire window function, not just the inner portion. However, exact implementation details, such as where in Django this fix should go or considerations around type compatibility, would be up to the implementer to determine. There is a sensible and clear path to a solution based on the information given, but the implementer is expected to fill in some gaps regarding integration with existing codebase structures.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a good understanding of Django's ORM internals, especially how window functions are compiled into SQL for different backends like SQLite. The fix would involve both understanding the SQL generation logic and making sure changes do not break other behaviors, especially regarding data types and window expression composition for SQLite and possibly other DB engines. While not an extremely large code change, it requires careful design and an understanding of framework-level abstractions, so it fits into the 1-4 hour range for an experienced engineer after onboarding.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The issue is appropriate for benchmarking coding ability, as it requires both diagnosis and framework-specific implementation skills. The provided information is enough to attempt a real solution and the expected fix is non-trivial but clear in direction.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13568": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides enough context for an experienced Django engineer to understand what needs to be fixed: the auth.E003 check currently enforces that USERNAME_FIELD must have unique=True, but in some situations, uniqueness can also be guaranteed via a UniqueConstraint (including total_unique_constraints) at the model level. The reporter explains the performance reasons for not using unique=True directly and points to the need for the system check to consider constraints. However, the text is somewhat informal and does not specify exactly how the new logic should be implemented or what constitutes 'a sufficient uniqueness guarantee'\u2014for example, how partial, conditional, or multi-field constraints should be handled. The intent is clear and sensible, but there is room for reasonable interpretation in implementation. Thus, it's a '1': well-specified with some gaps an engineer must sensibly fill in.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively contained issue that requires modifying the logic in the system check that enforces uniqueness of the USERNAME_FIELD. The core change involves examining additional model metadata (total_unique_constraints) and adjusting the existing check accordingly. While it takes some familiarity with the Django ORM metadata APIs, the required code changes are localized, and the main challenge is in reasoning about where and how to expand the check to look for uniqueness via constraints rather than only the field attribute. Implementing and testing this, for an engineer familiar with Django, would likely fit into the 15 minutes to 1 hour range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are unaddressed. The issue is usable for coding benchmark purposes. The main challenge lies in understanding Django's model constraint APIs and in reasoning about 'unique' enforcement semantics. Test coverage is included, and the problem/solution are constrained enough for clear evaluation. The issue's slight ambiguity is of the kind that commonly arises in real bugfixes. There are no external dependencies or project-specific blockers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13807": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It describes a specific crash occurring when table names in SQLite are SQL keywords (such as 'order'), provides the error message (OperationalError: near \\\"order\\\": syntax error), identifies the root cause as missing quote/backticks around table names in SQL statements, and gives detailed file and line locations within django/db/backends/sqlite3/base.py. It also instructs that the problematic code is in 'check_constraints', showing the exact code snippet and logic flow. Any experienced engineer can reproduce and understand the problem and what is required: properly quoting table names in SQL statements to handle reserved keywords, using the correct method (as django offers quote_name). The desired outcome (not crashing, supporting SQL keyword table names) is clear from context.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves identifying where table names are interpolated into SQL statements and updating the code to quote them properly using the existing Django API. The code to update is localized, and the main required logic\u2014quoting table names in SQL\u2014is straightforward for someone familiar with the codebase and database handling. The task requires understanding how Django constructs SQL statements for SQLite and being aware of quoting strategies, but does not demand a major refactor, multiple file changes, or high-complexity investigation. It should take less than an hour for an engineer familiar with Django internals and SQL.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue is concrete, technical, and well-captured by the given description and example. The nature of the bug and appropriate solution are both clear from the report. The only extra knowledge needed is familiarity with SQL injection/quoting and Django's API for quoting names, which are expected from engineers in this context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13810": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. The submitter identifies a problematic side effect with the Django middleware loading process in an ASGI context when a MiddlewareNotUsed exception is raised. Specific line references are provided (django/core/handlers/base.py#L58), and the symptoms (TypeError with HttpResponse in 'await' expression) are described. The reporter explains the sequence of events and posits a root cause: that the handler variable is overwritten, potentially contaminating the middleware chain with an incompatible handler type (sync used as async). However, there is ambiguity in the scope of the fix required\u2014whether to only fix the variable assignment or to also alter documentation, add async support guidance, etc. The minimal, core code change is clear, but a fully robust solution might require clarification. Nonetheless, an experienced engineer could make a reasonable and sensible interpretation of what is expected.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding Django middleware mechanics in both sync and async (ASGI) contexts, as well as tracing handler assignment logic and exception handling flow. Fixing the handler assignment to ensure async/sync compatibility is not trivial, but the main change is localized to a function in one file. The bug, cause, and fix are contained and testable; a skilled developer familiar with Django's middleware lifecycle and ASGI concepts could implement a solution in 15\u201360 minutes after understanding the codebase and the issue's context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues. The issue, while slightly ambiguous in breadth (code fix vs. documentation), provides enough technical detail and context for a meaningful attempt at a technical fix relevant to the benchmark's requirements. Tests can be constructed or reused from the patch to verify correct handling of sync/async middleware under ASGI. The sample is suitable for evaluating coding ability in the outlined setup.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14017": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and direct. It provides a minimal, clear example demonstrating the inconsistency in operator behavior between Q and Exists objects in Django's ORM, specifically highlighting that 'Exists(...) & Q(...)' works but the reverse raises a TypeError. The expected behavior is stated explicitly: the & and | operators should be commutative on Q-Exists pairs. The error traceback shows where the TypeError is raised, and the user suggests a likely missing __rand__ implementation or similar mechanism. An experienced engineer familiar with Django or operator overloading can clearly see the problem and what a successful solution would look like: ensure commutative usage of the bitwise operators between Q and Exists. No code ambiguities or requirement uncertainties are present.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is narrow in scope\u2014it's an operator overload issue involving type compatibility and commutativity for custom ORM expression objects. Solving it primarily requires (1) understanding how Q and Exists interact with Python's __and__/__rand__ (and/or) overloads, (2) modifying a single function to extend its type acceptance or to handle conditional objects generically, and (3) possibly adding or adjusting tests. For an engineer familiar with Django internals and Python's data model, identifying and implementing the fix would likely take less than an hour, though testing would be necessary.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample meets all requirements for use in a coding benchmark: the issue is clear; its scope, cause, and resolution are well-delineated; and it is suitable for evaluating understanding of Python dunder methods and type handling. No other issues identified. The context is self-sufficient and does not reference external documentation or require extra clarification.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14140": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified but requires some filling in of blanks. It states that combining Q() objects with boolean expressions causes a crash due to how single-child Q objects are deconstructed, specifically when the child is not subscriptable. The traceback explains the technical problem (attempting to subscript an 'Exists' object that is not subscriptable). The description also references a proposed patch and an alternative backward-compatible approach, but the main point is clear: the special case in Q.deconstruct for single-child Q objects leads to a bug when the child is not a tuple. However, the full technical context about Q objects and how they are meant to be deconstructed in Django requires some inference (i.e., that deconstruct() should treat all children as tuples). Still, an experienced Django developer would be able to interpret the core requirement and write a fix, even if some minor details (like legacy concerns) would need to be decided independently.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required fix is relatively straightforward for someone familiar with Django's ORM and the Q object. The core task is a small logic change in 'Q.deconstruct', removing a special case and ensuring all children are handled uniformly, as reflected in the patch (just a few lines changed). The primary challenge lies in understanding the implications of altering how Q objects deconstruct, but the stated problem and provided traceback make the issue concrete. Ensuring backward compatibility and proper test coverage requires some care, but the actual code change is modest. This could be reasonably completed in under an hour by an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The sample is suitable for use in a coding benchmark for Python engineers familiar with Django. The only requirement is that the engineer should have some awareness of how Q objects and query composition work in Django's ORM system, but this is expected in such benchmarks.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14238": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a specific error traceback and pinpoints the failure to the handling of subclasses in Django's AutoFieldMeta.__subclasscheck__, particularly with custom subclasses of BigAutoField and SmallAutoField used as DEFAULT_AUTO_FIELD. It also explicitly suggests that fixing subclass checks in AutoFieldMeta.__subclasscheck__ would resolve the issue. However, it does not provide detailed examples or a minimal code snippet that triggers the problem, nor does it deeply explain why the current logic is insufficient or what the expected semantics should be for \\\"subclass of AutoField\\\". Nonetheless, for an engineer familiar with Django internals, it is relatively clear how to proceed and what behavior needs to be fixed. Thus, there are some blanks to fill in (exact reproducer, expected vs actual for custom subclasses), but there is one sensible interpretation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves understanding Django's metaclass system, specifically how __subclasscheck__ is used for AutoField and its custom subclasses. The actual patch is a one-line change, switching from an 'in' check to issubclass in the __subclasscheck__ method, which is a small code change. Some thought is required to ensure correctness\u2014particularly in recognizing that the old check only accepted registered classes, not subclasses, and the intended semantics is to accept all subclasses. The required understanding is moderate, but the code change is simple. Implementing and testing would likely be accomplished in less than an hour by an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues outside what has been noted. The sample references Django internals, but with the issue text and traceback provided, and basic Django domain knowledge, it is suitable for benchmarking coding ability. The test coverage in the patches also seems comprehensive for this fix. No blockers identified.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14351": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly technical and assumes familiarity with how Django ORM Q objects, aliasing, and field selection work during ORM query compilation, specifically with respect to changes introduced between Django versions 2.2.5 and 3.2. It presents a specific error message and reproduces the problem context by describing differing behavior when using '__in' vs '__id__in' field lookups in Q-object OR conditions, leading to a multi-column subquery where only a single column is expected. However, the description doesn't fully specify target filenames or expected outcome beyond the error avoidance. While an experienced Django developer could reasonably interpret what needs to be fixed (ensure subqueries only include one column, likely the PK), there are some blanks regarding exactly how and where to implement the fix, so some judgment is required in choosing the appropriate module and method to patch. Overall, there is a sensible (if technically deep) path to a solution. \",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires nontrivial investigation into the Django ORM codebase, understanding of how query compilation and field selection work, and knowledge of backwards-incompatible changes between Django versions. Implementing the fix is not just a matter of a one-line change; it involves identifying the flow that selects columns for subqueries involved in Q object OR conditions and ensuring that in those circumstances only the correct columns are selected. The patch is isolated to a single file but introduces a new class method to manage the group-by columns correctly, and this behavior must also be tested, likely requiring significant time understanding the ORM internals and crafting a passing test case.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues stand out for use as a coding benchmark sample. The patch and test case are focused and traceable to the described bug, albeit niche.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14493": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a clear traceback indicating the specific error occurring (`UnboundLocalError: local variable 'substitutions' referenced before assignment`), and identifies the likely problematic line in the codebase (`staticfiles/storage.py`). It also explains the context in which `substitutions` may be undefined (if the loop is not entered), and why `max_post_process_passes` might be set to 0. However, while it pinpoints the cause of the bug, there is a small blank to fill in: the reporter does not explicitly state what the fix should be (e.g., initializing `substitutions` before the loop), although this is a very reasonable inference. The overall intent and the target function/file are all clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires analyzing a traceback, understanding a straightforward code path, and applying a small fix (initializing the variable before the loop). Everything is localized to a specific area in the code, and the necessary change is small but requires some thought\u2014especially to understand why the bug only occurs under certain configurations. An experienced engineer familiar with the codebase could apply and test this fix in around 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no blocking issues or reasons why this sample should not be used for the coding benchmark. While it references some external material (a code motivation link), it is not necessary for understanding or solving the immediate technical issue. The issue and solution are clear and self-contained.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14580": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified. It describes a concrete error: migrations generated by Django's migration system are missing an import statement for 'models', leading to a NameError when referencing 'models.Model'. There is a clear error message and context, and the scope of the affected component is suggested (django.db.migrations.writer). While the reporter is not 100% certain of the module to fix, and the circumstances under which the bug occurs are not described in detail (i.e., what kind of model setup triggers the missing import), the main problem\u2014generated migration files missing a necessary import\u2014is clearly stated. This provides a sensible basis for an engineer to attempt a solution, with only minor blanks regarding the precise triggers of the bug.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized and relatively straightforward: it involves updating the serializer logic so that references to 'models.Model' in generated migrations also include the required import statement. The patch shows a one-line change and some added tests. An engineer would need to understand how Django serializes models for migrations and where imports are injected but, for someone familiar with Django's migration system, this would take between 15 minutes and 1 hour to read the code, identify the responsible code path, implement, and test. Some initial investigation is required, but the change is neither conceptually complex nor broad in scope.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues here. The sample is suitable for benchmarking coding ability: the problem is concrete and the solution requires a precise code change as well as addition of test coverage to prevent regression. The primary challenge is situating oneself within Django's migration code, but the problem and solution are technically clear.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14672": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified for an experienced engineer familiar with Django's internals. It clearly identifies the affected area ('ManyToManyRel' in reverse_related.py), the method/property involved ('identity'), and what has been missed (a call to the 'make_hashable' function on 'through_fields'). It provides both the relevant context (recent changes introducing the 'identity' property, hashing for comparison, why hashability matters) and a concrete error trace demonstrating the bug ('TypeError: unhashable type: 'list''). It also gives a direct proposed solution, specifying exactly where to add the 'make_hashable' call. While some background knowledge of Django models and their field relations is presumed, engineers with this background would have no ambiguity about what needs to be changed and why.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This problem requires finding the right spot in the code ('ManyToManyRel' in reverse_related.py) where 'through_fields' is included as part of an 'identity' tuple and making sure it is hashable (by wrapping it in the 'make_hashable' function). While it requires understanding how Django model fields and reverse relationships are implemented, the fix is technically simple (a one- or two-line change). The error trace directly links the location of the bug. Thus, it is a small change requiring minimal research for someone familiar with Django, and should take 15 minutes to an hour, accounting for investigation, local testing, and review. There are only very minor subtleties (e.g., making sure the fix doesn't break other behavior), but the main coding task is straightforward.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blocking issues. The sample provides all required details\u2014code location, context, and rationale\u2014for a meaningful coding benchmark. The stack trace and proposed solution leave little room for ambiguity.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description makes clear what the specific problem is: when using method_decorator on a method, the inner function that ends up being decorated is a functools.partial object which lacks attributes like __name__, __module__, etc. The consequence (an AttributeError when accessing __name__) is provided, and it's apparent that the goal is for method_decorator to ensure the wrapper has these attributes as expected. However, the description does not state explicitly that functools.wraps or a similar technique should be used to preserve these attributes. Thus, there are some blanks for the engineer to fill in (the exact mechanics and strategy for copying/wrapping), though a sensible solution is implied by Python's best practices and the error cited. So, it fits criteria 1: some blanks to fill in, but a clear and actionable problem.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution involves understanding Python's decorator mechanics and how functools.partial interacts with function attributes. An experienced engineer would likely know about functools.wraps and understand where to apply it. The fix, as shown in the patch, is a localized one-liner, but may require some code investigation to properly locate. Thus, it is more involved than a trivial typo or assertion, but is still a small, focused change that is realistically achievable within an hour, likely less.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major blockers. The issue is clear enough for a single-feature coding benchmark. There might be ambiguity if the engineer is not familiar with Python decorators or partials, but that's within expected skill requirements for such a benchmark. The test patch provides a clear check that verifies the intended fix, which is a strong positive for its inclusion.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15104": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides specific details about the problem, including the error message, the stack trace, and a clear diagnosis of the cause: a custom ForeignKey field that hardcodes and then removes its 'to' argument, triggering a KeyError in the migration autodetector. The description also proposes a simple fix (replace 'del deconstruction[2][\\\"to\\\"]' with 'deconstruction[2].pop(\\\"to\\\", None)'). However, the description assumes some prior knowledge, such as understanding what 'deconstruction' is and why removing 'to' causes the KeyError, and it does not reference concrete filenames or line numbers for the actual patch, but with the traceback and explanation an experienced engineer could reasonably infer what to do. Therefore, it is not as complete as the best issues but still provides a sensible interpretation for a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the stack trace is quite long, the actual code change and the reasoning behind the fix are simple and directly indicated by the issue reporter. Locating the specific instance in the code and implementing the pop instead of del, plus making sure the change works with relevant test coverage, are straightforward efforts for someone familiar with the Django codebase. The core understanding and the solution would likely take less than an hour to implement.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues present. The error and solution are well-explained, and the accompanying patch shows the entire change localized to one function. An engineer evaluating this would experience a standard debugging scenario.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified for an experienced Django engineer familiar with the ORM and query building internals. The reporter explains the source of the AssertionError in Query.change_aliases, outlining when it is triggered (intersection between change_map keys and values due to conflicting alias changes when combining querysets). They also propose a concrete direction for a solution: incrementing the suffix of new aliases until there is no intersection, and note where documentation or clarification is missing in the code. While not every implementation detail is spelled out, a competent engineer would be able to sensibly reproduce and resolve the problem with what's provided, focusing on the aliasing logic in QuerySet combination code paths. However, ambiguities exist regarding exact expectations around test coverage and behavior in edge-cases, so it's not a 'perfectly specified' issue.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this bug requires in-depth familiarity with Django's ORM codebase, particularly the query aliasing and combining logic, and a correct understanding of the AssertionError's context. It is a non-trivial change: the solution involves adding logic to prevent intersecting alias remapping, updating alias prefix logic, optionally refactoring the functions for clarity, and ensuring side effects (such as subquery behavior) are properly considered. The test changes provided indicate additional test models and cases are needed. This would require careful setup and validation. For someone competent in Django, this would likely take 1-4 hours to understand the code, reason through the edge-cases, implement, and verify the fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional concerns. The issue is a good candidate for evaluating coding ability involving debugging and internal API understanding. Only note is that domain knowledge of Django internals is helpful for a timely solution, but this is in scope for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15277": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It identifies a concrete bug caused by instantiating a CharField with max_length=None, which results in a broken MaxLengthValidator being added and leads to a TypeError at runtime. The scenario and point of failure are described clearly, with references to the specific code involved (Value._resolve_output_field, CharField.__init__, MaxLengthValidator). The suggested fix\u2014to only add the validator if max_length is not None\u2014is both clear and justified by precedent (BinaryField.__init__). The engineer is told what is wrong, why it's wrong, and how it can be fixed in the code, so there is very little ambiguity about what a successful solution would look like.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires modifying the CharField.__init__ method to add a conditional check before appending a MaxLengthValidator. Although it touches a core area of Django, the change itself is small and local: it involves wrapping an existing line with an if-statement. Understanding the reason for the error and confidently making the change would probably take between 15 minutes and 1 hour for an experienced engineer familiar with the codebase, mostly to review usage context and potential impacts. It does not require deep refactoring or editing across many files.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is suitable for a coding benchmark. The test context is clear, and the problem scope is self-contained. The only minor complexity is ensuring that the change does not affect any other downstream logic reliant on validators, but this concern is standard in such micro-optimizations. No other major issues are present.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15280": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text gives a concrete example with Django models and describes both expected and actual behavior. The problem is that deferred fields are being accessed in such a way (through prefetches and relationship traversals) that Django issues additional queries when it should not. The specific offending behavior and evidence leading to the bug (via assertions and output of get_deferred_fields()) are provided, along with the context of relationship configuration (OneToOneField or ForeignKey). An experienced engineer can infer that the correct solution is to prevent the execution of unnecessary queries caused by incorrect deferred field metadata propagation. However, the ask to \\\"confirm it's actually a bug and point me in the right direction\\\" shows some uncertainty and means there is minor room for interpretation. The issue does not specify the resolution exactly, but sufficiently describes the high-level problem for a meaningful attempt. Thus, this sample falls into category 1 (some blanks, but a sensible interpretation is possible).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This bug involves nuanced behavior in the Django ORM's prefetching, deferred field handling, and caching mechanisms. Fixes here demand a solid understanding of Django's model relationship internals, deferred fields, and query execution logic. Identifying the root cause and making the necessary code changes, along with writing or tweaking targeted test cases (as the provided patch indicates), likely takes more than an hour but could be handled within a half-day by an experienced Django contributor familiar with the affected codebase. This is more than a simple tweak; it requires debugging through Django's internals and carefully testing results. I therefore assess the difficulty as '2: 1-4 hours'.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent. The scenario is well-suited for benchmarking, as it combines a clearly described ORM bug, a specific reproduction case, and precise requirements for 'passing' (i.e., no extraneous queries when accessing deferred fields via prefetches and relationships). The provided test demonstrates the new guarantee under correct behavior. The only additional context required is familiarity with Django's ORM internals, which is reasonable for the skill level evaluated here.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15315": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear high-level account of the problem: Field.__hash__() changes value when a field is added to a model class, which can cause issues when the field is used as a dict key. The description references a previous PR where the change was introduced and recommends reverting that aspect. It provides context about why having an immutable hash is critical. The affected method and class are specified (Field.__hash__), and it is clear that the desired behavior is immutability of the hash value regardless of assignment to a model. However, the description assumes some familiarity with the previous PRs and Django's model system, and does not spell out exact implementation details, so there are minor blanks to be filled in by the engineer (e.g., how exactly the original hash was implemented before the bug and what aspects need to be reverted). It is well-specified enough for an experienced engineer to successfully design a meaningful solution, though not exhaustively prescriptive.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core of the fix is in re-implementing the __hash__ method of the Field class to be immutable\u2014likely reverting it to its previous definition using only the creation_counter, rather than mutable properties affected by model assignment. This requires understanding the consequences of object hashes changing, but the code change itself is relatively contained to one class and one method. There may be a need to consider test cases but overall, this is a small-scale change that does not require major rewrites or deep design work, and could be reasonably completed in less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent. The description and associated context are sufficient, as are the test modifications validating the change. The engineer may need to familiarize themselves modestly with Django's model and Field machinery, but this does not pose a major obstacle given the scope and directness of the issue.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15375": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it identifies a concrete bug where using the new 'default' argument in aggregation after annotate() in Django 4.0 causes an SQL syntax error. The issue provides a minimal reproduction, the version affected, and the unexpected SQL. However, the scope is implicit\u2014while it is clear the aggregate 'default' argument interacted badly with annotate(), it's not 100% explicit what the desired behavior is, e.g. should it mimic the old aggregate behavior, or just not error. Still, an experienced engineer would reasonably infer what is expected, and the code/tests in the patch confirm this understanding.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding Django's ORM aggregation and annotation machinery, identifying why the 'default' argument causes malformed SQL, and patching the treatment of the default argument within the relevant class (aggregates.py). It is not a trivial fix, but not a major refactor; the core solution is a targeted change within a function, a level of change and research expected to take under 1 hour for a familiar engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues with the sample. The bug and fix are reasonably isolated and the information provided in the issue text is sufficient for a meaningfully scoped benchmark task. Only minor ambiguity about precise expectations, but nothing that blocks a solution or test authoring. No external/systemic blockers present.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15380": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete error traceback, including file names and line numbers, pinpointing that the crash occurs in the migration autodetector when renaming both a model and a field. The stack trace shows that there is a KeyError when trying to access a model key in self.to_state.models. This gives a knowledgeable engineer a clear starting point: investigate the internal logic of renames inside the autodetector and determine why model keys may not be present after renames. However, there is no explicit description of the user's exact intended workflow or how the renames are performed in practice, only the resulting error. Thus, while there are some blanks to fill (in inferring that the fix must allow for successive model and field renames, and must not assume the original model name is always present in to_state.models), the context is sufficient to reasonably interpret the root problem and devise a fix, as seen in the proposed patch and corresponding test.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue ultimately requires tracking down a KeyError in the migrations autodetector, reading the related code, understanding the control flow, and correcting a reference to the model's post-rename name, rather than the original name. While this takes some familiarity with how Django tracks state and renames, it only involves modifying one line of logic and is isolated to a single function. The solution also does not involve changing fundamental algorithms or coordinating changes across multiple modules\u2014just ensuring the correct key is selected. For an engineer familiar with Django or similar systems, this is a small, localized change that would take under an hour, though it could take a bit longer for someone unfamiliar with Django migrations.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues with the sample for evaluating engineering ability. The issue is self-contained, has a concrete error and stack trace, and the expected outcome can be inferred with reasonable confidence. The patch and test changes back this up, showing the problem is well-localized and can be properly tested by extending an existing suite.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15503": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified, but not perfect. It clearly describes that JSONField lookups such as has_key, has_keys, and has_any_keys do not handle numeric keys properly when used on certain databases (SQLite, MySQL, and Oracle) but behave as expected on PostgreSQL. The description includes a concrete symptom (test failure with key '1111'), an example error message and relevant environment details. However, it does not go into extensive detail about the expected behavior for all JSONField lookups or across all possible input types, nor does it specify desired edge-case handling (e.g., string vs integer keys, nested keys, array behavior, etc.). Still, there is a sensible interpretation for a successful solution: ensure these lookups work consistently for numeric string keys on all supported database backends. There may be non-trivial implementation details left to the engineer, but the high-level requirement is clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This is a moderately difficult issue to resolve. It involves understanding Django's ORM internals and implementation of JSONField lookups, as well as the differences in how various database backends (SQLite, MySQL, Oracle, PostgreSQL) handle JSON field queries, especially with numeric string keys. A solution requires both investigation and code changes to ensure consistent cross-backend behavior. Editing or extending test coverage is also needed. An experienced engineer would likely take between 1 and 4 hours to resolve this, including time to familiarize themselves with the relevant code, devise and test a fix, and validate solution correctness through tests across multiple DBs.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues found. The sample is suitable for estimating coding ability, as long as the benchmark user is expected to bring their own understanding of Django QuerySet and backend differences. The description maps to a concrete and testable functional regression, which is validated by changes in test files. The most complex aspect is ensuring the fix is done in a cross-backend compatible way, but the PR test changes also help clarify expectations.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15525": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified: the problem (loading a fixture fails on non-default database when natural keys use FKs) is described in concrete terms, with example models/code, an error traceback, and context of the environment (multiple DBs, Django, custom natural keys). The underlying cause is not explicitly identified in the issue text, but it is clear that the loaddata command with a fixture fails when using a non-default db, specifically when natural keys involve a foreign key. For an experienced Django engineer, these details are enough to understand what needs investigation and what a successful solution entails (making loaddata with such natural keys work on alternate DBs). Still, a bit of investigation is needed to clarify edge cases and whether the bug lies in model, manager, or serialization logic.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix requires understanding of Django internals\u2014specifically the serialization/deserialization process and how natural keys, managers, and database routing interact. The bug is fairly deep in the framework, including understanding how models' natural keys and their FKs are resolved during deserialization, and how the 'using' parameter propagates through that stack. While the actual code fix is small, arriving at this diagnosis and solution will take significant time, especially for someone not already familiar with Django's serialization, test infrastructure, and multi-DB setup. Locating exactly where to patch, verifying correct database routing, and writing an appropriate regression test all require non-trivial work and working knowledge of Django's codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues beyond those mentioned. The sample is appropriate for coding ability evaluation, as it requires reading stack traces, reading and reasoning over ORM code, and understanding serialization and deserialization edge cases, especially in a multi-DB context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15695": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified but has minor ambiguities. The problem is described clearly: RenameIndex crashes if re-applied when an unnamed index (often auto-generated for constraints like unique_together) is being renamed to the same name (i.e., a noop). The key error is given (relation already exists), and the direction indicates the fix: the operation should be a noop if the name doesn't actually change. However, full familiarity with how unique_together, unnamed indexes, and migration operations interact in Django is assumed, and the description does not explain the full context for those unfamiliar, nor does it specify every relevant edge condition. An experienced Django engineer could fill those details in, but a beginner may find it lacking. There's enough information for a senior engineer to attempt a fix faithfully to the intent.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change is relatively small\u2014a simple conditional early return in the RenameIndex operation. The main challenge is understanding precisely when this condition (old name == new name) applies and why it results in errors on repeated application. Once that is clear, the fix and test are straightforward, requiring perhaps 15\u201345 minutes for someone experienced with Django migrations. No deep refactoring or multi-file edits are required; the conceptual insight is straightforward if one is familiar with Django's migration operations.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue sample is a suitable benchmark for coding tasks requiring architectural and contextual understanding but not complex code rewrite. No additional problems prevent its use in benchmark evaluation. The problem, solution, and verification by test patch are all well-aligned.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15732": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a reproducible setup (a unique_together constraint on a primary key field) and describes the mismatch in constraints between unique_together and primary_key, referencing concrete database constraints and how the migration system handles them. The problem (migration cannot drop the unique_together constraint because there are multiple unique constraints on the same field) is clearly described. However, it does not specify the _exact_ behavior required to resolve\u2014i.e., whether the desired fix is to allow dropping the unique_together constraint even if another (primary key, unique) constraint exists, or whether there are edge cases. An experienced Django contributor could reasonably infer the ask: the migration logic should distinguish between different unique constraints on the same field. Nonetheless, there is some interpretation required around intended scope, so I chose '1'.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires understanding how Django constructs and drops database constraints, how unique_together maps to constraints, and how to safely distinguish between constraints that are functionally overlapping (primary_key, unique, unique_together). The core code changes are not vast, but require an understanding of database migrations, Django internals, and how constraint names and checks work across multiple databases. Crafting the tests also requires knowledge of migration operations. Overall, this is nontrivial and likely to take between 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The example is suitable for benchmarking: it has a focused, technically interesting migration/ORM bug; success criteria are testable; and the original test changes are tightly scoped. There are no additional issues with using this for coding benchmark purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15814": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue report provides a clear description of the problem: a crash when using select_related() and only() together on proxy models in Django. The traceback is given, along with model definitions, the problematic code, and a proposed fix. While the core cause and one line fix are shown, the issue isn't as precise in terms of what broader behavior is expected for all use cases (e.g., are there unusual models to consider, or only the example provided). Still, it's reasonably clear for an experienced Django developer familiar with this part of the codebase to attempt a solution. The description falls into category 1, where there are some blanks to fill (such as implications for other types of models or potential side effects), but it is clear enough to proceed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is small and located to a single line in the codebase (django/db/models/sql/query.py). An experienced Django developer would need to understand the relation between proxy models, the ORM 'only' and 'select_related' query features, and how metadata resolution happens. Given the clear traceback, model definitions, and working fix, it is a focused, nontrivial, but relatively quick modification, likely feasible in 15-60 minutes (level 1). Some time may be needed for testing broader cases or ensuring no regressions are introduced.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues. The issue and context are sufficiently specified for use in a coding benchmark, assuming test files are available and the candidate is familiarized with Django ORM internals. The only minor caveat is that the fix may seem very mechanical, but comprehending why it is needed does require understanding of Django's proxy models and the ORM's internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15930": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue describes a crash that occurs when using a Case expression with a negated Q object (specifically, ~Q(pk__in=[])), triggering a syntax error in the generated SQL. While the problem statement is concise, it depends on domain-specific knowledge: knowing how Django's query expressions generate SQL, and the meaning of ~Q(pk__in=[]). The reporter explains the SQL error and indicates the root cause (an empty condition resulting in an invalid SQL 'CASE WHEN THEN' construct). However, it doesn't explicitly state what the correct behavior should be\u2014only that this sentinel occurs and leads to a crash. An engineer would need to infer that the desired fix is to ensure an always-true predicate is generated in such cases, preventing the SQL error. This is a reasonable and sensible interpretation, though the description could be clearer and more explicit about the correct result. Therefore, I rate it as a 1: mostly well-specified with small gaps to fill.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch demonstrates that the required code change is very localized: it adds a check to replace an empty SQL condition with a predicate that's always true. For an experienced developer familiar with Django's codebase, understanding the crash, tracing the empty condition, and applying the fix would likely take 15\u201360 minutes. The conceptual complexity is limited, but requires understanding how conditions are translated into SQL expressions, which may also involve tracing through some of Django's ORM internals. This is not a trivial one-line change, but also not a major refactor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues: the problem is code-level and testable. It is also internally consistent, and the related code and impact are clearly demonstrated by the crash report and reproduced in the tests.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15957": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description explains the technical scenario (using Prefetch with sliced querysets in Django raises an AssertionError). It describes an error message, the intended use case (displaying a list of categories, each with a limited number of related objects), and the deficiency in Django\u2019s current behavior. However, the description does not provide detailed step-by-step expected behavior, nor specific API or class/method names. It assumes familiarity with Django\u2019s ORM, Prefetch, slicing, and query evaluation. While a knowledgeable developer could infer the main goal\u2014to enable prefetching on sliced querysets without raising errors\u2014some implementation details would be left to the engineer\u2019s discretion, such as the mechanics of how sliced prefetching should operate, and what edge cases must be supported. Thus, there are blanks for the engineer to fill in, but the intent is clear enough to sensibly attempt a solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue is moderately challenging. The solution required identifying the ORM\u2019s internal behavior that blocks filtered queries after a slice, then rewriting/refactoring logic to support such use cases (potentially requiring an understanding of QuerySet internals, lookups, window functions, and query compilation). The solution created a new helper (_filter_prefetch_queryset), added windowed row number logic, and made changes in several locations handling Prefetch. The test additions are substantial and non-trivial, requiring an understanding of Django\u2019s ORM and its testing conventions. While many experienced Django contributors could manage this in 1-4 hours, it\u2019s definitely more involving than a basic tweak or one-liner fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues with the sample. This is a realistic and meaningful engineering problem, well-suited for evaluating intermediate to advanced Python and Django skills, including familiarity with query optimization, ORMs, and unit testing. The only note is that the original issue omits exact class or filename references, but the context provided suffices for a meaningful coding benchmark exercise.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a precise error message and traceback, making the immediate error clear. It specifically describes that defining the \\\"through\\\" model of a ManyToManyField in a different app (rather than the same app) causes an AttributeError: 'str' object has no attribute '_meta' during migration. This context is detailed enough for someone familiar with Django's migration process and ManyToMany relationships to understand what is failing and under what conditions. However, the issue does not provide code snippets for the model definitions or the exact project structure, and it assumes the reader understands how Django resolves 'through' relationships. While a sensible approach to diagnosing and fixing the issue can be taken, there is some room for interpretation on the precise \\\"right\\\" fix (such as whether the resolution should adjust model definitions, migration code, or underlying logic). Thus it's not fully specified, but sufficiently clear for a skilled engineer to proceed and create relevant fixes and tests.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Diagnosing this bug requires familiarity with Django internals, specifically how ManyToManyField with a 'through' model works across separate apps, and how model references are resolved during migration generation. The patch involves updating migration autodetector logic to handle the lookup of the through model correctly instead of passing a string. Understanding the traceback, reasoning about the interaction between apps, and writing a fix (plus corresponding tests validating the fix across migration scenarios) makes this a substantial, nontrivial code change (but not a rewrite of major components). This is a moderately hard issue, likely to take 1-4 hours for a skilled engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues. The test patch also thoroughly adds a regression test that models the problematic scenario, so the issue is testable and concrete. The codebase context is typical for a moderately advanced Django bug involving migrations, so it is appropriate for engineering skill evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16032": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is quite brief, provides a one-line summary ('__in doesn't clear selected fields on the RHS when QuerySet.alias() is used after annotate()') and lists an OperationalError that results: 'sub-select returns 10 columns - expected 1'. There is no reproduction code, no concrete example, nor details about the affected Django models or queries. The text expects familiarity with Django's ORM internals and assumes that the engineer knows exactly how annotating and aliasing interacts with subqueries and the __in lookup. While the description gives a clue about the cause (selected fields not being cleared), it does not specify exactly what behavior is desired, what correct output would look like, or the contexts where the bug manifests. This leaves room for ambiguity, as one must make educated guesses about the expected behavior. Hence, I rated this '2'.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the final code change is not extremely large, the underlying problem involves subtle aspects of Django's QuerySet evaluation and how subqueries interact with annotate() and alias(). Understanding why extra columns are being selected, how this leads to an OperationalError, and what effect clearing the select fields has, all require a careful reading of Django's internals. Locating the right place to fix this and ensuring no regressions (hence the substantial new tests) would likely take 1-4 hours for an experienced Django contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Despite the vague issue description, the resulting code and tests show a focused, meaningful problem involving subqueries, annotation, and aliasing. There are no additional barriers (e.g., external dependencies, missing information for test setup). If the high-level requirement was made clear (i.e. what behavior is expected when annotate() and alias() are chained, and a subquery is used in __in), it is reasonable as a coding benchmark for a Django-savvy engineer.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16136": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear factual report: an async View subclass of Django that only defines an async 'post' method raises a 'TypeError: object HttpResponseNotAllowed can't be used in 'await' expression' when a GET request is issued. The trace and error message are highly specific and point to a type error involving await/async and an HttpResponseNotAllowed object. However, the description implies familiarity with Django's async view machinery and the error's underlying causes. It does not explicitly prescribe what the async fallback or correct fix should look like or what method(s) specifically need patching. Nonetheless, for someone versed in Django and async/await handling, these gaps are small; a sensible interpretation is that the fix is to ensure that the fallback response for not allowed methods is compatible with both sync and async paths. Thus, it is mostly well-specified but leaves some minor blanks for the engineer to fill in.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding how Django handles HTTP methods in class-based views (CBVs), particularly for async versus sync cases, and how response objects interact with async call stacks. The fix is essentially to wrap the HttpResponseNotAllowed in an awaitable if in an async context, which is a small change, though it demands care. The amount of code change involved is minimal (a conditional around the return value), and the hardest part is reasoning about the async/sync boundaries\u2014a task that should take about 15 minutes to an hour for an experienced Django engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain with this sample. The original issue and solution align well, and the changes and test addition confirm that the problem is addressed directly and straightforwardly. The only caveat is that a user unfamiliar with async/sync HTTP machinery in Django may need more context, but this is normal for such a task.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16429": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified. It identifies a reproducible TypeError when calling timesince() with timezone-aware and naive datetimes, and it points to the general cause (pivot in the code failing to set tzinfo). The exact fix (adding tzinfo) is suggested, and a link to the line of code is given, although external code references should not be followed. The description provides enough information, context (USE_TZ, timesince, datetimes > 1 month ago), and an interpretable reproduction scenario. The major edge case (difference between naive and aware datetimes) is called out clearly. However, the reproduction is described in words, not code, so there is minimal work filling in missing details (e.g., how is timesince called?). Still, a developer familiar with Django/timezones will interpret it sensibly and know what to fix/test. Thus, score 1.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change is a one-line fix: adding tzinfo=d.tzinfo to a datetime.datetime constructor. Locating the issue in the code may require a little time, but the description provides a direct hint of where the problem lies. Understanding the difference between naive and aware datetimes in Python, and how timezone handling works in Django, likely requires some thought for someone not already familiar. But the scope is small, affects a single function, and is straightforward to test/verify with unit tests once the symptom is understood. This is a 15min-1hr fix for someone at the intended experience level.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers for using this sample. The issue is clear enough about what breaks and how to fix it. The only minor note is that it depends on the developer having baseline familiarity with timezones in Python, but this is reasonable for the benchmark's intended use.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16454": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description clearly describes both the problem (Django's management command subparsers fail to retain error formatting due to missing arguments, causing stack traces instead of user-friendly messages) and the context (how CommandParser subclasses ArgumentParser, and how subparsers are created via add_subparsers().add_parser() without passing extra arguments). It provides an example of the current failure scenario and specifies the desired outcome: subparsers should properly copy these formatting-related arguments so error messages remain user-friendly. For an experienced engineer working with Django management commands, the area of the code that needs to be fixed (the way add_subparsers constructs subparsers) is clear, and the behavioral requirement (preserve user-friendly error formatting) is explicit. All essential details and reasoning for a solution are provided.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution involves understanding how Django extends argparse's ArgumentParser for enhanced CLI error formatting, and realizing the discrepancy between main parsers and subparsers as constructed by add_subparsers(). The main change is ensuring extra arguments relevant to error formatting are propagated when creating subparsers. This requires familiarity with how Django wraps argument parsing, but involves a fairly small code change (overriding add_subparsers to set the parser_class keyword argument appropriately). An experienced engineer could identify the code location and implement the fix in 15 minutes to 1 hour, possibly longer if unfamiliar with Django\u2019s management command internals, but likely on the shorter side for someone familiar.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other substantial issues with this sample for use as a benchmark coding test. The problem is well-specified, the solution does not depend on external discussion, and the requirement is clear for an engineer working from the issue text alone. The test patch verifies expected functional outputs. This is an exemplary benchmark scenario for evaluating coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16485": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is brief and lacks context. It states that floatformat() crashes on \\\"0.00\\\" and shows a ValueError related to a valid range for 'prec' being [1, MAX_PREC]. However, there is no explicit sample input/output beyond mentioning the error and the problematic input string. There is an implied expectation to fix the function so that it does not crash for this input. While an engineer could deduce the goal is to handle this edge case gracefully, some blanks regarding expected output could remain. Overall, there is a clear, sensible interpretation of the main requirement: fix floatformat() so it does not crash on strings like \\\"0.00\\\". But, more context or explicit expected behavior in the issue text itself would make it fully well-specified.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is specific and highly localized. It involves handling a particular input ('0.00') that causes a ValueError in floatformat(). An experienced engineer would likely spend some time identifying the control flow that leads to the error, write or adjust a conditional check to handle this input correctly, and possibly add or adjust tests. This falls into the 'small change that requires a bit of thought' category, as it is not a completely trivial fix and requires understanding what is going wrong for this type of input, but would likely be solved within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue could be improved by providing explicit input/output examples and more detail on expected behavior. However, for engineers familiar with the domain (template filters, float formatting), the intent is clear enough for this to be a workable benchmark item. There are no major blockers to using this issue for evaluation, as long as a test case is provided (as it is in the patch).\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16569": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It precisely outlines both the context (the usage of the add_fields method within Django formsets when can_delete is True and can_delete_extra is False) and the problem (TypeError: '<' not supported between NoneType and int when index is None). It provides a concrete example of when the error manifests (FormSet.empty_form()), points to the relevant line of code, and proposes a direct solution (checking if index is not None before the comparison). This is sufficient information for an experienced engineer to understand the bug, reproduce the error, and fix it accordingly.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a targeted logic bug in a conditional statement; the solution only requires revising the guard clause in a specific function. While the engineer needs to familiarize themselves with Django formsets and the context for this logic, the correction is both local and straightforward. There is some reasoning required to check for side effects, but the required code change is simple and quick to implement for someone already familiarized with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues. The problem is clearly described, isolated, and testable. The test amendment is also simple and focused, confirming the correct field presence for empty_form. The sample is representative of coding ability evaluation tasks for bug fixing and regression testing.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16667": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes the context (SelectDateWidget in Django forms), the triggering context (user input processed in value_from_datadict), the problematic code (datetime.date(int(y), int(m), int(d))), the nature of the bug (no guard against OverflowError when user provides extremely large input values), and provides an example error traceback. The location (django/forms/widgets.py, method value_from_datadict) is spelled out explicitly, and the desired outcome is clear: prevent the unhandled OverflowError so that input validation fails gracefully. There is no significant ambiguity; an experienced engineer would know to wrap the date-creation logic in an appropriate try/except to handle OverflowError and provide a fallback/validation error.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial bug fix. The engineer needs to add exception handling for OverflowError around the datetime.date instantiation and ensure that invalid user input results in a proper form error instead of crashing the server. It requires understanding the data flow in value_from_datadict and how form validation/reporting works in Django. However, the code changes themselves are minor\u2014likely not more than a few lines. The main challenge is ensuring that the form reports errors as expected with bad input, but this would generally take less than an hour for someone familiar with Django forms.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers for using this sample in a coding benchmark. The issue is clearly described, focused, and solvable from the given text. The expected behavior is clear and can be validated via associated test cases. The only thing to note is that familiarity with Django forms and exception handling is assumed, but this is acceptable for a coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16950": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description lacks critical details needed for a well-grounded solution attempt. It states that adding a subthing with a main Thing fails due to the value of \\\"id\\\" being set to null and references an exception (via an external dpaste link), but does not include the code for the models involved, the configuration of the Admin inlines, how UUIDField is used, or the actual stack trace. There is no mention of the intended default behaviors, nor how inlines are set up, meaning an engineer would need to make educated guesses about the structure and desired handling. As such, the description is quite vague with significant ambiguity about the root cause and requirements for a solution, only providing context that there\u2019s a problem involving UUIDField default handling within Django admin inlines.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Even assuming the problem requirement is clarified afterwards, this is a non-trivial issue. Debugging and fixing the handling of default values for UUIDFields in Django admin inlines requires understanding several core Django mechanisms (model field defaults, formsets, admin inlines, and how model instances are created from forms), as well as the interaction between parent and inline child records. While the code changes may not be large, understanding and ensuring correctness would require reading and stepping through relevant Django internals, designing targeted test cases, and confirming non-regression, making it a multi-hour (1\u20134 hours) fix for someone reasonably experienced with Django internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent for benchmark use, except for the fact that the issue is under-specified without clarifying additional details, so the original test patch is vital for unambiguous evaluation. Also, this is somewhat Django-specific, so the evaluator should have a corresponding level of Django familiarity.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-20859": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The title and summary indicate that there is a bug when trying to add a legend to a SubFigure object in matplotlib, specifically that a TypeError is raised because Legend expects its parent to be an Axes or Figure, but SubFigure is currently not accepted. A traceback is given, pointing to the exact line in legend.py, and a workaround is suggested to check against FigureBase instead of Figure. The version/environment information is provided. Overall, the task is clear: allow the legend to be attached to SubFigure, modifying the type check as described.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a straightforward update to a class/type check so that SubFigure (which is a subclass of FigureBase) is recognized as a valid parent for Legend. The issue reporter even points out the exact place in the code (with a line range), and the workaround suggests exactly what needs to be changed. For an engineer familiarizing themselves with the codebase, this is a moderate but not complex task. It would require a little time to verify the SubFigure/ FigureBase hierarchy and run basic tests, but overall the nature and fix for the bug are clear and narrowly scoped.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The issue is unambiguous, the code changes are localized to type checking in the Legend's constructor, and tests can be written to confirm correct behavior. The environment and version details provided add helpful context.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-22719": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified: it describes the context (using axes methods with empty data and string unit converters in Matplotlib), the observed behavior (a deprecation warning), and the traceback. It also analyzes why this warning is likely a false alarm and points out that empty data should probably be handled distinctly. However, it does not specify in detail what ought to happen instead (e.g., should there be no warning? no exception? Should empty data be silently accepted?), nor does it reference the specific place in the code to fix. It leaves some room for interpretation, but an experienced Matplotlib developer should have little trouble inferring the required fix. There is a sensible, actionable interpretation, but a few minor blanks to fill in about edge-case handling.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual fix, as shown by the patch, involved adding an additional check (for non-empty data) before issuing a deprecation warning or performing certain operations. This is conceptually straightforward, but requires understanding how unit conversion works in Matplotlib's internals, some debugging, and fairly targeted changes. For someone familiar with the codebase, this would typically take between 15 minutes and 1 hour to identify, implement, and test\u2014especially since tests must also be updated or added.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present that would prevent using this sample for benchmark purposes. The described behavior is reproducible, the problem is sufficiently clear, and the solution is testable with code and unit tests. There is enough technical detail in the original issue to drive a focused fix, even though the desired behavior for empty data is assumed rather than strictly dictated by the reporter.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23299": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description has a clear bug summary: calling matplotlib.get_backend() inadvertently clears all figures from Gcf.figs when the first figure was created in an rc_context. The minimal reproduction (an assert comparing before and after) and the associated stack trace make the core problem clear (loss of figures in a specific context). However, it does not specify the cause in the code or suggest a target for a fix \u2014 the engineer must investigate the interaction between get_backend(), rc_context, and Gcf. This requires some assumption about where to focus the fix and what invariants to preserve, but there is a reasonable, clear problem and expected behavior. Overall, some blanks remain (no pseudocode or clear reproduction snippet), but a sensible interpretation is possible for a meaningful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue targets an unexpected side effect between rc_context and figure management in matplotlib. While it requires a working knowledge of the interaction between configuration contexts, figure state, and backend handling, the underlying fix (as evidenced in the patch) is focused and touches only a couple of lines related to how rcParams are saved/restored in the context manager. For an engineer familiar with matplotlib's architecture, tracking down and correcting the mishandling of the 'backend' key is a small change that requires a bit of thought but would not take more than an hour. It involves investigation, understanding the context manager logic, and a targeted code update.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues noted. The issue is reasonable for evaluating coding ability at tracking and resolving configuration/context management bugs in a medium-to-large codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23476": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified, but not perfectly so. The observed behavior (figure DPI doubling every time a figure is unpickled) is clearly reported, and concrete error messages\u2014including the stack trace leading to OverflowError\u2014are provided. The problem is reproducible and is somewhat isolated (specific to M1 Mac and MacOSX backend, independent of Python version). However, the issue does not directly point to the location in code or components responsible, nor does it speculate on the underlying cause (such as device pixel ratio or how DPI is stored/restored), so the fix requires a degree of inference and investigation into how figures are pickled/unpickled and DPI is managed on different backends. Still, the report is detailed enough for a competent developer to reproduce and reasonably interpret the expected behavior: figure DPI should not change after unpickling. Thus, there are some blanks to fill in, but there is a sensible interpretation of the required fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Although the fix itself turned out to be minimal (change a couple lines to restore the original DPI upon unpickling), the engineer would need to spend time tracing the DPI handling logic through the figure serialization/deserialization process, understand the impact of device pixel ratio, and identify where DPI may be modified inappropriately. Given that the problem can be isolated through reproduction and stack traces, this constitutes a small change that requires focused investigation rather than a broad refactor\u2014likely to take 15 minutes to 1 hour for someone with the right familiarity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be any other major issues with using this issue for evaluation. The problem is concrete, involves debugging, and leads to a code+test solution that can be checked automatically with tests. As a benchmark sample, it is suitable. One subtlety is that the ability to reproduce the bug may depend on the environment (M1 Mac), but the bug and the required fix can both be understood from the textual description provided. This is a solid real-world debugging/coding problem.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24026": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-described: the user explains a specific context (using matplotlib's stackplot alongside other plot types, aiming for consistent color usage via property cyclers and CN aliases). The stackplot function currently throws a ValueError when passed a color string like 'C2', indicating mishandling of color cycle references. The traceback and error point toward set_prop_cycle behavior in stackplot. However, while the core problem and desired outcome are evident, the issue does not provide precise requirements for how stackplot should handle color cyclers and 'CN' aliases, e.g., whether it should accept both direct color strings and CN aliases interchangeably, or if only one pattern should apply. Nevertheless, a sensible approach can be inferred: update stackplot so that supplying 'C0', 'C1', etc. does not crash and works like plot or Rectangle. Details would need some interpretation by the implementer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The main code change only requires amending how the colors argument is processed in stackplot and involves understanding how matplotlib handles color cycles and color string aliases (like 'C2'). For someone familiar with matplotlib's internals and color management, this is a targeted change, not requiring major structural alteration or multi-file refactoring. The patch size and logic confirm this: the main function is modified with a small logic adjustment, and a single test is added to verify it. Thus, while requiring careful understanding, it should take between 15 minutes and an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers for using this issue as a coding benchmark sample. While the issue does not exhaustively specify edge cases (e.g., what to do if more colors are requested than there are CN aliases), the core requirement is clear enough for a meaningful solution, and the PR/test patch clarify what is expected well. The sample checks color handling and the avoidance of undesired side effects to the Axes property cycle.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24149": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified. It describes that calling ax.bar([np.nan], [np.nan]) in matplotlib version 3.6.1 raises a StopIteration exception that breaks expected downstream behavior in Seaborn. The exception traceback is provided and pinpoints that the issue is related to _safe_first_finite not handling the all-NaN case properly. The user and traceback make clear that the problem is about handling all-NaN input for bar plots. The precise code paths (matplotlib/axes/_axes.py and cbook/_safe_first_finite) are deducible. However, there is slight ambiguity on the intended behavior in all-NaN cases (should it silently ignore, or warn, or fallback?) and whether the 'phantom bar' trick for color cycles should definitely be respected. But for a matplotlib developer, the correct approach (avoid crashing; behave no worse than before) is a sensible and actionable interpretation. Thus, it's not perfectly explicit, but is clear enough for a skilled engineer to fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is modestly complex, requiring understanding how NaN values are handled in the Axes.bar method and the helper functions it calls. The main task is to patch a special case for when all input elements are NaN\u2014essentially catching the StopIteration and falling back to a default/fallback value. Identifying the root cause in the code from the traceback and implementing a suitable guard is not completely trivial, but would not require substantially rewriting logic or in-depth research beyond what's provided. The fix itself involves only a few lines in one function and basic Python exception handling, which should take an experienced engineer well under an hour, though not a completely trivial quick fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified. The test scenario is reproducible and testable with the information given. The description is good enough to use for benchmark coding tasks, as it covers both the failure and the context (Seaborn's usage). The only minor ambiguity is the idealized behavior with all-NaN input, but the sensible fix is well-motivated by the traceback and typical plotting library conventions.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24970": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it provides a clear reproduction of the observed deprecation warnings, including the exact warning message and the relevant code lines from matplotlib that trigger them (lines 730-732 in colors.py). The messages describe that out-of-bound integers will not be auto-cast to uint8 in future numpy versions and recommend use of np.array(value).astype(dtype) for overflow casting. It is clear the current code triggers warnings in NumPy 1.24 when assigning out-of-bounds values to unsigned integer arrays. However, it doesn't specify the exact desired behavior beyond 'remove deprecation warnings'. It is a reasonable expectation that the solution should update this logic so it works cleanly with new NumPy versions. An engineer would need to inspect the code to understand the context, but a sensible and actionable fix is clear. This fits squarely in category 1: some details must be filled (how the casting should be adjusted), but the goal is quite clear from text and warnings.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial fix. The main challenge is understanding how the NumPy type conversion rules changed, and what changes are needed to avoid the warning while retaining the same code behavior (in particular, overflow assignment to integer arrays). The actual change is a few lines, but some reading of the numpy documentation and careful code adjustment is needed. Given direct pointers to where the warnings arise, it is a targeted bugfix, not requiring a large code rewrite. It can be solved in under an hour for an engineer familiar with numpy and the matplotlib codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue is sufficiently specific, has actionable code context, and clearly corresponds to the test and code changes. The only missing element is specifying if overflow-casting is actually the desired behavior for matplotlib (but it's strongly implied by the explanation in the warning text and compatibility with previous numpy behavior).\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25311": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a reasonably clear description of the bug: pickling a figure with a draggable legend in matplotlib fails with a TypeError involving an unpicklable 'FigureCanvasQTAgg' object. It is implied that a similar issue occurs with draggable annotations. While the report is concise, it is straightforward in its expectations: make it possible to pickle figures that have draggable legends/annotations. There's a bit of ambiguity regarding whether only legends and annotations are affected (or possibly other draggable elements), and no minimal code snippet or explicit step-by-step reproduction, but an experienced engineer would likely interpret the required behavior correctly from this context. Some details would need to be deduced from the codebase, but the problem and success criteria are reasonably deducible.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual fix required a small but non-trivial change: removing an attribute holding a reference to the canvas (which is not picklable) and replacing it with a property to maintain access while avoiding serialization issues. This requires understanding of both class design and Python's pickling process, as well as debugging pickling errors. While not a trivial change, it can be implemented in under an hour by an engineer familiar with the domain and the codebase. It does not require substantial rewrites or deep research but requires careful reasoning about pickling and attribute management.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that disqualify this sample for evaluating coding ability. The main requirement\u2014to make draggable legends picklable\u2014is unambiguous enough for an experienced contributor to tackle with some exploration of the relevant code. The test change provided further clarifies expectations for a valid solution, if used in evaluation. The issue is well suited for assessing Python skills relating to serialization, attributes, and class design in a mature codebase like matplotlib.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25332": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue describes a concrete and reproducible problem: attempting to pickle a matplotlib Figure after calling align_labels() results in a specific TypeError (cannot pickle 'weakref.ReferenceType' object). The user provides a clear stack trace with a function call (pickle.dumps(fig)) and indicates the relevant context: Windows, matplotlib 3.7.0. However, it does not specify all steps for reproduction (the minimal code example is not included and version/backend/jupyter info is incomplete), so an engineer would have to infer or experiment to reproduce. It is also implicitly assumed the objective is to make such figures pickleable again. Thus, the intended fix is clear, but some details (which internal objects are problematic\u2014e.g., weakrefs) are not spelled out.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"While the code change required is not very large (adding __getstate__ and __setstate__ methods to Grouper in cbook.py), identifying where to add custom pickling support for weakref-heavy data structures (and understanding the structure of matplotlib's layout/label alignment handling) requires time. The engineer must trace the error, find where the problematic references reside, and implement serialization logic correctly. Writing and running the corresponding tests (as shown in the patch) would also take additional time. This is more than a quick fix or a trivial bug: it requires reasonable investigation but isn't a massive overhaul.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is sufficiently concrete, and the context provided (error message, method call stack) allows an engineer to proceed toward a solution. Details such as the reproduction code could be fleshed out, but the sample is suitable for evaluating coding ability with a focus on debugging and internal data handling.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25479": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified, though there are minor ambiguities. The user clearly explains the symptom (colormap lookup failing due to a mismatch between the registered colormap name and the internal name stored in the Colormap object). They provide a traceback, detailed error message, and a summary of the observed/expected behavior. However, the root cause and desired solution are not given in formal terms, so the engineer must infer that the fix involves ensuring the internal name of a registered colormap should match the registration name if different. The core 'what' is present, but details of the 'how' are implicit. The problem context (matplotlib colormap registration and lookup) is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires a small and targeted change to colormap registration logic: updating the Colormap object's name attribute if it is different from the registry name. It also involves understanding matplotlib's colormap handling, but the relevant code is localized. The change is not trivial, as it involves some reasoning about how name lookup interacts with custom colormaps, but the required code modification is limited, and corresponding updates to the equality logic and new tests are straightforward. Altogether, this is likely to take between 15 minutes and 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No blockers or major issues for using this as a coding benchmark sample. The issue is understandable from the text and fixable without hidden context. The error scenario is reproducible and the asked fix is in line with real issues in open-source projects. The only minor caveat is the engineer will need to reason through library internals based on the error and narrative, but this fits the challenge level for a quality benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-26291": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. The bug summary, stack trace, and precise context (OS, Python, Matplotlib version, test environment) are provided. The error message and traceback indicate exactly where in the code the failure occurs, which points to an AttributeError relating to the 'renderer' being 'None' during a call to 'get_window_extent'. However, the issue does not provide an explicit minimal reproducible code snippet or clear step-by-step instructions, and only refers to 'code following the first example on the website'. Still, given standard repo practices, an experienced engineer can reproduce and interpret the failure, as well as deduce a logical solution. Thus, it's not perfectly specified, but sufficiently clear for a reasonable solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch is a small code change (adding two lines to check if 'renderer' is None and initialize it), and a simple new test is introduced. Understanding the root cause (that renderer may be None and needs to be set based on context) takes a bit of analysis but is straightforward for an experienced Matplotlib contributor familiar with how rendering works. The code change itself is local and uncomplicated, requiring limited familiarity with the codebase. Time required is likely between 15 minutes to 1 hour, primarily to locate the correct place to add the fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue could be a bit clearer by including a minimal code sample directly in the description, but the stack trace and context are sufficient for an experienced contributor to understand and address the problem. The issue is a suitable benchmark for coding ability within the described setup.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-1724": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly describes the symptom (a UnicodeDecodeError occurs when passing a unicode method name to requests.request in Python 2.7.2), provides a full traceback to the underlying error, and points to the code line where the error likely originates (assignment of the possibly unicode 'method' argument). The user even hypothesizes the cause ('u'POST'' infecting the header with unicode) and mentions the specific line of code ('req.method = method.upper()') and affected file ('sessions.py:313'). For an experienced engineer, it is clear that the method parameter needs to be normalized to a str (not unicode) before passing it down into the stack, in order to prevent the ascii codec error. The solution scope and requirements are clear; there is little meaningful ambiguity here.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is straightforward (coerce 'method' to str/builtin_str at the correct location in the code). The problem is well-motivated with a simple reproducible failing example, clear stack trace, and a focused hypothesis as to the root cause. The code change is small (a one-line conversion before further processing the method argument). The main effort is confirming how/where to stringify the argument and ensuring it resolves the error without breaking compatibility or behavior elsewhere. An experienced developer familiar with the codebase would recognize this pattern and implement the fix within 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified. The bug and required fix are clear, and test coverage is straightforward to add or verify.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-5414": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It describes a specific situation: making a request to a URL with a leading dot in the host (e.g., http://.example.com) results in a UnicodeError. The reporter clarifies that instead of raising a generic UnicodeError, the code should raise an InvalidURL exception, as is apparently the project's convention. The text also references the relevant line of code and exception. While there is a brief allusion to historical ambiguity (another issue), the present report is clear about what change is expected: catch the UnicodeError in this case and raise the appropriate requests exception. Sufficient context is provided to identify which file and functionality is at play.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is relatively simple for an experienced engineer to address. After familiarizing oneself with the codebase, the central requirement -- to raise InvalidURL instead of UnicodeError for malformed hostnames such as those starting with \\\".\\\" -- can be addressed with a small edit to the URL-processing logic. The gold patch confirms this is a straightforward pattern addition. Locating and editing the appropriate conditional should take no more than 15-60 minutes for a familiar engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present. The issue, problem, and expected outcome are described clearly and the sample is suitable for benchmarking coding ability regarding small bugfixes and error handling in request processing logic.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3151": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well specified: it describes a concrete error message that occurs when using xarray's combine_by_coords with a non-monotonic, but otherwise identical, coordinate. The reporter provides the expected behavior (as implied by documentation: 'coordinate dimensions which do not vary between each dataset' should be ignored for monotonicity purposes), contrasts this with the observed error, and includes an example setup that triggers the bug. However, the example is minimal and does not include fully runnable code, nor does it explicitly state the desired output, so some interpretation is required. Nevertheless, an engineer with access to the codebase and documentation can reasonably infer the requirement: combine_by_coords should not check monotonicity for coordinate dimensions that don\u2019t vary among datasets.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires identifying where the monotonicity check happens in the xarray combine_by_coords code, understanding the roles of dimensions and their selection for the monotonicity constraint, and then modifying the logic so that only relevant dimensions (not bystander/identical ones) are checked. The change is small (a few lines in the function), but it requires a clear understanding of the difference between concat_dims and all dims. Writing a new unit test is also straightforward. Thus, it is somewhat beyond a trivial fix but should be solvable in under an hour by a competent engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no further major concerns. The only minor note is that the reproducibility of the error depends on the underlying version of xarray, but the issue and test patch together provide sufficient context for an engineer attempting to fix the issue in a benchmark environment. The sample is suitable for evaluating coding ability as it tests both bug-finding and comprehension of specification versus current implementation.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3677": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it describes that merging a Dataset and DataArray using the top-level merge() works, but using ds.merge(da) raises an AttributeError. The traceback provides clear information about where the error occurs. It is clear that the requirement is to make ds.merge(dataarray) behave like xr.merge([ds, dataarray]), and avoid the AttributeError. The function and file references are explicit (Dataset.merge method in xarray/core/dataset.py, interaction with DataArray input). No clarifying questions are needed to understand the bug or what the fix should accomplish.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change that requires a bit of thought. The fix is essentially a one-line check/cast to convert a DataArray argument into a Dataset before proceeding, as the top-level function does. The majority of the work is understanding the context, identifying where to add the check, and ensuring correct behavior. Writing/adjusting a test is straightforward. An experienced developer with some codebase familiarity would likely complete this in 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The sample is suitable for evaluating coding ability as it tests understanding of API symmetry and error resolution.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4094": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description identifies a specific bug: to_unstacked_dataset fails for single-dimension variables with a MergeError. The scenario (stacking and unstacking variables that only have a single dimension) is clear, and the expected correct behavior is reasonably deducible: stacking and then unstacking should return the original dataset, regardless of variable dimensionality. While the issue doesn't specify code locations or exact steps to reproduce, the description is targeted enough for a competent engineer familiar with the xarray codebase to attempt a fix. There are some blanks (e.g., sample code, function details), but the intended high-level behavior and nature of the bug are sensible and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The diff shows a small, localized one-line change (adding drop=True to self.sel in to_unstacked_dataset) and a corresponding, simple unit test addition. This indicates the fix does not require knowledge beyond the function's context and expected behavior. Some thought is needed to identify the cause and correct usage of the sel and squeeze functions, but the problem is straightforward for an experienced engineer familiar with xarray or similar data-wrangling Python libraries. The fix and test can likely be implemented within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present beyond those discussed. The issue is sufficiently specified for benchmark use, and the patch represents a clean, isolated fix. While sample code could improve clarity, the combination of description and nature of the bug allow for meaningful evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4695": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue communicates that naming a dimension 'method' in an xarray DataArray causes an incorrect error when using '.loc', because the dimension name collides with a keyword argument expected by downstream code. The user suggests that the dimension name should be irrelevant, and provides the error message. While not all the reproduction steps or exact code are given, the nature of the failure and its root cause are clearly articulated: a collision between dimension names and method parameters in underlying functions. A reasonable engineer could infer that the offending code path is interpreting the dictionary of indexers with keys that may conflict with parameter names ('method', etc), and needs modification to avoid passing dimension names as named arguments. The explanation is clear enough for an experienced engineer to enact a fix, likely in the DataArray.__getitem__ or .loc handler logic, making a sensible solution straightforward to identify.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires understanding how DataArray indexing methods work (in xarray), reviewing how keys are passed to underlying indexing functions, and modifying the logic to avoid collisions between dimension names and keyword parameters. The change is surgical\u2014converting a call from sel(**key) to sel(key)\u2014and the tests are simple to add. Investigating the source of the collision may require reading through some code, but the issue is isolated, and the patch is small (only a one-line code change and a test addition). For someone familiar with xarray, this would be a straightforward fix likely to take less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent. The sample tests a subtle and real interaction in the codebase (dimension names vs. method parameters), and the issue and required fix are both realistic and instructive for coding benchmarks. The changes to both implementation and tests are clear and standard.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6599": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description specifies that the problem arises when using `polyval` with timedelta64 coordinates, and that the results differ between the previous stable and the development versions. The user expects consistent results regardless of version. However, the description does not include an explicit example of code input and output, nor does it pinpoint the specific place in the code where the change should occur. The expected output is clear: the results of `polyval` on timedelta64 coordinates should match previous behavior. An experienced engineer familiar with xarray/core/computation.py and its testing infrastructure would interpret this as an issue with type handling (specifically of timedelta64 values), but would have to do some investigation and perhaps consult the code and documentation to pin down exactly what is different and what correct behavior looks like. Thus, there's some room to interpret and generalize the fix, but the core expectation is sensible and well-motivated.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively straightforward bugfix. Although it requires understanding how timedelta64 coordinates are handled in the context of the polyval function, it only involves a small addition to an existing type checking block and corresponding casting logic. The scope of the code changes in the sample patch is minimal, and does not require major architectural change or deep research. However, the engineer does need to confirm expectations and choose the right casting mechanism, so it's not entirely trivial. This places the task in the 15min-1hr range for someone familiar with the codebase and underlying libraries.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues present. The issue is sufficiently concrete for evaluative use: it targets a clear bug, includes a regression from a specific working version, and expects consistency between versions. Test changes corresponding to the issue are also direct and well-focused.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6721": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified. The user clearly reports that accessing the 'chunks' attribute on a Zarr-backed xarray Dataset causes the entire dataset to be loaded into memory, which is undesirable. The provided stack trace narrows down the problematic area\u2014the 'chunks' attribute of the Dataset and specifically walks through the relevant function calls in xarray/core/dataset.py and xarray/core/common.py, involving variable data access. Although the specific expected behavior (\\\"should not load all data in memory\\\") must be inferred, any experienced engineer could reasonably interpret the issue as wanting to access chunk information without triggering array loading. The environment and xarray version are detailed, leaving little room for ambiguity about the software setup. However, it is a step less specific than a perfect issue: the failure mode is clear, but the exact semantics (e.g., what should 'chunks' return in case of non-dask arrays) may prompt a minor implementation decision.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue amounts to a small refactor in how chunking information is accessed for variables in the Dataset. The code change required is limited in scope (a single line change: using v._data instead of v.data) and does not require a deep rearchitecting, but it demands familiarity with the distinction between raw and accessed data, and with how property methods might eagerly load data. Reading through the stack trace and relating it to the code is non-trivial but should be achievable within 15 to 60 minutes by an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The stack trace and user description are clear enough for benchmarking. The problem is tightly scoped and does not require domain-specific knowledge beyond being familiar with xarray data structures and lazy evaluation in Python. There are no major blockers to use this for coding ability evaluation. The existence of a specific regression test in the test patch strengthens this as a benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6386": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a good account of the problem: the short '-v' (verbose) option in pylint incorrectly expects an argument, unlike the long '--verbose' option. The user supplies a detailed description, error output, and an expectation for similar behavior between the short and long options. However, the underlying configuration mechanism and specifics about how options are parsed or what the help message 'suggests' are not specified with absolute clarity. An experienced engineer would interpret the meaning, but there remains a slight chance of interpretation needed regarding whether '-v' should take no argument (act as a simple flag), what the actual help text should be, and how the command-line argument parsing is structured. Still, the intent is clear: make '-v' behave like '--verbose'. Therefore, this sample is at level 1.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires modifying how the '-v' short option for 'verbose' is handled in the argument parsing logic, so that it does not expect an argument, matching the behavior of the long option. The code changes are spread across multiple files (primarily focused on argument setup/configuration and some test files). The work involves updating metadata for argument parsing and adding a targeted test, but does not require fundamental changes to complex logic or in-depth rework. For a familiar engineer, these changes would take from 15 minutes to one hour, largely in locating the relevant places in the codebase and making adjustments.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues precluding this sample from use in a coding evaluation setup. The problem is sufficiently clear, and the patch plus test clearly correspond to the described behavior. The only minor ambiguity resides in the specifics of how the help message should appear, but this does not significantly impact the suitability for benchmarking coding ability. No additional blockers are apparent.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6903": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. The author describes the environment (running pylint in a Kubernetes pod with --jobs=0), what goes wrong (a division produces 0 CPUs due to integer cast, causing a crash), and where the problematic code is (pylint.lint.run._query_cpu, lines 34, 55, 60). The error message from Python's multiprocessing.Pool is clear. The expected behavior is unambiguously stated: never allow a computed CPU count of 0; fallback to 1 in that case. The issue even offers a concrete proposed fix, but leaves room for engineering judgment as to the best way to apply it. An experienced engineer with access to the codebase could readily locate the relevant function and make the described change, and also create a test to replicate the issue with the specific cgroup file values provided. This is a textbook example of a well-specified issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change required is small: add a simple conditional to avoid returning 0 CPUs, and use 1 instead. The fix is localized to a single function and file, and requires no extensive re-architecting, but does require understanding the environment specifics and the logic in resource query code. The necessary test involves mocking filesystem reads and is a bit more involved than a trivial one-liner, but still straightforward for someone familiar with testing in Python. Overall, the task is small but requires a little thought about what might trigger the bug (e.g., test with cpu.shares == 2). Thus, a reasonable solve estimate is 15\u201360 minutes for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional concerns. The problem and fix are self-contained and do not require external context or documentation. The change and the bug are unambiguous, and regression is unlikely if implemented and tested as described. The sample is well-suited for use in a coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-8898": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified: it clearly states that the problem is with how comma-separated regular expressions are parsed in the 'bad-names-rgxs' option of pylint. It explains that splitting on commas leads to incorrect parsing when a regex itself contains a comma (for example, in quantifiers like {1,2}). It also provides an example where pylint crashes and produces a specific error trace. However, it does not specify exactly what the correct parsing behavior should be in all edge cases, nor does it give a minimal configuration or command-line example. Still, an experienced engineer can infer that the solution requires more sophisticated splitting of comma-separated regex values, so the requirements are adequately interpretable for a sound fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires analyzing and reworking the logic for parsing comma-separated regexes, which involves understanding both string parsing and regex syntax. The solution involves changes in more than one file and adds new utility logic to handle the splitting appropriately. Some careful thought is needed to ensure that commas within curly braces are not incorrectly interpreted as delimiters, but the overall scale is moderate and does not require a fundamental redesign. Writing new tests to capture edge cases further adds to the task's scope.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blocking issues are present. The issue is relevant, the requirements for the fix can be reasonably inferred, and the problem is distinct and code-related. The test modifications also target the bug described. This makes it suitable for use as a coding benchmark. The only minor shortcoming is the lack of specific command/config file showing the issue, but this can be deduced from the traceback and description.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10051": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a specific description of the problem: after calling caplog.clear(), caplog.get_records() no longer tracks log records properly, as the records list is replaced instead of cleared. The explanation pinpoints relevant code lines and describes the cause (divergence from using .clear() vs replacement). While clear about the nature of the defect, it leaves minor blanks: some details on expected semantics (should all get_records calls reflect only the latest events, or should there be greater configurability?) require filling in. However, it's readily solvable, as the implication is that get_records should always reflect the current caplog.records. The code references are thorough enough for a competent engineer to implement an appropriate fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is isolated to a small part of the codebase\u2014involves modifying the behavior of caplog.clear (and the underlying handler's reset method) to ensure that clearing is done in a way that preserves the connection between caplog.get_records and caplog.records. The change itself is a fairly small code adjustment (switching from list replacement to using .clear/keeping object identity intact), which is not trivial but does not require deep refactoring or understanding of many modules. An experienced developer can identify, implement, and test the fix in less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues were found with using this sample. The issue demonstrates a concrete, localized bug and is directly testable. The expected fix and logic can be arrived at without requiring context outside the original issue statement and the repository code.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10081": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it clearly states that when using `pytest --pdb` with unittest.TestCase classes decorated with `unittest.skip` at the class level, the `tearDown()` method is erroneously being executed. There is a concrete reproduction command, a reference to a similar previously fixed issue (#7215), and a traceback showing the problem. The filename `test_repro_skip_class.py` and the affected method (`tearDown`) are included. The scenario is specific (skip at class level during --pdb), and the problem is articulated in a way that allows an experienced engineer to both reproduce and understand the bug's scope. The required behavior (setUp and tearDown should not be called for skipped tests/classes) is clear from both the description and from the parallels to the function-level skip issue mentioned.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem, while not entirely trivial, is contained and similar to a prior issue referenced. The solution involves modifying the logic that determines whether to run tearDown() for skipped classes, building on a pattern already established for function-level skips. The engineer needs to examine how skips at different levels are processed in pytest's unittest integration, but the change is relatively small: refining/expanding a conditional. The affected code path is isolated, and the patch is concise, so an experienced engineer could complete this fix in 15-60 minutes depending on familiarity with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is reproducible, clearly described, and self-contained. The solution scope is sufficiently bounded for a benchmark coding task.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text clearly describes the error triggered by an incompatibility between youtube-dl's expectations of the file object's mode (specifically, the presence of 'b' in the mode string of sys.stdout/sys.stderr during pytest's capture), and pytest's EncodedFile's 'mode' property advertising 'rb+' when it doesn't actually support bytes via write(). There is detailed stack trace, context about relevant versions, and a concrete reproduction. However, the description does not specify exactly how the 'mode' property should be changed (i.e., that it should omit the 'b'), nor whether anything else needs to be changed in the class's behavior. Nonetheless, an experienced developer would interpret the clear intent: make sure EncodedFile.mode doesn't include 'b' if write() can't handle bytes. There is a small gap (interpretation of 'what the mode should be'), but otherwise it is well specified.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution requires tracing the flow from the failure, understanding why youtube-dl calls write() with bytes, realizing that it checks 'b' in mode, and recognizing that pytest's EncodedFile exposes an incompatible mode. Then the fix is to override the property to remove the 'b'. This is a targeted change to a single property of a single class, requiring some knowledge of Python file interfaces, but otherwise straightforward for someone comfortable with Python's IO model. Time required is likely less than an hour after orienting in the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The only non-trivial part is realizing that the mode property should be overridden and must match the actual supported input types. The issue text gives all the stack trace needed to trace the problem and hints at the solution, even if not step-by-step. No other blockers identified.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5631": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. It describes a specific problem that emerges with pytest version 3.6.0 when using the @patch decorator and providing \\\"new\\\" as an array. The error trace is included, showing the point of failure, and the root cause is explained (the code is performing a check that leads to a ValueError when p.new is an array due to ambiguous truth value semantics in NumPy-like objects). However, there are some missing details: no references to relevant code locations (aside from the stack trace), and it assumes reader familiarity with pytest's plugin system, patching, and internal APIs. The core requirement\u2014modify the sentinel-checking logic to not fail on array-like objects\u2014is clear, but there's some interpretation needed as to the best strategy to resolve the issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves a small but careful change: altering the logic so that sentinel comparison (with p.new) does not rely on potentially ambiguous equality or membership checks, but instead uses identity (is) checks. This requires understanding the nature of the objects in play but is limited in scope to a single helper function. The code change and test addition are contained and straightforward for someone familiar with testing frameworks and mocking in Python.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues preventing this example from being used. The sample successfully includes both the problematic scenario and the test that would fail before the patch and pass after it, offering a solid benchmark. The only minor point is that the issue subtly assumes the reader's understanding of how mock patching interacts with pytest internals, but this is standard for contributors to such a codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well specified: it makes clear that the problem is that when using pytest-xdist to run tests in parallel, the full chained exception traceback is not being displayed, unlike when running without xdist. The provided examples show what output is expected versus what is currently being seen, and it is clear that the expectation is that chained exceptions (via PEP 3134, exception chaining) should be preserved during serialization and reporting even when xdist is used. However, there are some implementation details left to the engineer, such as precisely how tracebacks and chained exceptions are serialized/deserialized. The problem to solve (exception serialization should include chained exceptions) is sufficiently clear for a confident attempt, but the exact boundaries (such as performance or cross-version expectations) are not exhaustively laid out. Still, for someone with context in pytest internals and xdist, the requirements are actionable and a sensible interpretation exists.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix here requires a fairly deep understanding of how pytest and pytest-xdist handle exception tracebacks, including how exceptions are represented, serialized, and deserialized across process boundaries. The solution involves factoring out and extending (or re-implementing) serialization code to correctly capture exception chains, updating/reporting code accordingly, and writing (and updating) multiple test cases. This is more substantial than a quick code or config tweak: the developer will need to trace through multi-file interactions, manipulate complex objects, and ensure backwards compatibility and correctness. While not a many-day refactoring, this level of work (new helpers, cross-file changes, substantive test expansion) is well above a routine small change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues have been identified. The scenario is directly testable, as shown by the accompanying test changes. The sample is well-suited for benchmark evaluation since it exercises serialization logic, testing best practices, and some familiarity with Python exception internals.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5809": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It describes exactly where the problem occurs (the --pastebin feature in pytest code base, specifically at src/_pytest/pastebin.py, referencing a direct code section), what causes the problem (using 'lexer=python3' when posting to bpaste.net results in HTTP 400 errors for some inputs), what the intended behavior is (the output being uploaded is not Python code, but rather arbitrary text), and what change should be made (switch the lexer parameter from 'python3' to 'text'). The description is actionable without needing more details or context. An experienced engineer could reasonably resolve the issue based on the information provided.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a very simple change. It requires only updating the value for the 'lexer' parameter in a dictionary used when posting to bpaste.net. The relevant lines and context are provided in the issue. There is no substantial logic to be rewritten and no need to edit multiple files. The required change is well-localized and requires very little time to implement, test, and verify.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues exist with using this sample as a coding benchmark. The requirement is explicit, the change is minimal and well-defined, and no ambiguous interpretation is required. The related test changes further reinforce the clarity of what needs to be done. This is an excellent benchmark for verifying basic ability to interpret issue descriptions and implement targeted changes.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5840": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description provided is quite minimal and leaves a lot of ambiguity regarding the root cause and intended fix. It mentions that after upgrading to version 5.1.2, there is a path casing issue on Windows when loading 'conftest.py', specifically that the path was converted to lowercase and leads to a 'ModuleNotFoundError: No module named 'python''. However, it does not provide further context about the expected behavior, steps to reproduce, or any information about how paths should be handled on different OSes. There is also no mention of what 'python' means in this context (is it a folder, module, etc.?). An engineer would have to make several assumptions to proceed, such as knowing this is a Windows casing/normalization bug and being aware that conftest discovery in pytest relies on precise path handling. This is more vague than ideal and leaves room for uncertainty about a fully correct solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Given the patches, the issue required changing how conftest paths are handled to address case sensitivity problems on Windows. The code changes are mid-sized, touching logic for unique path calculation and the associated test suite to verify the fix across various casing scenarios. Understanding and solving this would require a reasonable familiarity with file system nuances, pytest internals, and path normalization on Windows vs. other OSes. This is not a quick fix; it requires understanding the bug, making systematic code changes, and updating/additional tests. For an experienced engineer familiar with the codebase, this would likely take 1-4 hours (level 2).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional major issues preventing evaluation, but the vagueness of the issue text means it is not ideal for benchmarking unless you are willing to assume background knowledge about file path handling problems on Windows, especially in the context of pytest's plugin loading mechanism. The sample would test not just path handling but also ability to reconstruct problem context from minimal cues.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6197": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear problem: pytest version 5.2.3 erroneously tries to import any __init__.py file under the current directory, which can cause problems if the file cannot be imported (e.g., Windows-only packages on a Linux build). The failure log explicitly shows an AssertionError on importing foobar/__init__.py, supporting this claim. The regression is also confirmed by the fact that the behavior did not occur in pytest 5.2.2 but appears in 5.2.3. While the description is concise and specific about the problem and context (import collection regression in test discovery), it does not dictate exactly what the right fix is. However, it is reasonable for an experienced engineer to know the correct behavior (don't collect or import arbitrary __init__.py files unless necessary) and infer what a successful solution should prevent. Thus, some interpretation is required, but the task is fairly well defined.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires diving into pytest's test collection internals and understanding the mechanisms behind module collection, especially around package __init__.py files. The engineer needs to pinpoint where incorrect eager collection/import occurs and implement a targeted fix to restore correct behavior. It likely requires tracing through a couple of core functions and ensuring compatibility, plus adding/adjusting tests. This is more involved than a trivial patch or a simple bugfix and spans core implementation skills with some investigation, but it should not take as much as a whole day. The number of code and test touched lines supports the 1-4 hour estimate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major outstanding issues have been identified. The only caveat is that full context of the ideal pytest collection semantics must be internally reconstructed from standard expectations, as the issue reporter doesn't spell out every edge case. Still, this is within normal expectations for a regression bug in a major library.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6202": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It describes an observed bug in the headline formatting of test reports where '.[' is replaced with '['. There is a clear description of the visible symptom (the incorrect headline in test output), an explicit analysis of the cause (a specific line in src/_pytest/python.py returning s.replace('.[', '[')), and a suggested solution (replace the code with return s). It also confirms how to check a successful fix (run tox -e linting,py37 and pass all tests). Relevant source files and lines are referenced throughout. While some lines about the justification/mechanism of the change aren't fully explored, the symptoms, location, and fix are all clear and actionable for a competent engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a relatively straightforward bug fix. It asks for a small code change in a known location, with the erroneous behavior, file, and line number all listed explicitly. There is some domain-specific context (pytest's codebase) but for an experienced developer familiarized with the repo, tracking down and changing the line is simple. Some time may be spent double-checking side effects or test coverage, and confirming why the replacement was present, but overall this should take less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues are present. The description is detailed, includes explicit references, outlines the bug, and suggests the fix.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7205": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is quite focused: it reports a BytesWarning that occurs when using pytest's --setup-show with a bytes parameter in a fixture. The traceback is complete, including the code location (src/_pytest/setuponly.py) and the specific line where str() is used on a bytes object. The user suggests that 'saferepr' should be used instead of str() for better handling. While the issue doesn't formally specify acceptance criteria or an exhaustive set of scenarios, it is clear what the error is, where it happens, and a reasonable direction for solving (replace str() with something like saferepr). An engineer familiar with the codebase would have a sensible interpretation to implement a fix, even if there's some detail to confirm (e.g., edge cases for formatting). Thus, some blanks, but overall clear what change is wanted.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change itself is quite contained: identifying where str() is being used on fixturedef.cached_param and replacing it with saferepr, ensuring compatibility and that the warning is resolved. There may be small ancillary adjustments (such as maximum string size/representation), and it requires understanding of how pytest's reporting works and possibly what saferepr does. This requires a bit of codebase familiarity, but does not involve major refactoring, API changes, or a rewrite of logic. Therefore, it's comfortably in the 15min-1hr range for someone familiar with Python and the pytest code layout.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues observed. The bug is clearly defined and the technical context is sufficient for solving without outside information. The user accurately located the bug and offered a viable suggestion. The steps involved do not hinge on hidden dependencies or external context outside of the shown files.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7236": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text provides a clear, reproducible example (with a command and a stack trace) showing that unittest.TestCase.tearDown is executed on skipped tests when running with --pdb, even though it wasn't previously. The user shares expected behavior (tearDown/setUp should not be called for skipped tests) and indicates recent regression between versions. While the issue doesn't pinpoint the deep cause or reference specific internal details (filenames, functions), it is clear what is expected: restore previous behavior where setUp/tearDown are not called on skipped tests with --pdb. Thus, this is a well-scoped behavioral regression, though the implementer must investigate internals to understand why this happens.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue involves understanding the interaction between pytest's handling of unittest-style test skips and the --pdb debugger option. Diagnosis requires reading into the test collection, skip handling, and pdb logic, and investigating changes between versions. The change and test patch indicate a small-to-medium code fix\u2014but to produce this efficiently requires researching internals across a few files and understanding tricky details of test execution order. For an experienced engineer, this is likely a 1-4 hour problem: nontrivial, but not especially deep.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues prevent this sample from being used to evaluate coding ability. The issue is clear, is about a regression, and is testable. The test files do a good job of ensuring the regression is fixed.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7324": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description provides a symptom ('Pytest crashes the interpreter on debug build for 3.8+') with a Python C-API assertion failure message, but it is vague about what in Pytest triggers the assertion or what actual feature/component causes this to occur. It doesn't reference any particular file, function, or context\u2014nor does it describe the user action or Pytest feature being used when the bug occurs. The related CPython issue is mentioned, but according to instructions, we must not follow external links. The description does not specify what constitutes a successful solution, what behavior is actually expected, or how Pytest should behave instead. An experienced engineer could reasonably guess (especially seeing the assertion relates to disallowing the names 'None', 'True', or 'False' as Python identifiers), but there is substantive ambiguity on the immediate cause within Pytest, what the fix should accomplish (apart from 'do not crash'), and what specific functionality is affected. This qualifies as vague and ambiguous, as the engineer needs to fill in major blanks and guess intent.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Despite the lack of specification, the actual fix is fairly local: introduce a prefix for certain identifiers to avoid clashing with Python reserved words, and propagate this change in a targeted part of the code. The patch is small, touches only a few lines, and mainly updates the way certain tokens are handled while parsing expressions. While understanding the bug might take some investigation, the task of patching and testing is a focused 15-min to 1-hour problem for someone familiar with Pytest's marker expression parser.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The main issue is the vagueness of the description, as detailed above. For assessing coding ability, this sample tests the ability to reason from a crash and assert message, but not interpret end-user requirements or design decisions. However, there are no other blockers such as unclear solution validation, irrelevant context, or dependency on external info that prohibit using this for benchmarking coding skill\u2014as long as some ambiguity is acceptable in the test pool.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7490": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is generally well-specified: it describes a regression in pytest 6, where dynamically adding an xfail marker using request.node.add_marker(mark) no longer causes the test failure to be treated as xfail, but raises instead. The description provides concrete reproduction steps (a minimal test and stack trace), versions, and the prior/expected behavior. However, there is a small blank to fill: the issue doesn't specify exactly how the error occurs (i.e., what exception is raised by pytest), or how subtle differences between static and dynamic xfail should behave beyond this example. But overall, the engineer can infer that dynamic xfail should behave like static xfail again. There is little room for major ambiguity about what constitutes a successful fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue likely requires a moderate amount of work. The engineer must understand both static and dynamic xfail application, dig into the code where test markers are evaluated, and ensure parity for dynamic markers. The fix touches the test running internals (e.g., hooks like pytest_runtest_setup and pytest_runtest_call), which might require nontrivial understanding and validation. It isn't a simple one-liner or adding assertion \u2014 rather, it involves understanding the test lifecycle in pytest and possibly debugging marker handling. Thus, 1-4 hours is reasonable for a motivated, experienced engineer new to pytest internals but familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues outside the requirement for high familiarity with pytest internals. The problem is sufficiently specified, the reproduction steps are clear, and the test scenario is concise. This sample is suitable for benchmark use. Do note: ability to set up pytest's dev environment may introduce practical (not conceptual) friction for those unfamiliar, but this does not affect the coding evaluation suitability.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7521": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well specified because it includes precise failing test cases and asserts, showing the exact difference in captured output (\\\\r replaced by \\\\n) with the use of pytest's capfd fixture. It also specifies the pytest version and the environment (Fedora, Python 3.9, borgbackup tests). The reader can infer that the intent is for capfd.readouterr() to preserve carriage returns as written to stdout/stderr instead of normalizing \\\\r to \\\\n. However, some additional context about the intended platform behavior and precise motivations is not included, so there is a small blank for the engineer to fill in. The examples and assertions make the requirement clear enough for a fix, but do require some minor interpretation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding of Python's IO handling, pytest's capfd fixture internals, and why carriage returns are not being preserved. The patch shows it's a one-line fix (changing the newline argument for a TextIOWrapper used by capfd), but understanding the root-cause and confidently making this change may take some thoughtful reading of the code and relevant Python IO behavior. For an experienced engineer, this is likely a \\\"15 min - 1 hour\\\" problem.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent. The sample closely matches real-world debugging and patching of interactive test fixtures, so it is a good fit for a benchmark evaluating practical coding/debugging ability. The only mild note is that knowledge of encoding/newline handling and capfd internals may slightly increase the time for engineers unfamiliar with these subsystems, but there is enough information for a meaningful attempt.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly states the observed behavior (TypeError: __init__() got an unexpected keyword argument 'store_cv_values'), the missing parameter in RidgeClassifierCV, and references that documentation claims the flag is supposed to be supported. It explicitly asks for 'store_cv_values' boolean flag support to be added to RidgeClassifierCV, in line with similar support for RidgeCV. There is also an unambiguous example of the error, as well as detailed context about the related attribute cv_values_ and its purpose. The implementation requirements are clear: add the missing parameter, ensure its usage is as described, and update the relevant documentation and tests. This is sufficient for an experienced engineer to make a meaningful attempt at a fix, without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, targeted change that involves updating the RidgeClassifierCV class to accept the new 'store_cv_values' parameter, passing it where necessary, updating docstrings, and adding test coverage. The implementation is conceptually straightforward, largely mirroring behavior already existing in a related class (RidgeCV). It does not require a full rewrite or in-depth research, but does require touching multiple places (init, docstring, forwarding the parameter, and tests). This would generally take between 15 minutes and 1 hour for an engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues: The sample is well-specified, scoped, and is a good candidate for evaluating coding ability, as it involves a common type of parameter propagation bug in a codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10908": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, unambiguous, and provides enough context for an engineer to understand both the problem and the desired behavior. It states that when a CountVectorizer is initialized with a 'vocabulary' parameter, calling 'get_feature_names()' currently raises NotFittedError unless the vectorizer is explicitly trained first (i.e., via fit or transform). However, supplying a vocabulary should suffice for these introspective methods, and this expectation is well articulated in the description. The expected logical fix\u2014a change to allow 'get_feature_names()' to work if 'vocabulary' is provided\u2014can be reasonably deduced. The relevant methods and behavior are identified, and there is no need for extra clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small but nontrivial code change. The solution involves reviewing the logic in get_feature_names() and potentially making sure the vocabulary is validated/set if a custom vocabulary is provided. This doesn't require major redesign or deep algorithmic work; rather, it's a matter of correcting and aligning the state management around the vocabulary attribute, and adding or updating tests. For an experienced engineer familiar with scikit-learn's vectorizer codebase, it should take 15 min to 1 hour, accounting for understanding the class initialization and testing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are evident with this sample. The test changes clearly verify the resolution and the scope is contained with no evident blockers or ambiguity.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-12585": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear explanation of the problem with the clone function failing when parameters are estimator types (i.e., classes, not instances). It includes a code traceback indicating exactly where and how the error manifests, and proposes a potential fix, pointing to a specific line in base.py. However, there are some blanks to fill in, such as considering potential side effects for the proposed solution and edge cases not fully covered by the description. The main requirements are clear but aren't fully definitive about all expected behaviors (e.g., class-type handling elsewhere in the library). Overall, there's a sensible interpretation of what is required for a successful solution, but minor ambiguities remain.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core of the fix is small (one line), and the reasoning is straightforward once familiar with sklearn's estimator cloning mechanism. It does require some thought to understand the implications of handling class types and to ensure this special case doesn't break user expectations or related functionality. The codebase's size could add minimal overhead, but an experienced engineer could address this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent. The issue is clear and the test and gold patches directly address the described problem, fitting the benchmark's requirements for testing coding ability around bugfixes and minor enhancements.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13135": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue describes a specific error with KBinsDiscretizer using strategy='kmeans', where bin_edges end up unsorted and np.digitize fails as a result. The error message is provided, the strategy and code section involved are clear, and the root cause (unsorted centers leading to unsorted bin_edges) is explicitly named. The reference to the structure of KBinsDiscretizer and its transform method is specific enough that, as an experienced engineer with codebase access, I would know exactly where to look and how to address the issue (ensure the centers are sorted before being used to construct bin_edges). There is no ambiguity in what behavior needs to be fixed or what result is expected for a correct solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires identifying where the bin_edges are constructed from kmeans centers in the code, understanding that np.digitize needs sorted bins, and ensuring sorting before further use. It's a targeted logic/data preparation fix in a well-contained place, requiring careful thought but not extensive code changes or design work. An experienced engineer could reasonably diagnose, implement, and test the fix within 15 minutes to 1 hour once familiar with the relevant codebase section.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The issue has a clear root cause and solution path, and the behavioral expectation for the fix aligns with established use and documentation. The sample is suitable for a benchmark coding ability task.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13142": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a concrete symptom: fit_predict and predict on a GaussianMixture disagree when n_init>1, and includes an example error message and cites the relevant unit test as not covering n_init>1. However, it doesn't explicitly spell out what the desired behavior should be (fit_predict(X) should equal fit(X).predict(X)), nor does it specify the root cause or precise location of the bug. As an engineer, you would need to make the reasonable inference that consistency is expected between these two outputs for all values of n_init, and the remaining work is to diagnose, fix, and add a suitable test for this expectation. There is some room for interpretation but a sensible, clear target.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires reading and understanding the GaussianMixture fit_predict, fit, and predict workflow, reasoning about why n_init affects the result, and finding where the discrepancy occurs. The actual patch is small, but it requires careful thinking and a bit of debugging/tracing through code, rather than simply adding a missing line. For an engineer familiar with the scikit-learn codebase or Gaussian Mixture Model routines, this would probably take less than an hour but more than 15 minutes due to need for reading and confirming edge cases and test coverage.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a good benchmark sample because it exposes a subtle, realistic bug that is likely to require understanding of both code and numerical algorithms, not just syntax. It does not depend on information outside the issue description for a meaningful resolution.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13328": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it describes a TypeError that occurs when boolean predictors (X_bool) are passed to HuberRegressor.fit() in scikit-learn. The error traceback is provided, showing exactly where the failure happens and what the error message is. The implication is that HuberRegressor does not support boolean data for X, but ideally should, or at least should cast/conform it to a supported dtype. The reproduction is implicit (fit HuberRegressor with a boolean X), so the requirements for a fix are clear. There is no ambiguity about the expected input, class to modify, or the symptom being addressed. This is sufficient for an engineer to attempt a meaningful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires knowledge of input validation and how scikit-learn handles data types in estimators. The likely fix involves updating input checking to convert or reject boolean inputs explicitly, which is not a complex or wide-ranging change. It requires tracing through the data validation and ensuring the model works as intended with boolean input, but this is typically no more than a one-file change and doesn't require deep research. The problem is very localised and the test addition straightforward.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns. The issue is minimal, clearly scoped, and testable, with no ambiguity in requirements. The test case for boolean input is also easy to create and validate. This makes it an appropriate sample for coding benchmarks.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13779": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well-specified. It explains that the Voting estimator fails at fit time if weights are passed and one of the estimators is None, due to not checking for None in sample_weight support. The error traceback provided gives a clear starting point. However, there are minor blanks to fill in: the reporter assumes knowledge of how Voting estimators are set up in scikit-learn and what the effect of a None estimator should be. An experienced engineer familiar with ensemble estimators and scikit-learn practices would interpret that None signifies a dropped estimator (as allowed by scikit-learn's pipelines/ensembles). The desired fix is to properly skip None estimators when checking sample_weight support. The issue does not reference the precise function or file, but these can be narrowed down easily with the information given.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change requiring a bit of thought. The engineer must recognize the need to skip None estimators in an existing loop over ensemble members when sample_weight is passed, to avoid an AttributeError. It does not require a major refactor or changes to multiple files, just an additional check in the appropriate place. Locating the logic and understanding sample_weight checks takes a bit of time, but the actual code change and corresponding test updates are limited.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues not covered for coding evaluation suitability. The task requires interpreting standard use of None for dropped/skipped estimators in sklearn ensembles, which is reasonable for a benchmark. Test description is clear enough for validation. Some scikit-learn context is assumed, but not excessive.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14053": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it reports a specific error (IndexError: list index out of range) encountered in export_text when there is only one feature. It also includes the actual error, a brief description, and a version breakdown. However, the reproduction details are somewhat implicit: it does not include a code sample or step-by-step instructions to reproduce the bug, though an experienced engineer can infer that the issue is triggered when passing a tree with a single feature to export_text. The function and context are unambiguous, but the omission of a direct minimal reproducing example makes filling in some blanks necessary. Thus, this fits category 1 (some blanks to fill in, but a sensible interpretation is doable).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Once the problematic line is identified (list comprehension accessing feature_names[i] for i in tree_.feature), and understanding that the issue arises when there is a TREE_UNDEFINED index, the fix is a small conditional adjustment in the list comprehension. This requires understanding the input to export_text and how features are referenced, but it is localized and small in scope. For someone familiar with the codebase, this should be solvable in 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified. The issue is specific, and the problem is clear for an engineer familiar with scikit-learn's tree export utilities. The codebase context is straightforward, and although the sample could have benefited from a code reproduction, it remains valid for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14087": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is quite specific about the context: using LogisticRegressionCV with refit=False throws an IndexError. It provides a reproducible error snippet, stack trace, and environment details (Python, scikit-learn, numpy versions). However, the sample does not provide the exact code/data for X, y, so reproducing the error requires making reasonable assumptions about the test case. The problem is clear (IndexError with certain LogisticRegressionCV params); the exact fix can be inferred from traceback and doc/code reading. For a sufficiently knowledgeable engineer with access to the codebase and tests, the requirements are sensible.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires medium debugging in a mature codebase. The solution involves understanding how parameters interact in LogisticRegressionCV (refit, penalty, multi_class, etc.) and diagnosing an IndexError in numpy indexing. The actual code change is a small conditional around adding l1_ratio_, plus test updates to ensure coverage. For someone familiar with scikit-learn this is a small but non-trivial change, likely to take under an hour including test updates. The fix is less about large code changes and more about careful conditional checks and knowledge of the model's API.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The only ambiguity is the lack of the full minimal reproduction inputs, but the stack trace and error context are sufficient for an experienced developer. The issue is suitable for a coding ability benchmark as it involves a clear bug and an associated logic fix, rather than broad refactoring or API design.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14629": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly identifies a bug when using cross_val_predict(method='predict_proba') with MultiOutputClassifier in scikit-learn. The error, AttributeError: 'MultiOutputClassifier' object has no attribute 'classes_', is fully described, and a root cause is identified: the code tries to access estimator.classes_ rather than the list of per-output classes. The user even points out the relevant line of code and suggests the correct way to obtain the classes_, namely by iterating over the underlying estimators. The issue is well-diagnosed. However, the text leaves a few blanks: it doesn't say if there might be implications for other methods or subclasses, nor if any backward compatibility or other edge cases should be tested. It doesn't clarify if adding the classes_ attribute to MultiOutputClassifier is preferred, or if cross_val_predict should be patched differently. Still, a sensible, realistic fix is clear from the context: MultiOutputClassifier should provide a classes_ attribute as expected. High-level requirements are quite actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is fairly narrowly scoped\u2014an AttributeError due to a missing attribute. The main tasks are to (1) make sure MultiOutputClassifier exposes a classes_ attribute that returns a list of classes for each underlying estimator, e.g., via self.classes_ = [est.classes_ for est in self.estimators_], and (2) ensure proper testing. The patch and test diff show this is a single-class change with minor edits. An experienced engineer would need to consider if the attribute is available at the right time and review any inheritance chain, but overall, the change is conceptually simple. The fix and comprehensive test are straightforward but require some familiarity with scikit-learn's multioutput system. Total time: 15min-1hr.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain unresolved. The dataset sample is a solid case for evaluating the ability to read the codebase, diagnose a scope-limited bug, and implement/verify a Pythonic solution. The test case is clear and directly tied to the fix. The only minor ambiguity is whether the fix should be here or in validation, but the candidate solution is reasonable given the information in the issue.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14710": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clear. It provides a direct explanation of the bug encountered (mismatch between y_true and y_pred types during early stopping in HistGradientBoostingClassifier when the target is a string), a traceback showing where the TypeError occurs, and even a suggested code diff that demonstrates the necessary fix (encoding y_true and y_val back to their original string classes before scoring). It references the relevant class (HistGradientBoostingClassifier), file (sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py), and methods. The context is sufficient for an engineer to understand the root cause, replicate the issue, and determine what a successful fix entails.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires understanding where prediction and scoring operations happen in the training loop of HistGradientBoostingClassifier, but the problematic code locations are thoroughly referenced in the traceback and the description. The change is localized and does not require substantial refactoring (adds a conditional to map integer-encoded labels back to original classes before scoring). For an experienced engineer familiar with the codebase, this is a relatively straightforward bug fix that should take between 15 minutes to 1 hour, depending on familiarity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blocking issues with using this sample for evaluating coding ability. The description gives all the necessary context and the issue is representative of real-world bugfixing in a mature codebase. The engineer is expected to read stack traces, understand type mismatches, and adjust usage of class attributes accordingly while being careful not to break other scorers or regression cases.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14894": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified. It describes a specific problem: a ZeroDivisionError in _sparse_fit in sklearn's SVM implementation when support_vectors_ is empty. The error traceback pinpoints the file (sklearn/svm/base.py) and line where the division by zero occurs, as well as the offending operation (dual_coef_indices.size / n_class). The description, traceback, and version are provided. However, while the error is clear, the desired fix is only implicitly stated\u2014one must infer that the correct behavior is to avoid the division when support_vectors_ is empty, rather than, for example, raising a different error or returning a default value. Thus, there's a small gap in terms of exactly what the new behavior should be (e.g., should dual_coef_ be an empty matrix?), but the most sensible solution is clear for an experienced engineer. Hence, this is a '1' on the scale: it's straightforward to interpret sensibly, though a minor assumption must be made.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is very localized: it requires handling the special case when support_vectors_ is empty during SVM fitting in sparse mode. The patch shows that only a handful of lines need to be guarded by a conditional. Once familiar with the code (after a brief reading), it would likely take 15-30 minutes to identify the location, implement the condition, and write a test. The conceptual complexity is minor, requiring only care around array shapes for the empty case.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The problem is reproducible, code locations are clear, and the proposed solution is straightforward. The sample is suitable for benchmarking coding ability related to bug fixing in scientific Python libraries.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25747": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified for an engineer familiar with scikit-learn's pipeline API and pandas interoperability. The problem is illustrated with a stack trace, a code snippet, and clear indication that changing output from numpy (working) to pandas (failing) triggers the bug. However, the issue does not provide a minimal reproducible example, and details about the custom transformer or the precise data shape are missing. Still, a sensible and correct interpretation can be made: when using set_output(transform_output=\\\"pandas\\\") and a custom transformer in a FeatureUnion, pandas DataFrame index/column assignment goes wrong, leading to a ValueError. The engineer can locate the relevant handling in sklearn/utils/_set_output.py and infer the need to refine index/column logic. Overall, the goal is clear despite needing to fill in minor blanks.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch involves a small, targeted change (removing two lines from a function and clarifying the docstring), which directly addresses the error scenario described in the issue. The emergent behavior is clear from the error message. The actual solution requires understanding index/column handling in pandas and scikit-learn's set_output logic, but doesn't require substantial changes or wide-reaching codebase updates. After familiarization, the actual code and test modifications could be completed well within an hour by an experienced contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent. The only minor caveat is the lack of a minimal reproducing code sample, but this does not prevent comprehension or successful resolution for a benchmark task. Stack trace and version information are sufficient for context. Engineer must be comfortable with scikit-learn and pandas internals.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25931": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description explains a concrete and reproducible problem: when fitting IsolationForest with a pandas DataFrame (with feature names), a warning is raised about feature names being invalid if the 'contamination' parameter is set to a non-default value. The issue goes into detail on the context and hypothesizes the likely code path causing this behavior. It identifies the code file and lines involved (sklearn/ensemble/_iforest.py), and describes both what happens and what should happen. However, it leaves some blanks about the exact way to fix it: while one can infer that the problem relates to input validation and improper propagation of feature names in internal calls, the exact interface or implications of a fix (e.g., private vs. public API) are not fully spelled out, and the engineer must fill in these blanks based on context and codebase knowledge.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires understanding the relationship between feature names and how fit/predict/score_samples are called internally in scikit-learn's IsolationForest, especially when non-default contamination is used. Once the problem is identified (the call to score_samples during fit removes feature names due to validation), the solution is to bypass extra validation by calling a private method, preserving feature names. This change is limited to a few lines, mainly introducing a private method and minor edits to existing ones. The conceptual hurdle is moderate, but the overall change is not large in scope and should not require more than an hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues are present. The test patch shows a clear, non-regression test for the bug, and the requirements can be reasonably understood by someone with knowledge of scikit-learn internals, pandas, and how feature names propagate. The sample is relevant to coding ability assessment, as it requires navigating code paths, careful editing, and understanding side effects, with both a bug fix and a test addition.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly identifies the object (SequentialFeatureSelector), the parameter (cv), the input that causes a problem (passing an iterable of splits rather than an int), and includes a stack trace indicating where the error occurs (IndexError in _aggregate_score_dicts in sklearn/model_selection/_validation.py). The expected behavior is also described, referencing similar behavior in other scikit-learn components. However, the reporter asks whether it is a bug or a user error, leaving open a (small) question as to intent\u2014while it's sensible to assume it is a bug and that the correct fix is to support generators of splits, a fully specified issue would state the required new behavior definitively. Still, there is a clear sensible path to a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires understanding how cv parameters (both ints and iterable/generators of splits) are handled in scikit-learn, and ensuring SequentialFeatureSelector correctly supports both by wrapping the cv argument in check_cv as other estimators do. The structural code change is limited to a few lines, although it requires some understanding of the cross-validation handling utilities and the overall consistency in scikit-learn. This would be a small but non-trivial change, likely managed within 15-60 minutes by someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The example, expected result, and error logs provide sufficient context. The PR also suggests the solution is internally consistent with sklearn design patterns and the user intent.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10323": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue describes a concrete problem with using the 'literalinclude' directive in Sphinx when combining 'prepend' (or 'append') and 'dedent': specifically, that the dedent operation currently affects prepended/appended lines as well as included code, causing inconsistent indentation and warnings. The context provided, especially the minimal example and error message, makes clear what behavior is desired\u2014'dedent' should apply only to the included code, not to prepended/appended lines. However, it doesn't specify exactly where in the Sphinx codebase this occurs, nor does it provide expected output beyond the error, leaving some details for the engineer to infer. The issue is still actionable for a competent engineer, but requires interpretation of how the directives and their options interact.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change requires understanding the order in which filter functions are applied when processing included code in the literalinclude directive of Sphinx. This involves a code review to ensure 'dedent' is applied before 'prepend' and 'append', and a small, localized change to the codebase, followed by updating or adding a test. It is not trivial (since it relies on knowledge of how filter chains interact), but is unlikely to take more than an hour for an experienced developer familiar with Sphinx-like code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues that would prevent this sample from being part of a benchmark for coding ability. The problem is well-bounded, actionable, and comes with a clear test case to verify correctness.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10673": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear error message occurring when trying to add 'genindex', 'modindex', and 'search' to the Sphinx toctree in reStructuredText, and specifies the expected user-facing behavior: that this directive should work without warnings/errors. However, it references external discussions on Stack Overflow rather than summarizing conclusions, and does not describe precise technical details regarding why these special page references are not recognized as valid documents. The desired outcome is clear: the directive should support these synthetic documents and not warn about missing references. An engineer familiar with Sphinx and its document generation system could reasonably interpret what needs to be achieved, but some internal technical context may need to be inferred (how indices are registered/generated). Overall, there is some room for interpretation on 'how', but the 'what' is acceptably clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The solution involves making changes to several locations in the Sphinx codebase to recognize and handle synthetic/generated docnames ('genindex', 'modindex', 'search') as valid entries in the toctree. This touches multiple files (directives, adapters, collectors) and requires a solid understanding of the Sphinx build and registration process for these pages. It is not a quick fix; it likely involves reading through the internals, determining at what points these synthetic documents are registered, and ensuring they are respected in all relevant places. Therefore, it is more than a one-file or trivial edit and would likely take a few hours for even an experienced contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other substantive issues are evident. The problem, as described, is reproducible from the issue statement and the expected outcome is well validated by associated tests. The test patch clearly adds coverage for the expected behavior, ensuring that the solution is properly evaluated. The only minor drawback is the assumption of some prior knowledge about Sphinx internals, but for a benchmark test meant for experienced engineers, this is acceptable.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7440": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a concrete error message from Sphinx indicating that there is a duplicate glossary term with different case sensitivity (e.g., \\\"mysql\\\" vs \\\"MySQL\\\"). It also identifies the file and approximate location causing the problem (doc/glossary.rst). While it is clear that the duplicate glossary term is an error, the description does not specify whether the desired solution is to make glossary terms case sensitive, normalize them to the same case, or something else. However, the most sensible interpretation is that case sensitivity should be respected, so that \\\"mysql\\\" and \\\"MySQL\\\" are treated as different terms. An experienced engineer can reasonably infer this, though there is a minor blank to fill in.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires a small change to how Sphinx handles glossary term uniqueness\u2014removing the case-normalization step when storing or checking term names. This is not a complex change but does require understanding which part of Sphinx's code is responsible for term registration and reference. Once located, the implementation is straightforward and should take between 15 minutes to 1 hour for an experienced engineer after some codebase familiarization.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample does not have major problems for use in the benchmark. The only small ambiguity is the implicit assumption about whether case-sensitive glossary terms are desired\u2014which fortunately is the most reasonable interpretation due to the error and context. Test files provided confirm and anchor this requirement, so it is suitable for assessing coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7462": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is specific about the error encountered (IndexError: pop from empty list in sphinx/domains/python.py, unparse function), and describes the input that triggers the error (a function annotated with return type Tuple[()]). The user provides not only the code snippet that causes the failure, but also information about their environment and the version of the relevant software (Sphinx, Python, extensions). While the issue is generally clear\u2014prevent the IndexError when parsing an empty tuple in a type annotation\u2014there are some minor blanks: the user does not explicitly specify what the expected behavior/output should be when parsing Tuple[()], though the implication is clear (it should correctly render/parse instead of crashing). With the available context, an experienced engineer could make an informed and valid fix, but some small interpretation is required on what the output/rendering should look like.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug fix is localized and conceptually simple: detecting the special case where an AST tuple is empty, to avoid calling pop() on an empty list, and provide the correct output for an empty tuple. The diff shows that only a small conditional is added in two places. This requires basic understanding of AST handling and Sphinx code structure, but for an experienced developer familiar with Python's AST and medium-sized codebases, the solution is straightforward and doesn't require major investigation or rewriting.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be any major blockers or disqualifying factors for using this issue as a benchmark for coding ability. The core requirements are clear, and the fix and tests are representative of a typical, realistic bugfix task.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7985": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified. It clearly states that the Sphinx 'linkcheck' builder should also check 'local (internal) links', which it currently does not do. There is an explicit mention that adding support for checking local files as links would be useful. However, there are some blanks to fill in: the description doesn't specify precisely what constitutes a 'local link' (e.g., is it just plain filenames? subdirectories? relative paths?), nor what 'checking' entails (e.g., simple file existence, more sophisticated validation?). But with reasonable assumptions (paralleling how remote links are checked), one can sensibly interpret the requirements and attempt a straightforward implementation that checks whether local files exist. There is a test log fragment that hints at how broken links are reported, which helps clarify expectations.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small feature addition with moderate complexity: it requires reading the code that processes links (likely in the Sphinx linkcheck builder), identifying the place where remote and special URI schemes are handled, and adding a branch to check for local file existence. The PR shows that the change is mainly in one function with a handful of lines, plus corresponding tests. For an engineer familiar with Sphinx or after a brief orientation, this should be achievable in 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present that would disqualify this as a coding benchmark sample. The goal is reasonably clear, there is a sensible default interpretation, and test cases further clarify intent. Some ambiguity exists around finer details (e.g., exactly which URI patterns should count as local), but these are not sufficient to undermine the exercise. This is a good example for assessing the ability to implement a concrete, bounded feature from a short but slightly under-specified ticket.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8269": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified, but there is a small amount of interpretation required. It clearly describes what happens: when linkcheck_anchors is True, a 404/500 HTTP error is encountered, but the system reports 'anchor not found' rather than reporting the HTTP error. The desired behavior is for HTTP errors (like 404/500 from the server) to be surfaced, not just the anchor status. However, it does not explicitly state the format or exact text for the error reporting, what constitutes sufficient 'reporting' (log, exception, return?), or if there are any backwards compatibility or config considerations. Still, it's a common enough pattern that an experienced engineer can fill in the gaps. The relevant code and context are readily discoverable (linkcheck command, Sphinx), so a sensible interpretation is possible without more clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change is relatively straightforward: adding a check so that HTTP errors (non-2xx responses) result in an HTTP error being reported, rather than falling through to a misleading 'anchor not found' report. This likely involves modifying the flow in a single function to raise for HTTP status codes before anchor checking. The solution requires some understanding of request handling and exception management in the surrounding Sphinx code. Some additional work is required to ensure a proper test is constructed, perhaps with a local server, but overall the patch is small. For an experienced engineer familiar with the codebase, this would take 15 minutes to 1 hour, including testing and validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no significant issues preventing this sample from being useful in a coding benchmark. The scenario is realistic, the problem is concrete and the test setup for verification is clear. The only minor ambiguity would be in precise output formatting and error text, but that does not undermine its use in a benchmark context.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well specified. The description outlines the exact bug: when using :type: and :rtype: info fields in Sphinx Python documentation, ambiguous class lookup warnings appear due to how cross-references are resolved. The warning messages and the environment provide diagnostic information, and the 'Expected behavior' section clearly states that no warnings should be emitted and the types should resolve to 'mod.submod.A'. The problem involves XRef targeting for Python types in Sphinx. While knowing the Sphinx codebase helps, the requirements for the fix are unambiguous, and the engineer knows exactly what success looks like (no warning, correct resolution). This makes the issue suitable for a coding benchmark.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding both how Sphinx handles cross-references for info fields and the difference between implicit and explicit resolution mechanisms. A successful solution will require code changes in more than one file and adjustments to the cross-reference context propagation logic, likely involving a deep dive into Sphinx internals (domains, docfields) and RE structures. Writing and running tests to validate the fix adds complexity. Given these factors, it would likely take an experienced engineer 1-4 hours to orient themselves, identify the root cause, and apply a robust fix (as seen from the patch affecting multiple core files and tests).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues identified. The problem is precisely stated, representative of real-world issues in documentation tooling, and the engineer is given everything needed to attempt a high-quality solution. The sample is appropriate to evaluate coding ability, specifically debugging and understanding a mid-sized Python codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-9711": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it clearly explains the bug (version comparison is done as strings, not as versions) and gives a specific, concrete example of incorrect behavior. The expected behavior is also clearly stated: version '0.10.0' should be accepted as >= '0.6'. The relevant area of the code is also described ('needs_extensions' check) and there is no ambiguity in what is being asked\u2014change the logic to compare versions correctly as versions, not as strings. An experienced engineer would have no trouble understanding what needs to be changed to resolve this issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change involves modifying the logic where version numbers are being compared. While it requires identifying where the comparison happens and switching it from string comparison to semantic version comparison (using something like packaging.version.Version), this is a targeted change, localized to part of one function, and doesn't require deep changes or extensive rewrites. The change may require updating or adding some tests, but overall, for an experienced developer, this should take 15\u201360 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is well suited for use in evaluating coding ability: the requirements are clear, the expected result unambiguous, and it exercises an important but not trivial skill (handling version comparisons properly). There's a concrete, testable outcome. No other major issues exist.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13372": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete Python exception (UnboundLocalError with a specific variable), along with an excerpt from a stack trace indicating precisely where it occurs (sympy/core/evalf.py, line 1308) and under what condition (handling Max in evalf). The author of the issue also hypothesizes the cause: that certain elif clauses fail to assign reprec and imprec, and suggests an else clause to raise NotImplementedError. Thus, a developer has enough guidance to find and fix the bug, although there is some slight uncertainty about whether the suggested fix is best practice or whether more investigation is needed. There is a sensible and actionable interpretation, but a small blank remains as to exactly which branches should be handled and if the else solution covers all cases.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution involves identifying an unassigned variable due to incomplete conditional branches and safeguarding the assignments by adding an else clause that raises NotImplementedError. The stack trace and code pointers make finding the location straightforward. The actual change is small and local, and does not require extensive research or major refactoring. It would likely take 15 minutes to an hour for an experienced engineer familiar with the codebase, possibly at the lower side of that range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is reasonable for a benchmark: the bug is concrete, the fix is scoped and the new test demonstrates correctness. Issue is suitable for assessing debugging and patching skills in a realistic codebase context.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13480": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete problem: an error occurs when calling .subs on coth(log(tan(x))) for certain integral values, with a specific NameError and a filename and line number. It lists specific input values that trigger the failure. The developer is pointed to the file sympy/functions/elementary/hyperbolic.py and line 590 (which may be off by a couple of lines depending on the code changes). The missing name 'cotm' is the cause, which is a typo or variable mismatch. While the description does rely on familiarity with SymPy and .subs (and would be clearer if it stated precisely what the function is expected to do or what the correct output is), an experienced engineer could reasonably deduce that the issue is a variable naming error in the method. Thus, the problem is somewhat underexplained but fixable, and there's a sensible, clear path to resolution.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a simple, nearly trivial bug fix: a variable naming typo (cotm should be cothm). It takes only a few minutes to discover the mistake and correct it. The test addition is similarly simple. Any experienced developer familiar with basic debugging and reading stack traces can resolve the issue quickly after familiarizing themselves with the code context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blocking issues or reasons to disqualify this sample. The bug relates directly to a typo and surface-level fix. The one caveat is that some high-level familiarity with mathematical functions helps, but no deep domain expertise or difficult reasoning is required, making this suitable for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13877": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a clear description of the problem: calculating the determinant of a matrix in SymPy with certain symbolic entries causes a TypeError ('Invalid NaN comparison'), and includes a reproducible code sample (lambda function with Matrix and det). It also suggests a potential underlying cause (Bareiss algorithm validity for integer matrices), which points in the direction of the kind of fix required. However, it leaves some blanks; for example, it does not say exactly how the determinant function should behave in the presence of symbolic/non-integer matrices, nor exactly what changes are needed to handle these cases (so an engineer may have to inspect related code or infer requirements/behavior from context). Still, given the traceback, code, and the hint, a sensible interpretation for the fix is present.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding the matrix determinant implementation within SymPy, tracking down why the Bareiss algorithm is being applied inappropriately (i.e., symbolic, non-integer matrices), and finding a robust way to perform symbolic zero-checking (using expand_mul as in the gold patch). The engineer must know the SymPy codebase, design and implement an appropriate correction, and update/add tests. This requires navigating multiple files and algorithms, reasoning about symbolic mathematics, and potentially debugging recursive algorithm logic. This is non-trivial and likely to take 1-4 hours for most experienced engineers.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain\u2014the setup and goal are clear, the core problem is isolated, and patches/tests are provided. The description could provide slightly more context on expected behavior, but the presence of a demonstration, traceback, and the hypothesis concerning the Bareiss algorithm make the requirements adequately interpretable for a skilled engineer. The solution also appears to be self-contained within the determinant logic, avoiding broader project ambiguity.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-16792": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is generally well-specified. It clearly explains the context (autowrap with the Cython backend), the problematic behavior (the wrong C signature is generated when array arguments are present but unused), and gives a concrete example of both the error and the incorrect code generated. It also explains the correct behavior expected (array arguments should have the correct pointer type in the C signature regardless of usage in the expression). The only minor ambiguity is that it doesn't specify precisely which source file or method is responsible, but it identifies 'codegen' as likely, and a qualified engineer could reproduce the issue and have a clear understanding of what a successful fix would look like from the description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small to medium bugfix requiring some thought, as it involves understanding how argument types are generated in codegen for the cython backend. The engineer would need to trace argument handling even when unused in expressions and adapt signature creation logic, which may require familiarizing oneself with internals, but the changes are ultimately limited in scope (a few functions in the code generation pipeline) and the test is straightforward. Likely solvable in under an hour for someone familiar with the codebase or with some inspection.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue is relevant, reproducible, and the desired fix is testable. The engineer should be able to meaningfully solve the task with the information given and test the fix using the provided pattern.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17139": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified: it gives a specific stack trace and error message ('Invalid comparison of complex I'), along with the code context, indicating which files and functions are involved (e.g., fu.py, _f, TR6, _TR56). The immediate cause of the error (attempting to compare a complex to zero) is clear from the traceback. However, there is no concise high-level statement of what the correct behavior should be\u2014one has to infer from the stack trace, code, and context that the code should not attempt a real-number comparison for complex exponents, and that simplification should effectively be a no-op in that case. So the reader is left to fill in some details, but the problem is clear and there is a sensible interpretation of how to fix it.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required is localized to a small function in a single file (fu.py, _f). The error and its cause are clearly indicated in the stack trace (an attempt to use '<' on a complex exponent). The solution\u2014a type/property check to early-return in the problematic case\u2014is not subtle, once you understand the call stack and the mathematical requirement. This is a standard fix that requires some familiarity with SymPy's expression mechanics and Python's type properties, so it is a small change but does require piecing together the context, reading and understanding a few functions, and testing. Thus, for an experienced engineer familiarizing with the codebase, it would be a modest, sub-1-hour task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue sample is suitable. The only minor challenge is that the issue description is presented primarily as a stack trace, and it would benefit from a brief high-level summary of the intended 'correct' behavior. Still, the trace and the error are clear enough to infer this. No other blocking issues are apparent; tests are present in the patch.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17318": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a detailed error traceback originating from a call to sqrtdenest in sympy/simplify/sqrtdenest.py. It identifies the specific IndexError ('tuple index out of range') and pinpoints relevant functions (_sqrtdenest0, _sqrtdenest1, _sqrt_match, split_surds, _split_gcd) and their code locations. Furthermore, it states the desired behavior: if an expression cannot be denested, it should be returned unchanged. However, while the error and requirement are clear, the precise input that triggers the failure and which expressions should be returned unchanged require some interpretation. The example fix test confirms the desired behavior, but from only the issue description (without looking at PR/commits/tests), one would need to fill in the blank regarding which kinds of inputs cause this issue. For these reasons, there are 'some blanks to fill in' but a sensible solution can be constructed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix involves tracing through a specific function call stack, understanding how the error occurs with certain inputs, and guarding/handling a special case (when the function cannot proceed with denesting, it should return the expression unchanged). The scope is limited to a well-defined area of code and requires only a modest change and some careful handling, not a major rewrite or broad investigation. Thus, an experienced engineer familiar with the codebase could likely diagnose and patch this within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues; the problem, trigger, and required behavior are fairly clear based on the trace and summary in the issue (particularly as the code is mature and functions and files are well-located and named). The engineer would simply need to determine from reading the code and/or adding a unit test what expression/input triggers the bug.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17630": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified. The user clearly demonstrates an exception that is thrown when multiplying BlockMatrix objects containing ZeroMatrix blocks and shows (with two different code paths) how the error occurs. The stack trace and the direct cause of the problem (Zero vs ZeroMatrix) are described. However, there is a small gap: the issue does not specify what the precise semantics should be for multiplying such matrices (i.e., what the expected output should be), but uses context and examples to make a sensible guess possible (e.g., BlockMatrix multiplication should work, normal multiplication should not produce an exception, and Zero blocks should remain ZeroMatrix). An engineer familiar with SymPy matrix internals would reasonably interpret the requirement. The reference to Python and SymPy versions is adequate context, but there is no pointer to a desirable end-state beyond 'no exception' and correct result type. \",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The underlying cause is a type confusion/handling issue between Zero and ZeroMatrix during multiplication, specifically in block multiplication logic. The fix in the gold patch is relatively small: it involves enhancing post-processing logic for MatAdd to ensure the correct type (ZeroMatrix) is used and the right methods are called. The changes are limited in code size and scope, though locating the exact fix may require some debugging and understanding of the matrix expression internals. For an experienced engineer familiar with the codebase, it would take about 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The bug is clearly shown via code samples and stack trace, the affected files can be determined, and the fix/test scope is reasonable for use in benchmarking coding ability. Assumptions about matrix semantics are standard for this library domain, so lack of ultra-detailed requirements is not a blocker.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17655": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: The user describes a specific error that occurs when multiplying a geometry.Point object by a number when the number appears first (i.e., number * point), as opposed to point * number, which works as expected. The traceback and code snippets clarify exactly what operation is failing and why. The expected behavior is explicitly stated: both forms of multiplication (point * number and number * point) should yield the same result. The error messages and the context make it clear what method needs to be implemented: the reverse multiplication method (__rmul__) for the Point class. All key information (classes, methods, operation, expected vs observed behavior) is present.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires adding an __rmul__ method to the Point class that delegates to the existing __mul__. This is a minor, well-contained change that requires a basic understanding of Python's data model for operator overloading. The engineer needs to identify where the multiplication logic resides and implement the reverse operation analogously. There is some need to interpret the error and traceback, but overall, the scope is limited and does not require significant refactoring, research, or changes beyond a single method.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues. The sample is suitable as it describes a reproducible, concrete problem, and the corresponding fix is well-contained. Tests are provided to verify the solution. The problem is a representative test of knowledge about operator overloading in Python, and there is no ambiguity about what is required.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-18211": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it points out that when using the solveset function, a NotImplementedError is currently raised in certain cases, but it would be preferable for solveset to return a ConditionSet instead. This is a clear and reasonable expectation, and the intent is mostly unambiguous, especially for someone familiar with SymPy's solving tools and the ConditionSet object. However, it is not perfectly specified because it doesn't give concrete examples or discuss edge cases of what mathematical situations this should apply to, or details about why ConditionSet is the appropriate type to return in all NotImplementedError cases. Still, it is a sensible and understandable request with little room for misinterpretation for someone experienced with the codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is relatively targeted: catch NotImplementedError inside a relevant evaluation method (here, _eval_as_set) and return a ConditionSet instead. This requires some familiarity with how exceptions are handled, the types involved, and the function's intended purpose. For someone acquainted with SymPy and its solver interface, this represents a small code change and a straightforward test addition, likely to fall between 15 minutes and 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major concerns. The patch and the issue are in sync, and the problem is well enough specified for inclusion in a coding benchmark. The new tests cover the adjusted behavior. The main challenge would be for someone unfamiliar with ConditionSet, but given access to documentation/code, it's reasonable. No extraneous complications are present.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20428": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description offers a reasonably clear outline of the problem: a 'Poly' object with an underlying DMP having an unstripped leading zero is causing errors in subsequent polynomial methods (e.g., ZeroDivisionError, IndexError). Specific functions and files are mentioned, such as clear_denoms(), terms_gcd(), and the DMP representation, which provides context. However, there is some ambiguity: the exact requirements for when and where to strip leading zeros, and for which methods or classes this is necessary, are not exhaustively defined. The examples and tracebacks provide concrete illustrations, so an experienced engineer could sensibly interpret the requirements, but a small degree of interpretation/assumption is needed for edge cases or implementation details. Overall, the issue is fairly well specified but not exhaustively prescriptive.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch involves a targeted change to the expressiondomain's __bool__ method (from 'f.ex != 0' to 'not f.ex.is_zero'). This is a fairly contained adjustment that does not require major refactoring, investigation across many files, or deep algorithmic work. Understanding the error requires some familiarity with how Polys and DMP representations interact in SymPy, but an experienced engineer could likely diagnose and apply the fix within 15 minutes to an hour, after familiarizing themselves with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The problem is reproducible based on the description, the context is clear, and the associated test provides concrete evidence of the intended fix. The issue could be used in a benchmark setting as it effectively reflects a real but approachable software engineering task with a precise outcome.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20438": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue gives a clear error traceback arising from incorrect handling of set operations in SymPy: a 'Complement' object is missing an 'equals' method, leading to an AttributeError in the simplify logic. It is apparent from context that is_subset is not returning correct results for certain set combinations\u2014specifically, for ProductSet and FiniteSet cases. While the root cause and the exact sets or use-cases to fix are not thoroughly described, there is enough directional content (error, function names, line numbers, context) for an engineer familiar with the codebase to meaningfully investigate and propose a solution. The issue could be more explicit about which cases of is_subset fail and what the expected outputs are, but sensible interpretation is possible.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix requires a good understanding of SymPy's set algebra, relational logic, and the multiple dispatch hierarchy. Changes are spread over several files; adjustments to issubset logic, relational simplification, and fuzzy logic are needed, as well as updating or adding comprehensive tests. Navigating these abstractions and ensuring no regression with subtle set logic means this would easily take 1-4 hours for a competent engineer after a short ramp-up period. The underlying bug isn't trivial and correct handling of all edge cases is nontrivial. However, it is not so complex as to require days of research or massive code rewrites.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues: the main blockers are a somewhat terse issue description, but the engineering challenge is reasonable and the solution/test patches clearly correspond to improving reliability and correctness of set comparison logic. As a benchmark, it reasonably tests interpretation, codebase navigation, and logic implementation ability.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20590": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it describes a change in behavior between versions where Symbol instances unexpectedly acquired a __dict__ attribute, apparently in violation of the intention to use __slots__ (which should prevent __dict__). The user points out that this is likely due to a missing __slots__ declaration in a parent class, and implies that Symbol objects should not have a __dict__. An experienced engineer familiar with Python internals and multiple inheritance/mixins should be able to interpret this as 'ensure Symbol does not have a __dict__ by fixing the slots in the class hierarchy.' However, it does not explicitly say which class to change or exactly what code change to make, so some deduction is needed. Thus, this is a '1' (some blanks to fill in, but a sensible and clear solution exists).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"After understanding the problem (Symbol now has a __dict__ due to a missing __slots__), the fix is to ensure __slots__ is defined in the relevant parent class (Printable), which is a simple code change. The main challenge is identifying where in the hierarchy to set __slots__, but after that the code edit is minimal (add __slots__ = ()). This is a classic 15 min - 1 hour task for someone familiar with SymPy's structure.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a good sample for evaluating the ability to recognize issues arising from Python class hierarchy, slots, and how they influence __dict__ attributes. The engineer will need to be aware of Python's data model, but no other issues stand out.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-21379": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly identifies that an unexpected PolynomialError is being raised when using subs() with certain expressions involving hyperbolic functions and piecewise arguments. It lists a set of very specific conditions under which the issue occurs (e.g., usage of cosh/tanh vs sinh, removal of division or exp affects the behavior, real symbols are required), which provides a sensible and reproducible scenario. However, it does not specify exactly what the 'correct' behavior should be (should it return an unevaluated expression, raise a different error, or something else?). Additionally, it is not clear where the fix is intended to go (i.e., which module or method is suspected to be the culprit), nor does it provide a minimal reproducible script. Still, an engineer familiar with SymPy could interpret the goal as preventing PolynomialError in these scenarios, making the issue 'fillable' for a motivated solver. Therefore, it falls under category 1: some blanks to fill in, but sensible interpretation is possible.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is localized and closely tied to specialized usage patterns in the SymPy codebase, but the description provides strong clues about how to reproduce the error and what triggers it. The actual code fix (as shown in the gold patch) is a modest try/except guard in a relevant codepath (handling PolynomialError when calling gcd), which only requires a bit of code reading to find the appropriate place. While reproducing and validating the issue requires some knowledge of SymPy's piecewise and polynomial handling, this should be within reach for an experienced developer. Therefore, it would likely require between 15 minutes to an hour to (1) identify where the error is raised, (2) propose/implement the catch, and (3) write or update a regression test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers are evident. The only minor caveat is the absence of an explicit minimal reproducible code sample or an explicit desired output in the issue description, but the detailed behavioral clues and the resulting test written in the patch compensate adequately. An engineer can reasonably infer the needed fix and test for it. This sample is suitable for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-22714": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides the context that using 'sympify' or 'Point2D' with 'evaluate=False' unexpectedly causes a crash with a ValueError about 'Imaginary coordinates are not permitted.' It supplies a clear traceback indicating where this occurs and describes the problematic behavior. However, it does not explicitly specify the desired new behavior\u2014whether the code should work in 'evaluate=False' mode and how (e.g., should it return a valid Point object, skip the check, etc.). The solution requires sensible interpretation: fix whatever causes the crash so Point2D with evaluate=False does not fail, but the specifics are inferred rather than directly stated.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change only affects a condition in the constructor of the geometry Point class in sympy. The patch involves modifying a single line to correctly check for imaginary parts in point coordinates in a way that is compatible with evaluate=False. The code and logic involved are straightforward for anyone familiar with SymPy. The solution does not require understanding of complex algorithms or changes throughout the codebase. It likely requires <1 hour, mostly tracing and understanding the evaluation logic and updating the boolean check.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues preventing this sample from being used to evaluate coding ability. It is a realistic example of debugging and making a targeted fix in numerical logic for an open-source math library, using clear traceback info, and verifying correct behavior with a simple test case. A solver is required to interpret the error and use Python/SymPy idioms appropriately.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23824": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite clear. It explains the expected mathematical behavior of the kahane_simplify() function in handling products of gamma matrices, specifically the bug where leading uncontracted matrices are reversed. The description includes concrete examples of input and output expressions, what the function is supposed to do, and how the current incorrect behavior manifests. Furthermore, it pinpoints the likely source of the bug in terms of code logic (removal and reinsertion of matrices in the wrong order in kahane_simplify()). This provides enough context and specificity for an experienced engineer to attempt a meaningful and correct fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is well-localized: it is clear which function is responsible and what the correct vs. buggy behavior is, including the gist of the algorithmic mistake (backward insertion of leading indices). The bug fix involves making a relatively straightforward adjustment to the manipulation of a list of indices/terms. Since the logic and functional expectations are clear, it is a small change that requires some thought but would likely not take more than an hour for an experienced engineer familiarizing themselves with the codebase. The patch itself modifies just a few lines of code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. It provides a clear description, context, code identifiers, and concrete examples, and is adequate for evaluating coding ability, particularly with respect to debugging, mathematical logic, and careful list handling. The mathematical background with gamma matrices might pose a domain challenge, but all relevant expectations are spelled out in the issue.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23950": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified in that it identifies an incorrect method return value (Contains.as_set returns Contains, which is a boolean, not a set). It also mentions a downstream error (AttributeError: 'Contains' object has no attribute 'as_relational') with a traceback, and provides concrete usage code. However, the desired behavior is implied rather than explicitly stated: it expects Contains.as_set to return the underlying set. The engineer is left to infer that the correct implementation should return self.args[1], i.e., the set argument to Contains. More context about the Contains class's purpose and interface is assumed, but given the snippet, a reasonable solution can be inferred.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required is relatively small and localized: altering the as_set method of the Contains class to return the set argument. The issue provides a clear error scenario and enough information to locate the relevant code. For an engineer familiar with the codebase, this would not take substantial time, though a quick check of related class contracts might be in order. The fix itself is likely under 10 lines, but some investigation of class assumptions is needed.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are evident. The issue text gives a clear code location and use case leading to the failure, and it is testable. While some base knowledge of how sympy sets work would help, the engineering challenge is reasonable. The downstream test patch shows that the test coverage is sufficient.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24066": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text is concise and identifies a concrete problem with SI._collect_factor_and_dimension(), specifically that it fails to detect dimensionless exponents when calling exp() with physical units. The example given \u2013 showing the incorrect and expected result for exp(second/(farad*ohm)) \u2013 clarifies the nature of the bug. However, the explanation does not fully describe the internal logic or intended contract for SI._collect_factor_and_dimension(), so some reasoning about expected unit handling and familiarity with the codebase is required. Still, the example output and input are sufficient for an engineer experienced with SymPy and physical units to sensibly interpret what needs to be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is isolated to a single internal function and involves handling the output of an exponentiation operation when the exponent is dimensionless. The patch modifies a single code block and the associated test only adds a specific test case. An engineer familiar with the codebase would likely need some time to review the function and dimension-handling logic, but the core change is small and targeted. This places it in the 15 min - 1 hour bracket for difficulty.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are present. The test and code fix are both clear and address the reported bug directly. The issue is suitable as a coding benchmark sample for engineers familiar with units and dimensional analysis logic in scientific Python libraries.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24443": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives enough context for a reasonably experienced engineer to make a sensible attempt at resolving the bug. It specifies the function with issues (`_check_homomorphism`), the nature of the bug (\\\"ValueError: The given images do not define a homomorphism\\\"), and extremely helpful details, e.g., where and under what condition the check fails (when dealing with inverted generators in the `_image()` function for `PermutationGroup`). There is also a code pointer to the relevant file and lines which, although external, tells the reader exactly where the problematic code is. While the text mentions simplification is possible (\\\"the whole thing can be greatly simplified\\\"), it does not strictly specify what the solution should look like\u2014i.e., the engineer must decide how to simplify or fix the issue. The primary bug and its conditions are clear but the full solution details are left open. This fits level 1: sensible, targeted problem, but some details/interpretative work required for implementation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized: it requires understanding how the mapping for generators and their inverses fails for `PermutationGroup`, updating the code (potentially simplifying it), and reviewing the test impact. The patch mainly reworks a single internal function and rewires how generator images are handled. This takes some domain knowledge and care around group theory terminology and internal representations, so it is not completely trivial, but the code to be touched is largely limited in scope and conceptually contained within one logical functionality. The changes would likely fit comfortably into a 15-60 minute coding/debugging window for an engineer familiar with the codebase and basic group theory.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues need to be flagged. The problem is well-scoped, and the test changes target the precise failure case. The only possible ambiguity is whether the simplification refers to code style, performance, or logical correctness, but the primary bug is clear. Requirements are thus met for coding evaluation in a benchmark setting.\",\"q2_5_confidence\":5}"
    }
]