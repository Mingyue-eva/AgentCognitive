[
    {
        "astropy__astropy-13033": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is clearly specified with a concrete example, expected and actual behaviors, and runnable steps to reproduce. The problem is narrowly scoped: when a required column (in addition to 'time') is removed from a TimeSeries, the resulting exception is misleading, and instead there should be an error message that clearly informs the user about the missing required columns. The affected class and method signatures are evident (TimeSeries, potentially remove_column, _check_required_columns or related logic). There are no ambiguous requirements; an engineer can confidently address the problem as described.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding how required columns are tracked within astropy.timeseries.TimeSeries, and then improving error reporting when a required column is dropped. The change involves a small, well-localized update to the error message formatting logic so it shows a clear list of required columns when they're missing. This is a relatively minor change, without the need to change the public API or perform a large refactor. Most of the time would be spent getting familiar with the codebase and writing/adjusting a concise logic change. Total time: 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major concerns with this sample. The scenario is realistic and provides a good test of ability to understand both API semantics and error messaging, using a concrete example of reproducible code. The error message logic is obviously important for usability. The only minor caveat is that manipulating the _required_columns attribute directly is not usually part of public API, but the sample is internally well-motivated and testing for that is valid.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-13977": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified. The reporter describes in detail the problem they are encountering: when using astropy.units.Quantity with custom duck types, Quantity.__array_ufunc__ raises a ValueError when the operands have incompatible units, rather than returning NotImplemented. The desired behavior is also articulated: return NotImplemented so Python's operator dispatch will attempt the reflected operation (__radd__, etc.), matching Numpy's ufunc API expectations. The issue includes a motivating example, code snippets, and justification for the requested change. However, there are some blanks to fill in, such as whether this should occur for all ufunc operations or only addition/subtraction, and what exceptions or operand types this impacts. But a sensible and specific interpretation is easy to formulate for an experienced engineer.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This change is moderately difficult. It requires an experienced engineer to understand the interplay between numpy's ufunc protocol, astropy.units.Quantity, duck-typing in Python, operator overloading, and exception handling/dispatch rules. The implementation is more than a plainly mechanical change: it requires thoughtful handling of exception cases, awareness of when to return NotImplemented vs when to raise, careful modification of function internals, and updating or writing new tests (possibly across multiple files), as seen in the gold changes. Getting this right without introducing subtle bugs requires a solid understanding and careful verification; thus, '1-4 hours' is appropriate if the engineer is already familiar with the concepts.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue and resolution appear suitable for use as a coding benchmark, as it tests a good range of Python and numpy interoperability concepts, as well as careful software engineering in a mature open source codebase. The main technical risk is if the candidate is totally unfamiliar with numpy's ufunc dispatch protocol, but the issue provides enough context for an experienced engineer to research and implement a solution. There are no other major barriers as long as the high-level requirements remain as described.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14096": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The author clearly demonstrates the problem by creating a subclass of SkyCoord with a property that accesses a missing attribute. The user expects that accessing c.prop should raise an AttributeError for 'random_attr', but the error instead misleadingly refers to 'prop'. The issue provides a minimal reproducible example, indicating both the class being subclassed and the specific mechanics of the issue. This is sufficient information for an experienced engineer to both understand and resolve the problem.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires some thought about how Python's attribute access works, particularly how exceptions are propagated from __getattr__ and __getattribute__ methods in class hierarchies. Once the engineer identifies that the error comes from the way __getattr__ currently raises AttributeError, the necessary change is straightforward: delegate attribute access to __getattribute__ so that standard error reporting is preserved. This is a small, focused change requiring knowledge of Python's object model but not extensive codebase changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample; the example is clear and targeted. The minimal code and expectation make it suitable for a benchmark task focusing on Python's attribute mechanics and error reporting. The test case from the patch further verifies that the scenario is self-contained and evaluable.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14182": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is largely well-specified, providing explicit examples of the desired behavior and showing both the current state and proposed usage for the 'header_rows' argument with the RestructuredText output. The Python REPL snippets clarify the intended output format. However, the issue does not specify all possible corner cases or error handling requirements, nor does it address how to read versus write for these tables beyond a mention in the initial comment. Nonetheless, the requirements for changing the write method to support 'header_rows' for ascii.rst output are sensible and interpretable. An experienced developer could productively work from these details, although some minor assumptions would be needed (e.g., how to treat types or missing headers).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The patch modifies logic in a core format writer (and indirectly its reader) for an astrophysical table output, allowing custom header rows in the RestructuredText format. This involves updating class constructors, inheritance chains, and adapting line offsets and table marking logic, as well as updating (or adding) tests to verify round-trip behaviors. The code changes span multiple methods, class initializations, and require careful coordination with unit types and dtype parsing. While it's not a massive refactor, it is more than a trivial fix and would likely take 1-4 hours for an engineer new to this codebase to analyze, implement, and test thoroughly.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no serious reasons to exclude this issue from the coding benchmark. The problem is sufficiently contained and the observable behavior is well-demonstrated by the given examples; tests can clearly verify requirements. The only (minor) ambiguity is in precisely how to interpret unspecified edge cases, but this is manageable. The feature is self-contained to the format writer subsystem.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14309": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete failure case (the failing test code), context about a recent upstream change that could have caused the problem, and a reproduction snippet. However, it does not explicitly specify the exact expected behavior or explicitly state what the correct return value should be. The reader must infer (from the code and the quote from @saimn) that the intention is for identify_format (and specifically, the is_fits check within it) to return False (not None or other values) when given a file with an unknown extension, rather than attempting to call isinstance on an argument, which may be incorrect or cause errors. While this leaves some blanks for the implementer to fill in, such as how best to match the old/new behavior and what side effects to account for, the combination of background, reproduction, and hint makes a sensible solution feasible without clarification. Thus, this is not fully well-specified, but not ambiguous enough to be a 2: it's a solid 1.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This problem can be addressed by inspecting the code handling file extensions in is_fits/connect.py and ensuring the function returns the correct Boolean for the given input. The issue is isolated; there is a concrete reproduction code snippet and a clear part of the codebase to examine (identify_format and is_fits). An experienced engineer familiar with the code would have little difficulty crafting this fix, since it mainly entails adjusting a conditional and possibly improving regression tests. The primary research here involves understanding the change in behavior and correcting/defending the conditional logic, which would typically require less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues are apparent. The sample is suitable for the benchmark as it involves logical reasoning about function output, a non-trivial but small code change, and the associated test adjustment, which would test candidate ability to interpret ambiguous problem statements and trace regressions.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14365": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. The problem is that the current implementation of ascii.qdp expects QDP commands to be uppercase, while the actual QDP format is case-insensitive. The desired behavior is to allow commands like 'read serr 1 2' (lowercase) as well as 'READ SERR 1 2' (uppercase) without causing the parser to crash. The reporter provides a concrete example of the problematic file and how to reproduce the crash, as well as what the correct handling would be. The affected functionality is in reading/handling command lines in QDP files. All details necessary for a successful fix are given.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a moderately simple fix: it primarily involves making the QDP command parsing case-insensitive, which in Python typically means adjusting the relevant regex or string comparison logic. The change is localized and does not require large-scale changes to the codebase, but does require some understanding of the parsing code and usage of regular expressions or string methods. Familiarity with the code structure around file parsing and error handling is helpful.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues. The issue is clear, the expected behavior is well specified, and the problem is reproducible. The test patch demonstrates coverage of the new behavior. This is a good sample for evaluating coding ability related to parsing and case-insensitive matching.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14508": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes:\\n- The problem: When creating a fits.Card object with a float value, the float is unnecessarily expanded in its string representation, causing truncation of the comment field.\\n- The root cause: _format_float in astropy/io/fits/card.py uses over-precise formatting (f\\\"{value:.16G}\\\").\\n- The exact code location is identified.\\n- Concrete examples are given, including an input (header card with 0.009125), and the actual (incorrect) and desired (correct) output string representations. \\n- The reporter even suggests a possible fix (use str(value) unless it exceeds 20 characters).\\nThus, any experienced engineer can deduce what is required: modify _format_float so the float string fits within FITS Card formatting, using minimal necessary digits, without expanding floats unnecessarily, thus preserving comment length. Test scenario is also described. There is no ambiguity in required behavior.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is confined to a single function (_format_float) and is conceptually simple: change the way floats are converted to strings. While the engineer must be careful to preserve the FITS formatting constraints (handle cases with or without scientific notation, limit to 20 chars, etc), the test case and examples make intended behavior explicit. The scope of the problem and the existing code structure are clear. The engineer can quickly iterate on a solution and rerun the described/tested scenario. Writing or adapting the tests is straightforward, as the patch example reflects. For an experienced engineer, this is a small single-function change that could be completed in less than an hour, but requires some understanding of float formatting and edge cases.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. This is a clean sample for a coding benchmark: the requirements are clear, the affected code is localized, the expected behavior is testable, and there is minimal ambiguity. The only note is that domain familiarity with FITS formatting might help, but is not strictly necessary due to the detailed description and examples.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14995": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. The reporter provides a clear context: a regression in astropy v5.3 with NDDataRef mask propagation when using arithmetic operations with 'handle_mask=np.bitwise_or' and one operand lacks a mask. Expected behaviors are explained (\u201cmask that exists should just be copied over to the output\u201d), with clear examples in both working and failing scenarios. The test code section precisely reproduces what fails and what should work, and the desired outcome is unambiguous. Although the reporter admits not knowing the internal implementation details, enough explicit input/output cases are shown for an engineer to infer what code change is required and how to validate a fix. The filename (NDDataRef, likely in astropy/nddata/mixins/ndarithmetic.py) and function interaction are evident from the context and reproducer. This meets the standard of being clear enough for a meaningful solution attempt without outside clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue, once understood, is fairly contained: it requires modifying the NDDataRef arithmetic mask propagation logic so that if either operand lacks a mask, it avoids invalid operations and just returns the mask from the masked operand. The patch confirms this is a conditional logic fix touching only a few lines in a single function. Reading/understanding NDDataRef's behavior and rerunning the provided reproducer/test would take most of the time. For an engineer familiar with numpy and astropy, the patch and appropriate testing are likely to fall within the '15 min - 1 hour' bucket.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major caveats. The issue concerns mask propagation logic with clear input/output cases and a well-specified reproduction. The only note is that some familiarity with NDDataRef and the purpose of handle_mask is useful, but this is not unusual given the context and the repro examples.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7336": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and describes exactly what goes wrong (an exception occurs when using the astropy.units.quantity_input decorator on a constructor with a type-hinted return value of None), what causes the error, how to reproduce it (providing full sample code), and even suggests a possible direction for a fix. The affected decorator (units.quantity_input) and the signature scenario are clearly described. Expected and current behaviors are both explicit, and the workaround further clarifies the scope. An experienced engineer can produce an appropriate fix based solely on this description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, targeted code change that requires understanding the effect of Python's type hinting (specifically, return annotations), the behavior of the decorator, and implementing a simple additional conditional check. The core challenge is to recognize when a return annotation is None and ensure no attempt is made to convert the (non-existent) return value to a unit. This may require briefly familiarizing oneself with the related decorator code and its interaction with Python's inspect module, but it is a self-contained update likely requiring less than an hour for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The sample is appropriate for benchmarking reasoning and light debugging skills. The issue is well-scoped, and the required behavioral change is both testable and meaningful. No ambiguity or problematic dependencies are present.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7606": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is concise but clear. It identifies that comparing a Unit object (specifically an 'UnrecognizedUnit') to None via '==' currently raises a TypeError, but that the expected and correct behavior is that the result should be False (i.e. the comparison should not raise). The example code succinctly demonstrates the problem. An experienced engineer could interpret this as needing to safely handle (or delegate) comparison with None in Unit equality methods. The issue is both specific in its problem and the expected result, so there is no meaningful ambiguity or uncertainty in understanding what needs to be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the issue itself is very well defined, its resolution requires some understanding of Python data model methods (especially __eq__, and how to ensure fallback comparison semantics) and some familiarity with how errors are handled in the relevant codebase. Adding the appropriate catch to return NotImplemented, rather than raising, is a minor but non-trivial change\u2014some investigation is needed to ensure the solution is correct according to Python best practices and does not affect other equality operations. This is a targeted fix that an experienced engineer could accomplish in 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue is suitable as a benchmark: it is narrowly scoped, tests an engineer's ability to correctly update method behavior according to Python conventions, and involves standard repo navigation.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7671": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text provides a clear problem statement: after a particular PR, the minversion utility fails in a case where it previously didn't. Clear examples are provided with interactive sessions, showing the old and new (buggy) behavior, alongside the suspected root cause\u2014LooseVersion does not handle dev/rc version strings as expected, while pkg_resources.parse_version did. Although no explicit solution is demanded, it is clear that the goal is to fix minversion so that it works properly with post/dev version strings. The problem domain and function to patch are unambiguous, but the implementer must decide whether to patch the comparison logic or change the parsing method. Thus, there are 'some blanks to fill in,' but there is a sensible interpretation that enables a meaningful solution attempt.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted bug fix relating to version string parsing. It requires reading the problematic function and understanding how Python's version comparison tools work. Coming up with a fix (e.g., extracting the numeric part of the version string before comparing) may require a bit of research (such as checking PEP 440 or version parsing recommendations), but it does not require substantial rewrites or multiple-file changes. An experienced engineer should not need more than an hour to implement and validate a solution, given the clarity of the example and location of the bug.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues. The only possible limitation is that the issue is somewhat domain-specific (knowledge of version comparison quirks and third-party utilities), but that is typical for this type of benchmark. The test provides clear before/after behavior. The example input/output is sufficient to guide the implementation. The only minor ambiguity is whether to use an external library or patch the comparison, but that is solvable within engineering judgement.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-10554": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified: it provides concrete code examples that demonstrate the problem with unions of querysets and the interaction with ordering, particularly when combining derived querysets and then applying order_by and values_list. The actual code snippet demonstrates the query API calls and the misbehavior (i.e., how re-evaluation of the queryset after applying order_by produces an incorrect result or even breaks). However, the underlying cause or the precise expectation (e.g., what the correct output should be) is implied rather than explicitly stated, and the mention that the models are not relevant means the reader must be familiar with Django ORM internals to understand the problem context. An experienced developer can sensibly infer that the expected fix is to make union() on querysets compatible with order_by and values_list, preventing breaking during re-evaluation, but there's still some interpretation involved in how exactly to fix this within the Django codebase.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires a detailed understanding of Django's ORM implementation, especially the SQL compiler and query composition logic. The fix is not confined to a trivial change; it involves reasoning about how columns are selected and ordered in combined queries, ensuring that columns used in ORDER BY clauses are handled correctly when constructing SQL for unions. The patch involves adding new logic in compiler.py and introducing a new method in query.py, reflecting a moderate and nontrivial code change (but not huge). Given the interdependencies and need for a correct ORM-level solution without introducing regressions, it's likely to take a few hours for someone new to this segment of the codebase, especially considering test writing and validation. It's not an esoteric >100 line refactor, but clearly more than a minor patch.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major blockers are present. The issue demonstrates a legitimate bug with clear testable symptoms. The main limitation is the need for Django ORM/domain-specific knowledge, but for this evaluation setup (with a codebase and tests provided), it's appropriate. It does not assume undocumented context and the reproduction is clear.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11087": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear summary of the observed problem after upgrading Python versions and describes the conditions in which a UnicodeDecodeError occurs during a Django .delete(). The root cause is well analyzed, particularly that Django's delete process retrieves unnecessary fields (including corrupted unicode-values) for the cascade delete, leading to failure when decoding under Python 3. The text further describes that only referenced fields should be fetched in the relevant query. However, it lacks very specific guidance about how or where in Django's source this logic should be implemented. Nonetheless, for an experienced Django developer, the provided details (model/field references, expected and actual SQL, behavioral expectations) give a workable high-level spec and a clear desired behavior: optimize .delete() so it only fetches required fields, avoiding unnecessary UnicodeDecodeError and improving performance. The engineer would need to determine the best place to change the queryset generation logic for delete cascade/select, but the outcome is unambiguous. There are some blanks to fill (design/placement of the fix), but the goal is clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding not just the model code but also Django's deletion mechanics and how cascades, signals, and deferred fields interact. The fix involves modifying logic deep in the ORM's deletion machinery to only select referenced fields for certain cascade operations, unless deletion signals are present. This requires reading, understanding, and carefully editing complex, critical code (as shown by the gold patch touching deletion.py and models). It is not a trivial or superficial change; it requires familiarity with how Django fetches related objects during deletions, safe usage of QuerySet.only(), and making sure not to break signal-driven or other complex behaviors. Proper testing for this kind of core-ORM change is necessary, as evidenced by the involved test updates. For an experienced Django developer with some time to familiarize themselves, this is in the substantial (1-4 hour) range, as it means revisiting cascade logic, understanding related code, and validating with new/updated tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are present. The issue is relevant, the requirements are sufficiently clear, and the solution can be implemented without need for clarifications, albeit requiring judgment on implementation details. The test patch also shows that a clear, verifiable behavioral test was possible, making it suitable for coding assessment purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11265": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description includes a clear and concrete example (with code) illustrating the bug: when using exclude on an annotated FilteredRelation in Django, it raises a FieldError on the annotation name. The reporter describes what happens, gives the code that triggers the bug, and identifies the function (split_exclude) suspected to be at fault, providing good starting guidance. However, the description does require the reader to have context on Django's ORM internals, Annotate, FilteredRelation, and exclude mechanics. The issue could also be clearer about precisely what the FieldError is and under what more general conditions it arises. Still, an experienced engineer would have a sensible and actionable problem statement, so this is between well-specified and 'some blanks to fill in,' best fitting level 1.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The patch touches internals of Django's Query class, affecting how filtered relations are handled across queries, and modifies the behavior in split_exclude and trim_start. The fix is non-trivial: it spans multiple lines, alters the handling of filtered relations, and requires understanding of how query objects are cloned and modified. It requires a good grasp of Django ORM's internals, query generation, and perhaps careful manual QA/testing to avoid introducing regressions, especially given the subtlety of join behavior. This would likely take more than an hour for someone not already familiar with this part of the codebase, but could be done within 1-4 hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues beyond those described. The issue is suitable for evaluating coding ability, as it involves debugging, understanding non-trivial code, and writing targeted tests. The primary challenge here is the codebase complexity, not lack of clarity or missing context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text provides a clear description of the problem, including how Django's CheckConstraint generates SQL with fully qualified field names when using a mix of AND/OR conditions, resulting in invalid migration SQL for SQLite (and Oracle). The description also gives a full code example that can be executed, as well as the actual and desired resulting SQL queries. Moreover, there is a technical analysis of what causes the problem in the code (difference between Col and SimpleCol), which offers ample guidance to resolving it. A competent engineer would understand the technical context, the error, and the expected outcome without needing to ask for further clarification.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires understanding the details of SQL constraint generation across backends (at least SQLite and Oracle), digging into Django's ORM and migration internals, specifically in how check constraints are compiled and rendered in SQL. The fix (as shown in the gold patch) affects query-building internals and is not a trivial change. Although it only touches a few lines of code, a correct patch requires careful thought and significant familiarity with Django's query/constraint generation logic, as well as SQL and differences among supported backends. Diagnostic effort to validate the actual and expected SQL further adds to the complexity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a great benchmark issue: the specification is very robust (with technical root cause and input/output examples), and the changes are significant enough to require engineering skill and ORM familiarity. The only caveat is that this is mainly relevant to systems with detailed ORM/SQL constraint generation logic, such as Django, but that's acceptable for this context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11532": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description explains both the context (mail sending in Django, problems with non-ASCII hostnames when using non-unicode email encodings) and the cause (headers, specifically Message-ID, being improperly encoded). It provides a clear test to reproduce the problem, explains the impact, and even suggests a specific solution: convert domain names to punycode before using them. The filenames and functions implicated are called out directly (django.core.mail.utils, django.core.mail.message), and the test describes exactly the expected behavior. This level of detail is appropriate for a well-specified engineering issue.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This problem requires changes across multiple modules (mail message handling, address sanitization, and utility code), probably requiring an understanding of Django's encoding and email sending internals. While the problem and fix are well-defined, implementing it correctly involves careful changes to encoding routines, appropriate utility function creation, and integration with places in the code that manipulate domain names. There are also multiple places to update and test. For an experienced developer, this is a 1-4 hour task, depending on familiarity with Django's email/encoding internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues with using this sample: the issue is technical, well-scoped, testable, and concrete, and has a sensible, implementable solution. The resource links in the issue text don't break the self-containment because the key information is separately summarized in the issue body itself. The specificity of the reproduction steps and test case contributes to the sample's suitability for a coding evaluation task.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. It describes the problem in detail, referencing specific behaviors before and after a given Django commit, and clearly outlines where the logic is incorrect after the change. It gives concrete code samples of the models, admins, and even a patch with the intended logic. The chain of logic for each possible condition that could arise in the _check_list_display_item function is presented in a table, and the required behaviors are described in terms of error codes (E108, E109) and their meanings. Furthermore, it even provides a suggested correct implementation (pseudo-code or actual implementation) for the function, which matches the fix in the Gold PR. The error is described in the context of Django admin validation, and is precise about the root cause (misinterpretation of hasattr and get_field behavior).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the bug involves nuanced behavior in Django's admin validation code, the scope for the fix is limited to a single function (_check_list_display_item). The issue text provides thorough background and a logical breakdown of all edge cases, as well as a suggested logic for the solution which greatly reduces the need for deep exploration. Replicating and fixing would require a little thought about exception handling and field lookup in Django, but for someone familiar with the codebase, the explicit table and thorough write-up mean this falls into the '15 min - 1 hour' range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns. This is an exemplary benchmark issue: clear error definition, detailed context, sample code, explanation of edge cases, and a clearly-scoped patch. While domain knowledge of Django's admin and field validation is required, the guidance in the text makes the target behavior extremely clear.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11734": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description presents a minimal code example that demonstrates the bug, showing that using OuterRef within exclude() or ~Q() in Exists subqueries leads to a crash, whereas filter() with OuterRef works. The precise failure scenario is described, including which calls succeed and which fail. However, the issue text does not deeply elaborate on why this happens or the intended internal behavior correction, so some understanding of Django's ORM internals and OuterRef handling is required. An experienced engineer can interpret this as indicating that OuterRef is being resolved using the incorrect model context in these cases, and that a fix should address this bug so that usage is consistent across filter()/exclude()/~Q().\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This problem requires familiarity with Django's ORM query construction and its internals, especially how OuterRef operates in subqueries and exclusion clauses. The fix involves making non-trivial changes across multiple core files. Understanding and debugging ORM's complex query building logic takes time. The patch edits multiple core modules, and the correct solution requires careful analysis to avoid introducing regressions. Testing is also not trivial as regressions may not be obvious except in particular ORM query constructions.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues beyond what is already noted. The test and solution are clear enough for an experienced Django developer to resolve. The example in the issue, while concise, covers the minimal reproducible scenario. The sample is suitable for evaluating coding ability in the context of framework-level bugfixing.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11740": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly concrete: the user describes a scenario where they have two Django apps, and in one app they change a model field from a UUIDField to a ForeignKey pointing to a model in another app. They notice that the Django migration autogenerated for this change does not include a dependency on the app being pointed to (testapp2 / App2), which should happen, since ForeignKey introduces a dependency across apps. The reporter provides a clear minimal example and steps that would reproduce the issue, including mentioning the specific change. While the description is a bit informal and questions whether it's truly a bug, the required behavior (FK alteration should introduce a dependency in the migration) is clear enough for a competent engineer familiar with Django's migration system. Nonetheless, it leaves some interpretative work around exactly where and how to fix this within Django's codebase and whether other edge cases (nulls, blank, existing data) should be tested, so it is not a completely prescriptive 'write X in file Y' style request.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this bug requires understanding Django's migrations framework, specifically how migration dependencies are calculated and applied when fields are altered to ForeignKey types. The patch affects core migration detection logic, and it is not entirely trivial: reasoning about how dependencies work and why they may not be detected, investigating the autopopulation of dependencies, and ensuring this doesn't break circular dependencies, etc. Furthermore, the patch and required new tests show that the engineer needs to write both logic and new tests. For an experienced engineer familiar with Django, this should be solvable in a few hours, but is neither a quick nor trivial fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no blockers or external requirements omitted from the sample. The issue contains all relevant information including a clear example. While the user is unsure if it is a true bug at first, the concrete details communicated make this a suitable and realistic benchmark for coding ability in Django and Python, especially if focused on the migrations subsystem.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11815": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It summarizes a problem in Django migrations: when serializing an Enum for a CharField's default, the migration uses the Enum value (which may be a translation) instead of the Enum member's name. This causes issues if translations change or the value is not unique/stable. It provides concrete code examples both in the model and in the generated migration. The desired behavior (serialization should use the Enum name, e.g., Status['GOOD']) is explicitly stated. Any experienced Django/Python engineer can understand what is wrong and what a valid fix must achieve. There is no ambiguity in the requirements, and core context is provided without need to reference outside discussions.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is clearly described, and the required fix (change Enum serialization in migrations to use the Enum's name, not its value) is conceptually straightforward. However, identifying the correct place to make this change (likely in the serializer logic) and ensuring it handles all Enum-using code paths (with suitable test additions/changes) requires some codebase reading and careful handling. It is more than a trivial single-line fix but remains a targeted adjustment in a centralized serialization area, which should not require deep refactoring or extensive research. This fits well into the '15 min - 1 hour' category for an experienced developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues with the sample. The issue definition, expectation, and affected code area are all clear and self-contained. There is no external dependency or ambiguity that would undermine its suitability for a benchmark evaluating coding ability. The description includes code samples and clearly distinguishes 'before' vs. 'after' behavior, making this a strong example.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11964": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It presents a clear and concrete example of the current behavior, demonstrates exactly what model and model field types are involved (using Django's models.TextChoices and models.CharField), and provides actual test code that exercises and demonstrates the bug. The expected result is stated both in prose and in executable assertions: the field value should always be a str, not an Enum. The undesirable case and the desirable case are both illustrated explicitly. As an engineer, I would know exactly what to do: ensure that accessing the value of a field defined via a Choices enum always yields a str (or int for IntegerChoices), not the Enum type or instance. No clarification is necessary, and there is no ambiguity about which files or classes are involved.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is moderate because the engineer needs to understand how Django's Choice enums interact with field serialization/deserialization and model attribute access, which may require some reading of Django's source code and knowledge of the Enum protocol. However, the change itself is small (essentially customizing __str__ on the Choices class) and localized to a single file. Most of the time will be spent verifying understanding and running tests, rather than making code changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues detected. The sample is conceptually clear, tests are provided, and there is nothing that appears problematic with regard to using this sample in a coding-benchmark context. No external systems or API keys are required, and no missing context is apparent.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12155": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well specified. The reporter describes a specific problem: the admindoc utility only works with docstrings where the first line is empty, and fails when the docstring text starts on the first line. The reporter provides an explicit code snippet that shows typical docstring formatting, identifies the problematic line of code in the trim_docstring function (the calculation of indentation), and even gives a suggested fix (skip the first line when calculating indentation). The affected filename and function are named explicitly, and the minimal context surrounding the bug has been provided. Any experienced developer familiar with the codebase would be able to interpret and implement the fix based on this description alone.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is straightforward and requires a small change to the logic for processing docstrings. The issue is isolated to a single function (trim_docstring), and the solution flow is given in the issue description. The candidate solution (skipping the first line when calculating indentation) is detailed, so the change can be made quickly. Ensuring correctness might require minor testing, but overall the expected implementation time is well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other impediments to benchmarking coding ability using this sample. The issue is clearly described, the motivation and proposed solution are understandable without external context, and an engineer could write appropriate code and tests to demonstrate the fix. The scope is self-contained.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. It describes a TemplateSyntaxError that occurs when using Django custom template tags with keyword-only arguments that have default values. The examples highlight failure scenarios (e.g., using a keyword argument multiple times, or using keyword-only arguments in simple and inclusion tags) and even show the decorator and signature style that fails. The text also references all affected versions and implies the code is shared between simple and inclusion tags. However, it could be clearer about the exact desired behavior, and misses explicit details of the correct parsing, and error message specification, leaving some interpretation up to the developer. Still, a sensible solution path is clear for an experienced engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix consists of a minor change in how keyword arguments are handled during template tag parsing, ensuring keyword-only arguments with defaults are properly recognized. The affected logic is largely contained, and relevant context can be traced in one or two files. The change is subtle but straightforward for someone experienced in Django's template system, likely requiring less than an hour for reading, testing, and implementing, especially with the informative bug examples in the issue.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues prevent this sample from being used for a coding benchmark. The issue text is sufficient for an experienced engineer to understand and resolve, with clear failure examples and contextual cues. The test coverage provided in the corresponding patch demonstrates how to check for regressions and edge cases as well.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12325": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description provides a partial code snippet and explains a confusing behavior around the order of OneToOneField declarations in Django model inheritance. It points out that changing the order of fields in the subclass definition causes different behaviors when working with parent models, then asks whether the order should matter when the parent_link field is explicitly set. However, the issue is written in a highly condensed, almost note-like form, lacking a clear statement of the expected behavior, context, or actual/expected results. There is no explicit description of what the wrong outcome is, or what a correct solution should verify (e.g., is it a bug, or just a confusing API?). The person reading and implementing the fix needs to reconstruct assumptions about intent and desired behavior (\\\"First issue is that order seems to matter? Even if ordering is required 'by design' (It shouldn't be we have explicit parent_link marker)\\\"). It is not completely impossible to infer a solution direction, but a lot of knowledge about Django internals is being assumed. Thus, this fits best under rating 2: there is room for ambiguity and it's unclear exactly what a successful solution would look like.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a deep understanding of Django's model inheritance, the metaclass machinery, and how OneToOneField with parent_link operates. Even once clarified, the task likely involves tracing through meta programming logic, modifying class creation behavior, and updating related test cases. The patch affects critical model metaclass initialization code. While the code changes in the PR are not extremely large, determining the correct behavior is non-trivial, and verifying subtleties would require significant domain knowledge and careful testing. Therefore, an experienced engineer would need 1-4 hours, fitting a difficulty of 2.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Despite its vagueness, the issue can be addressed with sufficient Django knowledge, though it requires strong internal context. There is no evidence in the text of extraneous blockers (such as missing files or project context). However, poor specification could lead to solutions that are correct in one interpretation but not intended by the original author. If used, this sample should be paired with clear test cases (as in the PR) to ensure objectivity in evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12663": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description specifies that a regression occurred in Django after a particular commit: It broke the use of SimpleLazyObject within a queryset with a nested subquery annotation. The description provides concrete models (A, B, C), imports, and a clear self-contained test case that reproduces the problem. However, while the test is 'somewhat arbitrary,' it is replicable, and the problem statement is focused: using SimpleLazyObject in this context fails, but did not previously. What is not clear is exactly how (e.g., exception/traceback or silent bug) it fails, leaving a small blank about the outcome, and the issue presumes knowledge of Django ORM internals and recent history. Sensible interpretation would be to make whatever change is necessary for the test to pass as it did before the given commit, but there is room for ambiguity regarding the 'correct' internal behavior versus just restoring the previous surface behavior.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue deals with normalized interaction between Django ORM constructs (SimpleLazyObject, Subquery, annotations) and a regression tied to a recent core change. Understanding this bug and solution requires knowledge of Django's queryset internals and testing the regression. While only a small code change is required, locating the correct fix, understanding the context, and confirming that it doesn't introduce regressions elsewhere likely requires reading core Django code. There is also a need to understand why the change after the indicated commit cascaded to break this behavior. This depth of investigation and the specialized context place it in the 1-4 hour range for an experienced Django developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blocking factors: The issue gives a self-contained (albeit arbitrary) test case and models, so a competent developer could successfully implement the fix and demonstrate it via tests. The only minor concern is the lack of explicit output or error in the description, but this does not critically reduce usability for benchmarking skilled engineers familiar with Django ORM and test writing.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12708": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified: it describes a crash/bug in Django migrations when deleting index_together from a model if the same fields are also in unique_together, and gives clear reproduction steps and context about model definitions and the migration operations that trigger the bug. It also explains the desired behavior (that deletion should work, and indexes shouldn't be re-created unnecessarily), but is a bit open-ended as to whether the solution should be in making migration operations coherent or preventing outright deletion, etc. The issue requires the engineer to interpret Django model and migration mechanics, but from the provided reproduction steps and expectations (\\\"deletion should be possible without affecting unique_together\\\"), the requirements for a fix are fairly clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small logic bug that appears in Django's migration system when handling the interaction of index_together and unique_together constraints. The patch is small (editing a function to ensure unique constraints are not deleted when only the index is affected). For an engineer familiar with Django's schema/migration machinery, this would take some investigation of the relevant functions and test cases, but doesn't require rewriting large sections of code or deep algorithmic changes. Understanding and properly fixing the interactions between different model constraints takes some attention, but is not likely to require more than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues stand out with respect to using this issue as a coding ability benchmark. There is a clear problem, testable behavior, and a fix that can be verified via tests. The engineer does need some basic familiarity with Django migrations and schema constraints. The expectations are reasonable and the test provides a direct way to validate the solution.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12754": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete example with before and after models, noting that Django's makemigrations command generates the migration successfully, but migrate fails due to the order of operations (CreateModel before RemoveField). The report prescribes the correct behavior: the migration operation order should be reversed so that RemoveField comes before CreateModel. The description also mentions that the auto-detector should find this correct order, making it clear what needs to be fixed and why. There is a slight gap in that a reader unfamiliar with Django migrations or model inheritance might need to do additional investigation to fully grasp the implications, but any experienced Django developer would know the context. The issue is not overly vague, but it leaves some behavioral details and corner cases up to the implementer. Thus, it is closer to a '1': some blanks but a sensible and clear goal.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires understanding the Django migrations framework, especially how migration dependencies and model inheritance are handled in the autodetector. Making the correct adjustments involves nontrivial reasoning about operation ordering, inspecting model states, and implementing logic to handle base/remove field interactions. This is not a trivial or cosmetic bug; the fix, while not massive, touches on internal migration graph logic and requires care to not introduce other bugs. The solution is relatively concise, but identifying the correct place and logic for the dependency management (as seen in the patch) and ensuring it doesn't break other migration scenarios would take thoughtful investigation and targeted testing. For an experienced Django engineer, this would generally take 1-4 hours, as some time would be needed to write and confirm appropriate tests as well.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues blocking the use of this sample. The example and context are solid, and the solution is testable and meaningful for backend engineering skill assessment. The only mild caveat is that the topic is specialized (Django migrations), but within that domain, the question is clear and technically interesting.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12858": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified and gives detailed information about where the problem lies: models.E015 is raised (likely incorrectly) when ordering uses lookups (such as 'isnull') that are not transforms in Django models. The issue provides concrete examples of queries that should work (with field resolution via multiple foreign keys), lists the involved model relationships, and clearly references an associated change (PR #29408) that introduced the regression. However, it does not explicitly say what the correct behavior should be (though it can be strongly inferred that it should allow ordering by lookups such as 'isnull'), nor does it indicate what type of test coverage is expected. There are some blanks the implementer must fill (e.g., exact locations for code change; subtleties, like support for custom lookups), but the intent is reasonably clear. Thus, I assign this a 1.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is relatively straightforward once the area of code is identified. It requires a small logic update to also recognize lookups (not just transforms) as valid for ordering in the relevant check method. The anticipated patch (as shown) comprises a minor logic change involving a single function and simple conditional logic update. A developer familiar with Django's codebase and model checking infrastructure will be able to implement and test this in 15-60 minutes. The main effort lies in identifying the correct place to update the logic and ensuring tests cover lookups in ordering. No major rewrites or multi-file changes needed.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues present. The sample is suitable for use as a coding benchmark, as it contains a well-framed issue, clear regression behavior, and a concise code fix. The primary challenge for an engineer would be quickly orienting themselves in Django's model validation / checking code.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13012": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it explains the context (using ExpressionWrapper with a constant expression in Django ORM), details the incorrect SQL query generated (SHOWING the problematic GROUP BY clause), and contrasts it with the desired behavior (what happens if ExpressionWrapper is not used). It also provides clear, minimal code samples demonstrating both the error and the working alternative. The failure mode (Postgres exception due to invalid GROUP BY) is explicitly described. All essential details needed for an experienced engineer to understand, reproduce, and attempt a fix are present.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a focused bug fix that should take between 15 minutes to 1 hour. The problem has been isolated to ExpressionWrapper and its effect on SQL generation, and the solution involves modifying or overriding its group_by behavior to match Value. The engineer does not need to substantially rewrite code or investigate an obscure subsystem\u2014everything is localized and there's a clear path to a fix, but it does require understanding of Django's expression system and a small code change plus test update.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues were identified. There is clear input (Python code), expected output (correct SQL), and explicit failure mode. The issue provides enough detail for benchmarking purposes and is well-suited for the described evaluation setup.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13028": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: It provides relevant model code, the context of a Django upgrade, and clearly describes the problematic query (filtering ProductMetaData with metadata_type). The key observation is that a model field named 'filterable' (BooleanField) seems to trigger a NotSupportedError when used on the right-hand side of a filter. It also includes the workaround (renaming the field), suggesting that the problem is tied to the field name clashing with some internal or ORM logic. However, the issue does not specify the exact error message or stack trace, and the title is a bit technical for those not familiar with Django internals. It is clear that either documentation or a fix is requested, but leans slightly on the reader inferring which part of Django to patch. Overall, there's a sensible and specific interpretation, though a few blanks remain.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change requires understanding Django model field resolution and how ORM expressions use 'filterable'. An experienced Django developer would recognize a namespace clash between user-defined model fields and Django's internal query-system attribute names. The fix involves adding an extra guard to avoid false positives. The scope is localized (a small conditional change and a corresponding test), making this a minor but subtle change likely to take 15-60 minutes for someone familiar with the internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The only challenge is the need for familiarity with Django's ORM internal mechanisms and naming conventions. The test patch provides a clear demonstration of the error. No other major issues or ambiguity present.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13112": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides concrete information about the observed crash when running migrations with a model referencing an app (\\\"DJ_RegLogin\\\") that uses a mixed-case name in INSTALLED_APPS and in the app config. It includes specific code snippets from models.py, settings.py, and apps.py, and references Django version changes (works in 3.0, broken in 3.1b1). However, it does not show the exact traceback or describe the details of the crash, nor does it explicitly state that the problem is with ForeignKey resolution due to case mismatches. An experienced engineer can deduce that the problem centers on string-case handling when resolving related models, but the exact requirements for solution (e.g., how to normalize case, whether to warn or fail on case mismatches, etc.) are left to interpretation. Hence, there is some ambiguity, but the scenario and sensible direction for fixing are clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The solution requires understanding not just the source of the bug (case-sensitivity in app label/model name handling in deconstructing related fields) but also changes to core Django internals, which is not trivial for someone unfamiliar with the area. Fixing this requires debugging, finding where app/model label normalization should happen, understanding the impact of changes to the ORM/metaclass machinery, and writing or updating tests to cover mixed-case behaviors. The code change itself may appear small, but reaching, implementing, and safely testing the fix is more substantial. For someone familiar with the Django codebase, this would likely take 1-4 hours between investigation, implementation, and thorough testing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The sample is suitable for evaluating coding ability in dealing with framework internals and subtle string-handling bugs. The only minor shortcoming is the lack of an explicit traceback or example error output in the issue description, but enough context is provided to diagnose the core issue.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a concrete example of the desired functionality: performing temporal subtraction of DateTimeFields via ORM annotations, without requiring the use of ExpressionWrapper. The class and field names are given (e.g., Experiment model with start and end DateTimeFields), and an example queryset is shown attempting to annotate with delta=F('end') - F('start') + Value(datetime.timedelta(), output_field=DurationField()). While the description is somewhat terse and does not explicitly state what goes wrong currently or provide expected/actual output, it is clear that the intent is to allow such arithmetic directly and transparently. An engineer familiar with Django's ORM and expressions could reasonably infer the task: make temporal arithmetic work as shown, without requiring ExpressionWrapper. Thus, there are a few blanks (namely current/expected errors and why ExpressionWrapper is needed), but the goal is reasonably clear for a meaningful attempt.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The task involves changes to the internals of Django's ORM expression code to support more direct temporal arithmetic (subtraction) between date and datetime fields, bypassing the need for ExpressionWrapper. This is not a trivial fix\u2014it requires understanding or learning about the ORM's expression system, operator overloading, output fields, and backend compatibility. The patch edits substantial logic in expressions.py and modifies relevant test coverage. This would likely require 1-4 hours for an engineer with some familiarity with Django internals, given the need for careful, correct changes and comprehensive testing of database backends and edge cases.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The problem is relatively self-contained and is a good test of understanding ORM internals and expression evaluation logic. The concrete class and field names anchor the issue sufficiently for benchmark purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13297": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably clear: it describes an incompatibility/bug introduced in Django 3.1 where kwargs passed to TemplateView.get_context_data appear as SimpleLazyObject and must be explicitly converted to e.g. str in user code; in Django 3.0 this was not necessary. The user gives a concrete code example, both 'broken' and 'fixed', and mentions how URL patterns feed into this view. The context around what changed and what fails is well-specified. However, it doesn't specify precisely what the underlying expectation is for all view kwargs (e.g. whether they should always be resolved to plain value types), nor does it reference the Django base code directly. An engineer can sensibly infer what the fix needs to be, but a little interpretation is required to generalize beyond just the example. Thus, this scores a 1 (some blanks, but a sensible interpretation).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change is small and local to the base TemplateView implementation. The required change is to alter how URL kwargs are handled before adding them to context, switching from wrapping with SimpleLazyObject to using a lazy decorator that evaluates to the correct base type. This requires some understanding of Django's internals and care in choosing the correct mechanism, but is within the capabilities of a typical contributing engineer. The patch demonstrates a focused and relatively simple change, so this would take less than 1 hour for an experienced engineer familiar with generic Django views, especially after reproducing the problem.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The core logic is self-contained, and the failure mode is easy to understand from both the description and provided code snippets. The issue does rely on the reader's context regarding Django view mechanics, but this is expected at this benchmark level.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13406": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it provides a clear description of the problem, the unexpected behavior, and demonstrates it with clear code snippets. It specifies the Django models involved, the operations that lead to incorrect results (pickling and reapplying query objects that use values/annotate), the code that breaks, the observed outputs, and why they are incorrect. An experienced developer would know the expected vs actual result, what is malfunctioning in the ORM's query/result machinery, and what a solution must accomplish (i.e., after deserialization, a values/annotate queryset must still yield dicts, not model instances). No additional context or clarification is required to implement a fix and test it.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue targets a specific Django ORM behavior involving pickling and restoring annotated 'values'/'values_list' querysets, which return wrong types after deserialization. The underlying fix is relatively small: ensuring the iterable class is correctly preserved/reapplied after reassignment of the pickled query. The patch changes just a couple of lines in a single method and does not require deep rewrites or a significant architectural change. However, it requires a solid understanding of the ORM internals, which is typical for a mid-level Django contributor and not trivial for someone unfamiliar with the codebase. Thus, it may take a little time to trace through, but the surface area is limited and the patch straightforward once the correct point is identified.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues spotted. The reproducible example, context, and code all appear complete and clear, and the solution is validated by appropriate regression tests. The problem is self-contained and does not appear to assume external state, databases, or non-obvious code paths. The sample is thus suitable for evaluating coding ability for the intended benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13449": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well specified. It clearly describes a crash when using the Lag() window function with a DecimalField on SQLite in Django, gives a reproducible sample model, the failing query, and even an explanation of what is going wrong in the generated SQL. It distinguishes the case that works (using FloatField) and offers a temporary workaround by setting output_field=FloatField. However, while the underlying bug (CAST occurring in the wrong place for DecimalField in the generated SQL) and the conditions for failure are specific, the ideal behavior is somewhat inferred: The expected fix is for CAST to wrap the window expression correctly for DecimalFields on SQLite, restoring parity with FloatField. There is some room for interpretation regarding the best solution approach, but overall, the scope, requirements, reproduction, and intended effect are clear to an experienced engineer.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix requires understanding the Django ORM's SQL generation for window expressions, including how type casting is handled differently in SQLite for DecimalField, and how to change this logic so that the generated SQL is correct. The fix requires modifying the expression/compilation logic (class Window or similar), ensuring that it only impacts the relevant cases (specifically, DecimalField on SQLite), and then adding/adjusting tests to ensure proper coverage. This requires reading and understanding several parts of the ORM, but not a complete overhaul. The code patch touches some core abstractions but is relatively concise, though subtle enough to demand careful handling. Writing or updating tests is also required. This is solidly in the 1-4 hour range for an experienced Django engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No substantial additional concerns. The issue is a suitable candidate for a coding benchmark, as it is both technically interesting (touches core ORM abstractions, requires understanding SQL expressions and type conversions) and sufficiently well specified. The test additions are straightforward and domain-appropriate. The reproduction scenario is well described in the issue, and the solution is self-contained without requiring esoteric knowledge outside of the ORM and SQL compilation. If anything, it might be a bit harder than average due to subtlety of SQL generation, but this is not a problem for an advanced benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13568": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified but leaves some room for interpretation. The reporter explains that when defining a custom user model in Django, it's preferable in some situations to enforce uniqueness of USERNAME_FIELD (such as 'username') via a UniqueConstraint in model Meta constraints rather than unique=True on the CharField. They point out that the system check (auth.E003) currently only inspects the 'unique' property and should be extended to respect such explicit UniqueConstraints. While no specific lines/functions are referenced and the description is slightly informal, the core requirement is understandable: update the system check to also consider uniqueness via Meta constraints instead of just unique=True. There are minimal blanks to fill in, such as exact API usage or how partial unique constraints should be handled, but a sensible path forward is clear for an experienced Django developer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires reading and slightly modifying the Django system check logic (likely in django.contrib.auth.checks.check_user_model) to inspect for equivalent uniqueness enforcement via Meta.constraints, not just the field's unique=True. This change is small in scope (a few lines added), and testing would be straightforward with new model classes defining username as unique via Meta.constraints. For a developer familiar with Django, this should take between 15 min and 1 hour, mostly covering understanding the check logic, making and testing the change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blocking issues identified. The issue is a little informal and does not specify exactly how to handle edge cases like conditional unique constraints, but the corresponding PR and tests make it clear how these were meant to be handled. The core technical challenge and requirement for the coding benchmark setup are valid.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13807": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly states the problem (crash in 'loaddata' when table names are SQL keywords in SQLite due to lack of proper quoting), gives detailed repro steps (create a model called 'Order', create fixtures, use loaddata), pinpoints the root cause in the code (django/db/backends/sqlite3/base.py, check_constraints function, lines with improper string formatting for SQL statements), and lists affected versions. The engineer is given sufficient context to find the problem area and fix the quoting of table names in the relevant SQL queries, likely using Django's standard quoting mechanisms. It is clear what a successful solution would involve: making SQL statements robust against reserved keywords as table names by quoting them properly.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires identifying all locations in the specified function where table names are interpolated into SQL strings, and ensuring proper quoting is used. For an experienced engineer familiar with Django and the codebase, this involves updates in one function and minor changes to how table names are injected into SQL, probably using an existing quoting utility. This takes less than an hour: understanding the problem from the provided repro and root cause, verifying existing quoting utilities, and modifying the queries accordingly.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The task is clear, focused, and offers a concrete, directly-solvable problem suitable for benchmark evaluation. The test provided is also clear and directly tests the topic discussed in the issue, ensuring reproducibility and verification.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13810": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. The reporter provides a detailed summary of the symptoms observed (strange behavior with ASGI, django-debug-toolbar, and a custom middleware), and identifies a key spot in the Django codebase (django/core/handlers/base.py, around the handling of MiddlewareNotUsed and the handler variable) where the problem likely originates. They point out that the code incorrectly overwrites 'handler' even when the middleware is not used, leading to side effects in the middleware chain. Though the reporter describes how to provoke the issue and the effect (that async support is mishandled for middleware that is only meant for synchronous requests), the explanation still assumes the reader has substantial context about Django's middleware initialization and async/sync concepts. While the high-level bug is clear, the precise acceptance criteria and the intended fix are not spelled out in full detail. An experienced engineer can reasonably deduce what needs to change, but some blanks require filling in.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix requires a nuanced understanding of Django's middleware loading mechanics, the async/sync middleware adaptation logic, and the effects of exceptions like MiddlewareNotUsed. The correct solution involves not just a trivial edit but restructuring how the handler variable is assigned and propagated in the loop. It also involves subtle reasoning about how this impacts the middleware call chain, especially under async contexts. Additionally, the change requires familiarity with Django's testing and error-logging facilities to implement and verify the change. While not a massive code change, it goes beyond a simple patch and requires thoughtful analysis and robust testing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major concerns that would prevent this sample from being suitable for a coding benchmark. The issue is technical but represents a realistic and meaningful bug fix scenario within a mature, complex codebase. It is a challenging but fair test of an engineer's debugging skills and understanding of framework internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14017": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: It explains an inconsistency in operator behavior when combining Q and Exists objects using the & (and |) operators, provides a minimal reproducible example, clarifies expected behavior (commutativity), and suggests where the codebase might be missing the necessary logic (the __rand__ method). An experienced engineer would know where to look, how to reproduce, and what a successful fix should achieve (making the operations Q() & Exists(...) and Exists(...) & Q() both work).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue describes a Python operator overloading inconsistency, typical in Django's query framework. The fix requires understanding of Python dunder methods and possibly the Django ORM code for Q and Exists expressions. For an experienced engineer with some prior knowledge or after a brief review of how Django's Q and query expressions work, the change itself is limited, potentially just extending type checks or operator behavior. The patch supports this, showing a minor extension of an existing type check. The solution requires a bit of attention but is not deeply complex or multi-filed. Thus, it's a moderate, small code fix likely to take less than one hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The issue is clear, the solution scope is well constrained, and the requirements are precise. The sample is suitable for benchmarking coding ability in this context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14140": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides concrete code examples that demonstrate both the current faulty behavior and the desired behavior in terms of how Q() objects are deconstructed. It specifically describes that currently, single-child Q objects are treated differently and details the problematic consequence when the child is not subscriptable\u2014as when using Exists(). It names the function in question (deconstruct) and the relevant classes (Q). A solution direction is mentioned (removing the special case for single-child Q objects or checking for a length-2 tuple), although the final level of backward compatibility consideration is left to the implementer. This just barely leaves some blanks to fill (e.g., determining how much backward compatibility is sufficient), but it is mostly clear what is required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is localized to a specific function (Q.deconstruct). An experienced engineer can understand the cause (special handling for single-child Q children) and make the required adjustment in a relatively small amount of code, primarily adjusting how children are passed as args/kwargs, as shown in the gold patch. It requires understanding Django's query framework and some familiarity with the use of Q objects, but the scope is limited and does not require substantial restructuring or deep research. It is more than a trivial fix but can be completed in less than an hour, given the clarity of the issue and patch context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blocking issues for using this sample as a coding evaluation benchmark. The issue is clear, and the solution is sufficiently scoped. The test changes also validate the fix effectively. While direct references to all internal structures aren't given, Django engineers working with query expressions and deconstruction should have adequate context from the samples and explanation to address the problem appropriately without outside information. The slight ambiguity over backward compatibility is minor, as the description discusses possible approaches and the lack of reliance on the old behavior.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14238": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The text describes an explicit failure scenario: when DEFAULT_AUTO_FIELD is set to a custom subclass of BigAutoField (MyBigAutoField), Django does not treat it as expected due to a problem in AutoFieldMeta.__subclasscheck__. It makes clear what code is failing, provides a minimal example (with code), and specifically points at the internal logic that must be improved (by allowing subclasses in the _subclasses property). The fix direction is technically precise: make __subclasscheck__ work for subclasses, not only direct class objects. This is enough information for someone familiar with Django's ORM and the relevant metaclass machinery to target the correct code and design a patch.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch is small in scope, changing essentially a single line, and the necessary fix is clearly described in the issue. There is some thought required to understand how __subclasscheck__ works and what Python metaclass behaviors are expected, but it is unlikely to require more than an hour for a Django-experienced developer familiarizing themselves with this part of the codebase. Modifications were required in both core and tests, but the code changes themselves are straightforward.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant unresolved issues or blockers are present. The issue sample is well-scoped and suitable for use in assessing coding ability, as it focuses on correct use of Python metaclass mechanisms and adjusting subclass recognition logic.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14351": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a detailed account of the problem, compares the behavior in different Django versions, and links the observed bug in SQL generation to specific query constructions using Q objects. Examples of working and broken code are given, as well as the unexpected SQL (with a highlight where the problem arises). Though it references internal debugging output and SQL, the issue does not pin down exactly how the ORM should function differently, nor the precise code location to fix. However, there is enough actionable information for an experienced developer familiar with the Django ORM internals to begin investigating, reproduce, and reasonably interpret the root cause and required fix. There are some blanks to fill in (likely needing to study ORM alias handling specifics), making this a '1' by the provided rubric.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires a deep understanding of Django ORM's internals, particularly its query building and aliasing for joins and subqueries in the presence of Q objects and .in lookups. The engineer must reason from end user code to SQL and identify subtle changes in how get_default_columns operates across joined models and query construction, and then implement a robust (not just patch/fix) solution. This goes beyond a trivial or a quick fix, likely requiring 1-4 hours: familiarizing oneself with the relevant ORM/lookup internals, adding or modifying the necessary logic, and ensuring the change doesn't break anything else. The patch is relatively compact but conceptually challenging.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major other blockers are apparent. The issue text is technical and detailed, but interpretable for someone familiar with Django internals. The problem scope is appropriate for benchmarking engineer ability with ORM internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14493": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well specified: it clearly explains how to reproduce the bug (by subclassing ManifestStaticFilesStorage and setting max_post_process_passes=0), what triggers the issue (running collectstatic), and pinpoints the problematic cause (substitutions is not initialized if the processing loop is skipped). The issue also references the relevant code. However, there is a slight ambiguity in what the best solution would be: whether to initialize substitutions outside the loop or if there are deeper implications to setting max_post_process_passes=0. There is also an external reference for motivation, but the description is sufficient to understand and attempt a fix. Thus, it's a 1: some blanks are left, but a reasonable engineer can sensibly address it.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required is small: initializing 'substitutions' outside the loop to avoid the crash when max_post_process_passes=0. While the engineer must understand the flow and reason for the bug, it does not require substantial code rewriting, multiple file changes, or deep research. The time to understand and implement (for an engineer familiar with Django) would be less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blocking issues for using this as a coding benchmark sample. The only limitation is the slight reference to an external issue for motivation, but for the purposes of coding ability, the main technical problem and fix are self-contained in the issue text.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14580": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. The reporter provides both the relevant user code (the definitions in models.py), and a full sample of the incorrect output file (the generated migration file) that is missing the import statement for models. The expected versus actual behavior is clearly stated: Django should generate a migration file that is valid Python, but instead generates one that fails due to a missing import. The components at fault are also suggested (django.db.migrations.writer). No critical clarifications are needed\u2014the expected result (a correct migration file with appropriate import statements for all referenced classes) is obvious. Any competent engineer familiar with Django migrations and Python imports will know how to approach fixing this.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires understanding how Django serializes models for its migrations and why the import is missing. With the problem well-scoped (adding a missing import for models.Model when referenced), and a pointer to the relevant code area (django.db.migrations.writer), the task involves identifying where the import statements for model classes are figured, and ensuring that models.Model is properly imported in generated migration files. The actual code change is small (as seen in the Gold Patch), mainly adding an explicit record for models.Model to the list of imports. While this takes some codebase familiarity, for an experienced engineer this should fall comfortably within the 15 min - 1 hour range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The test and gold patches both show the problem and verify the fix, and the issue is a typical candidate for benchmarking code understanding and bug fixing. The issue text presents all the context necessary, and no problematic ambiguities or extraneous factors are present in the sample.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14672": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well specified. It clearly states the bug (missing call to make_hashable on through_fields in ManyToManyRel), provides the background (why hashing is required, what changed in Django 3.2, where make_hashable is already used), demonstrates the failure scenario in a minimal reproducible example, and specifies exactly what code change is needed (add missing make_hashable call on self.through_fields in ManyToManyRel). The required class and property names are referenced, and the purpose for the fix is understandable. While some deep familiarity with Django internals is assumed, the instructions are clear for an engineer with context. The diff in the PR matches the instructions in the issue text.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual code fix is quite minimal (a one-line change in the identity property of ManyToManyRel), and while it requires an understanding of the role of hashing/identity in Django ORM relations, it's essentially an oversight fix. The MR shows only a short change, and the major challenge is confirming where and that the missing call is required. Investigating and validating the issue based on the minimal repro and tracing identity/hashing logic may take a little time but is not a substantial rewrite; it's more than a simple typo but does not rise above an hour for someone familiar with Django's architecture. The bulk of the work is in understanding and validating, not coding.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The issue is self-contained, addresses a concrete and reproducible bug, and describes a clear fix. The only challenge is that it assumes a moderate familiarity with Django ORM internals, but this is reasonable for a coding benchmark related to Django.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a fairly clear account of the problem: when using method_decorator with a decorator, the wrapped function loses attributes like __name__ and __module__, likely due to how partial objects behave in Python. The example provided is not fully minimal or entirely explicit (some code such as logger definition is partial), but with familiarity with common decorator/wrapping patterns in Python, it is reasonably clear what's required: method_decorator should ensure that wrapper assignments such as __name__ and __module__ are preserved. However, it does require the reader to fill in some details about standard decorator problems and behaviors with partials and wraps. Overall, a sensible and experienced engineer would know what to fix and how to verify correct behavior, but there's a small gap in the exact requirements and context, so it is not perfectly specified.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires some understanding of how decorators, functools.wraps, and partial objects interact in Python, especially in the context of Django's method_decorator utility. The fix itself, as seen in the diff, involves ensuring that wraps is used to apply function metadata to the partial object. This would take a bit of thought and some exploration, but does not require significant refactoring or complex debugging; it's more a matter of knowing the utilities and applying them properly. For an experienced engineer, this likely falls in the 15 min - 1 hour range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are noted. The description focuses on a technical detail specific to function decorators and attributes that is both a common problem and well-known pattern in Python. Although the repro code could be improved, the intention and expectation are sufficiently clear for benchmarking purposes. The test added in the PR is a reasonable confirmation of the requirements.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15104": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The problem is described in detail: when a custom ForeignKey field in Django hardcodes its 'to' argument and then removes it in deconstruction, this causes a KeyError in Django's migration autodetector. The issue provides a minimal, self-contained reproduction, including the relevant custom field code, how the error manifests, and even points to an exact (though not line-number specified) code location where 'del deconstruction[2][\\\"to\\\"]' fails when the key is missing. The user also suggests a simple fix (using pop instead of del). There is sufficient information for an engineer to understand both the bug and the expected resolution, without ambiguity. All relevant classes, methods, and context are included, and the test case reproduces the problem precisely.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a fairly targeted fix that requires understanding the interaction between custom model fields and the Django migration autodetector. The issue provides a reproducible example and code pointer; an experienced engineer can verify the traceback, confirm the cause (missing 'to' key in deconstructed field kwargs), and implement the defensive coding change. Minor investigation is needed to confirm side effects or implications, but the code magnitude is small and the solution straightforward (changing 'del' to 'pop' in a single place). Familiarity with Django's migration internals is helpful but not strictly necessary given the provided test case.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues. The test provided is self-contained and validates the fix. The sample is representative of a real-world bug and fix scenario in a widely used Python project. There are no apparent blockers for using this issue in a coding ability benchmark, as the reproduction, context, and solution requirements are clear.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is well-specified in the sense that a Django-savvy engineer can deduce what is going wrong and the problem is clearly reproducible. The issue gives concrete code for models, shows how to reproduce the bug, and dives into the potential root cause in Query.combine and related methods, referencing specific method and variable names (e.g., Query.change_aliases, alias_map, change_map, etc). It further offers a suggestion about how to solve the issue, e.g., making sure rhs aliases don't conflict with lhs aliases, which is a sensible direction. However, there are still some blanks: the assertion explained in the issue isn't fully documented in Django, and finer implementation choices are left open to the developer. Overall, though, this is the kind of bug where an experienced Django ORM/codebase contributor can produce a meaningful, self-contained attempt at a fix without more information. So, while not absolutely, pedantically specified (e.g., with explicit edge cases for all possible alias collisions), it's certainly clear enough to act upon sensibly.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Understanding and fixing this bug requires familiarity with the internals of Django's ORM query construction, especially regarding aliasing and the mechanics of QuerySet combination with | (OR) operations. One needs to understand how table aliases are managed, why conflicts arise, and how to modify the query-building machinery to avoid collisions. It isn't just a patch of a single line or obvious error, but requires comprehension of subtle state changes and test updates. The change touches core parts of the ORM; making mistakes could lead to subtle regressions. The patch also involves adding new test models and writing a new robust regression test. For someone already familiar with Django's models and query system, this would still take an hour or more to work through and test. It may be approachable within a few hours, but it's definitely not trivial or a pure mechanical fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is self-contained, and the main caveat is that it requires domain expertise and a thorough understanding of Django's query planning, but that's acceptable for a benchmark focused on advanced coding skill. The reproducibility and provided test case make it suitable. The problem scope is complex, but still decisively scoped enough to leverage for evaluating deeper coding and debugging ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15277": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: It describes a concrete performance and correctness concern where CharField's __init__ always appends a MaxLengthValidator, even when max_length is None (not meaningful). The issue gives relevant context, reproduces the diagnostic via code snippets, justifies why it's undesirable, and suggests the required fix: gate the validator append on max_length not being None (as BinaryField does). The affected class and method are precisely named, and the suggested change is explicitly illustrated. An engineer with knowledge of Django can implement the fix as requested using just this information, without ambiguity about intent or scope.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively simple change (conditional validator addition), though one must understand the field/validator instantiation logic. It's a focused, <5-line fix inside a single method. Familiarity with Django's internals would make this straightforward, and there is little need for hunting through the codebase. The main thought is reasoning through why and where to add the conditional. The issue is much more about making the code more robust and a micro-optimization than about complex design or large code changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No substantial issues remain. The issue is clear, the test demonstrates correct handling for various types, and the requested code change is well-scoped. Using this as a benchmark is appropriate: it tests ability to understand code, follow reasoning about Python class initialization, and make a targeted change based on described precedent.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15280": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. The user provides the model structure, the exact test case that demonstrates the bug, the current queries executed, the observed incorrect behavior (the deferred field is still marked as deferred after prefetching, leading to extra queries), as well as what the expected behavior is (no extra query when accessing the field because it should have been prefetched). The core problem is described at an appropriate technical depth, and there are strong clues on how to reproduce and trigger the bug. Any reasonably experienced Django engineer could work from this information, and the expected testable outcome is clear. While implementation details would need exploration in the codebase, the high-level 'what' and the manifestation of the bug are fully clear from the description.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue involves understanding the internals of Django's queryset prefetching, how deferred fields work, and how instances map cached or prefetched relations\u2014these are nontrivial aspects of Django's ORM. The actual code patch is small, but identifying where to make the change and ensuring correctness (given possible edge cases) requires significant codebase familiarity and conceptual understanding. It is likely to take 1-4 hours for an experienced Django engineer to fix, including debugging, testing, and possibly writing new tests to cover the scenario.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major blockers or issues detected. The issue is clear, reproducible, and has an objective success criterion using a test case. The setup is well-aligned with the benchmark's requirements for evaluating coding ability in a real-world setting, especially for frameworks like Django.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15315": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified: it identifies a specific regression or side-effect in Django's Field.__hash__ implementation following a referenced commit (#31750). The problem is demonstrated by a code sample showing how a Field object's hash changes after assignment to a model class, breaking its use in dictionaries. The issue also gives strong guidance about the cause (the hash including attributes that change on assignment) and a suggestion for the fix (to revert the hash change from #31750, making the hash dependent only on the creation_counter, which is immutable). However, some familiarity with Django's internals (the semantics of model assignment and hashing behavior) is assumed, and there are minor blanks (e.g., whether this change reopens any issues addressed by the reverted PR), but these are relatively minor. An experienced engineer should have little trouble understanding what is required for a successful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding the hashing protocol in Python, Django model field internals, and why mutating an object's hash is problematic. The actual code fix is simple (changing the hash function to rely only on an immutable attribute), and the impact and mechanism are explained in the issue. Some care must be taken to ensure no regressions and to provide suitable tests. For an experienced engineer familiar with Django, this should take between 15 minutes and 1 hour, mostly spent on validation and ensuring no contract is violated by reverting the hash behavior.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present. The sample assumes knowledge of Django's internals but is otherwise a solid coding ability test: the problem is real, the requirements are clear, and the patch/test provide verifiable evidence of a correct solution. No additional blockers identified.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15375": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete bug report, including version info, a clear reproduction using sample code in the Django shell, before-and-after behaviors with outputs, and a workaround. However, it does not specify exactly where the problem lies in the code or what the underlying cause is, nor what implementation is required to fix it. It is clear that the desired outcome is for aggregate() with a 'default' argument to work after annotate() just as it does without, but some knowledge of Django ORM internals or willingness to explore the codebase would be required to resolve the ambiguity around why default=0 causes a problem and what 'breaks' means in practice (e.g., exception details are omitted). Nevertheless, the sample is clear enough to attempt a solution and verify correctness via the expected API usage.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue involves understanding Django's ORM aggregation and annotation internals, specifically how the default argument interacts with aggregate functions post-annotation. The patch requires changing somewhat central ORM logic, understanding and extending how defaults are handled when aggregating over annotated querysets. This necessitates reading and tracing Django's aggregate/queryset code, not just a trivial change. However, it's not a massive architectural overhaul, so it's unlikely to take more than a few hours for a developer already familiarizing themselves with Django's code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The problem is meaningful, self-contained, and relevant for testing advanced Python and ORM codebase familiarity. The test patches added appear sufficient for checking the bug fix. Note: detailed stack trace is omitted, but workaround and code to reproduce clarify the ask.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15380": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a reasonable overview of the problem: the migration autodetector crashes when renaming both a model and a field in one step. It describes the action that triggers the issue (renaming model and field, then running 'makemigrations') and includes the interactive prompt and a suggestion that this is a regression introduced in a specific commit. However, it lacks specifics such as the error message, stack trace, affected lines/files, or detailed reproduction steps (e.g., minimal models or migration instructions). A developer could reasonably interpret what's needed: fix the crash when renaming a model and a field together. While some details are missing, it provides enough context to attempt a meaningful fix. Thus, it deserves a '1' on well-specified scale.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code fix is localized to a small section of one function, changing the way the 'old_model_name' and 'model_name' are used to index the to_state models mapping. The required understanding is moderate: the developer must understand how renames are tracked, and how to use the correct mapping when both models and fields are renamed. No substantial rewriting or multi-file edits appear necessary; it's likely achievable within an hour given the description and moderate codebase familiarity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue is suitable for the setup, as the bug is reasonably scoped, the description implies a specific desired outcome, and the accompanying tests (in the gold PR) confirm the required behavior. While the original issue text could benefit from more detail, for benchmarking coding ability, it still meets the criteria as a moderately specified issue with a clearly testable bug.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15503": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes a concrete bug: using JSONField lookups like has_key, has_keys, or has_any_keys with numeric string keys works on some databases (PostgreSQL) but fails on others (SQLite, MySQL, Oracle). Versions of Django, Python, and SQLite are provided. A reproducible test case is given, including code for the model and a test class that clearly demonstrates the problem. The desired outcome is also clear: the count for a numeric string key should return 1, like for the non-numeric string key. No ambiguity remains about what needs to be done to resolve the issue. An experienced engineer would know from this text what behavior is required, where to investigate, and what a successful solution must achieve.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding how JSONField lookups are implemented in Django across various database backends and why differing behaviors arise when dealing with numeric string keys. The solution involves substantial non-trivial work: editing the Django ORM code to detect and properly handle string keys that look like numbers, ensuring correct SQL path generation for different databases, and adding or modifying associated test cases. The solution involves changes in core logic and touches multiple locations, with careful attention to cross-database compatibility and correctness. This can easily take 1-4 hours given the need to analyze and work with complex code and ensure compatibility.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major reasons to reject this issue for use in the benchmark. The issue is well-specified, there is a clear expected outcome, and the specifics regarding reproducing and testing the issue are well covered. The problem is neither trivial nor overly esoteric, making it a suitable and valuable benchmark for testing a developer's coding ability and reasoning within a mature codebase like Django.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15525": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description contains a minimal but concrete Django project setup that models the problem: models with natural keys and foreign keys, an example fixture, and a command showing how loaddata fails for non-default databases. The classes, methods, and the failing scenario are clearly outlined. While the reporter expresses some confusion and does not give details about the exception stack trace or the exact nature of the bug in the underlying Django code, it is possible to reasonably deduce the likely issue (natural keys + foreign keys handling is inconsistent across databases in loaddata). An engineer familiar with Django serialization and multi-database setup should have a logical path to reproduce and address the problem, although some time would be spent confirming assumptions. Therefore, while there is some information missing (e.g., exact error message), there is enough context for a sensible, focused solution attempt.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires understanding Django's serialization and deserialization processes, handling of natural keys with foreign key dependencies, and the behavior of loaddata across databases. The required fix involves tracing how natural key resolution differs between databases, identifying where the bug is triggered, and updating the relevant code to support multiple databases. This is moderately challenging: it is more than a quick patch, likely requiring debugging through Django internals, confirming the fix for more than one database, and adding relevant tests. It involves making a targeted change to the core serialization logic and verification with new test coverage, which would likely take 1-4 hours for a familiar engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The only minor limitation is that the reporter provides a link to an external GitHub project, but enough sample code and data is provided inline for the problem to be understood and reproduced directly. The issue covers the critical background needed for the benchmark. The sample is therefore suitable for use.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15695": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified for an experienced Django developer. It gives a concrete description of the scenario (RenameIndex() crashes when unnamed index is moved back and forth); explicitly states the expected behavior (should restore the old auto-generated name when rolling back, and not crash if re-applied); and provides a focused code snippet showing the testing context and what is failing. There are some underlying assumptions (the engineer must understand how auto-generated index names work, and why a RenameIndex may be a no-op if the names match), but the necessary objects (operation, schema_editor, index naming) and the problematic scenario are clear enough to implement a meaningful solution. Some blanks are left (exact root cause of crash isn\u2019t spelled out step-by-step), but a sensible interpretation is possible with the given detail.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small logic change in the RenameIndex operation implementation to avoid a crash when the index names are unchanged. The solution only adds a short conditional check and does not require large code restructuring. Understanding the specific reason for the crash and how Django's migration operations treat index names requires some familiarity with the codebase, but for a developer familiar with Django migrations and index operations, this is achievable in 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers. The issue is a good sample for evaluation: it requires interpreting migration logic and writing a defensive code fix, with a clear target condition and a test-ready scenario. Minor gaps could arise if the engineer is unfamiliar with unnamed/auto-generated index naming, but in the test context and with a codebase to reference this is reasonable.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15732": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue describes a specific bug in Django's migration handling of unique_together constraints for single fields that already have their own unique or primary key constraints. The user provides clear information about what the unintended state is (duplicate constraints on a single column: one from unique_together, one from the PK/unique attribute), what fails (attempt to drop unique_together constraint fails because of multiple matching constraints), and which backend is involved (PostgreSQL, but may apply more generally). While some internal Django familiarity is assumed, the report is reasonably clear and actionable for an engineer familiar with Django's schema editor and constraint handling. There is a sensible, concrete interpretation for a successful solution, though a little bit of inference as to the expected system behavior is required.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix requires understanding Django's migration framework, the way indexes and constraints are created/removed, and how schema editors interact with database backends. The solution involved changing logic to more precisely identify and remove the appropriate constraint rather than assuming only one constraint exists on the field(s). This task requires tracing through the relevant code paths, modifying core schema-editing logic, adding a helper, and updating (or adding) tests for regression. Considering the need to understand constraints, migrations internals, and avoid breaking existing behaviors, it's a non-trivial engineering task\u2014likely requiring deep familiarity with Django internals and schema representation, and would probably take 1-4 hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blocking issues are apparent. The issue is well-focused on a concrete bug, the required areas of code are touched in the patch, and good test coverage is provided. It is a suitable benchmark for evaluating skill in understanding and fixing database schema migration logic in Django.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15814": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides enough contextual information to make a well-informed attempt at a solution: it describes the crash encountered when using select_related() and only() together on proxy models, gives OS, Python, and Django versions, and provides code snippets for the involved models and the command that triggers the error. It goes so far as to highlight the precise snippet in query.py that leads to the failure, and proposes a one-line code change that resolves the issue. However, it does not explain why the fix is correct, nor exactly what kind of error is raised, nor does it detail all edge cases or discuss the broader implications of the change. There remains some room for interpretation and diligence for a contributor to ensure completeness and compatibility, but overall, it is clear what is needed to address the described defect.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution involves a small but non-trivial change to Django's query construction code: modifying a single line to ensure proxy models are handled correctly when combining select_related and only. The issue is straightforward once understood, given that the problem localization and potential fix are clearly stated. Familiarity with Django ORM internals is helpful but, for an experienced engineer, the debugging and fix could plausibly fit within 15-60 minutes, including identifying the problem and proposing/testing the fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional reasons to disqualify this sample. The supplied information is sufficient for a competent engineer to reproduce, understand, and resolve the issue without needing further context. Tests are straightforward to construct based on the example, and the scope is well-bounded by the description.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15930": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite specific and focused. It provides a code snippet that causes a crash, specifies the Django ORM method (annotate with Case/When and ~Q(pk__in=[])), and clearly explains both the current behavior (crash/syntax error) and the expected behavior (all rows should annotate with True, since all should match the condition). The issue also gives motivation/context about the use of this pattern as a sentinel in application code, which aids understanding. An experienced engineer familiar with Django's ORM expressions can identify where the behavior needs to be extended or fixed in the codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While this issue does require some understanding of Django's expression compilation pipeline and the handling of Q objects in SQL generation, the actual change is local and concise. The solution involves checking for a specific edge case (when the compiled condition SQL is the empty string) and using a fallback that always evaluates to True. Adjusting the code to accommodate this case and adding a relevant test would likely take a focused engineer between 15 minutes and 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues present. The problem is well-scoped, the fix is isolated, and test coverage can be achieved based on the description.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15957": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a concrete problem: Prefetch objects do not work with sliced querysets in Django. The user specifies a sample code pattern (using Prefetch, queryset=Post.objects.all()[:3], ...) and describes the desired functionality: prefetching only a limited number of related objects per parent, not all of them. However, the issue does not describe the precise expected API or all the behavioral nuances, nor does it reference which internals are affected, but it does communicate the key need and common use-case. An experienced developer could understand what is meant and make a sensible attempt at a solution, but would need to fill in some gaps with assumptions (e.g., slicing semantics, how to structure the implementation).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This requires a substantial change to the data prefetching logic in Django, touching core ORM internals. It likely involves understanding how queryset slicing is represented and how prefetching works over related fields, possibly touching multiple layers (prefetch logic, query compilation, etc). The solution demands a clear understanding of the ORM's internals, window functions, and advanced query expressions. It is not just a trivial patch, but also not a multi-day rewrite. Estimated effort: 1-4 hours for a developer familiar with Django internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are evident. The problem is specific, and the test patch demonstrates that it is possible to write deterministic tests for this change. Although some domain knowledge of Django ORM is assumed, the described use-case and code sample anchor the expectations. \\n\\nIf allowed to make reasonable assumptions about slicing semantics and that the change should affect all types of relations supported by prefetch, an experienced contributor should be able to complete the task. \",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified: it describes a concrete problem involving Django's migration system when a ManyToManyField uses a 'through' model defined in a separate app. It includes code snippets for the models as structured across multiple apps and summarizes the exact scenario that triggers the migration issue. The author provides information about how the placement of the intermediary (through) model affects migration behavior, and gives enough context to infer the expected behavior. However, it stops short of outlining precisely what the correct migration behavior should be or what error is encountered (no error message or traceback is given). Thus, while there is some room for interpretation on 'how' the fix should behave, the problem is concrete and there is a reasonable path toward a solution. It fits best at level 1: some blanks to fill but likely solvable by an experienced developer.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires an in-depth understanding of Django's migration system and its handling of ManyToMany relationships with 'through' models across different apps. The fix requires inspecting and understanding how model dependencies are resolved during the migration process, which can involve reviewing several parts of Django's migration framework. The engineer may need to investigate and possibly test different model configurations. Thus, it's more involved than a quick 15-minute fix and likely needs coordination across multiple files/class definitions and migration logic. However, it should not require more than 4 hours for an experienced contributor, as it involves a targeted change in the autodetector logic and associated tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues that would disqualify this sample. The only potential concern is the lack of explicit error output in the issue description, but the contextual clues and model definitions provide a cohesive description of the problem. The sample is suitable for benchmark use, particularly for moderately advanced Python/Django engineers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16032": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a clear test case to reproduce the problem, specifying the Python code to add to test for the bug. It describes the operations that cause the incorrect behavior (using .alias after .annotate on a QuerySet and then filtering with __in on a related field). However, the underlying explanation of what exactly should happen in the ORM is not deeply described\u2014one must interpret from the test what the intended and actual behaviors are. An experienced Django engineer can sensibly infer that the issue is about select fields not being cleared properly when alias is used after annotate, based on the test and the sample models (Book and Publisher). Thus, while the issue is not as exhaustively specified as possible, it is sufficiently specified with a direct test and a reasonable expectation for correct output. I would rate this as a 1: there are some small blanks, but a sensible and concrete interpretation is possible.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding the Django ORM's internals, specifically how QuerySet annotations and aliases interact and how select fields are managed through subqueries and lookups. The fix involves careful changes in multiple files (`related_lookups.py`, `query.py`) governing advanced ORM query generation logic. An experienced engineer would need time to reproduce, trace the bug through the call stack, and ensure the correct changes are made without breaking other functionality. Testing and validation with edge cases are essential due to the subtlety of the bug. Thus, this is more than a 'small fix' and involves substantial investigation and some non-trivial changes, likely taking 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent\u2014the sample is appropriate for evaluating advanced Python coding and debugging skills, specifically those related to ORMs and Django internals. The test case is clear, and both test and production code are sufficiently accessible.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16136": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a minimal reproducible example, including code snippets for the Django view, URL configuration, and information about the environment (Django 4.1.1, Python 3.10.6). It clearly states the erroneous behavior: accessing an async-only view via an unsupported method (GET) causes an exception due to 'object HttpResponseNotAllowed can't be used in await expression'. The steps to reproduce are explicit, and the expected behavior is implied (i.e., GET requests to a POST-only async view should return the standard HTTP 405 response correctly, without causing an exception). A knowledgeable engineer would understand what needs to be fixed and could attempt a solution with the given information, without any need for clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue describes a mismatch in async/sync handling in Django's class-based views. Diagnosing and resolving the cause (i.e., ensuring the correct handling of 'HttpResponseNotAllowed' according to whether the view is async or not) requires some knowledge of Django's view dispatch method but is limited in scope (affecting one logical area of code). After understanding the root cause, the actual code change is relatively straightforward, so it fits into the '15 min - 1 hour' category for an experienced Django engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues. The description and the bug are well-defined and scoped. The test and code changes are direct, involve minimal alteration, and clarify exactly what's required. The sample fits well with the intended benchmark setup.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16429": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly identifies the problem: calling timesince() with a datetime at least one month in the past and USE_TZ=True causes a TypeError. A concrete code sample and test case are provided (test_long_interval_with_tz) that reliably reproduces the error. The suspected cause is also identified, namely that pivot instantiation in timesince.py does not transfer tzinfo from the passed datetime. A specific remedy is even suggested: passing tzinfo=d.tzinfo to the datetime.datetime call. There is enough context (filenames, methods, settings) for an engineer to understand and address the issue without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue only requires a minor change: adjusting a specific instantiation of a datetime object to preserve the tzinfo attribute. The code location is clearly identified, and reproducing/resolving the bug can be done with only a bit of investigation to confirm and implement the fix. While a little thought is needed to be sure about datetime/tz behavior, the fix itself is simple. Updating and/or adding appropriate tests is also straightforward, especially since the test case is already given in the issue description.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present; the sample is suitable for the benchmark. The issue has actionable steps, clear context, a reproducible test, and a narrow, well-scoped change. No external dependencies or vague requirements.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16454": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and provides a clear problem description: when using Django's management commands with subparsers, the specialized error formatting provided by CommandParser is not propagated to subparsers. It demonstrates the difference in CLI error messages when required arguments are missing in subparsers. The description also describes the desired behavior: subparsers created under CommandParser should retain the improved error formatting so user errors do not produce stack traces but instead produce human-friendly usage errors. The problem is concrete, references the specific class and method involved (CommandParser and add_subparsers), and explains the context. An engineer can clearly identify what must be changed, and a correct solution and regression test can be created from this information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change requires making sure that CommandParser's enhancements are propagated to subparsers. This involves understanding how Django wraps argparse.ArgumentParser and how subparsers are constructed; the code change itself is small (as in the patch). However, it does require the engineer to trace how parser_class is passed to subparsers and to ensure CommandParser's arguments are copied over. This adds some complexity but not an excessive amount. It is more involved than a trivial 1-line fix, but should be solvable in under an hour for someone familiar with the structure and Python's argparse module.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues with this sample. The problem is clear, self-contained, and suitable for evaluation. The issue does not require external resources or ambiguous design decisions. The regression test is straightforward to create given the issue description and behavioral expectations.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16485": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear and concrete. It identifies the function under discussion: floatformat from django.template.defaultfilters. It highlights the specific problem: calling floatformat with '0.00' or Decimal('0.00') and 0 as arguments causes a crash, which is demonstrated with minimal reproducible code. Even though it does not provide the exact stack trace, the description suffices for an experienced engineer to reproduce and pinpoint the issue. The intent is clear: floatformat should not crash on these inputs. No further clarification is required to attempt a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Based on the patch and issue, this is a small change in logic (adjusting a conditional check). The engineer would need to familiarize themselves with how floatformat parses and handles its arguments and error cases, but the fix involves changing a single conditional. The bulk of the work is identifying exactly where the logic fails for '0.00' input, but the scope is local to a single function and straightforward for someone familiar with the codebase. This should take between 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The issue can be reproduced, the required fix is clear from the description, and the patch scope is confined. The problem is realistic and represents a bug fix with a test case.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16569": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is quite clear and well-specified. It describes in detail the conditions under which the bug occurs (when calling FormSet.empty_form() with can_delete=True and can_delete_extra=False, and index=None). It pinpoints the problematic line in the codebase (line 493 of django/forms/formsets) and provides a rationale for why the current conditional logic fails when index is None. It even proposes a fix (amending the condition to check 'index is not None') and includes a minimal self-contained example to reproduce the issue, which is sufficient for independently verifying the problem and confirming a solution. There is enough context for a developer familiar with Django to locate the issue and understand both the bug and the intended outcome.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix is a small, targeted code change that involves correcting a logical condition in an existing function. While some understanding of Django forms and formsets is necessary, the problem is isolated and well-explained, and a code location and example fix are provided. For an engineer familiar with Python and the Django codebase, this change would likely take 15-60 minutes, accounting for localizing the bug, applying the fix, and verifying it with tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a suitable and self-contained benchmark sample that tests reading comprehension, bug localization, and implementing a precise logical fix. No significant blockers or ambiguities are present in the issue description.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16667": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear, concrete, and actionable. It describes a reproducible server crash when using forms with the SelectDateWidget in Django, specifies the example code setup (form, view, and URL configuration), and provides the exact HTTP GET request that causes the crash. It further pinpoints precisely where and how the issue happens: user-controlled input is cast to int and used for datetime.date construction without guarding against OverflowError. The problematic line in the SelectDateWidget value_from_datadict method is identified. An experienced engineer can understand what is wrong (no guard against invalid date input causing OverflowError), trace where it happens, and infer that a successful solution would involve guarding that code against such exceptions. The desired outcome (prevent crash on malformed input) is also evident. No significant ambiguity remains.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively common sort of bug: unsanitized user input causes an unhandled exception. The core fix is to add a try/except block around the int conversions or the date construction to catch OverflowError (possibly ValueError as well), returning a suitable invalid value or error. The scope is limited to a single method in a single class; while some thought is needed to match the existing code/error-handling style and to ensure coverage for different invalid input scenarios, this should not take more than an hour for an engineer familiar with Django. It does not require changes across multiple files or a substantial rewrite. Testing the fix is straightforward thanks to the provided reproducible steps.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues present. The issue is both clearly specified and well-scoped for a coding ability benchmark. Test modifications would be expected and are easily formulated from the issue description. No external dependencies or project-specific ambiguities.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16950": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a well-defined, minimal reproducible example, with detailed reproduction steps and an explicit account of the bug (UUIDField 'id' on Thing being set to null when saving an inline SubThing). It precisely describes the expected versus actual behavior and gives enough context about Django versions, Python versions, model code, and admin code. However, there is some room for interpretation regarding lower-level expected behavior (e.g., exactly how Django should treat UUIDFields when an inline form is submitted), and it requires some familiarity with Django's model and admin internals. For a non-Django expert, there may be a small learning curve, but the description is suitable for an experienced engineer with Django knowledge. Thus, there are a few blanks to fill, but overall it's decently specified.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Resolving this issue requires an in-depth understanding of Django's form handling, particularly how inline admin forms interact with models that use alternate primary keys or UUIDFields with defaults. The problem likely involves navigating Django's internal logic for model formsets and foreign keys, which may not be immediately obvious. The fix, as seen in the patch, is subtle and affects how defaults are only nullified under certain conditions. Identifying exactly where this happens, preventing regression, and updating/adding appropriate tests can be expected to take in the range of 1-4 hours for an experienced Django developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified. The issue is reproducible, clear, has enough technical context, and is well-suited to a benchmark for developers with experience in Django. Some familiarity with Django form internals is assumed, but this is reasonable for professional-level coding benchmarks.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-20859": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. The user describes exactly what feature is not working (adding a legend to a SubFigure in matplotlib raises an error). They provide a minimal code example that reproduces the issue, describe the expected outcome (should be able to add a legend to a SubFigure), describe the actual outcome (an error is raised), and even pinpoint the relevant lines of code and suggest a concrete fix (changing an isinstance check from Figure to FigureBase in legend.py). Environment/version info is also included. No ambiguity remains about what needs to be fixed or how to test success.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires knowledge of the matplotlib codebase and understanding inheritance/typing within it, but the issue is laser-focused and comes with a reproduction, expected/actual output, and even a suggestion for the fix. The actual code change is quite small (simple type/subclass check modification), but working out any subtle consequences may take a bit of time. It is not a totally trivial (<15 min) change, but should be solvable within an hour by someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is an ideal benchmark issue: it is clear, testable, and focused in scope. The detail, including the minimal reproducible example and a candidate code fix, make it particularly instructive. The test patch is also clear and focused. No major shortcomings for use as a benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-22719": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a specific example that causes a warning, explains the expected vs. actual behavior, and gives considerable technical background about the deprecation and API notes. However, the issue does not specify exactly how the warning should be handled (e.g., should there be no warning, or a more targeted warning message), leaving some decisions up to the implementer. The explanation does point towards catching the empty-data edge case specifically and suggests potential places in the codebase (category units, unit converters). Therefore, while there are some blanks to fill, it is specific enough for an experienced engineer to attempt a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix, as shown in the patch, requires a small change\u2014adding a check on the size of the data before emitting a deprecation warning. Understanding the problem might take a short time for someone familiar with matplotlib's units/converters system, but the actual code change is modest and highly localized. Writing the appropriate test is also straightforward. This level of effort fits the 15 min - 1 hour range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues are present. The problem, while domain-specific, is suitable for benchmarking typical open source maintenance or bug-fixing tasks. The behavioral ambiguity is minimal and consistent with reasonable benchmark samples.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23299": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly lays out the bug: calling matplotlib.get_backend() can remove figures from Gcf.figs if the first figure was created in an rc_context, and this should not happen. There is a clear code sample for reproduction, which demonstrates the current (bad) and expected (good) outcome. The context provided around why this matters (e.g., plt.close(fig2) doesn't work after this bug is triggered) explains the consequences. It also provides detailed version and environment info. The relevant classes and functions are named (matplotlib.get_backend, rc_context, plt, Gcf), and the expectation is that figures should persist as normal through get_backend calls, even if they were created in an rc_context.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This bug requires a focused but non-trivial fix. The core of the work is understanding why figures created within an rc_context are lost from Gcf.figs after get_backend is called, identifying that rc_context resets too much state (specifically 'backend'), and changing it so that the 'backend' is not reset. The engineer needs reasonable understanding of how matplotlib handles rcParams, context managers, figure management, and backends, but does not require heavy refactoring, new feature design, or editing files outside of rcparams and testing. This would take a bit of digging and reasoning but likely under 1 hour for someone already oriented in matplotlib.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The problem and expected solution are clear, and the test setup shown in the PR is straightforward and well aligned with the issue scenario. The dataset sample is suitable for coding ability benchmarking: recreating the bug, reasoning about rc_context's interaction with backend/rcParams, and patching to not reset backend is an appropriate task at this difficulty.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23476": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it contains a concise summary of the problem (DPI doubling on unpickling figures on M1 Macs), a complete minimal code example to reproduce the bug, actual vs. expected output, and system configuration details. The engineer knows exactly what the issue is, how to trigger it, what the correct behavior should be, and the environment to consider. There\u2019s no ambiguity regarding what a successful fix should accomplish\u2014the DPI should remain constant through repeated pickling/unpickling. The context and failure mode are explicit.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The reported bug involves DPI doubling on unpickling Matplotlib figures on M1 Macs. Given that there is a minimal reproducible example and that the code change in the patch is small (resetting the stored DPI on serialization), an experienced engineer familiar with Matplotlib's structure would likely only need 15-60 minutes to analyze the relevant code and craft a targeted fix. While some understanding of device pixel ratios and pickling in Matplotlib is needed, it doesn't require fundamental rewrites or deep research.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is highly suitable for a coding benchmark: it's a well-known codebase, the scenario is easily reproducible, the problem is clear, and the fix is challenging enough to test understanding of serialization, object state, and platform-specific issues. No significant caveats.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24026": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well specified. It clearly states the problem: calling stackplot with named color aliases like 'C2', 'C3', 'C4' raises a ValueError, whereas other plotting methods support this usage. The use case of wanting to synchronize colors across multiple plot types is explained. A minimal, reproducible code snippet is provided, showing exactly what triggers the problem. However, the desired behavior is only implied (stackplot should accept 'CN' color aliases just like ax.plot and Rectangle), and there are minor blanks to fill: e.g., whether the change should only avoid the error or also make use of the cycler in a specific way. Still, there is an obvious, sensible interpretation of what resolving the issue entails. No references to specific functions/classes are given, but the minimal example and context are sufficient for an experienced engineer familiar with matplotlib.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change requires understanding of how matplotlib's stackplot interprets the colors argument, and its relationship to the property cycler and color alias functionality. The core of the solution is to ensure stackplot can accept colors like 'C2' and not inadvertently set the axes color cycler, but rather use the provided color list directly. Implementing this logic and ensuring proper cycling is a small but non-trivial patch. Understanding the location to make the change is straightforward with knowledge of matplotlib's codebase. The main challenge is recognizing the property cycler side effect and making the change without introducing regressions.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues unaddressed. The core requirement is clear, reproducible, and appropriate for an engineer with access to the codebase and tests. The test patch demonstrates the fix can be validated. There is no ambiguity about what constitutes a correct solution: support 'CN' aliases in stackplot and preserve expected color cycling behavior. Some familiarity with matplotlib coding conventions would be helpful, but not strictly required.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24149": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well-specified. It gives an explicit description of the bug, the version where the issue is introduced, and clear code to reproduce the problem. The expected vs. actual behaviors are clearly articulated: previously, calling ax.bar([np.nan], [np.nan]) would create a BarCollection, but now it raises an exception. The user provides further test cases that highlight nuance (ax.bar([np.nan],[0]) fails, but ax.bar([0],[np.nan]) works), clearly indicating the problematic input space. These details allow an engineer to understand what is going wrong, what the legacy behavior was, and what behavior should be restored. There are no ambiguous requirements or missing pieces.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Given the explicit reproduction steps and clear explanation, the main challenge is to trace the internal logic in matplotlib's bar plotting code (likely lib/matplotlib/axes/_axes.py) and to identify where NaN handling has changed between versions. While inspecting changes and verifying the conditions requires some investigation, the fix (as shown by the patch) only requires a small code adjustment that handles the edge case when all data is NaN. For an experienced Matplotlib contributor, this would likely take between 15 min and 1 hour, including test writing and validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The sample is well-structured both as a description and in terms of the engineering challenge. It clearly targets a defined and reproducible bug, and the expected outcome is unambiguous. This makes it a strong example for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24970": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it reports a bug (NumPy 1.24 deprecation warnings) triggered in a specific context (a call to plt.get_cmap()(np.empty((0,), dtype=np.uint8))), provides a code snippet for reproduction, and states the expected outcome (no warnings). While it does not specify which warnings exactly occur, nor does it point to a specific part of matplotlib's source code, the reproduction is minimal and clear. An experienced engineer could reproduce the problem and track down the deprecation warning to the offending use of NumPy APIs in the code, and then seek to resolve it in line with the expectation (i.e., modifying code so that usage no longer triggers the warnings). The lack of explicit detail on what the warning is (e.g., full warning trace or message) leaves a small gap, but overall the report is actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The main challenge is reproducing the warning, identifying where in the codebase the warning originates, and then updating the relevant code to avoid the deprecated usage. The code changes (as in the patch) are quite focused and localized and do not require rewriting large portions or deeply understanding application-wide logic. The expected time is 15-60 minutes, as it involves tracking a warning, updating types and error handling, and testing the fix, potentially across different numeric types as in the test addition. The requirement to understand some NumPy deprecation mechanisms might require a quick reference to documentation, but not in-depth research.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers are evident. The test patch demonstrates a straightforward test for the fixed behavior. The only minor deficit is the issue description does not copy-paste the precise warning message, but the reproducible snippet and expected outcome are adequate.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25311": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well specified. It clearly describes the problem: attempting to pickle a matplotlib figure with a draggable legend (or draggable annotations) fails, whereas removing the draggable line makes pickling succeed. A complete minimal code example is provided showing how to reproduce the issue. The expected outcome is explicitly stated (\\\"Pickling successful\\\"). Environmental information is detailed (OS, matplotlib version, Python version). There are no ambiguities or missing requirements: the successful solution is simply that pickling such a figure must work without errors, as demonstrated by the sample. The requirements for a \\\"correct\\\" solution are unambiguous and testable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is relatively contained: pickling a matplotlib figure with a draggable legend fails, and the solution must ensure that this works. The provided minimal code example makes reproduction easy, and the root cause is likely to reside in how the draggable machinery (leg.set_draggable) stores references that break pickling. For an engineer familiar with matplotlib's artist and event/interaction system, diagnosing and fixing the reference (and making it compatible with pickling) requires some understanding of the architecture but primarily debugging and adjusting a small section of code (in this case, offsetbox.py). The fix is concise and local, so this fits within a '15 min - 1 hour' (level 1) timescale once the relevant code is found.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are evident: the sample provides all required information for reproducing and verifying the bug. The only possible note is that details like 'matplotlib Backend' are missing, but the bug is independent of this setting. The expected result is objectively checkable using the test.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25332": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly states the symptom (pickling a figure fails after calling align_labels()), includes a reproducible minimal code sample, and describes the expected behavior (\\\"Pickling successful\\\"). The code snippet provides everything an engineer needs to reproduce the problem and understand what success looks like (i.e., pickle.dumps(fig) runs without error after align_labels is called). The affected area is also clear from the code (after calling align_labels on a matplotlib Figure). The environment is specified (Windows, matplotlib 3.7.0). While some details like traceback or the precise error message are not provided, the reproduction code suffices to observe those directly. Overall, this is a textbook example of a good bug report.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue likely requires a moderate amount of investigation. Although the reproduction is clear, the root cause involves an internal change to enable pickling after complex internal state (from align_labels, which uses Grouper and weak references). It requires understanding how label alignment changes the figure's state, and why this state becomes unpickleable. The fix requires adding __getstate__ and __setstate__ methods to a support class (Grouper), which is a non-trivial serialization detail; this is not something discoverable by tweaking a single line, but involves understanding how weakrefs interact with pickle. Locating the exact class and figuring out how to serialize its state for pickling would take substantial time for anyone not already familiar with this part of the codebase. Thus, 1-4 hours is a reasonable estimate for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues or blockers found beyond what is covered. The issue is clear; there is a minimal reproducible example and the expected behavior is unambiguous. Tests are not mentioned in the issue, but for benchmarking this is fine, as they are supplied separately. The only possible, but mild, limitation is the lack of an error message in the issue, but it's easy to produce via the code. All relevant information is included for the benchmark task.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25479": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified but contains some possible ambiguities that require a bit of interpretation. The problem is clearly demonstrated with code examples, and the underlying cause is explained: when registering a colormap, the internal name stored on the colormap object and the name used for registry lookup can be different, leading to confusion and errors when retrieving or using the colormap elsewhere. The user provides a minimal example that illustrates the broken behavior and describes expected usage. However, the issue stops short of stating an explicit desired solution (e.g., whether to update the colormap's name on registration, or to improve how names are handled elsewhere), so there is a small amount of interpretation required as to what a complete solution looks like.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core difficulty is understanding matplotlib's colormap registration and name lookup behavior and then ensuring that when a colormap is registered under a specific name, it can always be used by that name, even if its internal .name attribute is different. This is a targeted change that mostly calls for updating the colormap object during registration and making sure name equality logic aligns with expected behaviors. While this does require some understanding of the codebase (especially the registry and colormap implementations), it is limited in scope, likely localized to a few methods and surfaces in the tests. Implementing and testing the change should not take more than an hour for an engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further major issues are present. The issue is reproducible with the problem scenario, and the requirements\u2014though not completely formalized\u2014are clear enough to enable a purposeful solution and meaningful test coverage. The test cases provided closely mirror the original example, ensuring the fix can be validated. The problem is valid and suitable for benchmarking coding skills, including understanding of object registry semantics and validation through tests.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-26291": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is generally well-specified: it provides a minimal code example, the environment details, and a clear statement of expected behavior based on a canonical example from documentation. However, the 'actual outcome' is left blank. The expectation is based on behavior shown in an example at the project's documentation site, but since no output/error/traceback is pasted, we must infer that something is going wrong (likely an exception or visible bug). There's an ambiguity as to whether the issue is a crash, a visual output mismatch, or other error; however, given the context and the nature of the request, a reasonable and experienced engineer could reproduce the issue, infer what is wrong (by following the reproducible example), and understand that the goal is to make the inset axes work as shown in the demo. So, this fits a Level 1: there are a few blanks to fill (specifically, the exact error or visual misbehavior), but it's still possible to make a sensible interpretation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a relatively focused bugfix. For an experienced engineer familiar with the relevant matplotlib code, understanding and reproducing the problem is straightforward using the provided example. The solution is a small change: handling the case where a renderer is None via a fallback. Identifying where to place the fix, and why the renderer is missing in this code path, takes some codebase navigation and reasoning, but does not require major refactoring or extensive new code. Writing an appropriate regression test is also straightforward. There is no need to alter multiple files significantly or deeply refactor logic.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. This sample is suitable for coding ability evaluation: the issue is clear enough to act as a real-world bugfix challenge. The main point of friction would be the lack of explicit error output or the precise failure, so the tester has to rely on running the provided snippet to see what goes wrong. But this kind of inference is common in real-world engineering.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-1724": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. The problem is clearly described with two concrete code examples: one that works, and one that fails due to a UnicodeDecodeError when a unicode method name (u'POST') is used instead of a string ('POST'). The submitter provides an explicit guess at the root cause, referencing the relevant line and variable (sessions.py:313, 'req.method = method.upper()'), and mentions the affected library version. The expected behavior is clear: using a unicode string for the method parameter should not cause a UnicodeDecodeError. All necessary information for a meaningful fix\u2014including where to look and what constitutes success\u2014is provided.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial bug fix. The developer needs to ensure the method argument is converted to the correct string type before being used\u2014a change involving one or two lines of code. While this requires some investigation into string/bytes handling in Python 2.7 and the requests library internals, the scope is very limited and isolated to method handling. Test coverage should be easy to confirm by replicating the unicode input scenario. For an experienced engineer familiar with the codebase, this would likely take less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional major issues preventing this issue from being used as a benchmark item. The environment (Python 2.7.x, requests 1.2.3) should be documented for reproducibility, since unicode/str handling differs between Python 2 and 3.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-5414": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It states that making a request to a URL such as 'http://.example.com' raises a UnicodeError, but should instead raise an InvalidUrl. It even directs to a relevant line in 'requests/models.py' where related logic occurs, and references a similar prior issue. Reproduction steps are provided and show exactly what the user did, as well as the expected result ('InvalidUrl: URL has an invalid label.'). The problem, the context, and the desired change in error handling are explicitly described, making the engineer's job straightforward: catch UnicodeError and raise the appropriate InvalidUrl exception for malformed hosts like those beginning with a dot or star. This gives ample information for someone familiar with the codebase to implement the required fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small but targeted change to input validation logic. The engineer must locate the `prepare_url` logic (already pointed out in the issue), understand how URL hostnames are parsed, and adjust the code to standardize how malformed input is handled. The change (as seen in the patch) is a few lines of code: broadening the condition so that hosts beginning with a dot raise InvalidUrl, similar to other bad inputs. It is a minor update with low risk and limited blast radius, and understanding the full problem and making the change should take well under an hour for an experienced engineer familiar with requests\u2019 URL validation code. The only slightly non-trivial part is making sure you catch and re-raise the right error, which is straightforward with the provided context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be any other major issues. The sample is suitable for a coding benchmark: the symptoms and desired behavior are both clear, and the expected changes can be made and tested in a straightforward way. Test changes are also simple and illustrative.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3151": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It provides a minimal code sample that triggers the problem, clearly states the expected behavior ('combine_by_coords should return without error'), and quotes the documentation which supports the expectation that non-varying coordinate dimensions are 'ignored.' The current (undesired) behavior is also explicitly described: identical coordinate dimensions still must be monotonic, which is not in line with expectations. The context is complete for an engineer familiar with xarray, and 'combine_by_coords' is an explicitly named function. The engineer is given all necessary details to construct a solution and proper tests.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding how 'combine_by_coords' handles coordinate dimensions, identifying where monotonicity is being checked, and modifying the check to ignore bystander dimensions that do not vary between datasets. For someone experienced with the codebase, it is a small but non-trivial change (identifying and adjusting logic), likely taking 15-60 minutes, especially since a clear MCVE provides reproducibility. No substantial refactoring or research is required.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues with using this sample for the coding evaluation. The problem is precise and testable and has a good reproduction case. The MCVE and expected behavior remove ambiguity. The only minor complexity is needing familiarity with xarray's combining logic, but this is expected for such benchmarks.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3677": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It provides a clear example of code that works as expected (using the top-level xr.merge function) and code that fails (using the instance method ds.merge with a DataArray). The expected output is shown explicitly. The context and the nature of the bug are directly described, and the expected behavior is clearly illustrated. Any software engineer familiar with xarray will understand that the goal is to make ds.merge accept a DataArray as argument in the same way that xr.merge works, and to produce the same output. No additional clarification is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required is straightforward: adapt the merge method on the Dataset class so that, if the argument is a DataArray, it is properly converted to a Dataset before proceeding. While it requires a bit of codebase familiarity and some careful handling, this is a standard small fix in a mature codebase that should be achievable in 15 minutes to 1 hour for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues that should prevent this sample from being used in the benchmark. The description is clear, the code is minimal and reproducible, and the expectations are well stated. No external dependencies or context is needed beyond standard xarray knowledge.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4094": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a clear minimal, complete, and verifiable example (MCVE) illustrating the problem: the failure of a roundtrip stack/unstack operation on datasets with only single-dimension variables. The expected behavior and actual problem are both described and demonstrated in code. While the description could be more explicit about expected output (\\\"a working roundtrip\\\" is a bit vague), the code sample makes the nature of the defect clear, and the engineer can sensibly interpret what needs fixing. The specific method impacted (to_unstacked_dataset) and the context (single-dim variables stacked then unstacked) are clear, though there is a minor amount of interpretation left as to what exactly constitutes \\\"working\\\" in the most general sense.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires identifying and understanding why the existing to_unstacked_dataset implementation fails for single-dimension variables after a stack/unstack operation, and then making, testing, and validating a (likely small) code change to fix the behavior. Most of the code change is a minor adjustment to a selection call, not a structural rewrite, but it does require a strong understanding of both the problem and xarray's internals. An experienced engineer could expect to diagnose and patch the method, plus write or adjust a regression test, all within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues with the sample. The test case and patch both directly address the described problem. The issue is a good fit for evaluating Python and library internals understanding.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4695": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The provided code sample succinctly presents the problem: a DataArray with a dimension named 'method' causes .loc to fail, whereas another name works. The issue text explicitly states that the name of the dimension should not interfere and suggests it is being used unsanitized. The minimal reproducible example allows an engineer to reproduce and diagnose the bug. The expected fix is clear: ensure that DataArray.loc works regardless of dimension names. No major ambiguities remain about what the successful outcome looks like (make the code sample work and ensure no dimension name collisions). The environment and version information is a helpful supplement for context.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a minor API bug where a special name ('method') collides with internal parameter names, causing a failure in .loc. With the provided code and clear description, an experienced xarray contributor could reproduce, debug, and patch the bug in 15-60 minutes. The fix likely involves adjusting how keyword arguments are forwarded internally to avoid name collisions. This is a fairly localized, well-scoped code change (even though it involves understanding how .loc and .sel interact, the problem is not fundamentally complex or large in scope).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue is typical of Python API/keyword argument bugs and should be suitable for benchmarking coding ability as described. The reproducibility, scope, and clarity make it appropriate for your use-case.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6599": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear and well-specified. The submitter provides the context (difference in behavior between versions of xarray for the `polyval` function with timedelta64 input), an explicit statement of the expected and actual behaviors, and a minimal reproducible example. They also provide reference outputs for both the correct (previous) and incorrect (current) cases. There is no ambiguity in what is required: make `polyval` with timedelta64 coordinates return the same, correct results as before. An engineer could understand precisely what must be fixed and could verify success by ensuring the code produces the given 'correct' output for the example provided. No additional clarifying information is required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is localized, with a clear difference in output between two versions given the same input. The minimal, reproducible example allows fast debugging and diagnosis. While some knowledge of xarray internals and handling numpy timedelta64 is necessary, the patch (as evidenced in the gold diff) is small and targeted\u2014a one-line adjustment in the data conversion logic with a corresponding test addition. For an experienced engineer familiar with or able to quickly familiarize themselves with the xarray codebase, this should take less than 1 hour, making it a difficulty level 1 problem.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue is exemplary for a benchmark: good specification, clear expectations, direct testable example, and no deep architectural or unclear requirements. The only potential caveat is that reproducing the exact output may depend on floating point precision and the environment, but this is addressed by providing the expected printout and environment details.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6721": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear minimal code example that triggers the unexpected behavior: accessing the 'chunks' attribute on a Zarr-backed Xarray object loads the entire array into memory, which is not the expected lazy behavior. The expected behavior is also specified via a reference: accessing 'chunks' should merely inspect the 'encoding' attribute and not trigger a full data load. However, the explanation of \\\"simply inspect the encoding attribute\\\" is somewhat brief and does not provide deep underlying rationale or acceptance criteria; an engineer familiar with Xarray's chunking and lazy loading would need to fill in some blanks and interpret what change is needed. There are no references to specific files or functions in the issue text, though the context and minimal example are sufficient for an experienced developer to generate a sensible solution, as confirmed by the concise code diff provided in the gold patch.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a small logic fix. An experienced engineer familiar with the codebase and the concepts of lazy loading in Xarray and chunking should be able to reproduce, diagnose, and implement the fix within 15 minutes to 1 hour. The core of the fix involves ensuring that chunk inspection does not inadvertently trigger full data loading, which is a matter of a small code adjustment (switching from using '.data' to '._data'). The tests are straightforward, and the scope of the code change is limited to a single location without requiring multi-file changes or deep refactoring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The environment is well specified, and the minimal example is clear. The engineer does need familiarity with the internal data model of Xarray to resolve the ambiguity of 'encoding' versus accessing 'chunks', but this is within the expected standard for this sort of benchmark task.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6386": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides specific information about the problem: the '-v' (short verbose) flag in pylint expects an argument, which is inconsistent with the '--verbose' (long) option that does not require an argument. The steps to reproduce are clear and version information is provided. However, the issue doesn't specify precisely what should happen with both the flag and its help message, so some implementation detail is left to the engineer (such as whether '-v' should mirror '--verbose' exactly, including its help and internal handling). Sensible interpretation is to align their behaviors. Thus, there are some blanks to fill in, but the high-level goal is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires making the short '-v' flag behave like '--verbose', i.e., not expect an argument and be internally mapped to the same action within the argument parsing code. This typically involves identifying where options are defined and processed, updating the argument parser configuration, and possibly adjusting the help message. The fix is small and localized\u2014modifying several lines\u2014but may require understanding how the argument system is implemented. For an experienced engineer, this is likely a 15 min - 1 hour change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues that preclude this from being used as a benchmark. The problem is understandable from the description and has definite boundaries. Expected outputs can be sensibly inferred. The example command and clear expected behavior (matching the long option) limit ambiguity enough for an experienced engineer.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6903": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes the crash scenario (running pylint with --jobs=0 in a Kubernetes pod), pinpoints the culprit function (_query_cpu), and explains how the calculation of available CPUs can yield zero, which is invalid for multiprocessing. The user includes relevant system file outputs, the exact line of code at fault, and even suggests a possible fix (ensure minimum returned CPU count is 1). The expected behavior is also made clear: pylint should not crash if zero CPUs is calculated; it should default to 1 instead. This is actionable and leaves almost no ambiguity for an implementer familiar with the codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted bug fix: it requires understanding the function _query_cpu and ensuring it never returns zero, applying a minimum of 1. The change is contained, affecting a single function. Testing involves simulating return values with mocks to reproduce the bug, so a small but precise test case is added. The engineering challenge is low\u2014just a bit of care is needed to not break other logic. An experienced engineer could complete this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No issues beyond the core bug are present. The sample is a strong candidate for coding benchmarks: it is focused, reproducible, has a clearly stated expected output, and comes with a test that can automate assessment. It is suitable for exercising skills in bug localization, reasoning under resource constraints, and writing defensive code.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-8898": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides all necessary information to understand both the current behavior and what the correct/desired behavior should be. It makes clear that the problem arises when using regular expressions with commas in pylint's 'bad-name-rgxs' configuration option, which causes the option parser to split on commas inside regex quantifiers and thus breaks parsing or causes crashes. The issue text includes: (1) the bug description, (2) a minimal failing configuration demonstrating the problem, (3) what command to run, (4) the expected behavior (ability to use any valid regular expression, including those with commas, or an escaping mechanism if not possible), and (5) relevant environment info. An experienced engineer could replicate, understand, and attempt a fix for this from the issue alone, without requiring clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires understanding how options are parsed in pylint, specifically how comma-separated regular expressions are handled. While it involves making a change to the string splitting/parsing logic to ensure that commas inside regex quantifiers (e.g., {1,3}) are not incorrectly parsed as list separators, the overall patch is relatively concise and touches only a few places (utility function and its use-site). It mainly requires careful parsing logic rather than major refactoring or research, so it's a 15 min - 1 hour task for an experienced developer after understanding the code flow.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The problem is clearly described, the solution is testable, and the scope of change is reasonable for the benchmark. The issue is suitable for evaluating coding ability related to parsing and string handling.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10051": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The problem is clearly described: the behavior of caplog.get_records() and caplog.clear() diverges because caplog.clear() replaces caplog.records instead of clearing it, causing get_records() to become disconnected from the current list of log records. The expected relationship between the functionalities is made explicit, and a minimal reproducible example is provided to demonstrate the bug. The relevant lines in the codebase are pointed out, which helps define what 'correct' looks like: calling caplog.clear() should clear all records, and subsequent logging should be reflected in both caplog.get_records() and caplog.records. There is no major ambiguity about what should be fixed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a moderate change in a well-contained section of the codebase (log capturing and record management in pytest). An experienced engineer would need to spend some time understanding the difference in how caplog.get_records() and caplog.records interact, but the root cause is pointed out and a fix involves refactoring clear/reset logic so both stay in sync. Implementing and testing the fix, given a good understanding of the code, should not take more than 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues. The issue provides a clear problem, steps to reproduce, and context. The fix can be tested independently, and the requirements are not ambiguous. The issue is suitable for benchmarking coding ability in this format.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10081": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a concrete, minimal reproduction of the problem, including the specific test case, environment details, and a clear contrast between the current (incorrect) and desired (correct) behaviors. It specifies that when running pytest with --pdb, tearDown() should not be executed for TestCase classes that are decorated with @unittest.skip, mirroring behavior for function-level skips (reference to #7215). No understanding of further context or external information is strictly necessary; the requirements and scope are clear, and there is a precise example to test against. The corresponding test and gold patch also show a tight fit between the issue and the code solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Although the problem is non-trivial (involving test discovery and fixture/teardown lifecycle in pytest/unittest integration), the scope of the change is limited. The engineer must reason about how pytest interacts with unittest's skip machinery and pdb, and extend the logic that checks for skipped tests to include class-level skips, which is a clear and focused modification. Only a small piece of code needs to be updated (one function), and the challenge lies more in tracing the code rather than making a large change. An engineer familiar with pytest and the codebase should be able to make the change within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues found. The issue is well-specified and suitable for evaluating coding ability: the requirements are clear, properly scoped, and can be tested directly. The connection between the reproduction steps and the expected output, plus the tight test/patch alignment, makes this a very clean benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5262": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly identifies a concrete bug with _pytest.capture.EncodedFile's mode property incorrectly including the 'b' flag despite its write() method not supporting bytes. The description details how this causes an exception with youtube-dl, and provides a minimal reproducible example (test.py with youtube_dl). This gives a clear source location (EncodedFile in _pytest.capture), indicates the property to be fixed (mode), describes the correct behavior (mode should not include 'b'), and supplies context for testing. A competent engineer could proceed without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue involves identifying the origin of the incorrect 'mode' attribute and ensuring EncodedFile's mode property reflects its actual capabilities (i.e., not advertising binary mode when unsupported). The code fix is small (replacing/overriding a property), but understanding the subtlety around file-like objects, modes, and python I/O can take some reasoning. However, with the clear guidance from the issue and a minimal reproducible test, the task is relatively straightforward for someone familiar with Python internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major concerns. The sample provides sufficient context and reproducibility for both diagnosis and validation. Its scope is appropriate for a coding benchmark, requiring both understanding of the Python I/O abstractions and the ability to make a targeted modification.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5631": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly specific: it describes the context (using pytest to run tests with a patch where 'new' is an array), the previous and current behavior, and the root cause of the bug (comparing 'p.new in sentinels' when p.new is a NumPy array leads to a ValueError, because the comparison returns an array of booleans). The link to the commit that introduced the bug, and an example code snippet, are provided. However, there are a few blanks for an engineer to fill in: for example, which exact function is affected is not stated (though a strong candidate can be inferred), the expected fix is implied but not outright stated. The concept of comparing against sentinels using equality rather than identity is implicit, and the engineer will need to interpret the intent themselves. No information is given about additional edge-cases or compatibility goals. Nonetheless, it is clear what behavior is wrong and what behavior is desired for this scenario. Overall, the issue is not completely self-contained, but has enough detail for a sensible interpretation and solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual fix, as seen in the provided gold patch, is relatively localized: it changes a section of logic in a single function to compare sentinel objects using identity (is) rather than containment/equality. This is a common Python pitfall, especially when working with custom/NumPy objects, and a moderately experienced engineer could understand and solve this in 15\u201360 minutes after tracing through the code, examining the collection logic, and testing the change. The changes required are not large, affect a narrow area, and do not require extensive rewriting or multi-file coordination. However, it does require understanding of Python object equality semantics and some knowledge of unittest.mock/patch, so it's not trivial. Hence, level 1.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are present. The main subtlety\u2014why an array of booleans is problematic\u2014is reflected adequately in the issue description. The code and test changes suggest that the issue and solution are both contained and sensible for evaluating coding ability. The only minor drawback is the reliance on some knowledge of the context (unittest.mock, patch, and what sentinels are), but this is reasonable for the level expected.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is clear about the observed problem: when running tests with pytest-xdist, chained exceptions are not displayed in the output as they are without xdist; only the final exception is shown. It provides example tests and shows both the desired (correct) and undesired (current, failing) output, making it clear what behavior is missing. While it doesn't mention a specific filename or function, the concrete nature of the observed difference in output, plus explicit mention of pytest and xdist versions, gives an experienced engineer enough clues to determine the cause and desired outcome. Some familiarity with the pytest codebase and its reporting/serialization process is assumed, but the description is focused enough to support meaningful resolution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a good understanding of pytest's internal representation and serialization of exception info, especially with regards to chained exceptions and how they are (or aren't) handled in distributed test execution with xdist. The fix involves not just a local change to a single function, but refactoring and enhancing how exception chains are serialized/deserialized between subprocesses. It likely requires editing multiple files, changing a non-trivial amount of logic, and writing or adapting tests to confirm the fix (as seen in the gold/test patches). For an engineer familiar with pytest but not this specific area, this would take several hours to understand, diagnose, design a fix, and implement/test it.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is specific, can be reasonably solved without needing external clarification, and does not rely on any hidden context. The provided minimal code examples make the desired behavior clear. It assumes familiarity with how pytest-xdist works, but this is within scope for the expected audience.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5809": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It identifies the specific file and lines of code involved (src/_pytest/pastebin.py, L68-L73), describes the bug (use of 'lexer=python3' in the --pastebin feature's HTTP POST causes HTTP errors for some inputs), and provides a concrete suggestion for a solution (switching lexer to 'text'). Context for reproducing the problem is included. The developer has enough information to both find and understand the bug as well as implement a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change to resolve the issue is local and straightforward: change the value of the 'lexer' parameter from 'python3' (or version-conditional python) to 'text'. The problem is isolated to a single function/parameter, and some reasoning about the appropriateness of possible lexer values is needed, but no substantial code rewrite or complex logic is involved. Implementing and verifying the change should take less than an hour for a developer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major additional issues with this sample. The description is clear, the fix is self-contained, and the test change is straightforward and validates the change. No external resources or ambiguous requirements are present.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5840": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description gives some context: after upgrading pytest from 5.1.1 to 5.1.2 on Windows, there is an ImportError related to loading conftest, and the path is now being converted to lowercase. However, the description does not specify the traceback, the exact ImportError shown, or the consequence (e.g., what breaks, what functionality is lost). It also does not specify which file or function is responsible for the change in path casing. There is no minimal reproducible example, no information about what the expected vs. actual behavior is besides the casing, and no direct guidance on what a correct solution should accomplish other than inferring that casing changes are problematic. There is enough here to sensibly interpret what the user is reporting\u2014that lowercasing/wrong-cased import paths with conftest.py on Windows is breaking something since the upgrade\u2014but it leaves the implementer to make many assumptions.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Given the patch, this required subtle changes to multiple places where paths and import keys were handled and some understanding of Windows path semantics, file system casing, and the plugin system in pytest. The engineer had to update multiple locations to shift from a deprecated/incorrect 'unique_path' approach to a more robust Path.resolve() pattern, and update how conftest modules are referenced internally. This is not trivial and likely required investigation, code understanding, and careful testing, hence 1-4 hours for a non-trivial but not huge fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Despite the vague issue report, because the test patch shows that the failing scenario was well-understood after triage, and the code change is targeted and deliberate, this is an example of an issue that could be solved with some initial investigation and codebase knowledge\u2014consistent with the benchmark's expectations. The absence of a detailed error report in the issue is a limitation, but the core problem is discernible.\",\"q2_5_confidence\":4}"
    },
    {
        "pytest-dev__pytest-6197": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified for an experienced software engineer familiar with Python tooling and pytest internals. The report describes a regression in pytest 5.2.3 compared to 5.2.2, where pytest starts to import/collect `__init__.py` files regardless of context, leading to failures when those files are not importable on a given OS (e.g., the file in the example always fails via 'assert False'). The user provides a minimal reproduction and comparative outputs between both versions. However, while it's clear that pytest should not import/collect arbitrary __init__.py files, it's not made explicit exactly when __init__.py should be imported or how it ought to be skipped in a cross-platform way. There is a bit of ambiguity in that the issue doesn't come with direct statements about desired exact behavior when collecting packages, other than it should not break builds as seen. Still, there is a sensible path for a fix: prevent pytest from eagerly collecting or importing __init__.py files that are not directly part of test modules. This puts it at a 1 per the scale: there are blanks to fill in about the precise collector logic, but a clear understanding and sensible solution are possible.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Addressing this regression requires understanding pytest's test collection internals\u2014especially how it handles Python packages, files, and import logic for __init__.py. The solution involves non-trivial investigation to discern what changed between versions, as well as editing lower-level collector classes and methods to modify collection behavior. Multiple areas must be considered, such as side effects of skipping __init__.py packages, customizing collector logic, and ensuring the fix does not break other test collection routes. Additionally, the associated tests require careful crafting to guarantee that both the bug is fixed and desired (non-)collections are respected. Overall, this is more complex than a quick patch and will likely require 1-4 hours for someone familiar with pytest's codebase, justifying a difficulty rating of 2.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No disqualifying issues were observed. The example and reproduction are strong for experienced pytest contributors, giving adequate context. The absence of explicit files, methods, or direct references in the description makes it slightly less friendly to total newcomers, but not inapplicable for the intended coding skill benchmark. The problem is representative of real-world regression bugs. There is some ambiguity in the specification, but the intent and main direction of a solution are sufficiently clear.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6202": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides clear context regarding the observed problem: an incorrect transformation ('.[' replaced with '[') in test report headlines occurring for parametrized test cases in pytest. The author includes example code, specifies the nature of the headline error, and even pinpoints (with code and file references) the exact line introducing the issue (a specific string replacement in getmodpath within python.py). The reporter suggests a concrete solution (remove the replacement), mentions test/linting results, and connects the symptom to a real-world negative consequence (vscode-python test discovery). References to file names and method names are present and clear. Any experienced engineer with access to the code can follow the rationale and expected fix without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual code fix is a small edit\u2014removing a single string replace operation in the getmodpath method, affecting only one file and a short section of code. Diagnosing the issue might require a bit of tracing the origin of the problematic formatting and confirming the fix through relevant test reports, but once understood, the solution is straightforward. The inclusion of both the failing and desired behaviors, plus code locations, means an experienced engineer could implement and verify the fix in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The provided example, the explicit bug cause, and suggested solution make this issue a good sample for benchmarking coding ability. The steps required to verify correctness are straightforward, and the test updates clearly assert the correct behavior post-fix.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7205": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified. The problem is described clearly with a minimal reproducible example and explicit steps to trigger the error (running pytest with `python3 -bb -m pytest --setup-show` and a test using bytes as a parameter). The error scenario\u2014a BytesWarning due to inappropriate use of `str()` on bytes\u2014is highlighted, and the suggested fix (using `saferepr` instead of `str()`) is explicitly mentioned. An engineer can reasonably infer which code path is problematic and what needs to be changed. The only ambiguity is that there is no detail on exactly where in the codebase to apply the change, but the example strongly suggests it's related to the output formatting for parameters in fixture setup, which is not difficult to locate via search. Thus, while there is a small blank to fill about pinpointing the exact code location, the intent and high-level solution are clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively straightforward issue once the engineer familiarizes themselves with the codebase. The test case in the issue makes it easy to reproduce the problem. The fix involves replacing a call to `str()` (or implicit string conversion) with `saferepr` (or another safer formatting approach) when displaying parameters for test setup. Pinpointing the exact spot may require some light codebase exploration (searching for --setup-show or the relevant output routines), but the actual code change is minor\u2014a one-line replacement. The associated tests are trivial to write once the bug is understood, so the entire task should fit easily within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present. The scenario is realistic, and the issue text provides enough detail for meaningful benchmark evaluation. The issue is focused, includes a concrete example and an actionable suggestion, and is easy to verify with simple testing.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7236": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well specified. The author provides a minimal reproducible example, details the exact environment (Python version, dependencies), the commands they ran, and the difference in behavior between two versions of pytest (5.4.1 and 5.4.2). The expected/desired behavior is clear: tests marked as skipped should not execute setUp or tearDown methods (in unittest.TestCase), even when running with --pdb, just as they do without --pdb. There are concrete observations of the erroneous behavior (tearDown being erroneously called), and the exact steps to reproduce, making it easy for an engineer to understand the bug and what a successful solution would look like.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively contained issue that would require an engineer to understand how pytest integrates with unittest.TestCase, investigate differences between the two versions mentioned, and identify why tearDown is being executed in the skip case when using --pdb. Because the problem is directly reproducible and the expected vs actual behavior is made explicit, it should take an experienced developer in the 15 min to 1 hour range to fix, mainly involving careful logic adjustment rather than broad or architectural codebase changes. The PR solution also confirms a relatively small modification.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is an excellent choice for the coding benchmark: the problem is clear and isolated, well suited to test understanding of control flow around skipping and teardown in unit test frameworks. The test patch is focused and targets the exact bug. No other outstanding issues observed.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7324": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is concise but leaves out several specifics. The main complaint is that Pytest crashes the interpreter on a debug build for Python 3.8+ when running Expression.compile('False'). The issue points at a minimal reproducer, but does not explicitly describe the desired behavior, the root cause, or all the contexts in which the bug may occur. The reference to another issue (bpo-40870) provides more context but cannot be used per instructions. However, based on the reproducer and implication of a crash, it is reasonably clear that the Expression.compile function should not crash, and should handle this input gracefully. The exact error, expected output, and domain of valid inputs are left for the engineer to determine. An experienced engineer can make a sensible interpretation but must fill in some blanks regarding error handling and the underlying cause.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix itself, based on the patch, involves introducing a prefix to certain identifiers to distinguish between match expression identifiers and Python keywords, and a minor change to how the matcher processes the keys. The scope of changes is contained to a single module with minor related changes to a test file. Understanding the reason for the crash and deducing this fix requires some initial investigation, but after recognizing the bug, the coding aspect is straightforward. This would likely take less than an hour for an experienced developer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"While the issue does assume some familiarity with the mark expression syntax and its implementation, there aren't any blockers or major issues with using this as a benchmark example, as long as the participant is familiar with Python and Pytest internals. The main challenge is understanding the function of Expression.compile and the expected treatment of reserved words in match expressions.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7490": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It details a behavioral regression between pytest 5.x and pytest 6.0.0rc0 when dynamically adding an xfail marker to a test via the request object. The example provided explicitly demonstrates the expected behavior (test should be marked as xfailed) and the new incorrect behavior (test now fails normally). The filenames (test_foo.py), relevant method (request.node.add_marker), and context (pytest version and OS) are all cited. There is no ambiguity about what needs to be fixed: restoring the previous behavior where dynamically marking a test as xfail within the test body should cause the test to be reported as xfailed, not failed. There is sufficient information for an experienced engineer to attempt a solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue likely requires a moderate amount of investigation and code changes within the pytest codebase. It involves understanding the internals of test item marking, the test lifecycle, and how the xfail/skip markers are evaluated dynamically during runtime. While the code change is not massive, the engineer must track where markers are respected versus ignored, and ensure backward compatibility. This will take longer than a trivial or small fix, likely 1-4 hours for someone familiarizing themselves with pytest's internal marker handling and test runner.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The use case is clear, the test case is reproducible, and the expected result is defined precisely. This is a good candidate for a coding benchmark relying on the provided issue text. The patch and new tests directly correspond to the described issue.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7521": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well specified: it describes the specific change in behavior observed between pytest versions 5 and 6 regarding the handling of carriage returns (\\\\r) in captured stdout. The sample test case clearly illustrates that with pytest 6, the output no longer preserves the carriage return character, whereas it was preserved in pytest 5. However, the issue does not state exactly what the new intended behavior should be or why this change occurred (e.g., whether it's a bug or a deliberate change), but it is strongly implied that the test should pass as it did before (i.e., the capfd fixture should preserve newlines like \\\\n, \\\\r\\\\n, and \\\\r). The level of detail is just sufficient for an engineer familiar with pytest's capture mechanisms to determine the required fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves understanding how Python's text stream and buffering work, especially in relation to newlines and file opening parameters. Once the behavior change is identified (likely related to how files/streams are opened in the capture mechanism), the actual code change is relatively minor\u2014a matter of specifying the newline parameter correctly for the internal buffer. This may require reading documentation and a bit of code spelunking, but is unlikely to require deep architectural changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are present. The sample is appropriate for evaluating coding ability: it is concrete, includes a reproducible test, and requires reasoning about Python's I/O subtleties. It does assume familiarity with how text streams handle newlines and pytest's capfd fixture, but thorough engineers should be able to handle this.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10297": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes that RidgeClassifierCV does not accept the store_cv_values parameter, even though documentation states it should; the existing discrepancy between docs and code is revealed. The minimal code to reproduce is provided, and the desired outcome is stated (RidgeClassifierCV should work with store_cv_values=True and expose cv_values_ as expected when fit). The generalization from RidgeCV's working implementation is suggested, and version context is given. No ambiguous or missing requirements: a patch must add this parameter to RidgeClassifierCV to make its API and functionality consistent with RidgeCV, matching documentation, and expose cross-validation values appropriately.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required fix consists of propagating an already existing parameter (store_cv_values) from RidgeCV to RidgeClassifierCV, updating the constructor and ensuring proper integration. Some understanding of the class structure and inheritance is needed, as is a quick review of docstrings and attribute propagation. The edit itself is conceptually simple\u2014a non-invasive extension of the API\u2014and the provided tests cover the expectation. The code and API change is minor but requires understanding inheritance and possible implications, so it is comfortably a 15min\u20131hr task for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No issues unaddressed: The issue summarizes the problem, a test can be written to check the patch, and the scope is reasonable for benchmarking. The expectations match the actual changes needed. The only minor note is that checking interaction with the 'cv' param (that only store_cv_values=True with cv=None is allowed) is expected knowledge for someone working in this area.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10908": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It clearly identifies the bug (CountVectorizer's get_feature_names raises NotFittedError when a vocabulary parameter is provided but the vectorizer is not trained), demonstrates reproducible code samples for both using and not using the vocabulary parameter, and provides an explanation of the underlying cause (vocabulary_ is created during transform with the vocabulary argument, but not before then). The reporter specifies what should happen instead: get_feature_names should not raise NotFittedError when initialized with vocabulary, even if fit/transform hasn't been called. This is a precise and actionable requirement. It clearly mentions relevant method names such as get_feature_names and _validate_vocabulary, making it straightforward for a software engineer to identify the location(s) to address in the codebase. The behavior before and after calling transform is well outlined, enabling a suitable fix to be devised with no further clarifications required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves a small change in the get_feature_names method to ensure that the _validate_vocabulary() method is called if vocabulary_ is not already set. This is a straightforward code adjustment once the bug is understood, requiring only understanding of the vectorizer's vocabulary management and the relevant class methods. The change is localized to a single file/function, doesn't require extensive refactoring, and doesn't involve intricate logic, but requires familiarity with scikit-learn's conventions.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is well-suited for evaluating the ability to investigate and patch a targeted behavioral bug in a mature open-source codebase, especially in a foundational library such as sklearn. It clearly checks understanding of Python object state, class methods, and basic debugging. The presence of a test patch which introduces new relevant tests covering the fixed behavior further supports its suitability for benchmarking coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-12585": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified: it describes a concrete problem with the clone function in sklearn when its parameters are estimator types (classes) instead of instances, provides a minimal code sample to reproduce, specifies what happens and what is expected (no error should be raised), and even suggests a change to the code. However, the description does not give a thorough explanation of why one would want this behavior, whether there are legitimate/strong reasons for allowing classes as parameters, or whether this change could have unintended side effects elsewhere. The author raises this concern, leaving some uncertainty about broader implications and required safeguards or tests. Still, the main requirement for resolving the immediate bug is clear, so it's suitable for a benchmark with minor interpretation left for the engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change is small and localized: it involves adjusting a condition in clone to also treat estimator types (classes) appropriately. The issue provides both a minimal reproducible example and a suggested fix, which reduces the need for significant investigation. However, due diligence would require thinking about side effects or implications elsewhere, and perhaps extending or adding tests, so it is not strictly trivial but should be manageable within 15-60 minutes for someone familiar with the project.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The sample is good for benchmarking coding ability. While the possible impact on other API components is pointed out in the issue, handling such consequences and writing appropriate tests is part of a standard engineering task.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13135": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It clearly describes the failure mode: KBinsDiscretizer with strategy='kmeans' fails due to unsorted bin_edges, which results from unsorted centers produced by k-means, breaking the use of np.digitize. Specific code to reproduce is provided, with an example and directions on how to modify an existing test. The expected and actual result are specified, with the expectation that no error should be thrown. The affected functionality and the key parameter ('kmeans' strategy) are described, and specifics about the file and test involved (sklearn/preprocessing/tests/test_discretization.py, test_nonuniform_strategies) are provided. There is no ambiguity about what the bug is or what correct behavior would look like.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a '15 min - 1 hour' fix for an experienced software engineer. The issue points directly to the cause of failure: unsorted centers from k-means lead to unsorted bin_edges. The solution is to ensure sorting of the centers before calculating bin_edges. The codebase is not trivial, but the relevant function and behavior are well constrained. The fix itself is minor (add a sort operation) and the associated test changes are relatively straightforward. It requires careful reading and validation but not major investigation or refactoring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The problem and desired behavior are clear, and would work well for the described benchmark setup. Test code and the area of the codebase that needs changing are clear from the description.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13142": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is clearly specified. It describes a specific inconsistency between fit_predict(X) and predict(X) in the GaussianMixture class when n_init > 1. The issue includes a code snippet to reproduce the problem, describes the expected (no exceptions/assertion passes) and actual (assertion fails) results, and points out a gap in the existing test coverage. An experienced engineer can straightforwardly identify what needs to be fixed: make fit_predict and predict(X) agree for all n_init values, especially n_init >1. File/class/function references are implicit through the code sample and the unit test mentioned.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a careful review of the GaussianMixture (and related BayesianGaussianMixture) implementation to understand why fit_predict and predict disagree for n_init >1, and ensuring that the correct prediction labels are returned after model fitting. The correct solution is not entirely trivial but is localized and requires a targeted modification (~a few lines) within the fit_predict method. Understanding the model's flow and why the E-step is needed again is required, but for an engineer familiar with scikit-learn or related codebases, this should be a 15-60 minute fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The scope and required changes are clear. The only minor complication is a full understanding of EM initialization and E-step, but this is expected knowledge for someone working on this code.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13328": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very clear and well-specified. It states that HuberRegressor.fit should not throw a TypeError when boolean arrays are supplied as input for X and provides a detailed expected behavior: the boolean X should be accepted, and internally converted to float (as is done in similar estimators like LinearRegression). It provides code samples that reproduce the problem (including positive and negative cases), outlines what is currently happening vs. what is expected, and references the affected public class/method (HuberRegressor.fit). With this information, it is clear exactly what the issue is and what the intended fix should achieve. An engineer could begin work without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively simple and isolated change in scikit-learn. The solution involves ensuring the input dtype handling in HuberRegressor.fit matches expected behavior (i.e., accepting boolean arrays and ensuring they are converted appropriately), which can be addressed by updating the call to the check_X_y utility and possibly adding appropriate dtype conversion or checks. The code change is small and the major intellectual effort is recognizing the core problem and altering a check or adding a dtype parameter. Tests are also straightforward to add, as shown in the test patch. It may take some time to familiarize oneself with scikit-learn's data validation utilities but the fix itself is small and circumscribed.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are present. The issue is technically straightforward, clearly scoped, and relevant for evaluating coding ability involving input validation and compatibility with numpy/scikit-learn conventions. Documentation or changelog updates, if required, would also be minor and do not affect the evaluation setting.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13779": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes a bug encountered when setting an estimator in a VotingClassifier (or similar ensemble class) to None and then attempting to fit with sample_weight. The provided code example is minimal and demonstrates the exact failure scenario: create the ensemble, fit with weights, set an estimator to None, fit again with weights, and observe the failure. The text states why the failure occurs (lack of None check), and the code snippet makes clear the location and flow that needs fixing. An experienced engineer would understand that a check for None should be added when iterating over estimators during fit, specifically when dealing with sample_weight support. The scope and intention of the required solution are unambiguous.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a relatively small change that requires understanding the structure of ensemble estimators, particularly how fit and estimator iteration work. The required fix is to skip or ignore estimators set to None when checking for sample_weight support during fit. The modification is localized, conceptually simple, and does not require major refactoring or extensive research. It is more than a trivial change (must carefully ensure correct skipping logic), but an experienced engineer can implement and test this in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The sample is concrete, directly testable, and appropriate for evaluating coding ability. The included code sample and description ensure clarity. Both the bug and the correct behavior to implement are specified.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14053": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a minimal but clear reproduction case, as well as the expected behavior: export_text from sklearn's DecisionTree does not handle the single-feature case properly. Although it does not exhaustively describe the expected output or error, the included code sample is sufficient for an experienced engineer to reproduce the problem and infer a reasonable expected output format. The core issue is localized to export_text handling of single-feature trees. Some blanks need to be filled in about exact string output, but overall it is sufficiently specific to be actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This bug is relatively localized: it relates to string exports in a well-defined part of scikit-learn (tree.export). Given the minimal reproducible example, and familiarity with the codebase, tracking down where the feature name mapping goes wrong and correcting it to properly handle the single-feature case is straightforward. The necessary change is small\u2014only a couple of lines\u2014in a single file, with correspondingly simple test changes. This would take about 15-60 minutes for an experienced contributor who has built scikit-learn before.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue and solution do not appear to have edge cases or dependencies outside export_text and its test cases. The sample is suitable for coding benchmarks. No extra blockers or problematic ambiguity is present for this setup.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14087": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it provides a clear minimal example that consistently triggers the bug, namely, that using LogisticRegressionCV with refit=False results in an IndexError. The code snippet for reproduction is complete and ready to run, the expected result is clearly stated (no error should be thrown), and relevant version information is provided. It is apparent that the intention is to make LogisticRegressionCV work correctly when refit=False, without throwing an unexpected error. No external clarifications are needed: simply ensuring that the operation does not fail in this scenario is the requirement.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue is related to internal logic and attribute handling in the LogisticRegressionCV class for the refit=False case, but it does not require deeply changing architectural structures or rewriting large portions of code. An experienced engineer familiar with the scikit-learn codebase should be able to reproduce, debug, and remedy the specific IndexError in under an hour. The fix in the gold patch is concise, modifying conditional handling to set attributes properly. Some time is needed to explore the code paths, but the patch is not large or complex.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues are apparent. The problem is well-scoped, the reproduction is clear, and the required solution is straightforward to deduce from the description. The patch and corresponding test changes confirm the single-focus nature of the issue. This should serve well as an evaluation sample for coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14629": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The description clearly states what function is being used ('cross_val_predict' with method='predict_proba'), what estimator is being used (MultiOutputClassifier), and what error is occurring (AttributeError due to accessing 'classes_' attribute directly on MultiOutputClassifier instead of on the underlying estimators). The information about the relevant file and code area is provided, with a precise hint about the likely bug: 'estimator.classes_' should be 'mo_clf.estimators_[i].classes_'. There is a reproducible code example included that demonstrates both working and failing cases. The expected result (array with prediction probabilities) is clearly stated. Version info is included to help reproduce the bug. An experienced engineer could move directly toward a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is 15 min - 1 hour. The issue requires identifying the incorrect usage of 'classes_' in the relevant part of the code, understanding the MultiOutputClassifier interface, and ensuring the correct attribute is accessed. The patch is relatively small (introducing 'self.classes_' as a list of classes for each estimator), and the main challenge is understanding how the internal structure of MultiOutputClassifier and downstream code interact. The fix is straightforward but requires familiarity with scikit-learn's estimator conventions. The related test change is also simple but ensures correctness.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are uncovered. The bug is clear, the fix is local, and the requirements can be translated directly into code and tests. The sample is suitable for use as part of an engineering benchmark for coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14710": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes an error scenario (fitting HistGradientBoostingClassifier with string target and early stopping), provides reproducible code to trigger the issue, describes the underlying cause (mismatch between y_true/y_pred types during scoring), and specifies what is expected (no error should occur). There is even a proposed code diff, indicating exactly where in sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py the change should go, backing up the technical details. A competent engineer could resolve the problem directly using this information without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires some domain-specific awareness (knowing about sklearn scoring, early stopping, and how class encoding works), but once familiar, the engineering effort is limited to modifying class/target conversion at scoring time. The patch demonstrates the change is localized and relatively minor -- recognizing the need to re-map encoded y values back to string representation before scoring. The challenge is mostly in understanding the flow, not large-scale code changes. Thus, it falls squarely into the 15min\u20131hr range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified. The error is reproducible, the patch is local and clear, and the new tests explicitly cover the failing scenario. The issue is a suitable benchmark for evaluating coding ability for an experienced engineer. The scenario is not too trivial and requires understanding interactions between APIs and internal code paths, but is not ambiguous or underspecified.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14894": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It states exactly what causes the error: fitting an SVM with sparse inputs results in a ZeroDivisionError when there are no support vectors. The steps to reproduce the issue are given as code, the expected result is stated explicitly (\\\"No error is thrown and self.dual_coef_ = sp.csr_matrix([])\\\"), and the actual result (exception) is described. The location of the bug (likely in _fit_sparse or related code for SVM) is implied from the traceback and description. For a developer familiar with sklearn, the description is sufficient to understand and resolve the bug without any further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively minor bug fix: the problem is a missing edge-case check for 'no support vectors' causing a division by zero. The solution is to add a check for n_SV==0 and construct an appropriately empty csr_matrix when this is true. The fix is conceptually simple and only affects a specific code path. Locating the exact place to add the fix requires some attention but is straightforward for someone reasonably familiar with the code. Writing/adjusting the regression test is also simple. The whole process would likely take 15-60 min for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues. The issue is well-specified, the code location and expected behavior are clear, and the provided minimal reproducible example means it's straightforward to verify the fix. The sample is suitable for evaluating coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25747": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clear. It describes a concrete bug encountered when using a custom transformer in a FeatureUnion pipeline, specifically when the sklearn set_output configuration is set to 'pandas'. The user provides a minimal, reproducible example, demonstrating code that works with the default output and code that fails with 'pandas' output. The expected result (no error when using pandas output) is explicitly stated, and the versions of relevant libraries are included. The bug pertains to the handling of DataFrame indices in the _wrap_in_pandas_container function within sklearn's codebase. The example clearly identifies both the context and required functionality, allowing an engineer to replicate, understand, and target the root cause for the fix, as confirmed by the focused changes in the gold patch.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix only involves a small change in a utility function (removing re-assignment of index if the data is already a DataFrame), which requires the engineer to understand the pandas/sklearn output interface but does not involve deep architectural changes or edits to multiple files. The engineer would need to understand the rationale for skipping the index assignment and confirm that existing tests cover expected cases, but overall the scoped nature of the code and the clear reproduction steps make this fix achievable within an hour. It is mostly an issue of careful logic and testing rather than a deep or sprawling codebase change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues present. The sample is appropriate for evaluating coding ability: it requires understanding of pandas, sklearn transformer outputs, pipeline internals, and some subtlety in handling DataFrame indices, but does not ask for context that isn't present in the issue. The given test modifications illustrate how the fix should behave. The reproduction steps are concrete and the expected vs. actual is very clear.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25931": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified because it details a specific bug (an unexpected warning about feature names when using IsolationForest with a non-default contamination parameter and a DataFrame), provides example code that clearly demonstrates the issue, describes both the expected and actual results, and includes relevant environment information. The relationship to the codebase is explicit: it's an issue in the IsolationForest estimator in scikit-learn, and the researcher is told what should happen instead of the observed behavior. This is sufficient context for an engineer familiar with the codebase to attempt a solution without any follow-up clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a small but careful change: identifying why input validation (and hence feature name loss and warning) was triggered during fit when contamination != 'auto', and refactoring the internal API to allow fitting to avoid the duplicate validation. The solution involves splitting out input validation from scoring so the fit routine can call a version that skips it, thus preserving feature names. This requires understanding some internal structure but is contained to a relatively small area (modifying or adding a helper/private method) and adding a regression test. An experienced contributor would likely identify and patch this within 15-60 minutes once familiar with the module.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. This is an ideal sample since it targets a typical subtle bug in handling pandas DataFrames vs numpy arrays, and the fix and test changes are localized and tightly scoped. The expected and actual behaviors are both clear, with a concrete snippet for reproduction and verification.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is relatively well-specified. The reporter clearly describes the problem: SequentialFeatureSelector in scikit-learn fails when given an iterable of splits (as provided by cross-validator.split), even though the documentation says it should work. The steps to reproduce, expected behavior (no error), and minimal code sample are given, including relevant scikit-learn constructs and parameters. The only blank is whether the error is due to a misunderstanding or a true limitation, but the core user expectation is unambiguous: SequentialFeatureSelector should accept an iterable of splits for cv, just as similar estimators do. Thus, there is a sensible, concrete interpretation, even if the precise technical root cause or implementation isn't detailed in the issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The underlying fix involves routing the cv (cross-validation splitting) argument through the appropriate utility (check_cv) so that both integer values (like 5) and iterable/generator splits are supported. This requires understanding how scikit-learn typically normalizes the cv parameter and then restructuring the relevant method(s) to handle it. Since the required code change is concise, affects a single class, and adheres to scikit-learn conventions, the fix can be reasoned through and implemented by an experienced engineer in 15 minutes to 1 hour. It is not trivially simple, but not highly complex or spread across multiple modules either.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues or blockers with reusing this sample for coding ability benchmarking. The provided test patch validates that the solution works. The only possible challenge is that the symptom (the error) is not quoted explicitly from the stack trace, but the issue and expectation are clear. The description also alludes to intended behavior as observed in similar scikit-learn classes. Overall, this is well-suited for the use case.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10323": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified, providing concrete examples of input files (index.rst, pom.xml), the precise literalinclude directive used, the undesired and expected output, and the reason for the problem (loss or incorrect application of indentation when using :prepend: and/or :dedent:). The user also sketches an almost-working workaround and describes the limitation. However, while the context and desired behavior are clear, there is some room for interpretation about the best implementation details (e.g., exactly how to sequence application of dedent and prepend, what warnings are acceptable, and how this integrates into the broader processing pipeline). An experienced engineer would likely have no trouble inferring the intent, but there are some blanks to fill in regarding the precise mechanism of solution. Overall, this fits squarely into level 1: some blanks, but the requirements and desired outcome are clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution involves modifying the order in which various filters (dedent, prepend, etc.) are applied in the code that handles literalinclude directives. It requires understanding the code flow and the interaction of different options/filters, but this is localized to a recognizable section of code and does not require major architectural changes. The relevant details are present, and the amount of new code or refactoring seems relatively minor. The task requires careful thought to avoid regressions and consideration of potential edge cases, but is manageable within an hour for someone familiar with the stack.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major concerns for suitability. The problem is well-motivated with a clear input and output. The main blank is implementation detail, not user requirement.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10673": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified. It describes that users want to include 'genindex', 'modindex', and 'search' in their Sphinx toctree without errors, provides example syntax, and explicitly asks that such a directive should 'be possible and not raise errors'. It does not describe exact internal mechanics, but there is an obvious and sensible interpretation: users expect these built-in/generated pages to be referable from the toctree as if they were documents. It is clear what correct user-facing behavior should be, but the solution requires an engineer to understand the internals of Sphinx's document referencing and labeling to ensure special documents are handled properly. So, while some implementation details need to be inferred, the high-level goal and testable result are clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This is a moderately complex issue: it involves understanding how Sphinx processes the toctree directive, handles special/generated documents like genindex/modindex/search, and why current logic rejects them. The solution touches multiple files and locations in the codebase, updating both toctree parsing and toctree resolution logic to include generated document names as valid entries. It also requires thoughtful updates to logic to ensure these documents are handled appropriately without unintended side-effects such as breaking globs or figure numbering. While not extremely difficult or requiring esoteric knowledge, it is substantially more than a trivial fix and would likely require several rounds of code exploration, testing, and validation, warranting a 1-4 hour estimate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues detected. The issue, while requiring some inference regarding Sphinx's internals, is suitable for coding ability evaluation. The goal is user-facing and easily testable with appropriate test files. Provided test changes have clear correspondence to the described user story.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7440": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description points out a problem of duplicate glossary terms in Sphinx documentation, with the main complaint being about terms like \\\"MySQL\\\" vs. \\\"mysql\\\" being treated as different even if the only difference is case. Although the issue report is not written in a detailed or step-by-step manner and lacks a direct code pointer or error message, it provides enough context through build steps and expected behavior. The central problem (case sensitivity in glossary terms) is sufficiently clear for an experienced engineer to act on. However, the description leaves some details to be interpreted (for example, whether all case distinctions should be respected or not), so I rate it a '1'\u2014it's not perfectly specified, but the intent is clear enough to allow a reasonable solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires changing how glossary terms are indexed for cross-referencing in Sphinx so that case-sensitive distinctions are preserved. This is essentially a small change in the logic of how terms are registered and stored, which does require knowledge of the codebase but does not involve extensive rewriting or multi-file modification. The patch and test change confirm the scope is localized and straightforward. An experienced engineer should be able to identify the relevant code and tests and implement the change within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues were identified beyond the partially underspecified nature of the issue description. The associated test ensures that the functional requirement is covered. The issue sample is suitable for benchmark use, with the main caveat being the slight ambiguity in the precise requirement, but the provided information is sufficient for a directed fix.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7462": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a concrete example demonstrating the error condition (IndexError: pop from empty list for empty tuple type annotation) and detailed reproduction steps: write the provided code snippet using typing.Tuple[()], set up Sphinx documentation with the given dependencies, and build the docs. The expected behavior is also clearly described: documentation should build with the function foo and valid type annotations. The only minor gap is that detailed knowledge of how Sphinx parses type annotations is left to the implementer, but there is a very reasonable, actionable interpretation of what requires fixing: Sphinx crashes when processing Tuple[()] and should instead process it without failure. Overall, the issue is clear and actionable, though minor details (e.g., precise output) are inferred from context.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code changes illustrate that the fix involved handling an edge case for empty tuples in two files. The main challenge lies in recognizing that 'unparse' methods must handle empty tuples as a special case to avoid IndexError exceptions. This requires inspecting the involved code, understanding why the error occurs, and making defensive programming changes accordingly. The patch touches two locations, but the extent of change is small and conceptually straightforward once the cause is understood. For an experienced engineer, this would likely take 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major concerns. The issue is self-contained and provides reproducible steps. The solution is targeted and has meaningful test coverage added.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7985": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. The main request is for 'linkcheck' to check local (internal) links in addition to external ones. The provided example in index.rst clearly shows both a broken external link and a supposed broken local link, and the expected behaviour is for local references (like 'doesntexist') to be checked just like external URLs. However, a bit of interpretation is necessary regarding details: what exactly counts as a local/internal link (any non-URL reference?), and how to handle potential edge cases (relative vs absolute paths, links outside the documentation root, etc.). The basic requirement\u2014to report broken local references\u2014is clear, but some implementation details are left to the engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This requires understanding how linkcheck currently parses and processes links, then extending it to also validate local/internal references, perhaps by checking for file existence. The amount of code changed in the original PR is moderate (adds a regex and a path check). The task requires some thought but is not architectural and does not require rewriting entire modules or deep cross-cutting changes. For an experienced developer, after some exploration of the codebase, this is likely a 15min-1h task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers or issues noted. The only minor ambiguity is the exact definition of 'local' or 'internal' links, but the engineer can make a sensible choice based on the typical Sphinx documentation usage.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8269": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes the unexpected behavior (linkcheck reports 'Anchor not found' even when the server response is an error). It details the exact commands to reproduce the issue, the actual (incorrect) output and the expected output, specifying the difference between the behavior with linkcheck_anchors=True and linkcheck_anchors=False. The desired solution is unambiguous: report the HTTP error (e.g., 404/500 error message) instead of, or in addition to, the 'Anchor not found' message when the server replies with a non-200 status code. No extra detail about the internals of the codebase or hidden requirements are suggested, so an engineer with access to the codebase could provide a solution. There is sufficient information to meaningfully solve the problem as described.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small change to the code handling HTTP responses in the linkcheck builder: after retrieving the server response, the developer must ensure that HTTP errors (non-200 status codes) are properly reported instead of only checking for the anchor. The actual fix is a one-line addition (calling response.raise_for_status()), which is not trivial for someone completely unfamiliar with the code, but should take an experienced engineer less than an hour\u2014including identifying the right location in the code and running the provided steps to reproduce and verify the fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue is directly testable, with clear expected output and a behavioral change that is both easy to reason about and to verify in automated tests. The reproduction example and desired outcome are explicit, so this is highly suitable as a benchmark sample for coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8551": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete report of a bug in Sphinx involving how implicit type references (via :type: and :rtype:) resolve unqualified names. It includes detailed reStructuredText snippets demonstrating the problem: the behavior of type links when switching current modules, and the specific false warnings generated. The expected behavior is reasonably clear (which type names should resolve where; \\\"No warnings, and the two mentioned types should resolve to mod.submod.A\\\"). However, it assumes familiarity with Sphinx internals and the xref resolution model, so while the examples define the symptoms and what 'correct' would look like, a new engineer would have to make sensible inferences about the precise location/logic of the bug and its fix. There aren't explicit references to filenames or functions, but the reproduction and desired outcome are clear enough for someone with Sphinx experience.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a solid understanding of Sphinx docfield handling and the internals of Python domain processing. The engineer will need to read through the logic of how xrefs are resolved for :type: and :rtype: fields versus explicit roles, trace the context handling for references, and determine how to pass module/class context through these references. The solution touches on both the docfields processing and the cross-reference generation logic, and the test patch shows non-trivial additions to the test suite. Overall, it is a medium-complexity bug that would require careful reading and modification of more than one module, as well as comfort with the reStructuredText/Sphinx ecosystem. Estimated 1-4 hours for an experienced Sphinx developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The test patch provides a clear check for the new and correct behavior. The issue is specific to Sphinx and so assumes the engineer is comfortable with its architecture and reST domain logic, but that's expected for this type of benchmark. The repro steps are reproducible, and the expected behavior is defined clearly enough for validation.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-9711": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The core problem is that the 'needs_extensions' check, as implemented, compares version numbers lexicographically as strings, which produces incorrect ordering for versions like '0.10' and '0.6'. The expected behavior is clearly described: 'sphinx-gallery 0.10.0 should be accepted if 0.6 is the minimum specified.' The issue even offers concrete reproduction steps and explains the motivation for the fix. From the issue text, it is clear that the solution should involve correcting the version comparison logic to compare version numbers semantically, not as strings. The relevant files and context (Sphinx, needs_extensions) are clear enough for an engineer familiar with the codebase to reasonably locate and solve the issue without ambiguity. No external info or guesswork is required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves changing the version comparison from a string-based to a version-aware comparison, likely by adopting a standard library or package for version comparison (such as packaging.version.Version). Though it requires care and a short investigation into where the check is performed in the codebase, it's fairly straightforward and doesn't involve refactoring multiple files or touching complex interdependencies. The modification is focused and small in scope. For an engineer familiarizing themselves with the codebase, this is a 15 min - 1 hour change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are apparent. The issue clearly describes what is wrong, how to reproduce it, and what the expected behavior should be. It is a suitable benchmark for evaluating coding ability related to bug fixing and understanding version comparison pitfalls.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13372": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified: it identifies an UnboundLocalError in the evalf function when calling Mul(x, Max(0, y), evaluate=False).evalf() and in a particular argument order. The reporter notes a potential solution -- raising NotImplementedError in specific elif clauses missing an else branch. While the error, where it manifests, and the context for the bug (order of arguments in Mul, links to a relevant PR) are all given, the issue does not provide a full traceback, deeper code context, or specifics on why the argument order is significant. Still, an experienced engineer could reasonably localize, reproduce, and fix the issue with the provided information, but would need to investigate the code to confirm details. This places the specification at level 1: there are minor blanks but a sensible interpretation is available.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required fix is to add 'else: raise NotImplementedError' branches in two places to prevent an UnboundLocalError. Identifying the precise locations may require some code reading and minor debugging, but the overall logic and implementation are straightforward and localized to a small number of lines. Once the bug is found (based on the hint), the fix itself is quick. For an experienced engineer with knowledge of SymPy's evalf, this is a small change that could be implemented within 15 minutes to an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues. The bug is reproducible based on the test case supplied in the issue and the solution aligns with the problem statement. One note is that the issue references motivation found in a different PR, but the actionable items are sufficiently described in the issue itself for this benchmark setup.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13480": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well specified: it gives a concrete code example using SymPy where calling `.subs` on `coth(log(tan(x)))` with certain integer values gives an error. The description lists several integer values of x for which the operation fails. The reader can infer the context (SymPy), the function involved (`coth` and `log(tan(x))`), and the method that fails (`subs`). However, it does not specify the error message or exactly what the correct behavior should be\u2014e.g., what the expected output should be when it 'fails.' Still, a sensible interpretation is that the operation should not error for those inputs and should return a mathematically valid result, as it does for other values. Thus, the issue has some blanks (expected output/behavior) to be filled, but is reasonably precise and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This appears to require understanding how SymPy's `coth` function handles symbolic arguments, especially with expressions involving `log` and `tan`. The error is reproducible with concrete values, and the solution only involved correcting a conditional in the code (switching an incorrect variable name), which is relatively small in scope. An experienced engineer familiar with the codebase and symbolic computation should be able to understand, diagnose, and fix this in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are present. The only caveat is that the expected output for the failing cases was not explicitly specified, but the test patch provides clear guidance that can be used during evaluation. The problem is otherwise representative of well-scoped symbolic math library bugs.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13877": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives sufficient detail for a knowledgeable developer to deduce what is wrong. The user provides code that calls Sympy's Matrix.det with symbolic entries and shows a progression of expected results that fails with nan for n=5. They suggest the Bareiss algorithm may not be valid for non-integer matrices, implying that Matrix.det is using an inappropriate algorithm for generic symbolic entries. The expected behavior and the nature of the bug (incorrect determinant computation with symbolic entries) is clear, and the test case is explicit. However, the issue does not directly specify what the solution should be, i.e., whether to use a fallback algorithm or how to detect/use a more appropriate implementation. While the symptom, context, and expectation are clear, some implementation interpretation is still required.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding of matrix determinant algorithms, how Sympy implements them, and where the Bareiss algorithm is (in)appropriate. The fix impacts computation for symbolic matrices and touches areas of logic responsible for symbolic algebra, so the developer has to carefully edit algorithm selection and might need to add a customized zero-checking function. The actual code changes are not huge but involve understanding both mathematical and library context. The work likely includes exploring algorithm selection (Bareiss vs. Berkowitz), changing or extending low-level routines, and updating or extending tests, totaling 1-4 hours for a developer familiar with the Sympy codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This is a solid benchmark sample: it represents a subtle but real-world symbolic math bug that a careful engineer could resolve with no extraneous ambiguity. The issue's context, repro, and expectation are clear. While specific implementation detail is not dictated, the technical depth is appropriate for a coding ability test.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-16792": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear and concise problem statement: functions wrapped using autowrap with the cython backend in sympy do not generate correct function signatures if an array argument does not appear in the expression being wrapped. A specific minimal code example is given that both demonstrates the failure and contrasts it with a working case. The underlying technical problem is pinpointed to the codegen step, describing the root cause (array arguments treated as scalars when unused). The expectation for the fix is also clearly articulated: the code should generate the correct C signature (\\\"double *\\\" instead of \\\"double\\\" for the argument type) regardless of whether or not the array variable is used in the expression. The immediate expected behavior/fix is straightforward from the description and examples, making it well-specified.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is localized to how codegen handles unused array arguments in routine signatures. The issue already identifies where the bug is (likely the codegen stage in sympy.utilities.codegen). The fix involves recognizing all array arguments in the list, even if unused in the expression, and ensuring they produce the proper metadata for correct code generation. This requires understanding the relevant code path and modifying or extending the logic that assembles function arguments, but is not a broad or esoteric change. It would require some inspection and a targeted update, likely taking between 15 minutes to an hour for a developer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The issue text provides clear context, motivation, examples, and expected behavior for both the bug and the fix. The solution is clearly verifiable via tests.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17139": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is somewhat underspecified as it largely consists of an example REPL session that demonstrates printing `simplify(cos(x)**I)` and points back to the file fu.py. The title hints at an 'Invalid comparison of complex I'. While there isn't much detail explicitly stating the intent, it can be reasonably inferred that the behavior of simplify() when given cos(x) raised to a complex power is incorrect or raises an error due to inappropriate comparisons. An experienced engineer could deduce that an error or unexpected simplification is occurring and that complex exponents are problematic, but the exact expected behavior (e.g., no simplification in the complex case, just return as is) is not stated and must be inferred. The location (fu.py) is helpful, but an explicit statement of what is 'invalid' is missing.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The underlying bug appears to be a logic error in fu.py related to how expressions with complex exponents are handled. The fix involves adding a straightforward check to avoid invalid comparisons for non-real exponents. While it requires some understanding of the relevant code path (including the simplify function and where comparisons on exponents are made), the change itself is quite small and isolated. The tests show this is a targeted fix. For an experienced engineer familiar with Sympy or similar symbolic libraries, this could be comfortably addressed in under an hour once the problem is recognized.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is generally suitable for the benchmark. While the issue description could be more explicit about the bug and expected outcome, the provided code example and file name allow for a reasonable interpretation. The main point of friction is the need to infer the shift in logic for complex exponents, but this is typical for real-world issues.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17318": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified for someone familiar with the codebase (SymPy, specifically its sqrtdenest functionality). The report provides the specific problematic input, the observed failure (IndexError), and the desired behavior: if the expression cannot be denested, it should be returned unchanged. It even provides example old and expected new outputs. The problem is mostly mechanical: prevent the IndexError and return the input unchanged when denesting is impossible. However, to fully implement this, the developer would need some familiarity with sqrtdenest\u2019s internals and what it means to leave the expression 'unchanged.' There is a minimal chance that there is ambiguity in the expected return format or edge-cases, but the main expectation is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves updating logic inside sqrtdenest to better handle cases it cannot denest, which is a relatively small and targeted change. For someone familiar with SymPy\u2019s codebase and sqrtdenest\u2019s implementation, this should take less than an hour\u2014likely even less\u2014once the failure is reproducible. The code changes are focused (around logical guarding and returning inputs untouched), and the test updates are only needed to check this edge case.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues besides the minor lack of detail about other possible problem cases, which is covered adequately by the explicit example and desired behavior. The sample is suitable for benchmark purposes, testing an engineer's ability to robustly handle edge-cases and exceptions in mathematical code.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17630": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear and concrete example of the bug involving BlockMatrix and ZeroMatrix blocks in sympy: multiplying a BlockMatrix containing ZeroMatrix blocks twice produces an exception because intermediate results convert ZeroMatrix blocks to scalars (Zero). The steps to reproduce are explicit, expected vs. actual behavior is demonstrated, and the cause is analyzed to a reasonable depth by pointing out that the zeros become objects of type Zero rather than ZeroMatrix. Although there is a small gap in that the desired behavior could be debated (e.g., whether or not block_multiplying multiple times should always yield BlockMatrices with ZeroMatrix blocks), the intent (avoid exceptions and maintain block structure) is evident. An experienced engineer could sensibly interpret and attempt a fix, but may need to make some design choices.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The core of the issue is identifying the incorrect conversion of ZeroMatrix blocks to scalar zeros (Zero) during block matrix multiplication and ensuring that zeros are preserved as ZeroMatrix where appropriate. The provided diff confirms that the solution was a small but non-trivial change in the _postprocessor function in matexpr.py and adjusting associated tests. This requires reading and understanding the relevant code paths but is contained and does not involve extensive rewrites or esoteric knowledge. Thus, it fits into the 15 min - 1 hour range for an experienced developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues are apparent. The example is direct, sensible for a coding benchmark, and includes a clearly testable effect. Engineer judgment is needed, but the problem is well scoped. Some context about sympy internals helps, but the required reasoning is fair for the intended evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17655": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it provides clear code examples of what works and what fails, and states the expected behavior (that multiplying a geometry.Point by a scalar, regardless of operand order, should behave the same). It is clear that the solution is to make the two multiplication syntaxes (Point * number and number * Point) behave equivalently. The context makes it clear exactly where the problem is located and what outcome is expected. No additional clarification from the issue author is needed to proceed with a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a typical small patch where one needs to implement the __rmul__ method for the Point class so that right-multiplication (number * Point) works the same as left-multiplication. It is clear what to do and does not require a deep understanding of the codebase, only knowledge of Python's operator overloading. It should take less than an hour for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is suitable for evaluating coding ability because it tests understanding of Python dunder methods and correct application of operator overloading. The changes are surgical and directly test the developer's ability to map a specification (those two expressions producing the same result) to a simple code fix. No additional issues or confounding factors were apparent.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-18211": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified: it directly describes the problematic behavior (solveset raises NotImplementedError when it should return a ConditionSet) and provides both a code example and the expected output. The test code and the PR changes confirm that the intent and solution are clear: when an equation cannot be solved as a set, a ConditionSet over the Reals should be returned, rather than raising an exception. The function/class/method in question (as_set on Eq objects in SymPy) is apparent from the example. No additional clarifications are necessary to meaningfully attempt a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a relatively straightforward fix for someone familiar with the codebase and the mathematical concept. It involves adding a try/except around a function call in one method to catch a specific exception and return a fallback result using ConditionSet. No large refactoring, new algorithms, or deep research is needed. The patch changes are small and isolated (one file, minor code addition). An experienced engineer could review and implement this in well under an hour after familiarizing with related code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues stand out. The issue is clear, self-contained, and a direct coding-oriented task. No significant ambiguities or blockers for using this as a coding benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20428": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description thoroughly explains the unexpected behavior in SymPy's Poly and clear_denoms methods. It gives a detailed code example that demonstrates the bug, shows what the expected behavior is (comparison with Poly(0, x, domain='EX')), and digs into lower-level representations and attributes like DMP and is_zero, as well as recent changes in primitive. This provides a clear understanding for an engineer on what is wrong (an incorrect zero polynomial that does not behave as zero) and what needs to change (the internal representation and checks for zero-ness), with enough context to go about fixing the bug. While some domain knowledge of SymPy's internals is expected, the issue is well-specified for someone familiar with the system.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is fairly isolated\u2014a property method needs to correctly detect whether a polynomial is truly zero when its underlying domain is 'EX' and its expression is zero (ex == 0 or ex.is_zero). The required code change is not extensive (as evidenced by the one-line fix in __bool__), but it does require the engineer to understand how equality, zero-ness, and representation work in SymPy's polynomial classes and domains. An engineer familiar with SymPy and abstract algebra concepts would likely take 15-60 minutes: some time to dig into related methods, confirm the root cause, and validate that changes don't introduce regressions, but not much more. The test case is quite self-contained.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is high-quality for a benchmark: it is technical, tightly-scoped, and focuses on correctness in mathematical software. An engineer must accurately reason about correctness and internal representations, making this a valuable and realistic coding assessment scenario. The problem is self-contained and its effects are easily validated by tests.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20438": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete example (with Python code) demonstrating the problematic behavior of the is_subset method regarding FiniteSet, ProductSet, and their interaction with set equality and simplification. From this, it is clear what the buggy behavior is and there is a reasonable interpretation of what constitutes the correct outcome: that is_subset should work properly for the case of ProductSet and FiniteSet, reflecting mathematical subset relationships. However, the description could provide more explicit detail about the desired output, e.g., specifying the expected result for each call. Still, the included REPL transcript makes it interpretable by an experienced engineer who can deduce expectations from standard set behavior.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires tracing through SymPy's set logic, understanding how subsets and equality are handled for different set types, and then identifying and implementing the correct logic (and dispatch paths) for ProductSet and FiniteSet combinations. The solution touches at least three files and likely involves understanding how multiple components interact. For an experienced engineer, this would take more than just a quick patch; it requires reasoning across several code and test files and verifying correct behavior in a symbolic math context. The engineer needs familiarity with SymPy's use of dispatch, predicates, and mathematical set theory.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is suitable for evaluating coding ability because it involves not just patching a bug but reasoning across the mathematical logic and implementing dispatches correctly. The test case patch confirms that coverage was added for the reported problematic case. The main issue not covered by the issue text is a detailed breakdown of all expected behaviors, but the provided example is sufficient for a competent engineer to infer the requirements.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20590": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified: it identifies a specific regression in behavior for sympy.Symbol objects\u2014namely, that in version 1.6.2 Symbol instances did not have a __dict__, but from version 1.7 onward, they do (though empty). The user suspects that an accidental change in class inheritance or slot definitions caused this and suggests that this is a bug. The issue gives a clear desired state (Symbol objects should not have a __dict__, as per their __slots__), and an experienced engineer can sensibly infer the correct direction for a solution. The only minor gap is that the issue does not specify precisely which file/class should be changed, requiring a reasonable amount of investigation into class hierarchies.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a moderate codebase understanding but is localized: it is about correctly configuring __slots__ and fixing the class hierarchy/mixin usage so that no __dict__ is present. An experienced engineer familiar with Python class internals and __slots__ can probably fix this within 15-60 minutes, once the relevant class (Printable or its mixins) is found. The required fix is quite small and does not require rewriting large portions of code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue sample is suitable for evaluating the candidate's knowledge of Python's class internals, especially __slots__ and inheritance effects. The only caveat is that for a candidate not versed in these, it might be unfairly tricky, but for an experienced Python engineer, the sample is appropriate.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-21379": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is decently specified. It provides a miniature working example (MWE) showing exactly how to produce the error, details the behavioral context, and clarifies the failing scenario versus those that 'work'. The user also speculates about what may be causing the issue (e.g. the order of assumptions or cache state). While the precise cause in the codebase isn't identified, the reproduction steps are clear enough for a developer to investigate and verify a fix. However, there is some ambiguity, as the user is not prescriptive about the exact solution or expected behavior beyond 'no PolynomialError should occur here'; this leaves some interpretation up to the developer. Nonetheless, the issue is concrete and well-scoped for investigation and resolution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix required only a minor change: wrapping the call to 'gcd' with a try-except block for PolynomialError. This is a small change that would take a bit of thought to correctly localize and test, especially confirming via the provided MWE and ensuring no regression elsewhere. It is not a trivial fix as it involves understanding why/where PolynomialError is raised, but for an experienced developer familiar with SymPy, it would likely take less than one hour to implement and validate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The sample is a good benchmark for debugging and minimal reproducible example investigation. The error is reproducible, the change is localized, and the test genuinely exercises the bug. There are no other major issues that preclude its usage as a coding benchmark, assuming the test files and environment are faithfully replicated.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-22714": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides specific information about what goes wrong (an unexpected crash with an error message), the context in which it occurs (using with evaluate(False) with Point2D in SymPy), and a code example that triggers the problem. It also provides the expected behavior by pointing out that the code works in similar cases when evaluate=False is passed explicitly or not used. However, it doesn't explain why the crash should not occur, or the deeper reason for expecting the call to work\u2014i.e., whether imaginary coordinates are ever permitted, or more detail about the expected outputs\u2014but it is clear that the error message should not be produced in this context, and that is sufficient for a well-motivated fix. Hence, there are minor blanks (such as why this behavior is required), but there is a sensible interpretation of the requirements for a successful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a 15 min - 1 hour difficulty. The problem is narrow in scope (one code path under a certain option), is triggered by a simple pattern in the code (use of with evaluate(False)), and is locally reproducible. The solution (as indicated in the patch) is just a tweak in the logic for checking imaginary coordinates at object construction time. No broad architectural changes, significant refactoring or multi-file debugging is required. The core of the fix is changing one line, plus adding a test, which should be easily accomplished by a developer with some understanding of this codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major concerns. The issue and patches fit the setup: there is a clear problem, a focused fix, and a relevant test verifying the behavior. The description, while not exhaustive about design intent, is sufficient to guide a competent solution. The test patch is also direct and checks for exactly the scenario described in the issue.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23824": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It clearly identifies the bug in the kahane_simplify() function: when there are leading uncontracted gamma matrices, they are re-inserted in reverse order after simplification, whereas they should be unaffected. A concrete example, including both code and expected outputs, is provided; the two cases illustrate the bug and clarify the desired correct result. The explanation of the cause in the implementation (\u201cinsertion loop is just backward\u201d) provides further useful guidance. The function, context (SymPy's physics.hep.gamma_matrices), and even the rough area of code to review are all named or clear. An experienced engineer could take direct action based on this information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue would likely require 15 minutes to 1 hour for an experienced engineer familiar with Python and symbolic manipulation libraries. The bug and its cause are both very clearly localized and described; finding and correcting the exact spot for the insertion logic (likely a loop or list operation in kahane_simplify) will take some careful attention but is conceptually straightforward once one is familiar with the overall dataflow in the function. The fix involves adjusting the order in which items are inserted/merged into a list, not a full rewrite or a major architectural change. The addition of concise, focused tests (as shown in the patch) makes validation straightforward, further reducing overall difficulty.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is well-specified, and the sample is suitable for use in a coding ability benchmark. The problem is both realistic and focused, requiring careful, targeted reasoning about a specific symbolic manipulation, which is a common form of bugfix in mathematical Python codebases. The example and context should not require any domain-specific knowledge beyond careful attention to the mathematics of list manipulation and order preservation.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23950": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and provides a clear description of the problem: the 'Contains.as_set' method is returning the 'Contains' object itself, which is incorrect since 'Contains' is a Boolean expression and not a set. The issue gives concrete examples in the Python REPL, such as 'Contains(x, Reals).as_set()' returning 'Contains(x, Reals)', and explains the undesirable consequences. It also references the relevant functionality regarding set containment and as_set behavior. An engineer familiar with the SymPy codebase would understand that 'as_set' should return the set involved in the containment and could identify and modify the appropriate method in the codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix primarily involves identifying the as_set method in the Contains class and making a straightforward change to return the correct set object from the arguments, rather than raising an error or returning itself. The required change is minor and concentrated within a single method. For an engineer with basic familiarity with the codebase or the ability to look up class/method relationships, this would be a small change that might involve a bit of thought (to ensure the right object is returned), but not much more.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The sample is suitable for use in evaluating coding ability, as it requires reasoning about existing code, understanding the mathematical abstraction, and making a concise, targeted fix. The presence of clear input/output samples and relevant context ensures a fair and focused coding task.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24066": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides enough context to understand what is wrong and what a successful fix should achieve. It pinpoints the SI._collect_factor_and_dimension() method as not properly detecting that an exponent is dimensionless, and supplies a concise code snippet that reproduces the problem. While the precise expected return values are not fully spelled out in prose, the meaning can be inferred from the code and assertions. The specific assertion and means to reproduce the failure give the implementer a concrete failing case to fix, making the requirements sensible and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change involves understanding and modifying an internal function that processes units/dimensions within a known subsystem (sympy.physics.units/system.py). The fix is localized, as shown by the small number of lines touched and concise tests. However, the issue does require some understanding of the domain (dimensional analysis), and investigation into how dimensionless quantities are represented and checked. It would likely take an experienced engineer between 15 minutes and 1 hour to implement and appropriately test the correct fix, after familiarizing themselves with the code and logic.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The test patch provides clear regression coverage, and the issue is self-contained and focused. The problem and fix are well suited for coding benchmarks evaluating bugfixing and reasoning.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24443": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a reasonably clear context and a concrete example of a problem using sympy's combinatorics module. It points out that _check_homomorphism (and specifically its internal _image function) is broken on PermutationGroups, and gives a failing use case. It further directs attention to the relevant function, file, and lines of code where the bug occurs. The core of the problem is specified: when an inverted generator is encountered, 'in gens' fails, suggesting some mapping/indexing or recognition logic is insufficient. However, the explicit details of how to fix the logic are not spelled out, and the suggestion 'the whole thing can be greatly simplified' leaves some implementation to the engineer's discretion. Still, the blend of example, context, and narrow focus on relevant parts makes it clear enough for a solution, assuming familiarity with the codebase and mathematical domain. Some blanks may need interpretation, but overall, it is well within the bounds of a typical open source issue.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The code changes involve significantly rewriting an internal function (_image) in a mathematically nontrivial part of the code (group theory/combintorics). Understanding the bug relates to generator mapping for PermutationGroups and rewriting logic to handle inverted generators properly requires understanding domain-specific logic. The diff also shows that the function was streamlined and generalized, with subtle changes that require strong understanding of both the mathematics involved and software implementation. This is more involved than a simple bug fix or parameter tweak and likely requires tracing through existing group theory abstractions. Although it's contained to one file and not massive in scope, it's meaningful enough to plausibly take a couple of hours for someone new to that part of the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The bug is sufficiently demonstrated and has both context and a reproducible example. The test changes are straightforward and verify the exact context from the issue. The sample is suitable for code evaluation benchmarks concerning mathematical software and internal API understanding. It does presuppose the engineer has some mathematical maturity, specifically with groups/permutations, but is reasonable for the target benchmark.\",\"q2_5_confidence\":5}"
    }
]