[
    {
        "astropy__astropy-13033": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified. It clearly states the problem: when removing a required column from a TimeSeries object, the resulting exception is misleading to the user. It also describes the expected behavior ('an exception that informs the users required columns are missing') and notes that the actual exception is confusing. While it does not give the exact wording of the (bad or expected) exceptions or explicitly name functions/classes, it is reasonable to infer that the relevant code involves removal of columns from TimeSeries and exception messages related to required columns. An experienced engineer could reasonably interpret what needs to change: make the exception message more informative when required columns are missing after a column removal. Some blanks must be filled in regarding ideal message wording and precise locations for exception raising, but the intent is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change involves modifying the error reporting for a well-defined case: ensuring the exception message when removing a required column from a TimeSeries object is clear and informative. Understanding the flow that leads to the exception and updating the message or logic should not take long for someone familiar with the codebase. There are only a handful of places to check (column removal and required column enforcement). The actual PR diff is small, confirming that this is a relatively quick and localized fix that should take less than an hour for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The sample works well for evaluating coding ability: the intent, context, and deliverable (clear error message) are reasonably clear from the issue text. The necessary knowledge about required columns and error handling is either sensible to infer or discoverable from the codebase. It tests ability to interpret requirements and improve user experience in error reporting.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-13977": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well-specified, though it does not prescribe the exact lines to change or specify test cases, it gives a concrete request: that Quantity.__array_ufunc__ should return NotImplemented (instead of raising ValueError) when it encounters incompatible input types/units. The author outlines the motivation: to better support duck typing so that reflected operations (__radd__, etc.) are properly invoked, as per numpy's protocol. The issue references current behavior, the desired standard (numpy docs), and provides sufficient context (specifically mentioning Quantity.__array_ufunc__). An experienced engineer could reasonably attempt a patch from this description, though some details like which exceptions to catch or when exactly to return NotImplemented might have to be inferred from the numpy docs or the behavior of similar classes. The solution will likely involve some reasonable interpretation, but the engineer is not left guessing high-level intent.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"I rate this issue as a '2: 1-4 hours' task. Addressing the issue involves understanding both the Astropy unit system and numpy's __array_ufunc__ protocol, as well as carefully tailoring the error-handling logic so that NotImplemented is returned at the appropriate time without causing unintended side effects elsewhere in the codebase. The engineer would need to identify where exceptions are currently raised, modify the error-handling to return NotImplemented when suitable, and ensure it does not interfere with existing logic. There is also a strong test requirement: meaningful tests for duck-typed objects must be written or adapted. This is not a trivial problem and requires some familiarity with both the codebase and numpy's duck-typing machinery, as well as defending backwards compatibility for existing Quantity behaviors. While not an all-day challenge, it is more than just a minor code tweak.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are present. The sample targets a well-understood interface (numpy __array_ufunc__) and involves code and tests that should be clear to anyone familiar with this area. The test patch confirms the nature of the fix, validates new duck-type behaviors, and ensures previous behaviors persist. If an engineer is familiar with Astropy/NumPy conventions, the issue is appropriate for benchmark purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14096": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly identifies the symptoms of the problem: when subclassing SkyCoord and creating a custom property that references a missing attribute, the error message is misleading\u2014it blames the property rather than the missing attribute. Even though the issue does not explicitly reference code lines, function names, or provide a minimal example, the behavior and expected change are clear enough for an experienced engineer familiar with Python's attribute access mechanics and error messaging to interpret. The essential requirement is to propagate the correct AttributeError reflecting the missing attribute accessed within the property, not the property itself. The engineer still has to infer exactly where in SkyCoord's implementation the problem arises and what the fix should be, but the description is sufficiently concrete.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The solution requires identifying how attribute errors are raised in SkyCoord when accessing non-existent attributes and adjusting the method to ensure the error propagates from the underlying attribute (e.g., by using __getattribute__ instead of manually raising AttributeError in __getattr__). This requires understanding of Python dunder methods but is localized and straightforward. The code change is a small, targeted refactor and would realistically take less than an hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are present. The problem is clear, the requirements are interpretable by an experienced engineer, and both the solution and the test are concise. The issue and its context are suitable for a benchmark coding task as formulated.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14182": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it requests supporting a 'header_rows' argument for the RestructuredText output format, similar to the fixed_width format. While the exact implementation details are not provided, and there is minor ambiguity in how multi-row headers should exactly be handled or what happens during reading vs. writing, the intent is quite clear. An experienced engineer familiar with the codebase and context can understand that the feature request is to enable specifying multiple header rows (such as units, column names, etc.) when outputting tables in RST format. Thus, while there are implementation details to decide, the requirements are clear enough to produce a meaningful solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Implementing support for 'header_rows' in the RestructuredText output format involves changing internal writer logic to accept multiple header rows, likely adjusting how tables are structured and printed for this format, and ensuring compatibility with the existing table infrastructure. This will touch on both writing and possibly reading logic for tables, and may affect more than one class or function. While not a highly complex change, it is substantially more involved than a simple bugfix or small refactor, as care must be taken to ensure backward compatibility and comprehensive testing for I/O. 1-4 hours is a suitable estimate for understanding, designing, implementing, and testing the feature.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The issue description is adequate for evaluating coding ability since it has a concrete requested feature, a sensible rationale, and can be tested with example input and output behavior, as seen in the test patch. There is little risk of major ambiguity affecting the coding assessment; the sample is suitable for use in a benchmark context.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14309": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a concrete error ('IndexError: tuple index out of range') and pinpoints the function involved ('identify_format' in io.registry). It also references a recent change in behavior (a commit) that altered the function's return from None to executing an isinstance() check, which provides technical context. However, the problem statement is somewhat indirect: it does not explicitly specify the expected behavior for the function, nor does it directly state what change should be made for the successful fix. The developer is left to infer, based on the error and context, what fix would be correct by studying the referenced function and its change history. Although this is achievable, there is an assumption that the engineer understands the FITS format handling, edge cases of extension checking, and what the correct 'identify_format' contract should be upon receiving a string without a FITS extension. Thus, there are 'some blanks to fill in', but a knowledgeable engineer should make a sensible interpretation of what's required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The reported issue is centered on a regression related to type checking and file extension handling in a function. The patch required only a small change: adjusting when the function returns upon checking a file extension, making it a straightforward conditional logic fix. While the developer needs to understand the context and the specific bug path, the change is conceptually simple and limited to a few lines. For an engineer familiar with the codebase, this should take 15 minutes to an hour, accounting for the time to explore the code, reproduce the issue, and validate the fix with relevant tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional concerns. However, some domain expertise (FITS files/astropy IO system) is helpful for interpreting the fix, even though the issue and fix are both very localized. The test included in the patch makes verification straightforward. The lack of explicit expected behavior in the issue text might be a minor concern, but is manageable for an experienced developer given the context and test patch.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14365": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it states that ascii.qdp is incorrectly assuming commands are upper-case, while the QDP format is case-insensitive. The example given (\\\"READ SERR 1 2\\\" vs. \\\"read serr 1 2\\\") makes the requirement concrete. The expected behavior is also explicit: files with lower-case commands should be parsed correctly and not cause the system to crash. All necessary requirements and context are provided for a developer to implement a suitable solution without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a straightforward change for an experienced engineer. It involves updating the parsing logic to handle commands in a case-insensitive manner, likely via changing or augmenting regex handling, or ensuring string comparisons are case-insensitive. Locating where this is handled and patching the code would require some familiarity with the file but is not a substantial rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns: the issue is suitable for assessing coding ability in a benchmark. The requirements are clear and concrete, and tests can be cleanly written to check for correct behavior. The scope is well-contained and does not depend on ambiguous outside context.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14508": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It explicitly identifies the file (astropy/io/fits/card.py), the function involved (_format_float), the undesired behavior (overly verbose string representation of floats that cause comment truncation in FITS Card), and offers a clear expected behavior (should be able to create any valid FITS Card). It suggests a sensible implementation direction (use str(value) if possible) and describes the criterion for success (float representations should fit within 20 characters and not unnecessarily limit comments). The example given (0.009125 leads to '0.009124999999999999') makes the formatting problem concrete, and there's enough context to understand what needs to change and why.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted code change requiring careful string formatting logic, but is rather contained in scope: identifying the misformatted float string, changing how it is formatted (preferring str(value)), and then ensuring backward compatibility and that the new behavior matches FITS standard requirements. There is a bit of nuance in enforcing a max length and considering scientific notation, but for an experienced developer, this falls under 15 minutes to 1 hour--just enough to require thought and testing, but not a substantial rewrite or investigation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other blocking issues. The sample is good: it concerns a well-bounded function, has a clear use-case and regression test outlines, and is directly tied to well-understood formatting behavior. The only subtlety is the possible side-effects mentioned, but the test patch demonstrates how to test for correctness.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-14995": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it describes the bug that occurs (mask propagation fails when one operand does not have a mask during arithmetic with handle_mask=np.bitwise_or), explains the expected behavior (mask from the operand with a mask should be used, as before in v5.2), and references relevant details such as NDDataRef, mask, and versioning. However, the issue lacks specifics like an explicit code example or precise description of expected vs actual failure (e.g., stack trace or test case), and the author admits to not fully understanding how the feature works. Still, a competent engineer could make a sensible interpretation and devise a meaningful patch based on this description, especially with access to the codebase and tests for validation. Hence, it merits a '1' for some blanks to fill in, but a sensible solution path is available.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty appears to be modest. The engineer needs to understand propagation logic for masks (including cases where masks are missing or None), identify the point of failure, and adjust the logic to behave as in the previous version. As seen in the gold patch, the fix is a small, local adjustment to an 'if' condition handling mask propagation, modifying only a couple of lines. The tests confirm the correct mask propagation scenarios. There is some need for thought to reason about propagation semantics but, for someone familiar with the codebase, implementation and testing should be possible within 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are uncovered beyond the ones examined. The issue aligns well with the benchmark goal: it tests for understanding of mask logic, conditional handling, and regression-style fixes. The lack of an explicit code example in the original issue might make for a slightly harder benchmark but not an ambiguous or broken one.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7336": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. The user describes the usage context (using units.quantity_input on a constructor with a type-hinted return value None), the exception (None has no attribute to), the workaround, and the desired behavior (decorator should check for None and omit the unit check). The high-level requirement is clear: make the decorator compatible with functions (like constructors) that have '-> None' annotations, by not attempting to call 'to' on a None result. The terminology and references are specific to astropy.units.quantity_input and constructor semantics in Python, but an engineer with some familiarity with the codebase and Python typing can understand what is needed and how to verify correctness.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves identifying where units.quantity_input checks the return type and causes a failure when None is annotated. It likely requires a focused change: adjusting an if-condition so that a None annotation skips the unit conversion logic. The code patch shows that a single line is changed. The engineer would need to locate, understand, and carefully update type checks, then add or update a test. For an experienced engineer familiar with Python typing, decorators, and the codebase, this would take 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is clear, the requirements are testable, and the solution scope is well-bounded. The sample appears suitable for evaluating coding ability in the context of real-world Python decorator and typing challenges.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7606": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is quite concise but clear. It reports that an equality comparison between an unrecognized unit and None raises a TypeError, but should instead return False. While it lacks extensive detail (such as stack traces or precise API/class names), the described scenario is sufficiently specific for someone familiar with the codebase to interpret and resolve. The correct behavior (return False) is also unambiguous. The test patch further reinforces that the issue expects 'unit != None' to be False instead of raising an error. Thus, the description is well specified for an experienced engineer, albeit with some blanks such as the exact location(s) to modify.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively minor fix. The engineer needs to adjust equality comparison logic in the relevant class (UnrecognizedUnit) to ensure that a comparison to None (and similar non-unit objects) returns False rather than raising a TypeError. The code change is localized (editing an __eq__ method and potentially fixing handling in a helper function), and the tests affected are straightforward. Familiarity with Python's equality methods and exception handling is sufficient. Thus, it would take a bit of investigation but should fit well within a 15-min to 1-hour window for an experienced developer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues. The test change shows the failure scenario and the fix aligns with expected Pythonic comparison behavior. The issue, though brief, is workable for evaluating coding ability and does not appear to require any external context or subjective interpretation.\",\"q2_5_confidence\":5}"
    },
    {
        "astropy__astropy-7671": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is somewhat well-specified but does require the engineer to fill in some details. The main problem is that the function 'minversion' fails in certain cases due to an underlying issue with Python's LooseVersion when version strings contain certain forms (like 'dev', 'rc1', etc.). The description provides context (the problem is triggered by a change in PR #7647 and relates to a bug in LooseVersion). It indicates that using pkg_resources.parse_version avoids the failure, but that functionality was previously removed. The description also notes that version strings without the '.3' do not fail, which gives a clue as to the problematic input. However, it does not specify precisely what the correct input/output should be, nor does it directly state how the code should be changed to work around the bug. There is just enough context and direction for an experienced engineer to infer a reasonable solution by narrowing the version string's format or pre-processing it. Thus, it fits level 1: some blanks, but a sensible interpretation is possible.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change requires identifying the root cause (LooseVersion limitation with certain version strings), deciding on a reasonable workaround, and implementing a small change to the minversion function, such as normalizing/filtering the version string before passing it to LooseVersion. The patch and test changes are minimal, indicating that the task is tractable in less than 1 hour for an experienced engineer. Some exploration and validation are required, but the scope is quite contained.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues are identified. The textual description is moderately well-specified and, when paired with the test, forms a reasonable benchmark sample. The main ambiguity is that an engineer would need to understand the specific failure mode and reason about how to work around LooseVersion's limitations without using pkg_resources.parse_version, but this is not insurmountable.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-10554": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is quite vague. It merely states that 'Union queryset with ordering breaks on ordering with derived querysets,' references another issue, and provides a single sentence suggesting that evaluating the queryset instead of creating a new one makes the code work. There are no concrete examples, no information about the stack trace, and no direct information about what the user did and what broke. Without additional context, it's not clear what the expected or desired behavior is, nor exactly where or how the code is breaking. Although a Django ORM engineer could guess at what is being discussed\u2014union queries with ordering and annotations\u2014they would still need to do a lot of digging and experimenting to understand the exact problem and reproduce it. This makes the issue specification vague and leaves room for ambiguity.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Despite the vague description, the solution required changes to core Django ORM code (compiler and query classes), and a new helper method was added. The engineer would need to investigate interaction between union queries, ordering, and possibly annotation/values interactions, reproduce the problem, and make changes that conform to Django's internal query building semantics. This is not trivial and could easily take 1-4 hours to diagnose and fix for someone already familiar with the codebase. The span of changes and the need for new tests suggest deeper investigation and non-trivial core logic adjustment.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Although the issue as stated is vague, the gold patch and test patch indicate a clear area of the code to work in and a precise problem and solution. For benchmarking coding ability, unless we supplemented the text with a distilled, concrete scenario (like the test), it is borderline: the ambiguity of the issue would not isolate candidate's coding ability as much as their skill in disambiguating requirements. However, technically, the setup could work by giving the provided test.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11087": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear discussion of the observed problem (unexpected UnicodeDecodeError in Django .delete()), describes the SQL queries that are generated, identifies the likely root causes, and provides hypotheses for both a workaround and an improvement (changing which fields are selected by .delete()). However, it does not explicitly specify the full set of requirements for what constitutes a successful fix (e.g., whether optimizations should only apply in the absence of deletion signals or other side-effects). The engineer is left to interpret that only the fields required for cascading should be selected unless signals are present. The connection to performance improvements and correctness is clear, but translating this to actual code requires some interpretation of the intended ORM behavior. Thus, while a sensible interpretation is very achievable, there are some blanks to fill in regarding precise behavior, especially given the contextual complexity of Django's ORM internals.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The issue requires a solid understanding of the Django ORM's deletion mechanics, careful attention to signals and database query optimization, and changes that must be extensively tested for correctness in edge cases. The patch involves new helper logic, non-trivial changes in deletion collection, and writing comprehensive unit tests to cover new behavior. This work is concentrated in a few files but is conceptually challenging and must consider correctness, backward compatibility, and subtle interactions in Django's architecture. An experienced engineer would likely spend 1-4 hours from understanding to implementation and validation, assuming prior Django familiarity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major disqualifying issues. The issue and context are typical for a benchmark assessing coding ability in a complex Python project. The description is good enough to devise and validate a fix, and the example demonstrates appropriate test coverage.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11265": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides enough context to reasonably attempt a fix, especially for someone familiar with Django's ORM. It identifies a clear failure case: using `exclude` on a queryset with an annotated `FilteredRelation` raises a FieldError on the annotation name. It further pinpoints `split_exclude` as the likely faulty function and indicates that extra data from the original query is not being passed to the subquery. However, the description is not fully detailed; it does not specify, for example, a minimum repro or what 'extra data' specifically means, nor does it clarify if there are other edge cases or interactions. Still, a sensible engineer could trace from these hints to the relevant code and understand the requirement well enough to attempt a meaningful solution. The precise requirements and expected behavior remain slightly implicit, but the main fix intent can be deduced from the symptoms and the function call-out.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix involves not only identifying the root cause by tracing how `FilteredRelation` state is lost in the exclude path, but also making a targeted change to correctly pass along `_filtered_relations` in query cloning (in `split_exclude`). The solution also introduces changes related to not trimming certain joins in another function, reflecting an understanding of the ORM internals. Thus, this issue demands major familiarity with Django's ORM architecture and searching through related code paths. It is more complex than a trivial bugfix and likely requires 1-4 hours for an engineer to fully comprehend, test, and implement, especially factoring in writing or updating tests for the changed behavior.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues outside what was already discussed. The issue is moderately specific and, given the associated test case, engineers in a benchmark setup would be able to reason about and attempt a meaningful fix. It is suitable for benchmarking intermediate-level problem-solving and codebase reasoning skills.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11299": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is relatively well specified: it identifies that CheckConstraint with a combination of OR and AND clauses generates incorrect SQL on SQLite and Oracle, due to differences in how columns are referenced (Col vs. SimpleCol), particularly with fully qualified names. It describes the error (malformed schema exception), the context (table swap on migration), and provides enough detail for someone familiar with Django's migration/constraint framework to infer what should be fixed (column name qualification within constraints, especially with OR/AND mixtures). However, there are still some blanks to fill in: there's no example model or exact migration shown, and the issue does not state precisely what the correct output SQL should be or exactly where the bug resides in the code. An engineer would likely still need to dig into the codebase and tests to confirm their understanding, but the scope and nature of the bug is clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a solid understanding of Django's internal SQL generation for constraints, especially how CheckConstraint expressions are serialized across different databases. The fix is not trivial: it involves understanding the difference between Col and SimpleCol, examining how the ORM builds constraint clauses, and ensuring compatibility across backends. Implementing and testing the solution would require familiarity with the migration system and possibly writing or adjusting multiple tests. This would generally take 1-4 hours for an experienced engineer familiar with Django internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is technical and would make sense to a Django contributor. The only limitation is that deep ORM/internal knowledge is needed, which may make it non-representative for less experienced candidates, but this is common for such infrastructure bugs.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11532": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear scenario: when the computer's hostname contains Unicode characters and email encoding is non-unicode (iso-8859-1), Django does not handle the domain name properly, leading to crashes. The fix suggested in the description is to convert the domain name to punycode before using it in the relevant email headers. While the description is clear about the cause, the desired behavior, and even suggests a location for the fix (django.core.mail.utils or django.core.mail.message), it leaves implementation details (such as all places this should be applied or needed test scenarios) to the engineer's discretion. Thus, it is reasonably well-specified, though a few blanks (e.g., exactly where to apply, or if other components are affected) remain.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The solution requires identifying and addressing all points in the Django codebase where non-ASCII domain names in email contexts must be converted to punycode. This touches multiple files and functions, not just a single fix, as seen in the patch (mail/message.py, mail/utils.py, validators.py, encoding.py, html.py, etc.). The engineer must also understand relevant standards (IDN/punycode), handle encoding correctly, and ensure proper and thorough test coverage. This is a medium-complexity change: not extremely hard, but it requires coordinated updates, careful refactoring, and thorough understanding of email handling internals.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The problem and expected behavior are well explained, and the proposed solution is technically feasible. It would be suitable for coding ability evaluation, as it involves a concrete bug and tests a candidate's understanding of Unicode, email standards, and Django's code organization.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11551": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides a clear and detailed explanation of the problem, its root cause, and the required correct logic. It references specific Django internals, the transition between Django versions, the nature of the erroneous check (hasattr on the model class), and describes the proper sequence of conditions to check. The solution is described at the level of function logic, specifically what conditions to check, in what order, and which exceptions to handle. This gives an experienced engineer sufficient information to implement a fix. Any references to filenames/classes/functions (e.g., _check_list_display_item, admin.E108, admin.E109) are present or easily found with minimal searching.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is small in code footprint and main logic changes, affecting a single method. It requires understanding some Django internals around model fields and admin validation logic, as well as correct exception handling and condition ordering. However, for a developer familiar with Django and field checks, the required update would take little time aside from setting up the test and confirming behavior. It's more about modifying conditionals than major refactoring. Investigation and implementation should fit within an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues are apparent. The issue is tightly scoped, the context and requirements are clear, and the original test verifies the fix. The sample is suitable for coding ability evaluation as it tests ability to follow detailed logic, manage exception handling, and work with Django's admin/model field APIs.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11734": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text describes a concrete problem in Django's query generation related to using OuterRef in exclude() or ~Q() within an Exists annotation, which results in a crash, whereas filter() works as expected. While the ultimate technical details are not fleshed out (e.g., error messages, stack trace, or detailed query examples are missing), the expected outcome is clear: make exclude() with OuterRef in Exists behave like filter(), i.e., not crash and work as users of the ORM would expect. An experienced engineer familiar with Django would reasonably infer the kind of bug and the desired behavior. However, there are some ambiguities on repro steps and expected output for arbitrary cases, so some blanks would need to be filled with sensible interpretations and existing ORM conventions.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue spans multiple files in Django's ORM internals (fields/related_lookups/sql/query), involves some subtle interactions with query compilation and advanced ORM expressions like OuterRef and Exists, and non-trivial test additions. The diff is not enormous, but understanding the root cause requires a solid grasp of Django's query planning pipeline and how lookups, annotations, and references are resolved. It's not just a trivial bug fix and would likely require digging into the existing implementation and ensuring any change doesn't break other ORM behaviors, so 1-4 hours is a fair estimate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues are apparent. The only slight concern is that some familiarity with Django's ORM internals is assumed, but this is expected for an engineer tasked with such an issue. The description could benefit from a more explicit example and error message, but is still usable for benchmarking.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11740": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified for an engineer familiar with Django's migration system. The reporter clearly describes the specific sequence leading to the bug: changing a UUIDField in testapp1 to a ForeignKey to a model in testapp2, running 'makemigrations', and observing that no dependency on testapp2 was created in the migration. It is clear what is expected (the migration should declare a dependency on testapp2 when such a ForeignKey is added/altered). However, the description does not go into full technical detail; e.g., does not cite exact class/method names or show example migration files before/after the change. There are some behaviors for the engineer to infer (such as whether this is specific only to UUIDFields, or applies to all changes to ForeignKeys), but there is a sensible interpretation for fixing the issue as described.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires understanding Django's migration framework internals, especially how migration dependencies are detected and generated. The change impacts the logic for determining dependencies when altering fields to foreign keys across apps, and requires non-trivial modification in the migration autodetector logic as well as extending or adding to the relevant test cases. Familiarity with Django's codebase and conventions is necessary, and the fix is not simply a one-line patch\u2014some investigation and validation are needed to ensure correct and robust dependency detection. For an experienced Django developer, the task should take 1-4 hours (likely at the lower end).\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are evident. The issue text is sufficient for someone familiar with Django's migrations. The necessary context is present and the developer can reasonably infer what must be validated and tested. As with all migration issues, care must be taken in solution and testing to avoid regressions or breakage in migration ordering, but the sample is fair for evaluating real-world coding ability in this context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11815": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is precise and clear. It explains that Django currently serializes Enum values in migrations using their localized value, which is problematic because translations can change over time, making migrations un-replayable if the translation changes. The issue identifies where in the codebase this happens: when Enum is used as a default for a CharField in migrations. The required fix is spelled out: migrations should serialize using the Enum member name, not its value. It gives a concrete example of the correct approach (Status['GOOD']) and highlights the practical implications. An experienced engineer has all the information needed to attempt a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the problem requires understanding how Django serializes Enum values in migrations, the change is conceptually straightforward: it needs the serialization to refer to Enum members by name, not by value, in the migration files. Implementing this requires adjusting the serializer and updating/adding corresponding tests to check for this behavior. The patch shows it required small changes in the serializer and moderate updates in the corresponding test suite. This is not a trivial 5-minute fix, but it should be solvable by an experienced engineer within one hour after locating the relevant code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present for use in coding benchmarks. The issue is clear, scoped, and the solution is non-trivial but approachable.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-11964": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a reasonable, if slightly indirect, explanation of the underlying problem: that Django model fields using TextChoices or IntegerChoices enums do not return their raw value when coerced to str, but instead return something like MyChoice.FIRST_CHOICE (i.e., their enum member object). This can be problematic when serializing model data or interfacing with external APIs, as the string representation is then inconsistent between objects and their enum values. The user provides an example class and describes the observed/expected behavior. However, while it's clear that the str(...) of a Choices member should return its value (e.g., 'first'), there are some blanks to fill (e.g., should all Enums in Django behave this way; is overriding __str__ globally the right solution). For an experienced engineer, a sensible interpretation is clear, but some domain judgment is required on the extrapolation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required change is relatively straightforward for someone familiar with Django's enum system. It involves overriding the __str__ method for the Choices class to return the value attribute. This small adjustment requires a targeted code change and possibly minor updates to tests, but does not require an in-depth rewrite or substantial research. The fix is simple yet needs good understanding of Python enums and Django model fields.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers or concerns are identified with using this issue for benchmarking. It clearly diagnoses a common API usability problem and can be tested well. The description is good enough for an engineer familiar with Django and enums to implement a reasonable solution without further discussion. Just ensure that clarification of the intended behavior of __str__ is noted in the benchmark context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12155": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified. It provides a clear summary of the problem: docutils fails to render view docstrings properly when the first line is not empty because the indentation calculation does not skip the first line, which may have zero indentation. The issue identifies the specific function (trim_docstring), provides the problematic code, explains why it fails, and even suggests the required code change (skipping the first line when computing the minimum indentation). For an engineer familiar with the codebase, this is sufficient to clearly understand both the bug and the appropriate resolution, without requiring further context or clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is rated as 1 (15 min - 1 hour) because although the issue is isolated, it requires some understanding of Python docstring conventions and possibly some familiarity with how docutils and Django's admindocs work. The change is small and well-localized (editing a function and possibly updating tests), but the engineer must verify the fix and ensure it behaves as intended across different docstring formats.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with this sample. The issue is clear, the context and goal are well-defined, and the solution scope is manageable. The test changes are directly related and reinforce the required correctness. This issue is suitable for benchmarking coding ability in this context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it identifies a specific bug regarding Django custom template tags and inclusion tags that take keyword-only arguments with default values. It points out that providing another variable or supplying a keyword argument twice leads to errors or incorrect behavior. However, the description lacks concrete code examples or specifics about the expected vs actual error messages, and doesn't explicitly say what correct behavior should be, only outlines what is broken. An engineer familiar with Django's template tag system could infer a sensible solution (make keyword-only arguments with defaults function as expected and handle duplicate keywords with proper errors), but there is some ambiguity left around edge cases and desired error messages.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue is a small, localized logic fix in the parser for custom template tags. The patch involves modifying a single conditional in a function to handle keyword-only arguments correctly and updating/adding related test cases. There are no major architectural changes or complex logic, but the engineer would need to understand how Django parses tag arguments and how keyword-only arguments work for custom tags, so some codebase familiarity is required. This sets the difficulty in the 15 min to 1 hour range for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues that would make this sample unsuitable for evaluating coding ability. The issue is interpretable with moderate experience in Django and provides a good test of understanding parser logic and template tag mechanics. While more context or code samples in the description would help, the problem and boundary are sufficiently clear when coupled with the provided test cases.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12325": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is vague and lacks critical details. It refers to 'pk setup for MTI to parent' and problems with 'multiple OneToOne references,' especially concerning ordering and the 'parent_link' marker. However, it doesn't explain the desired behavior, provide an example or specify what the erroneous and expected states look like. For an engineer unfamiliar with the codebase, it is not clear exactly what is broken or what the correct solution should be, as the description leans heavily on internal concepts without grounding them in actionable detail. There is room for ambiguity about what 'shouldn't it look from top to bottom' means, and what is meant by the order being required 'by design.' The text feels more like a partial observation or a note than a full bug description, making a concrete solution direction unclear without prior domain knowledge or further context.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Although the patch changes are small in terms of lines, solving this requires nuanced understanding of Django's ORM internals, particularly around model inheritance, OneToOneField, and the use of 'parent_link.' It involves editing logic in core model metaclass logic and understanding multi-table inheritance edge cases. Even with sufficient clarification of the problem, it would likely take 1-4 hours for an experienced developer to analyze the problem, write, and test the correct solution, especially considering the need for comprehensive understanding and potentially adapting or creating new test cases.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues beyond the specification concerns described. The main barrier is the ambiguity and lack of clarity in the original issue description, which could make it problematic if relying solely on the issue text, but otherwise there are no blocking problems with using this as a benchmark provided that the ambiguity is addressed in the setup as indicated above.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12663": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description states that using SimpleLazyObject in a queryset with a nested subquery annotation fails, and references a prior commit as well as calling the change a regression, but it does not specify what the expected or correct behavior is beyond implying it worked previously. There is no code sample, no explicit error message, no description of what exactly fails or how, and no mention of tests or observed output. The phrase 'somewhat arbitrary testcase' also implies that the concrete circumstances aren't well documented. For an engineer unfamiliar with the context, it's ambiguous: You'd need to infer the desired fix (restore previous working behavior?), but it's not clear what counts as a correct/general solution. Thus, while motivated, this description is not well specified, leaving meaningful ambiguity as to the specifics.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Assuming the requirements and scope were clarified (e.g., what the expected behavior is, and that introducing SimpleLazyObject should not break subquery annotation resolution), the patch is a small change (modifying an attribute access for select fields in a property getter), and does not involve many files or lines of code. The overall change is straightforward once the underlying bug and desired behavior are identified, so an experienced Django developer could solve it in 15-60 minutes, mostly spent identifying and minimally testing the change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues: the main caveat is that the ambiguity in the original issue text could result in multiple plausible, but subtly different, fixes being proposed if context from the PR/tests weren't available. As a benchmark, the sample could be problematic if used without providing all necessary context. Otherwise, no other blockers.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12708": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a reasonably clear account of the problem: deleting index_together on fields that are also under unique_together leads to migration failures in Django 1.11.10, especially when refactoring to use the new indexes feature. The issue mentions two main points: (1) deletion of index_together should work independently of unique_together, and (2) moving index declarations should not trigger unnecessary index recreation. While there is enough information to make a sensible interpretation of the required fix (ensure index_together and unique_together constraints interact coherently and avoid recreating indexes unnecessarily), there are minor ambiguities\u2014such as the edge cases and subtle interactions between these model options. However, for an experienced Django developer familiar with schema editing and constraints, the intent and direction for a solution is clear enough without being fully detailed; thus, I rate it a 1.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires an understanding of Django's migration framework, particularly how it processes index_together and unique_together changes at the schema level. The patch indicates that locating the relevant place in the code, understanding how composed indexes are created and deleted, and altering the logic to correctly handle their interaction is necessary. The developer must reason about the logic of index deletion and creation and ensure that unique constraints are not impacted when deleting index_together. Additionally, new or adjusted tests are needed to ensure that the fix works as intended. This level of effort, including targeted investigation and non-trivial changes to core migration logic and its tests, would likely take between 1\u20134 hours for a skilled engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues are apparent. The sample appears suitable for evaluating coding ability, as it tests understanding of schema management and migration logic. The only caution is that familiarity with Django's internals is helpful, but this is appropriate for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12754": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. It describes a specific bug encountered while using Django's migration framework when creating a model subclass and moving a field onto it in the same step. The bug is that makemigrations works, but migrate fails due to a field name clash, and switching the order of migration operations resolves the failure. However, it assumes prior knowledge of how Django migrations, field moves, and model inheritance work, and does not specify exact file names, classes, or specific inputs. Despite this, the core issue and desired high-level behavior of the auto-detector are clear, allowing for a sensible solution by someone familiar with Django's ORM and migration mechanics.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding how Django's migration auto-detector determines dependencies and operation order, and making non-trivial changes to its logic. It involves reasoning about the internals of the migration framework, model state transitions, and the subtle interaction between field removal and model creation. The changes are focused in a substantial and somewhat complex part of the code (migration auto-detection), and understanding/test-driving the solution requires care but doesn't require a rewrite of large swathes of code. Estimated effort is between 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The test case provided in the patch verifies the expected scenario and the sample is reasonable for evaluating advanced coding/debugging skills involving dependency management and framework internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-12858": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description references a specific error (models.E015) related to ordering and lookups/transforms, and mentions model structure and a recent PR (#29408) that may have caused a regression. However, it does not provide code samples, complete definition of the affected models, configuration of 'ordering', or details about the desired and actual behavior. The relationship between ordering, lookups, and transforms is implied but not directly articulated. There is a clear reference to the symptom (models.E015 being raised in an unintended situation), but it is ambiguous as to exactly what ordering expressions are expected to work, or how the check should function. Some assumptions must be made to proceed. Thus, it falls between a 1 and 2, but given the lack of direct example, I rate it a 2 (there is ambiguity and room for interpretation).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change required to fix this issue is local and conceptually straightforward: relaxing a conditional check to allow not only transforms but also lookups. With a basic understanding of Django's model checks and the meaning of 'transforms' and 'lookups', an experienced engineer could track down where the check is performed, compare it with recent changes, and adapt the logic accordingly. The patch is a simple modification to a conditional in a single place. However, it requires understanding of Django internals and model validation logic, so while not trivial, it is reasonable to estimate 15min\u20131h for a qualified contributor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Despite the lack of an explicit code sample, the issue's context is clear enough (especially with the model relationships described) for someone familiar with Django's model checks. The issue is subtle in that it relies on implicit background knowledge, but the problem and high-level solution are clear enough for use as a benchmark. The test added in the patch demonstrates how the issue manifests, justifying that the ambiguity is not fatal for evaluation purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13012": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified. It clearly describes an undesired behavior when using ExpressionWrapper with a constant expression and how this leads to PostgreSQL generating an invalid SQL query due to an unnecessary constant in the GROUP BY clause. The core expectation is that constant expressions (wrapped or not) should be omitted from GROUP BY when constructing the query. The issue mentions that without the ExpressionWrapper, Django already handles this correctly, which gives a clue for the required behavior. However, the report does not specify where exactly in code this logic should be implemented or altered (e.g., which methods or classes need to be modified). Nor does it specify all edge cases; still, for an engineer familiar with Django ORM, this is sufficient to make a concrete and correct change, so the main requirement is clear. Some blanks (like which part of ExpressionWrapper to change) need to be filled by the implementer, but the semantics are clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix itself, as reflected in the gold patch, involves a straightforward delegation method in ExpressionWrapper so that get_group_by_cols just calls its inner expression. The key is understanding where GROUP BY columns are determined in the Django ORM for expressions. For someone familiar with the codebase, the major work is identifying that ExpressionWrapper left get_group_by_cols unimplemented and thus needed to delegate. The code change is succinct and isolated (adding one method), so it is a small fix that requires some investigation but little actual code. Implementing and testing the fix would generally fit within 15-60 minutes for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The problem is concrete, testable, and reflects a common ORM edge case. It gives enough context for a meaningful coding exercise related to expression handling and SQL query generation. The solution is verifiable via a test. This is a good small benchmark sample for coding ability assessment.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13028": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue clearly describes a bug encountered when using a BooleanField named 'filterable' in a Django model: filtering on this field produces a NotSupportedError. It specifies that changing the field name resolves the problem and suggests that either the problem should be documented or fixed. The model and field definitions are provided, so a maintainer can identify and reproduce the problem. However, it does not explicitly describe how the error manifests (e.g., the precise query being executed), nor does it provide an ideal behavior, other than implying that filtering on a BooleanField named 'filterable' should work. There are minor gaps in specificity regarding exactly what kind of fix is desired (documentation vs. code), but a sensible interpretation for a code fix is possible.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue is moderately straightforward: the core challenge is to identify why using a field named 'filterable' causes an error and to provide a fix. The relevant context (field definition, error type, workaround) is given. The actual solution requires changing a type check in the ORM logic to avoid conflicting with user fields named 'filterable'. This is not a trivial typo fix, but requires a bit of consideration of how internal Django ORM filtering works versus field naming conflicts. For an experienced Django developer, this would likely be a 15-60 minute task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The provided test and code indicate that the problem is reproducible and that the fix only involves a small but thoughtful change to the codebase and test coverage. The issue is conceptually clear, and the challenge is suitable for the benchmark setup described.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13112": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear and reasonably detailed explanation of the specific bug: makemigrations is crashing due to mismatches in case sensitivity between ForeignKey definitions (which use a lower-case app label string like 'dj_reglogin.category') and the INSTALLED_APPS/AppConfig entry (which uses a mixed-case string 'DJ_RegLogin'). It references what used to work in Django 3.0 but now fails in Django 3.1b1, giving a specific example and enough context on the app names to infer the root cause. While not all implementation detail is spelled out, and the desired behavior is not stated in explicit requirements, it is quite clear that the tool should support mixed-case app labels and avoid being case sensitive when looking up related models for migrations. There are a few blanks to fill in, such as the exact intended relationship between case handling and what the code should normalize (app label vs model name), but overall it's a sensible, actionable problem statement.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires digging into Django's migration framework and understanding how related model references (especially ForeignKey fields specified with app labels) are constructed and normalized. The engineer has to investigate where string references are resolved and handled in migrations, test different case combinations, and ensure compatibility between the new and old behavior. However, the code patch is relatively isolated, the bug is well-scoped, and the issue doesn't require major refactoring or touching a large amount of code. The solution is primarily string manipulation/normalization and careful conditional logic, with verification against relevant migration test cases. An experienced engineer could likely identify and resolve this within an hour, assuming basic familiarity with Django internals. The additional tests needed are straightforward as well.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major concerns. The test clearly checks the fix, and the issue is appropriate for assessing debugging and reasoning about code that handles string normalization and compatibility. The only mild ambiguity is in precisely how much normalization is needed, but the intent is sufficiently clear for a meaningful exercise.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13128": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it describes a concrete problem (annotating the difference between two DateTimeField attributes plus a timedelta triggers an error) and a desired solution (make temporal subtraction work without requiring ExpressionWrapper, ideally natively support it). The mention of FieldError, mixed types, and output_field references make clear it is related to Django's queryset annotation/expression system. It is clear the user wants ORM expressions like 'F(completed) - F(assigned)' to 'just work' without needing ExpressionWrapper. However, some context needs filling in: the precise details of how expression resolution/compilation works for arithmetic with dates, and which types are involved, are not spelled out; likewise, it is not clear if all Datetime-typed arithmetic is targeted or only specific cases. Still, an experienced Django engineer should be able to sensibly interpret the requirements with good confidence.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This change affects core expression processing logic in Django's ORM, specifically supporting datetime subtraction without explicit wrappers. The engineering required includes understanding and editing nontrivial code in expressions.py, carefully handling type resolution/casting, and ensuring backwards compatibility. It spans several locations and demands precise reasoning about type constraints and database features. Regression coverage in tests needs careful adjustment as well. For an experienced Django engineer, this would likely take 1-4 hours: not extremely difficult, but it involves substantial reasoning, targeted changes in important code, and careful validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues for benchmarking purposes. The primary risk is if a participant is not already familiar with Django ORM internals, but the issue is appropriately specified and testable within the context of a framework evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13297": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly specific: it describes a breaking change in Django 3.1 where TemplateView.get_context_data's kwargs returns SimpleLazyObject, which causes crashes when filtering unless explicitly converted to a string. It provides what the problem is, the affected environment (Python 3.7.8, SQLite), and mentions the differing behavior between Django 3.0 and 3.1. It points clearly to the issue being the lazy evaluation of arguments in context data and provides enough context for someone familiar with Django's TemplateView to know where to look and what to fix. However, the exact fix required (how to convert or ensure the correct type) or which function/class to update is left up to the implementer, requiring some reasonable interpretation and investigation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change described and the gold patch support that this is a relatively small scoped but non-trivial fix. It involves understanding the effect of SimpleLazyObject in Django templates, identifying where kwargs are introduced as lazy objects and replacing the usage pattern, along with updating or adding a test. This would likely take 15 minutes to an hour for someone familiar with Django's codebase. There's no large algorithm, no refactoring across multiple modules, nor deep research required, but it does demand understanding some internals of Django and correct usage of lazy evaluation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues. The issue is sufficiently specified for use in a benchmark, and the required solution falls within a reasonable complexity for such tasks. Some minor context about how filtering is being used in downstream code (not shown in the issue) is left to the implementer, but this is typical for real-world bugfixes and the included tests in the patch confirm requirements.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13406": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly communicates that when a Django queryset utilizing values() or values_list(), potentially in combination with annotate() for GROUP BY-like functionality, is serialized via pickle and later reloaded, the returned query behaves incorrectly. Specifically, after unpickling, instead of returning the expected dictionaries (from values()) or tuples (from values_list()), it returns model instances with a broken state. The reporter articulates the situation, the API calls that are problematic, and the incorrect state observed (inability to access fields like .id). However, some blanks need to be filled in by the implementer, for example, determining the exact portion of the codebase that incorrectly reinstates the model instance rather than dictionaries after unpickling. There is also some room for interpretation about whether both values() and values_list() should be supported, and how to structure the fix, but overall the requirements are clear and actionable for an engineer familiar with Django's internals.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves adding logic to the queryset setter so that if the pickled query has values_select set, the _iterable_class is set to ValuesIterable. This is a targeted and localized fix in the internals of the ORM (one method in one file), requiring moderate analysis but no major refactor or new architectural pattern. Understanding why the bug occurs does require knowledge of Django's queryset evaluation and serialization but the concrete fix and the scope of code edits are relatively small. Most of the time would likely be spent understanding the ORM's relevant internals and testing, rather than writing a large amount of code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The core scenario (restoring querysets with values()/values_list() from pickled state) is well motivated, and the test changes provide clear ways to validate the fix against regressions. This is suitable for benchmarking coding ability on moderately involved framework internals.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13449": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified. It identifies a clear bug: using Lag() with a DecimalField on SQLite causes a syntax error because of incorrect placement of the CAST() function. The description distinguishes this from other field types (problem occurs only for DecimalField), and points out that the problem is unique to SQLite. It also provides a workaround (setting output_field=FloatField), which helps clarify the scope. However, it doesn't indicate exactly where in the codebase the fix should occur, nor does it provide example code or a traceback. Still, the issue provides a clear problem and a clue for a fix, and a reasonably experienced Django developer would know to look at the Window expression SQL generation code.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This fix requires intimate knowledge of Django's ORM and SQL compilation layer, especially how window functions and custom fields are translated to SQL for different backends. The required change involves properly overriding SQL generation for the specific combination of Lag() and DecimalField on SQLite, which is nontrivial. The developer needs to navigate the codebase, understand backend-specific expression compilation, and properly place the CAST(). Tests also require adding a new model field and test case for this scenario. This would likely take 1\u20134 hours for someone familiarizing themselves with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No blocking or problematic issues identified. The sample is suitable: the problem and expected high-level solution are clear, this is not simply a matter of bug triage, and there are code/test changes relevant for benchmarking coding ability. The issue, however, requires a moderate understanding of Django internals and isn't purely mechanical.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13568": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a clear description of the current problem and the desired behavior: when defining a user model with a UniqueConstraint on the username field, Django's auth.E003 check still requires unique=True, but this is not always desirable as it creates an extra index in PostgreSQL. It suggests extending the system check to consider UniqueConstraint on the USERNAME_FIELD. Although some technical knowledge of Django's system checks and Model Meta options is required, the objective is understood. The issue could be even clearer if it specified exactly which method/files to change and what the ideal new behavior would look like, but an experienced engineer working in Django will see what is needed. There is some ambiguity in interpreting the precise scope (e.g., should all UniqueConstraints count? How about partial? etc.), but overall, it's fairly well-specified.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this issue requires a good understanding of Django's ORM internals, system checks, and potential database-level consequences. The solution requires identifying where auth.E003 is raised, reasoning about unique=True vs UniqueConstraint, and ensuring correct detection logic while considering backwards compatibility and test coverage. Updating or adding appropriate tests is also nontrivial. For an experienced Django contributor, this would likely take 1-4 hours, as it involves nontrivial codebase navigation, learning the nuances of system checks, and adjusting tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant blockers for using this sample in a coding ability benchmark. The domain is a bit specialized (Django user model/system checks), but the problem is well-motivated and the solution sufficiently complex to be meaningful. The original description has some ambiguity regarding partial/conditional unique constraints, but the intent is clear for the relevant Django-specific context.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13807": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it provides a clear context, root cause, and exact file and function locations in Django (sqlite3/base.py, check_constraints) where the bugs arise. The problem is described precisely: loaddata crashes when table names are SQL keywords due to missing quoting (backticks) in PRAGMA statements. The code snippets show where %s formatting is used incorrectly, and the fix is straightforward: ensure correct quoting of table names. A competent engineer can identify the solution (use the quoting utility in Django) and check all places where PRAGMA statements might interpolate table names. The reproduction is version-specific, making debugging easier, and no ambiguous requirements or behavioral questions remain.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively small but non-trivial bug. The engineer needs to understand the problem with passing unquoted table names (particularly if they're SQL keywords), find the locations in code where table names are interpolated into SQL statements, and apply the appropriate quoting function. The task involves minor code changes in one function and corresponding unit test additions. All logic is straightforward and does not require extensive code changes or deep architecture work. For an engineer familiar with Django, this would take between 15 minutes and an hour, including running tests and verifying the effects.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. Both problem statement and validation (test patch) are clear and directly address the issue. The scenario is realistic and representative for a benchmark on Python/database skills. The test code and model cover the edge case thoroughly.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-13810": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description clearly explains a precise problem in Django\u2019s middleware chain in ASGI context: when MiddlewareNotUsed is raised during middleware loading, the handler variable is overwritten, and this causes side effects because the next middleware in the chain may not be properly adapted for async/sync compatibility. The report includes an explicit description of the undesired effect (a synchronous HttpResponse is returned instead of a coroutine in an async context), and describes the conditions under which this is triggered (ASGI, custom synchronous middleware, MiddlewareNotUsed). While the description could benefit from explicit pointers to code (e.g., method or line references), a Django contributor familiar with the middleware code could reasonably infer where this logic would reside (middleware chain setup) and what behavioral change is needed. The problem is precise, but the solution requires some interpretation (e.g., handler should only be overwritten if the middleware is kept). Thus, there are \u201cblanks to fill in,\u201d but it is reasonably clear what constitutes a successful fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch is localized to the function that sets up the middleware chain, with a relatively small code change: ensuring the handler is only overwritten if the middleware is not skipped. The main challenge is correctly understanding the control flow (especially around exceptions and how handler variables are set and passed in the loading logic), rather than implementing a large code or architectural change. There is some need for care in ensuring the adaption only occurs at the right time, but this is a typical refactor-level fix likely to take from 15 minutes to 1 hour for an experienced Django core engineer. Additionally, relevant tests are concise and only minimally extended, reflecting limited surface affected.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue requires some familiarity with Django\u2019s middleware system, especially in async contexts, but does not require deep architectural changes or multi-module coordination. The sample is appropriate for benchmarking since it requires careful reading, understanding exception-driven control flow, and a small but non-trivial refactor, all within reasonable scope for an assessment. There are no other major blockers or concerns for its use in coding ability evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14017": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description identifies a concrete problem: combining Q objects and Exists objects using & or | operators in Django ORM raises a TypeError when the Q comes first (Q(...) & Exists(...)), but not when Exists comes first. It explicitly states the desired behavior (commutative for & and | between Q and Exists). It also gives a likely direct cause (missing __rand__). However, the issue does not specify the full set of expected behaviors, mention relevant versions, or give code examples or affected files. Still, an experienced engineer familiar with the Django ORM would reasonably understand what is required and could meaningfully attempt a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is relatively localized (regarding operator overloading and class interactions), and the fix in the reference patch is indeed small\u2014a minor logic tweak in one method. An experienced Django developer could probably reason about and patch this quickly after reviewing the relevant classes, so 15 min\u20131 hour is appropriate.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no significant blockers for using this issue for coding ability evaluation, assuming the engineer understands Django's Q and Exists constructs. The scenario has a definite functional requirement and an observable failure (TypeError) and asks for making the operator handling consistent/commutative. No additional issues found.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14140": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a reasonably clear statement of both the bug and the desired outcome. It states that Q objects with one child are treated differently during deconstruction, leading to errors when the child is a non-subscriptable boolean expression. It references an existing patch (though we are told not to click through) and describes a possible approach for backward compatibility. However, there are some ambiguities: it assumes familiarity with the Django Q object internal logic, and does not cite specific places in the code, nor does it clearly state how test coverage should look or all the possible edge cases. An experienced engineer could confidently infer the required behavior (treat all single-child Q objects consistently in deconstruct, regardless of type) and the success criteria (no crash when combining Q() objects with boolean expressions), but would need to investigate precise implementation details in the codebase. So, while some extrapolation or research is required, the intention and direction for a fix are mostly clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch reveals a targeted change to a single function in one file, removing the special-case handling of single-child Q objects and cleaning up the code. The fix is conceptually simple but requires enough understanding of Q object deconstruction, boolean expressions in Django queries, and backward compatibility concerns to do carefully. While non-trivial, this is the kind of issue that could be understood, researched, and fixed by an experienced Django engineer in under an hour, especially since the required change mostly involves removing a conditional branch and ensuring consistency. The additional test changes suggest some diligence is needed to cover cases, but the fix itself is a focused logic adjustment.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The main risk is that the issue presumes some knowledge of Django internals, and the Q object mechanics are a bit advanced for someone with only surface-level Django ORM experience. However, for the stated evaluation context (engineer familiarizing themselves with the codebase), this is acceptable. No other major issues are apparent\u2014the problem is concrete and testable.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14238": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It describes a specific bug where specifying a custom AutoField subclass as DEFAULT_AUTO_FIELD fails due to an overly strict subclass check in Django. It even refers to the precise area to modify (AutoFieldMeta.__subclasscheck__). The expected fix is described in implementation terms ('allowing subclasses of those classes in the _subclasses property'), making the direction for the solution clear. An experienced engineer with access to the codebase could localize the problem and enact a targeted fix using just this description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The diff shows that the fix requires a small code change, replacing a membership check with issubclass in a metaclass. While understanding Python metaclasses can be a bit specialized, the change itself is simple once the area is located. The related test changes are minor and straightforward extensions. An experienced engineer would likely take 15-60 minutes to complete this, including verifying the tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues preventing this sample from being used in the benchmark. The issue text, code change, and tests are clearly related and traceable, and the engineering task exercises both understanding of Django's ORM internals and Python's class model/metaclasses at a basic level. This is an appropriately scoped real-world issue for assessing coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14351": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified for an engineer familiar with Django ORM internals. It describes a specific error ('subquery must return only one column') that occurs when using Q objects in a filter with certain related lookups (agent__property_groups__in vs agent__property_groups__id__in), and notes a change in behavior between Django versions. This clearly indicates the undesired difference in how query columns are selected and provides enough detail about the source of the breakage, although it lacks direct code samples or exact context about the model structure. While some environment exploration or guessing about the intended correct behavior is needed, the root problem is identifiable for someone with Django experience.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Solving this requires moderate expertise with Django's ORM and query construction internals. The engineer must locate where related lookups and Q objects interact and understand changes in column selection logic between versions. The patch shows that this involves adding a new method and handling group-by columns specifically in subquery contexts\u2014so non-trivial investigation, reasoning, and careful patching are needed. This is less routine than a basic bug fix and could take multiple hours (1-4) even with the codebase available.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent with this sample for use in a coding benchmark. The issue is clear enough for someone experienced with Django, and there is a sufficiently direct mapping between issue, patch, and test.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14493": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description points to a bug in ManifestStaticFilesStorage when max_post_process_passes is set to 0, specifically referencing a block of code (with a GitHub permalink) where 'substitutions' is only set if the loop is entered at least once. However, the description assumes the reader is already familiar with why setting max_post_process_passes=0 is desirable and what 'invalid CSS' refers to (with further information only available through referenced links, which are not part of our setup). The description does not spell out what failure or crash occurs, the expected behavior, or what a valid fix should be in clear terms. The actual code and place to look is given but without context, which means someone new to this part of the codebase would have to infer the intent and desired effect. Thus, while not totally opaque, the issue is vague and leaves room for ambiguity and interpretation. This corresponds to a '2'.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual code fix is a very small, straightforward one-line addition, which would be clear to someone who fully understood the code path and state invariants, and recognized the problem (i.e., uninitialized variable when max_post_process_passes=0). However, getting to that point may require some investigation for someone less familiar with the subtleties, but for an experienced engineer, the fix itself and locating the spot wouldn't exceed an hour of focused work.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Despite the vagueness, the test patch shows this problem can be diagnosed and fixed with minimal changes. However, the main risk here is the lack of clarity in the issue text for someone not already expert in this part of Django, but these are common ambiguities in production bug reports and testable for engineering diagnosis skills.\",\"q2_5_confidence\":4}"
    },
    {
        "django__django-14580": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue text provides a clear description of the bug: generated migrations omit the required import for 'models.Model', causing a NameError. It explains under what conditions the bug occurs (when using custom fields and mixins), the actual and expected behavior, and suggests the part of the codebase likely at fault (django.db.migrations.writer). An experienced engineer would know to ensure migrations generate valid Python code and include all necessary imports. There may be minor gaps (e.g., the specific serialization mechanism for types), but the expected outcome\u2014a valid migration that runs without NameError\u2014is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires identifying where type serializations for model classes happen within the migration writing code and updating it to ensure the relevant import is included. This is a targeted change\u2014likely only a few lines\u2014within a relatively well-contained area of the codebase. While some knowledge of Django's migration internals may help, the debugging and modification for this specific scenario (missing import for 'models.Model') is not complex once the precise area is located.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present that should disqualify this sample. It is suitable for evaluating coding ability, as the task is clearly specified and can be tested directly using migration generation and validation.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14672": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clear about what is required for a successful solution. It provides sufficient context: a hash is derived from the identity property of ForeignObjectRel, and for ManyToManyRel, the identity tuple is missing a call to make_hashable for through_fields, which can be a list and needs to be hashable. The solution is also spelled out: add a call to make_hashable on self.through_fields in ManyToManyRel. An experienced software engineer familiar with the codebase could reasonably locate the relevant logic in reverse_related.py, identify the identity property, and update it as described. There is enough provided context in the issue for a concrete, actionable implementation. No external information or clarification is strictly required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"I rated the difficulty as 1 (15 min \u2013 1 hour) because this is a small targeted bugfix. The issue pinpoints the missing functionality\u2014adding make_hashable to through_fields in the identity property of ManyToManyRel. The solution only involves updating a single line in an existing method, with no substantial rewrites or design decisions required. The changes to tests mainly involve adding or tweaking models to explicitly check hashability when through_fields is a list. For someone familiar with Django's ORM internals and the test suite layout, this is a straightforward task that could be completed quickly, with most of the time likely spent verifying and running the relevant tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major additional issues. The issue is specific, actionable, and the required change can be made and tested based on the information given. Test cases and code context are implied to be available, and there are no blockers such as ambiguous requirements or platform-specific dependencies. This makes the sample suitable for evaluating coding ability as described.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-14787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue describes a problem where method_decorator does not preserve wrapper assignments (such as __name__ and __module__) because it wraps a partial object rather than the original function. The description provides a concrete symptom and names the relevant function (method_decorator), but it does not specify the full context or the desired mechanism for preserving these attributes. However, for an experienced engineer, it is clear what property is missing (preservation of standard function metadata) and where in the code to start looking, so a sensible interpretation is feasible. There are blanks to fill in (e.g., which attributes, how to restore them), but the expected outcome is interpretable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a fix that focuses on the proper application of functools.wraps at the correct point in a decorator utility. The code change itself is small and conceptually clear once the problem is understood; it only requires the addition of a wraps() call to bind the appropriate function metadata to the partial object being created. Familiarity with decorators and function introspection in Python is needed, but the fix should take an experienced engineer less than one hour, including time to understand the issue and implement tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blockers or issues are present. Although the issue could be better specified (e.g., with a motivating code sample or error message), it is sufficiently clear for a strong engineer familiar with decorators and Python internals to solve it. The provided patch and test confirm that only a small code change and corresponding test coverage are needed.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15104": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well specified. The reporter provides context about encountering a KeyError when running the migration autodetector on a Django project with a custom ForeignKey field. They detail that the field hardcodes its 'to' argument and omits it from the deconstructed kwargs, which causes the autodetector to fail. They even suggest the precise line and the fix that solves their problem: replacing 'del deconstruction[2][\\\"to\\\"]' with 'deconstruction[2].pop(\\\"to\\\", None)'. However, the issue is somewhat underspecified for someone unfamiliar with the migration deconstruction process or the details of custom ForeignKey fields; it assumes a fair bit of prior knowledge and doesn't specify the expected behavior in all cases. Still, it lays out a sensible interpretation and steps toward a solution, though there are minor blanks to fill in.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is moderate (15 min \u2013 1 hour) for an experienced engineer familiar with Django. The fix is a one-line code change, but requires understanding the migration autodetector, the custom ForeignKey logic, and why the KeyError occurs. Tracing through to deconstruction and the logic of the fix takes a bit of investigation, but it doesn't require rewriting large sections or dealing with complex logic.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues preventing the use of this sample for evaluating coding ability. The issue text is a bit informal and assumes the reader is familiar with Django migration internals, but it is concrete enough for someone experienced to deduce and implement the solution.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15128": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear, precise, and specifies both the trigger (AssertionError during Query.combine with colliding aliases) and the expectation for the fix: ensure new aliases do not intersect with existing ones, suggesting a possible implementation and asking for documentation updates to clarify non-commutativity. The relevant methods (Query.combine, Query.table_alias, Query.join, change_aliases) are named directly. The description provides enough technical depth to make it straightforward for an engineer familiar with the codebase and Django's ORM to understand the required solution. No further clarification is necessary to attempt a fix.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"While the patch is not extremely large, it requires a deep understanding of Django's SQL generation, aliasing, and QuerySet combination semantics (including handling collisions and ensuring non-commutativity documentation). The solution touches multiple functions and needs careful changes to avoid subtle regressions in complex ORM operation scenarios. Writing appropriate tests to cover the edge case also requires setup and familiarity with Django testing patterns. For an experienced Django contributor, this might take 1-4 hours between understanding, implementing, and verifying.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues that would prevent this sample from being used as a coding benchmark. The problem, although advanced, is well-specified and tests are possible. The only minor note is that a strong understanding of the ORM and aliasing is required, making it less suitable for a beginner task, but that's not a disqualifier for the benchmark purpose.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15277": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is very well specified. It both motivates ('Currently...') and describes precisely the performance problem with how CharField.__init__ always adds a MaxLengthValidator, even when max_length is None. It gives specific context for where this occurs (Value._resolve_output_field), illustrates current behavior with inline pseudo-code, and suggests a concrete, clear solution: only add the validator if self.max_length is not None. The effect and intent are unambiguous, and it is clear that the solution should update CharField.__init__ to mirror BinaryField.__init__'s handling of validators. There are no unclear requirements, and an engineer would know exactly what change is expected and why.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change requires some familiarity with the Django codebase to find the relevant places, particularly CharField and its validator logic. However, the code change itself is straightforward: it is just a 1-2 line conditional. Testing the change may take additional time to ensure no side effects, but no complex rewriting or deep algorithmic thinking is required. This is a small change that requires a little careful attention, but is not trivial. For an experienced engineer, this would likely take 15-60 minutes including context review, implementation, and validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would block use of this sample in the benchmark. The requirements, context, and rationale are clear. The problem and solution are self-contained and verifiable via test changes, making this issue suitable for evaluating coding ability. The required code context is standard for contributors to Django or similar frameworks.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15280": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified for an experienced Django developer. It provides concrete model definitions for User and Profile, describes the precise queryset operation (using only and prefetch_related with nested Prefetches), and details the unexpected behavior (a database query occurring for a field that should have been prefetched). The issue explains that 'kind' is still being deferred on the nested User despite prefetching. It gives an example of the affected models and queries and makes clear what is failing; it expects that prefetching should correctly hydrate the nested objects and not defer fields unnecessarily. However, there are a few blanks: the developer will have to make an educated guess about the test scenario (using similar models or others, as in the patch), and the exact method of solution is up to interpretation. Overall, the requirements are reasonably clear about 'what' needs to be fixed but require the implementer to interpret 'how' to fix it and test it. Thus, it merits a score of 1.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue involves a nuanced aspect of Django's ORM, specifically the interaction between 'only', deferred fields, and multi-level prefetching. Understanding exactly why the problem happens (fields being incorrectly deferred across nested prefetches) requires knowledge of how Django caches and hydrates objects in related querysets, and familiarity with the prefetch framework's internals. Fixing it requires a careful, non-trivial change to how related objects are assigned during the prefetch process (as shown in the patch). It also necessitates writing or updating tests to handle the edge case and ensure no regressions occur. For a developer familiar with the codebase, this is a non-trivial 1-4 hour task, as it requires careful reading, debugging, and possibly running test cases to validate the change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no disqualifying issues with this sample; the models are specified, and the query is explicit. The missing piece is only the precise construction of the test, but given the audience's expertise, it is reasonable to expect them to fill in the blanks. The scenario is suitable for evaluating coding ability in Django ORM internals and understanding of deferred fields and prefetching logic.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15315": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is clear and well-specified. It describes a problem with the immutability of the hash value of Field objects in the Django ORM. Specifically, it explains that when a Field instance is assigned to a model class, its __hash__ value changes, which is undesirable because an object's hash should remain constant during its lifetime to ensure correct behavior when used as a key in dictionaries or sets. The issue points to a specific commit where the problem was introduced and suggests reverting part of that change. The requirements for a correct solution are unambiguous: ensure that Field.__hash__ is immutable regardless of assignment to model classes. This gives a concrete direction for implementation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a good understanding of how hashing and object identity work in Python and familiarity with the Django ORM system. However, the actual code change is relatively simple: it involves ensuring that the __hash__ method depends only on immutable or intrinsic attributes (here, just the creation_counter). Updating or adding an appropriate test is straightforward once the desired behavior is understood. For an experienced engineer, this fix and test update would take between 15 minutes and 1 hour, predominantly for analysis and validation rather than code writing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are present. The sample is suitable for this coding ability benchmark, as the problem is clear, tractable, and the solution can be objectively tested.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15375": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is somewhat well-specified, in that it describes precisely the operation sequence (aggregate() with 'default' after annotate()) and the error (crash) that occurs, including the context (Django 4.0.1, both PostgreSQL and SQLite). It also explains that using Coalesce directly works, but aggregate classes' new default arg (added in Django 4.0) does not. However, the issue does not provide details like the exact traceback, error message, or concrete code examples of what fails, nor what the expected output is. An engineer with domain knowledge (familiar with Django ORM/aggregation) could interpret this sensibly and attempt a fix, but there are still gaps, especially for someone not deeply familiar with Django's recent changes.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change is quite targeted, involving only a few lines in the aggregation logic to ensure that the `is_summary` attribute is correctly retained when wrapping an aggregate in a Coalesce as part of the fix for supporting the new default behaviour after annotation. The conceptual challenge lies in understanding how the new default parameter interacts with the annotation/aggregation machinery in Django 4.0+, but does not require rewriting major portions of code or affecting a large number of files. The test fix demonstrates new passing test cases that validate the specific bugs fixed. This is a small but non-trivial patch for a framework's ORM expert; likely solvable in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues to prevent this sample from being usable in evaluating coding ability. The only limitation is that some Django ORM-specific knowledge is needed to fully grasp the problem and solution. The spec could be improved by including code samples or tracebacks, but as it stands, it's usable though not flawless.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15380": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description indicates that the migration autodetector crashes when both a model and a field are renamed in a single migration step. While it doesn't provide a stack trace, error message, or specific usage scenario, it is explicit enough about the condition to reproduce: a model and one of its fields are being renamed at the same time, and this crashes the migration autodetector. An experienced engineer with access to the codebase should be able to construct a test case and reason about the code paths involved. However, more context (error message, minimal reproduction) would make it strictly well specified, so there are some blanks to fill but the intent is clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Diagnosing and resolving this issue likely requires a solid understanding of how Django's migration autodetector tracks changes across models and fields, especially concerning renames. The fix involves careful reasoning about state management and object references during the migration process, as well as writing/adjusting tests. While the change itself is small, arriving at the correct change and ensuring all edge-cases are covered requires substantial investigation and validation, likely taking 1\u20134 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers or issues that would preclude use in the benchmark other than typical expectations about codebase familiarity. The sample tests for expected behavior and the described bug is conceptually clear even if the minimal reproduction information is somewhat sparse. It is a reasonable case for measuring real-world debugging and targeted code changes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15503": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it describes a concrete bug in Django's JSONField 'has_key' (as well as 'has_keys' and 'has_any_keys') lookups when the key being queried is a number, and it explicitly notes that the problem occurs on SQLite but not PostgreSQL. The issue provides enough version and environmental info to reproduce the bug. However, details about what \\\"fails to find the keys\\\" actually means are not completely explicit (for example, does it fail for all integer-like string keys or only actual numeric keys, and whether the missing logic is in lookup SQL generation, Python side, or both). Also, \\\"has_keys\\\" and \\\"has_any_keys\\\" are mentioned in the title but only \\\"has_key\\\" is described in the text, so some minor inference is needed when interpreting the extent of the required fix. Still, any experienced Django developer would make sensible choices for a solution based on this description.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This is a moderately difficult issue. Understanding and fixing it requires: (1) digging into how Django generates SQL/expressions for JSONField lookups on different backends, (2) determining where the type mismatch or SQL generation bug occurs, and (3) ensuring consistency of key querying for numeric keys across all backends. The fix entails modifying logic in core field/lookup code and adding relevant tests. While not a huge patch, this is definitely not a trivial change; it requires careful reasoning about cross-database behavior and some depth of knowledge of Django's ORM internals. For an experienced engineer familiar with Django, it's a solid 1\u20134 hours with time to write tests and understand all ramifications.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues observed with the sample. The test patch covers the described scenario and the issue is narrowly scoped to a concrete, reproducible problem with specific symptoms. The only possible minor caveat is that the issue could be made even clearer by adding expected vs actual result examples, but the current level of detail is sufficient for an experienced engineer. The sample is suitable for evaluation purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15525": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text is written by someone who has encountered different behavior when using loaddata with natural keys and foreign keys across multiple Django databases. The problem context, underlying models, and the behavior discrepancy are all described. The provided minimal code and description make the core problem and a sensible target for a fix clear: loaddata works for the default DB but fails for non-default DBs in a setup with foreign key dependencies through natural keys. While the issue doesn't describe the root cause or exact desired change, a knowledgeable Django engineer should be able to infer what success means: loaddata should work the same on all databases. Some implementation choices (e.g., whether to patch serializers, how to structure the fix) are left for the engineer, but the \\\"what\\\" is answerable and reasonably specified. Thus, this is level 1: some blank-filling, but a clear sensible direction.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch is focused on ensuring that the database ('db') state is correctly set before invoking the natural key logic when deserializing/serializing models. The main code fix is a small, but non-trivial change (setting obj._state.db before calling natural_key). The engineer would need to reason about model state and deserialization, but the underlying issue is localized. Additionally, the test work is not extensive and mostly about adding coverage for multi-DB/natural key cases. Therefore, an experienced Django engineer could reproduce this in about 15-60 minutes, depending on codebase familiarity.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blocker or confounding issue is evident from the sample for use in the benchmark. The issue is understandable, demonstrates a realistic failure scenario, and the scope of the fix is reasonable. This sample appears suitable for a coding evaluation in the specified setup.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15695": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. It describes a crash in RenameIndex() when moving an unnamed index backward and forward. It clarifies that RenameIndex() should restore the old auto-generated name when this scenario happens, and that the crash occurs upon re-applying RenameIndex(). However, it does assume some familiarity with Django migration internals, as it references unique_together, unnamed indexes, and the behavior of auto-generated index names. While the expectation is mostly clear (prevent crash, restore correct name), an engineer may need to fill in some blanks about the exact expected behaviors for edge cases. The required changes are focused and sensible to interpret, but not as explicit as an ideal bug report with reproducible steps and input/output examples.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code fix is quite targeted once the right location is found: it essentially involves guarding against a no-op when the old and new index names match. This requires some thought about when and how index names are generated, as well as understanding the migration system's expectations. However, it's a straightforward logic fix (add a condition and return early) that shouldn't take much time once the engineer understands the affected methods and the scenario. It's slightly more involved due to the need to grasp some Django internals, but still within the 15 min to 1 hour range for a competent engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues identified. The sample is suitable: it requires some reasoning but not major guesswork, and covers a clearly-scoped bug in migration logic. The benchmark tests reliably check the resolution through code and test changes.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15732": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly technical and assumes familiarity with Django migrations and database constraints (unique and unique_together), but it does convey the essential problem: a unique_together constraint is present on a field that already has a unique or primary key constraint, and migrations cannot drop the unique_together constraint as expected. There's some ambiguity in terms of exactly which code or API must be changed, and the user hints at the underlying implementation details (Django migration internals, constraint detection), but the scenario and problem are clear for someone familiar with Django internals. The main gap is that it doesn't specify the desired behavior in a pseudo-spec or direct instruction form, but it does describe what goes wrong and the environment (PostgreSQL).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix involves understanding Django's migration and schema editor internals, specifically how unique constraints and unique_together constraints are tracked and deleted. The change requires careful manipulation of database constraint metadata, understanding the interaction between multiple constraints on the same field (primary key, unique, unique_together), and ensuring the altered logic is correct across supported databases. Editing multiple methods and ensuring a correct fix that does not introduce regressions requires careful attention and testing, likely taking 1-4 hours.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues are present. The bug, while a somewhat deep framework issue, is adequately testable, and the tests do a good job covering the edge cases involved (primary key field and plain unique field with redundant unique_together constraints). This makes it suitable for a coding benchmark targeting experienced engineers with Django or db-migrations experience.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15814": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well specified. It clearly states the problem encountered when calling QuerySet.only() after select_related() on proxy models in Django, and it points out the line in django/db/models/sql/query.py where the code fails, as well as what minimal code change ('opts = cur_model._meta.concrete_model._meta') resolves it. However, the description could be improved with a specific, minimal reproducible example or a step-by-step guide to reproducing the issue. While the context and fix are clear enough for a Django engineer familiar with ORM internals, someone unfamiliar with proxy models or the deferred fields logic may need to do additional investigation. Nevertheless, it is possible to produce a reasonable fix based on the information given.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change to the Django codebase, requiring the addition of a single line to ensure the code uses the concrete model's _meta. It is not trivial, as it requires some understanding of Django's ORM and proxy model mechanics, but it is limited in scope and local to a single function. An experienced Django engineer could likely understand and patch this in less than an hour once familiar with the context.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues beyond what has been discussed. The test provided in the patch gives a concrete usage scenario, which helps supplement the specification. This issue is suitable for evaluating coding ability, particularly knowledge of object-relational mappers and model inheritance.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15930": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description states that Case() crashes when used with ~Q(pk__in=[]). While the context is geared to someone with Django ORM experience, the description makes it clear what triggers the crash and why this pattern (~Q(pk__in=[])) might appear (as a sentinel value). It gives a concise minimal reproduction condition, and hints at expected behavior (it should not crash). While the precise error or desired result is not fully specified, it is sensible to interpret the goal as 'make Case() handle this predicate without crashing.' An experienced engineer can meaningfully attempt a solution based on this information, but there are some blanks, such as what the correct behavior should be (should Case() treat ~Q(pk__in=[]) as always true/false, or another way). Thus, I rate it as a 1: Some blanks, but a sensible interpretation is possible.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual fix is relatively small: it involves checking for an empty condition SQL fragment and substituting it with a predicate that is always true. This tweak is conceptually simple, though it requires understanding Django's conditional expression handling and when WHERE clauses may be empty. Locating the correct place for this logic and validating the fix with tests would likely take 15-60 minutes for someone familiar with the codebase. The scope of changes is small.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other issues present. The sample is appropriate for evaluating ability to interpret technical bug reports and patch conditional logic in a mature Python codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15957": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear context of the problem: using Django's Prefetch objects does not work as expected when combined with sliced querysets, leading to inefficiencies when one wants to select only a limited subset of related objects per parent. The user provides a use case (showing a few objects per category rather than all), and highlights a lack of both documentation and functionality. However, the description does not detail the exact expected API or behavior changes, nor is it specific about edge cases (e.g., how slicing interacts with prefetch in all ORM relationships). For a Django engineer, the problem is interpretable (make Prefetch respect slicing), but requires some assumptions. Thus, it is \\\"some blanks to fill in\\\" (well suited to a level 1 rating).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires a solid understanding of Django ORM internals, particularly how prefetch_related works in combination with slicing. The solution involves writing a helper function to filter the queryset accordingly, and ensuring compatibility with both forward and reverse relations, all while being tested against various edge cases using multiple test scenarios. It affects non-trivial codepaths, involves using advanced ORM features (Window, RowNumber, Q objects), and requires integration with both code and tests. Although not requiring major refactoring or new module creation, it is substantially more complex than a simple bugfix and would likely take several hours for an engineer to get right, especially considering testing and possible regressions.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The sample is appropriate for benchmarking coding ability in the context of Django ORM. The problem is self-contained, meaningful, and accompanied by comprehensive tests in the provided patch. However, some degree of experience with Django's ORM internals is assumed for a realistic attempt.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-15973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides details about three apps ('fonte', 'fonte_variavel', 'variavel') and describes a specific AttributeError during migrations that only occurs when the through model for a many-to-many relationship is defined in a separate app. It mentions that defining the model in the same file or app avoids the issue. While the text does not precisely specify what the correct behavior should be (e.g., it doesn't state the migration should proceed without error regardless of model app location), it is clear from context that the goal is to enable migrations to work correctly even when the through model is in a different app. An experienced engineer can reasonably infer the intended outcome: fixing Django's migration behavior so that such cases do not error. Some specifics (like exact file/class names) are not given, but the scenario is well-motivated and clear enough for a targeted fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required fix involves changing the migration autodetector to properly resolve the through model reference when it is defined in a different app. This requires understanding of Django's migration system and foreign key/model resolution, but the code change is relatively small (as indicated by the patch). Implementing, testing, and verifying this should fit within an hour for a familiar engineer. The difficulty comes from needing to trace how the through argument is handled across apps during migrations, not from extensive code rework.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is a reasonable candidate for a coding benchmark. The issue is not overly ambiguous, and it targets a subtle but practical problem in Django's migration framework. The description could be improved with a minimal reproducible example, but the test patch provides meaningful context for validation. No additional blockers are identified.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16032": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is somewhat concise but assumes a high level of familiarity with Django ORM internals, particularly how select fields are managed with QuerySet.alias() and annotate(). While the title and single-line description point to a flaw (fields on the RHS of an __in filter are not cleared as expected if alias is used after annotate), it lacks detailed motivation, context, or detailed reproduction steps. However, a well-informed Django contributor or core developer could make a sensible interpretation: there is an interplay issue when using annotate() followed by alias(), leading to incorrect behavior when filtering with __in. The required solution is to fix this interaction so select fields are cleared appropriately. Concrete examples would strengthen the issue, but the presence of test code (in the PR) helps clarify the expectation after-the-fact. Overall, the issue is not entirely self-contained or beginner-friendly, but clear enough for an experienced engineer familiar with the domain to attempt a solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Fixing this issue requires an in-depth understanding of the Django ORM and its internals (QuerySet, aliasing, annotation, and field selection mechanics). The fix itself touches non-trivial code paths within the core of Django's query construction machinery. Additionally, determining where and how fields should be cleared--and verifying it does not break other code--would likely require reading surrounding logic and testing various scenarios. While the eventual patch is not particularly large, it is subtle and relates to intricate behavior. Therefore, this is a solid 1-4 hour level issue, especially given the need to ensure fixes are correct and do not cause regressions.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers for inclusion in a benchmark, beyond the standard high-level domain knowledge expected for Django ORM work. The issue could have been more self-contained, but is sufficient for the target audience of an experienced engineer.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16136": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a clear minimal reproduction: defining a View subclass with only an async 'post' leads to a crash when a GET request occurs, because the object 'HttpResponseNotAllowed' cannot be used in 'await'. It details the symptom (an exception with a specific error) and the condition (async View with only one method overridden). However, it does not specify exact code, expected behavior, or context about the view or framework, but a knowledgeable Django dev can reasonably infer that http_method_not_allowed must work correctly in both sync and async Views, and that the crash should be prevented for unimplemented method handling. Thus there are some blanks (e.g., whether response should be made awaitable or if method should be async), but it's enough to guide a sensible and specific patch.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires understanding Django's generic View internals, the sync/async boundary, and adapting 'http_method_not_allowed' to support both usage patterns. The code patch is small (conditionally wrap the response in an awaitable coroutine), but requires familiarity with Django's View classes and Python async. For an experienced engineer, this would likely take between 15 minutes to an hour, including a brief investigation to confirm the root cause and test the change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues present. The issue is narrowly scoped to one function/method and addresses a specific bug. It is a suitable benchmark: it checks for understanding of async/sync boundaries and careful handling in framework code.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16429": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it explicitly describes the error (TypeError due to mismatch between offset-naive and offset-aware datetimes in timesince() with USE_TZ=True for datetimes over 1 month ago), the relevant file (django/utils/timesince.py), and the precise cause (pivot datetime constructed without accounting for input's tzinfo). It even specifies the fix: adding the input datetime\u2019s tzinfo to the pivot call. An engineer would have clear, actionable information and a good sense of both where and how to fix the problem. No ambiguities exist in terms of replicating or solving the core problem.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a targeted bug fix in a single utility function. While it requires understanding the context (timezone-awareness in Django), the problem is clearly described, and the fix is well scoped (adding tzinfo to a specific datetime construction). For an engineer familiar with Django's timezone conventions, this is a small change, though verifying the fix may require running existing/fixed tests. This level of difficulty lands squarely in the 15 min - 1 hour range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further concerns for benchmark inclusion. The issue is self-contained, directly checks for correctness by surface symptoms (TypeError), and does not depend on external context or require additional information to solve. This is well-suited for evaluating coding/debugging ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16454": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and clearly describes both the problem and the required high-level correction. It explains that Django management command subparsers, created via CommandParser.add_subparsers(), do not inherit the improved error formatting intended by CommandParser. As a result, missing arguments to subparsers produce stack traces instead of user-friendly error messages. The issue further specifies that the subparser action returned by add_subparsers() needs to copy relevant arguments to constructed subparsers to resolve this issue. An experienced engineer familiar with argparse and Django's management command internals would understand the affected area (CommandParser and its add_subparsers method), the required fix (ensuring subparsers inherit error formatting arguments), and the desired end user effect (better error reporting for subcommands). The description contains enough detail to make the expectations for a successful solution clear without needing further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue likely requires reading and understanding how Django's CommandParser works and how subparsers are currently constructed. The solution involves overriding or modifying CommandParser.add_subparsers() so that the relevant arguments (such as called_from_command_line) are passed down to constructed subparsers. The gold patch shows this is a focused, targeted change (a short method override and use of functools.partial), and does not require major rewrites or editing many files. There is a modest level of reasoning needed about argparse and how subclassing and argument passing works, but the change would take 15 minutes to 1 hour for an engineer familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues preventing this issue from being suitable for benchmarking coding ability. The problem is practical, the expected output and effect are clear, and the solution should be independently implementable by an engineer with relevant experience.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16485": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description \\\"floatformat() crashes on '0.00'.\\\" is somewhat minimal, but it still specifies a concrete observable bug (a crash) with a particular input to a named function in the codebase. As an experienced engineer, it is reasonable to interpret that providing the string '0.00' to the floatformat() function with default or typical arguments leads to a crash, and that the goal is to fix the function such that it no longer crashes in this case. While more details (tracebacks, context, expectation for returned value) would be ideal, it is still sensible to attempt a fix with the information given. Therefore, this is a level 1 issue: some blanks, but a sensible interpretation and clear next steps for the engineer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires identifying the crash that occurs in floatformat when passed '0.00', understanding why, and making a small but correct change to prevent it. For an experienced engineer familiar with the codebase, this would likely take 15-60 minutes: the bug is localized, the function is named, and the error scenario ('0.00') is specific. The fix itself is a simple logic adjustment, as seen in the patch. The most time-consuming part is likely reproducing and understanding the crash.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major problems with using this issue for benchmarking: the function is clearly identified, input is concrete, and the expected behavior can be inferred from the context. While it would be better to explicitly state the desired output, the main requirement (no crash) is clear. The brevity of the report is a minor challenge but not a blocker.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16569": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is very well-specified. It precisely describes the context (the add_fields() method of a FormSet with certain can_delete/can_delete_extra attributes), under what circumstances the problem arises (calling add_fields with index=None, particularly via empty_form), what the symptom is (TypeError when index is compared to initial_form_count without checking for None), and offers a concrete example for how the condition should be modified. Although it references some Django formset internals (which would require a little codebase familiarity), it gives enough information to both understand and implement a fix without ambiguities.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a minor logic fix requiring a small conditional change in an existing method, guarded by a check for None. The issue is easily reproducible, clearly explained, and the area of code to change is indicated directly. For an experienced engineer with some codebase familiarity, it is straightforward: locate the method, update the condition, check relevant tests. The patch itself is minimal.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is well-suited for use in a benchmark evaluating coding ability. It tests the engineer's attention to detail and understanding of conditional logic in code. No issues beyond the small scope of the described bug.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16667": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description concisely and clearly explains both the bug (OverflowError in SelectDateWidget.value_from_datadict when extremely large year, month, or day values are passed) and the area in which it occurs (form validation, specifically in SelectDateWidget.value_from_datadict). The problem is described as inputs causing OverflowError in datetime.date due to integer overflow, and the cause (user-controlled values passed without overflow checks) is stated. The text specifies that supplying extremely large integer values triggers datetime.date's OverflowError. For an engineer familiar with Django forms, this directs attention to adding overflow checks or handling the raised error in the indicated function. The requirement\u2014prevent the crash due to OverflowError when parsing date inputs in this widget\u2014is unambiguous and actionable for a fix with minimal need for interpretation. No further information is needed to attempt a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code change affects only a targeted part of the codebase (handling user-provided year, month, and day values when constructing a date). The required solution\u2014handling OverflowError in SelectDateWidget.value_from_datadict, likely via try/except or input validation\u2014is relatively straightforward for someone familiar with Python and Django forms. The fix consists of adding a brief error check and ensuring a fallback value is returned, making it a small change with little complexity. The changes are localized to a single function and do not require broad knowledge of the system or large-scale refactoring. Some basic familiarity with Python exceptions and Django forms is necessary but the time to implement and test should remain comfortably under one hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The issue is clearly specified, amenable to direct testing, and solutions can be unambiguously validated based on whether OverflowError is avoided and appropriate error behavior is enacted. There are no architectural or documentation concerns, nor are there dependencies on unspecified design decisions. The tests confirm the requirements for a successful patch. Overall, a strong candidate for evaluation of coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "django__django-16950": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified, but there are some blanks to fill in. The author describes a concrete use case involving a Django abstract base model with an auto primary key and a UUIDField default, a Thing model, and a SubThing model with a ForeignKey to Thing\u2019s UUID. The problem occurs when creating a Thing and its SubThing in the Django admin via inlines: the Thing\u2019s UUIDField default is not being applied, resulting in null IDs. While the issue provides enough specifics (model structure, admin configuration, problematic case), it does not show the actual model code or admin registration, leaving the engineer to infer the likely structure and intended Django patterns. The high-level requirements are clear: fix the behavior so that the default on Thing.UUIDField is correctly applied when using inlines in the admin. The scenario can be reasonably reconstructed by an engineer familiar with Django, but there is a small amount of ambiguity around the exact model setup and Django version quirks.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue likely requires substantial understanding of the Django admin, the form/model interaction, and the handling of default values in relation to how inlines operate. The fix is not trivial: it involves changing the core way in which related models (with UUID PKs) and default values are handled during admin inline form creation. An engineer would likely need to read through Django\u2019s model/form logic, reproduce the problem in a minimal test case, and test several edge cases to ensure no regressions. This depth of investigation and modification typically takes 1-4 hours, especially for someone new to this part of Django\u2019s codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There do not appear to be any disqualifying issues that would prevent this sample from being used to evaluate coding ability. The scenario is well-motivated, rooted in real Django usage, and sufficiently detailed to allow reasonable attempts at a solution. The test patch further clarifies the edge cases. The only minor caveat is that deep Django internals knowledge is required\u2014this could make the sample less accessible to those unfamiliar with framework internals, but it is not disqualifying.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-20859": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. The author reports that adding a legend to a SubFigure does not work, expects that it should, and suggests a concrete change: updating a type check to use 'FigureBase' instead of 'Figure' at a specific line (L437). There's enough context to identify what part of the codebase to inspect (the legend-creating logic in matplotlib), and a suggested fix makes it clear what is being targeted. However, there isn't an explicit code example showing the failure or steps to reproduce, and the reasons for the type check are only implied (not fully explained), so the engineer must fill in some implementation details and verify that the tests cover the general intent. Thus, there are some blanks to fill in, but the high-level goal is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively minor change but does require a bit of thought. The fix is changing a type check from 'isinstance(parent, Figure)' to 'isinstance(parent, FigureBase)', and updating an error message. The engineer must understand the object model in matplotlib (that SubFigures inherit from FigureBase but not Figure) and must ensure that this doesn't have unintended side-effects. The solution is small but requires understanding the relevant classes and interactions, so it may take around 15-60 minutes for an experienced engineer familiarizing themselves with the codebase to make, test, and verify the fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues with using this sample in a benchmark for coding ability. The scenario is realistic, the issue is clear enough, and the required fix tests both the ability to navigate an object hierarchy and to properly patch and test a widely used library. The test provided is clear and directly checks the behavior in question.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-22719": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified: it indicates a specific deprecation warning is being triggered in Matplotlib when empty data is passed to an axis using string unit converters. The description includes expected behavior (either produce empty artists or a more accurate warning), and points out where in the implementation the issue likely resides (broad exception catching and a warning that should not occur). Although the exact location in the code is not pinpointed and some domain familiarity is assumed (understanding unit converters and Matplotlib's warning system), an experienced developer could reasonably interpret that the fix should prevent the deprecation warning when empty data is used, by refining exception handling or the relevant logic. Thus, there are some blanks to fill, but the requirements are sensible and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The primary challenge is understanding the logic for when the deprecation warning is issued, and modifying it to avoid warnings on empty data. For a developer familiar with Matplotlib or similar codebases, this involves reviewing the relevant category unit converter code and adding a guard (such as checking the data isn't empty before issuing a warning). The change is not entirely trivial, as some understanding of the converter's context is needed, but it should be solvable in under an hour, especially with a focused issue and small code surface area involved.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major other issues that would make this sample unsuitable. The issue reflects a realistic minor bugfix workflow in a large codebase and would provide a meaningful benchmark of reading comprehension and patching code involving edge cases and warnings.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23299": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a clear problem statement: calling matplotlib.get_backend() clears figures from Gcf.figs if the first figure in Gcf.figs was created in an rc_context. It explains the expected behavior (the figure should not be missing from Gcf) and describes the consequence (plt.close(fig2) can't work). While there are some blanks engineers might have to fill in\u2014such as the precise internal mechanics of rc_context, Gcf, and how they interact with figure creation and backend management\u2014the description is concrete and actionable. The scope is quite clear, as the affected functionality, classes, and the error mode are all spelled out. It is not at the level of \\\"obvious one-line fix\\\" but provides engineers with a sensible interpretation and actionable steps to investigate and address the issue.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue appears to require a targeted change\u2014identifying why rc_context interferes with backend handling and Gcf figure retention, and preventing backend reset by the context manager. For a developer familiar with matplotlib's configuration mechanisms, this would require tracing some functions and understanding the interaction between rc_context and backend handling, but not rewriting large parts of the codebase. The fix, as seen, touches a small section (~2-3 lines) of one function. Therefore, it would likely take about 15 min to 1 hour for an experienced engineer to reproduce and verify, including test adjustments.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This sample is suitable for use in the benchmark. The description, while not spelling out every minute implementation detail, is precise about what is broken and what is expected, and the fix scope is localized. No major blockers or missing information.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-23476": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description reports a concrete bug with clear symptoms: upon unpickling a matplotlib figure on macOS M1, the dpi value doubles (and continues increasing if pickled/unpickled in a loop), leading to potential integer overflows. The conditions (only on M1 Macs, OS version, Python version, library version, and installation method) are specified, which helps to localize the issue context. However, the description does not explicitly specify the root cause, what the dpi is supposed to be set to after unpickling (is it the original value?), or give test-case style inputs/outputs. There are some plausible interpretations (dpi should not be multiplied by device pixel ratio on load), but the engineer must fill in details and validate the intended/correct dpi post-unpickle. This makes the issue \\\"some blanks to fill in, but a sensible interpretation\\\" (score 1), not perfectly specified.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves understanding figure state serialization in matplotlib and identifying where the dpi change due to device pixel ratio adjustment leaks into the pickled state. The actual code change is very small (a conditional reset of dpi in the getstate method before pickling), and requires only modest inspection and testing. An engineer familiar with matplotlib and pickling could likely resolve this in 15-60 minutes, especially as the bug is isolated to a specific platform and symptom. Debugging might involve some trial and error verifying platform-specific behavior, but this is not a major rewrite or highly esoteric/root-internals issue.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The only caveat is that the bug only manifests on M1 with a specific backend, so cross-platform issues may complicate testing, but this is not a reason to avoid this sample for the benchmark. The problem, context, and necessary knowledge are appropriate for a coding skills benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24026": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text describes a concrete problem with stackplot in matplotlib: unlike ax.plot() and Rectangle(), stackplot does not support supplying a 'CN' alias, and throws a ValueError. The use case (synchronizing colors across various types of plots) is described, and there is a pointer to a usability inconsistency between stackplot and other functions. However, it is not 100% explicit about how stackplot should behave\u2014should it accept 'CN' style color strings? Should it match ax.plot()'s property cycle behavior precisely? Nevertheless, a sensible and reasonable solution can be inferred: stackplot should use the Axes' property color cycle (and accept color inputs in a compatible way) similarly to other plotting methods. The example, while not minimal, is clear enough for an experienced engineer to proceed. Some ambiguity exists (e.g., does this cover all color formats, how should color assignment be prioritized?), but overall the description is sufficient to produce a meaningful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small change to how the stackplot function handles colors. The engineering challenge is mainly in ensuring that the color handling in stackplot aligns with how ax.plot() and Rectangle() handle colors, especially wrt the property cycle and color aliases like 'C0', 'C1', etc. This likely involves a small modification to the function and possibly its interface, and can be accomplished in under an hour by someone familiar with the codebase and matplotlib's color API. The final PR diff is also quite concise (essentially switching to using an itertools.cycle of the colors). The change is meaningful but not substantially complex or large in scope.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers or unaddressed issues exist for this sample. The problem and solution align well with a test of practical coding and codebase familiarity. The only caveat is the lack of a minimal reproducible example, but the context is clear and the solution scope is well understood. This is suitable for use in a coding benchmark as intended.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24149": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description specifies a clear bug: ax.bar in matplotlib version 3.6.1 raises an exception when passed all-NaN data, which in turn breaks seaborn's histogram function. It hints that the cause is related to recent changes around barplots and NaN handling. While it does not specify the precise exception message or exactly where in the code the issue occurs, it provides enough information for an experienced engineer to reproduce the error using matplotlib's bar plotting with NaN values and to reason about handling such cases gracefully. Some knowledge of matplotlib's internals is implied, but there is a sensible and targeted interpretation of what is required for a successful solution: ensure ax.bar([nan], [nan]) does not raise. However, there are a few missing specifics (e.g. exact expected behavior, exception details), thus not fully clear-cut, so rating = 1.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug fix is relatively localized: it involves updating error handling in the relevant axes plotting function to gracefully handle cases where all inputs are NaN. The patch shows only a minor code change (handling StopIteration), and the test patch adds a simple new test. For an engineer familiar with matplotlib's plotting code, this is a fix requiring a bit of investigation (locating where the exception occurs and addressing it), but does not require rewriting major structures or deep architectural changes. Therefore, it is a 15 min - 1 hour task.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues stand out that would prevent this sample from being suitable for a benchmark coding task. The issue is specific, the fix is meaningful but not trivial, and the requirements for success are testable. While it refers to another library (seaborn), the core fix is in matplotlib and can be clearly scoped.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-24970": "{\"q1_1_is_well_specified\":3,\"q1_2_explanation\":\"The issue text is very vague: it only states that there are 'several deprecation warnings' starting from NumPy 1.24, but it does not include any example of the warnings, stack traces, or which features/APIs are deprecated. There is no reproduction example, no information about when the deprecations are triggered, or what kind of code or usage patterns are involved. The only additional information is related to OS, package versions, and backend used, but it's not clear what to fix or where to look. Without further information, it's nearly impossible for an engineer (even with repository access) to know exactly what is being asked to fix or what code to touch, unless independently discovering the deprecation warnings themselves.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Assuming the high-level requirement is ultimately made clear (that the task is to resolve new NumPy 1.24 deprecation warnings triggered by matplotlib code), the actual code changes required are minor\u2014likely simply modifying a few lines to avoid deprecated behaviors, as seen in the patch. However, the initial ambiguity in the issue text would make it very time-consuming to even identify what needs to be fixed, unless the expectations are clarified outside of the issue text.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Due to the sparse issue description, this sample is not suitable as-is for benchmarking coding ability. It severely under-specifies what is broken, provides no context for how to reproduce, or what the correct behavior is. There is also a risk that different engineers would patch different things or make untestable assumptions. Only if the test suite is provided and is very comprehensive could it possibly be attempted, but the minimal specification is a serious concern for benchmarking purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25311": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified. It clearly states that figures with draggable legends (and annotations) cannot be pickled, which is a concrete, actionable bug report. It does not, however, provide a minimal, reproducible example or specify what constitutes the 'error' (stack trace, exception message), nor does it stipulate whether this should work for all backends or just some (though the info panel suggests Agg and possibly others are implicated). It leaves some blanks for the engineer to fill in, but the intent and affected functionality are quite clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change involves understanding why figures with draggable legends cannot be pickled and modifying how canvas references are stored (by making it a property instead of an instance attribute) to avoid breaking picklability. While some knowledge of the codebase's pickling and canvas mechanisms is required, this change is limited in scope (essentially adjusting how one attribute is set and accessed). The test fix also only adds coverage for pickling figures with draggable legends. For an experienced engineer familiar with matplotlib, this could be achieved in under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The sample describes a measurable bug, and the provided patches demonstrate that the fix required limited and localized changes. The only possible ambiguity is in not stating the exact exception thrown, but the problem and expectation (that pickling a figure with a draggable legend should work) are clear enough for a meaningful coding benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25332": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description identifies a bug that occurs when trying to pickle a figure after calling align_labels(), but it does not specify the error message, traceback, or the root technical reason for the failure. However, it is reasonably clear that the main problem involves incompatibility between aligned label functionality and pickling; an engineer familiar with the Matplotlib codebase could make a sensible interpretation\u2014namely, to investigate what internal state align_labels() changes that could affect pickling. The engineer will need to inspect the implementation details and maybe reproduce the error locally, but the core requirement (\\\"Unable to pickle figure after calling align_labels()\\\") is clear.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix involves understanding both the figure alignment mechanism (specifically, how align_labels and related label groupings are stored internally) and how pickling/unpickling works for the affected objects. This requires identifying the relevant state that prevents pickling and providing custom __getstate__ and __setstate__ methods to make the object serializable. This is less straightforward than a simple bugfix, requiring a bit of architectural understanding. It may involve some investigation and changes to internal data handling, as evidenced by the patch which modifies the Grouper class, and ensuring the fix works as intended by updating/adding tests. Such work is substantial and likely to require between 1-4 hours for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues with using this sample for evaluating coding ability. The main requirement (enabling pickling after using align_labels) is clear enough, and the patch/test are direct. The lack of an error message or traceback makes for a slightly less desirable sample, but not a disqualifying one, since the core bug is still explicit.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-25479": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. It clearly describes a bug involving registration and lookup of colormap objects under different names in matplotlib: when you register a colormap under a different name than its '.name' attribute, pyplot functions (like imshow or set_cmap) may fail to find the colormap unless both names match. However, while the issue provides examples of the behavior and expected outcome (registered colormap should be usable by its registered name regardless of internal name), it does not specify the exact mechanism for resolving this mismatch, nor exact expectations for all affected APIs. Still, there is a sensible interpretation: ensure cmap.name and registration name always match when registering, so user experience is consistent. Naming of involved classes and methods (e.g., Colormap, register, cmap.name) is mentioned or inferred from the text.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is relatively straightforward to fix for an engineer familiar with matplotlib's internals. It requires updating the registration process to ensure the colormap's name matches the registration (a relatively small code change) and updating associated equality testing. The solution impacts a couple of files, but the required logic isn't complex; it needs care (not to break other uses), but is not a major refactor. It likely takes 15-60 minutes to identify where to make the fix, update behavior, and add/refactor tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues present. The scenario is common in visualization libraries. The issue text doesn't describe API surface in detail or fully enumerate how all matplotlib interfaces should behave, but for the purposes of code evaluation, it is reasonable and typical. The test patch given ensures the fix is actually tested. Suitable for benchmark use.\",\"q2_5_confidence\":5}"
    },
    {
        "matplotlib__matplotlib-26291": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description gives some helpful context, such as the affected function/module (`mpl_toolkits.axes_grid1.inset_locator.inset_axes`), the OS, Python, Matplotlib, and Jupyter versions, and a general sense that creating inset axes results in an error (although the precise error message and the actual code snippet causing the bug are omitted). The user refers to a 'demo on the website' and 'the first example', but doesn't include the example code, describe the error, or give a traceback. Thus, it takes some interpretation to deduce the likely context and expected output. While a knowledgeable engineer could make a reasonable guess (especially if familiar with matplotlib), there is ambiguity in what exactly is broken and how to verify it (e.g., is it a crash, silent failure, incorrect positioning, etc.?). Therefore, the issue is vague and there is room for ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Assuming the high-level problem is clarified (i.e., that creating inset axes fails or errors due to improper handling of the renderer argument), the fix is small: adding a check to set the renderer if not already present. The code change is minimal and localized to a utility class/function, and the required test is straightforward. Still, it requires some understanding of the call stack and when the renderer may be None. This likely falls into the '15 min - 1 hour' range for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Despite the lack of details in the issue report, there are no other major blockers for inclusion, provided the high-level problem is assumed as clarified above. The associated patch and test are straightforward, and the sample tests programming ability related to bugfixing in a large Python library.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-1724": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear context and describes the problem: Passing a unicode value as the HTTP method causes UnicodeDecodeError in Python 2.7.2, because unicode 'infects' headers when method.upper() is used in sessions.py. It references the relevant file (sessions.py) and line (where req.method is set via method.upper()). However, it does not provide a concrete example of how to reproduce the issue, nor does it state the exact desired behavior\u2014should unicode method names be supported gracefully, or should an error be raised? The sensible interpretation is to ensure the requests library can accept unicode HTTP method names in Python 2.7.2 by converting them to native strings. While the issue lacks detailed reproducible steps, it is well-specified enough for an experienced engineer to make a meaningful attempt at a solution, given the information about the problem in the description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small but non-trivial fix. Identifying the need to cast the method to a built-in string would likely require reading the function in sessions.py, understanding the implications of unicode in Python 2, and then applying the fix. The code change is very small (one line). However, recognizing where to make the change and why unicode method names are problematic may require a bit of investigation. For someone familiar with Python 2 and unicode/str issues, this would likely take 15 minutes to one hour, including writing a test.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are apparent. The issue provides enough context for the intended fix, and the required files and functions are clearly referenced.\",\"q2_5_confidence\":5}"
    },
    {
        "psf__requests-5414": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is generally clear: when making a request to a URL such as 'http://.example.com', a UnicodeError is raised, but the expected behavior (in line with similar cases and existing code in requests/models.py) is to raise an InvalidURL exception instead. The user provides a concrete input that causes the problem, references relevant lines of code, and mentions precedent (previously discussed similar issues). The main ambiguity is whether other edge-case host patterns (besides the shown input and '*') should be covered, but there is a sensible and actionable interpretation for what must change to resolve the bug: ensure URLs with leading dots are detected and cause InvalidURL to be raised. It is not a full 0 because the scope of host patterns to cover is not exhaustively specified, but a reasonable fix is straightforward to infer.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix consists of extending an existing conditional check in the code that already handles a special-case host ('*'). With guidance to the code location and error mechanism, and with a clear input that triggers the problem, an experienced engineer could reasonably diagnose and patch the function within 15-60 minutes. The change is limited in scope, requires updating a line or two, and possibly adding/changing a test. It isn't trivial (0: <15min), since it requires some familiarity with error handling in the url preparation process, but it is a small and local code change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers or issues remain. The only caveat is the list of invalid patterns is not exhaustively specified, but the example and code context make the requirement clear enough. The system information is provided for reproducibility but is not relevant for implementation. The sample is suitable for coding evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3151": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified for an experienced engineer with access to the xarray codebase. It describes that xr.combine_by_coords currently requires coordinate dimensions to be monotonic even if they are identical across input datasets. According to the documentation, these should be ignored, and so the code should not require monotonicity for such cases. The engineer is clearly told what the undesired behavior is (raising ValueError when non-monotonic but identical coordinates are present) and what the expected behavior should be (do not raise; ignore those coordinates). This is enough to sensibly interpret what code changes are needed, but there is still some room for interpretation about precise edge cases or possible interactions with other combine logic, so this is a '1', not a perfect '0'.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a targeted code change in a high-level core routine. The solution mainly consists of adjusting which dimensions are checked for monotonicity when combining datasets. This requires understanding the logic for combining coordinates but is relatively localized, not requiring rewriting large portions of the codebase or making changes across many files. However, some understanding of the combination logic, coordinate handling, and the capability to write an appropriate test is necessary, making this more than a trivial fix and thus appropriate for the '15 min - 1 hr' category.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues. The sample is suitable for evaluating coding ability as the requirements are clear enough and the scope is well-bounded. There is a relevant test case that exercises the previously problematic behavior as well.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-3677": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text identifies a specific and reproducible behavior mismatch: merging a DataArray into a Dataset via the ds.merge() method fails, while using the top-level merge() function succeeds. The scope of the desired fix\u2014making ds.merge() accept DataArray as well as Dataset\u2014is implied by contrasting the two behaviors. However, there are some minor gaps: the issue does not specify error messages or edge case expectations (e.g. what should happen with conflicting variable names, or attributes). Fortunately, the expectation to mirror the behavior of the top-level merge() is a reasonable and actionable interpretation. Thus, the issue is mostly clear with minor blanks to fill in.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change that requires investigating how ds.merge() processes its arguments, ensuring it can accept a DataArray, and adapting it accordingly\u2014similar to the top-level merge(). The code patch shows a one-line change, but one must understand the correct type conversions and confirm that existing functionality is not broken. An experienced engineer should be able to identify and implement this change, including the required test(s), in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The issue is clear enough for benchmarking, and the patch and test directly illustrate the expected behavior. No ambiguity regarding the success criteria exists once the test is provided (though the issue description alone leaves a little to interpretation).\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4094": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue text describes a concrete problem: to_unstacked_dataset does not work for single-dimension variables. It specifically notes that the user's workflow involves stacking variables and later unstacking them, but the unstack fails when variables only have a single dimension. However, the issue does not go into details such as error messages, expected input/output, or code examples. The general expectation is clear: fix to_unstacked_dataset so that it works for single-dimension variables. Still, the absence of code snippets or a more thorough explanation introduces some ambiguity, requiring the implementer to interpret what 'doesn't work' means (e.g., does it error, does it produce the wrong shape, etc). Nevertheless, there is a sensible route to a successful solution: to ensure that stacking and unstacking works for single-dimension variables, maintaining round-trip consistency as for the multi-dim case.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The actual patch is a one-line change affecting a specific method and involves changing a single argument in a sel call. For an engineer familiar with xarray, stacking/unstacking logic, and the codebase, this would be a modest and localized modification\u2014requiring reading the code, understanding the single-dim edge case, and updating the invocation. Given the test patch, it is evident that a standard regression test is added. Overall, this is a typical small bugfix that does not require major design/restructuring.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues were found for this benchmark scenario. The patch and test confirm that the issue as described can be reliably checked for correctness; the lack of extensive context in the issue description is mitigated by the straightforward nature of the bug and the test.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-4695": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified: it clearly states that naming a dimension \\\"method\\\" in xarray causes an error when calling \\\".loc\\\", and indicates that the dimension name should not matter. It mentions that the error is likely due to unsanitized dimension names being passed to a method. However, it does not specify precisely what the error message is, provide example code to reproduce the issue, or include expected/actual behaviors or stack traces. While a knowledgeable engineer familiar with xarray could make a sensible interpretation and attempt a fix (i.e., ensuring that dimension names do not conflict with method parameter names), some gaps remain. Nonetheless, the description is concrete enough to guide an actionable solution, especially by inspecting the handling of dimension names in selection/indexing functions.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This fix requires identifying where the \\\"method\\\" dimension name causes issues in the call to .loc and modifying how selection keys are handled, which is a small code change but does require an understanding of how xarray passes keyword arguments and how this might collide with function parameter names. For an experienced engineer familiar with the codebase, reviewing the relevant selection/indexing code and changing the relevant function call would likely take between 15 minutes and 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major additional issues are present. While the original description could be clearer with a reproducible example or stack trace, it is sufficient for the purpose of the benchmark. The issue fits the intended evaluation scenario well: the problem is narrowly scoped, concrete, and has a testable end condition. Test coverage for the collision case is provided in the patch.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6599": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue describes a regression in the behavior of the polyval function when working with timedelta64 coordinates. The reporter notes the problem exists when moving from a stable release to the latest version and expects both versions to return the same results. The issue details the data type involved (timedelta64) and specifies the expected outcome (results invariant across versions). However, it lacks a specific example of usage, traceback, or explicit details of the incorrect result, so the developer must infer the root-cause and correct behavior based on general expectations and niche knowledge. Overall, while it doesn\u2019t lay out a minimal reproducer, nor specifies inputs and outputs, it does give enough context for a competent engineer to identify and address the regression, particularly if familiar with xarray and data type handling.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The change ultimately required adding explicit handling for the timedelta64 (dtype.kind == 'm') case in an internal conversion function. The bug was essentially a missing branch for timedelta64, fixed with a simple isinstance check and a cast. While not entirely trivial, as it required knowing how xarray's internal numeric conversion works and some understanding of numpy dtypes, the amount of code to change was minimal (a few lines in a single function). For an experienced engineer familiar with xarray internals and numpy dtypes, this could be diagnosed and fixed within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues present in the sample. The regression context and expected behavior are clear for an experienced contributor. However, the issue would be improved with a minimal reproducible example. As it stands, it's still reasonable for benchmarking, especially since passing tests will verify correctness. This is a fair way to assess ability to reason about data type handling and bugfixes in a Python scientific library.\",\"q2_5_confidence\":5}"
    },
    {
        "pydata__xarray-6721": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is vague and does not provide a minimal reproducible example, clear code snippets, or direct references to the expected versus actual behavior. It only states that accessing chunks on a zarr-backed xarray seems to load the entire array into memory. There is no sample code demonstrating the erroneous behavior, and the exact expected fix is left mostly to inference by the engineer. The environment details are provided but do not aid directly in problem reproduction. An experienced developer could reasonably infer that chunk access should not trigger full data loading, but there is considerable ambiguity in exactly how this should be achieved and verified just from the description.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Based on the gold patch, the required change is a small modification (changing 'v.data' to 'v._data' in one place) in the code. The main difficulty is recognizing the object model and attribute usage in xarray's internals; the code change itself is relatively simple. For someone familiar with xarray internals, this would likely take less than an hour, with the majority of time spent understanding the chunk loading process and confirming that the right attribute is being accessed. The test patch implies that the expected fix is relatively straightforward to implement.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"While the issue description lacks specifics and clarity, the paired patches suggest that the bug and its solution are limited in scope. There are no other major issues that would disqualify this as a benchmark coding ability task. The test provided verifies (regression-style) that accessing 'chunks' does not load all data into memory. Documentation on 'InaccessibleVariableDataStore' is assumed part of the test setup and is not shown here, but this is not a blocker for the coding benchmark.\",\"q2_5_confidence\":4}"
    },
    {
        "pylint-dev__pylint-6386": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes a specific bug with the 'verbose' option in pylint's command line interface: the short option ('-v') expects an argument, unlike the long option ('--verbose'), which does not. Additionally, the help message incorrectly suggests a value should be provided for the verbose option. The expected behavior is that 'pylint mytest.py -v' should behave like 'pylint mytest.py --verbose', i.e., not expect an argument. There is sufficient information for an experienced engineer to understand what is wrong, what is expected, and how to approach fixing the problem (i.e., align the behavior of short and long verbose options).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue likely requires inspecting the argument parsing code for the command line options in pylint, understanding how options are registered and handled, and making corresponding adjustments so that '-v' behaves like '--verbose'. Also, the help message likely needs to be adjusted to not show a metavar/value for this flag. The patch affects several files, but the changes are fairly localized and not conceptually complex. It would require some knowledge of argument parsing in Python and the pylint codebase, but for an experienced engineer this would probably take between 15 minutes and 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no blockers or additional issues with using this sample for a coding benchmark. The issue is specific, the requirements and expected outcome are clear, and the necessary changes are well scoped. The added/modified tests in the patch confirm the fix is verifiable.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-6903": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description provides clear and actionable information for resolving the bug. It identifies the precise failure scenario: using pylint with --jobs=0 inside a Kubernetes pod, where the autodetected CPU count is 0 and this causes pylint to crash. The description points to a specific function (pylint.run._query_cpu()) and explains the faulty logic (conversion to integer causes 0 CPUs, which breaks multiprocessing). It even proposes a reasonable solution ('or 1' at the end of the calculation). This gives enough context (including how to reproduce the issue, the environment, and a possible code fix) for an engineer to implement a solution successfully without further clarification. The relevant area of the codebase is indicated, and the expected behavior is clearly described. There are no substantial ambiguities or missing requirements. The only minor assumption is the need to ensure that at least 1 CPU is always used for multiprocessing, but this is a logical inference from the provided information.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a small change to the existing logic in a single function (_query_cpu) to ensure it returns at least 1 CPU instead of 0. The task is straightforward: detect when the result is 0 and substitute it with 1. Understanding the bug and implementing the check requires reading and modifying a small section of code. Some thought is needed to handle edge cases and testing (such as handling fractional CPUs or non-integer values), but the change is conceptually simple and isolated. The tests use some mocking but are not overly complex. A capable engineer should be able to implement and verify a fix in less than an hour after understanding the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no significant issues preventing this sample from being used to evaluate coding ability. The sample tests the ability to make targeted bug fixes in response to a clearly specified problem, which is appropriate for coding benchmarks. The only minor point is it assumes some familiarity with multiprocessing and system resource querying, but this is commonly in scope for Python engineering work. No further concerns were identified based on the provided information.\",\"q2_5_confidence\":5}"
    },
    {
        "pylint-dev__pylint-8898": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it describes a specific bug where regular expressions given to the 'bad-names-rgxs' option are incorrectly processed if they contain commas. It explains that Pylint splits on commas and thus mangles regex patterns that contain commas, since the option takes a CSV list. The user provides a concrete example of the problem. However, no specific input flags or files are shown, and the exact expected behavior is implied (robust handling of regex patterns with embedded commas) but not spelled out with explicit examples of broken vs. correct behavior. Still, the description is clear enough for a maintainer to investigate and develop a fix, especially when combined with familiarity with Pylint's option parsing and the 'bad-names-rgxs' configuration. There is some interpretation required, but sufficient information is present to proceed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires identifying how comma-separated regex strings are parsed in the configuration logic and modifying this parsing to avoid splitting inside regex quantifiers with commas. It involves understanding the configuration parsing code but results in a targeted change (~30 lines of code in multiple files, mainly in utility methods), plus associated test coverage. For a maintainer reasonably familiar with the Pylint codebase, this is a small, focused bug fix requiring some care in parsing logic but not a deep re-architecture or highly complex logic. Realistically, this is an hour-level bug fix for an experienced engineer but not a trivial one-liner.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are apparent with this sample. The issue and the solution cover both the application logic and the extension of tests, ensuring that the regression is guarded against. The testcases are specific and check the problematic condition (regexes with and without commas). It is suitable for a benchmark sample, as it requires non-trivial reasoning about parsing and escapes.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10051": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes a concrete inconsistency between `caplog.get_records()` and `caplog.clear()` in pytest's logging capture fixture. The description explains the diverging behavior: after `caplog.clear()` is called, `get_records()` returns a frozen or outdated state, not reflecting further log records. The technical reason is mentioned: `caplog.records` is replaced, not cleared, breaking the link, and the expected behavior (that `get_records()` returns newly captured log records after `caplog.clear()`) is clear. An experienced engineer could recreate, diagnose, and fix this issue\u2014with unambiguous understanding of what 'correct' looks like. No filenames are explicitly given, but the contextual use is clear for someone familiar with pytest's logging codebase.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires understanding how caplog's internal state management works, specifically how log records are tracked and cleared across test stages. The fix is relatively small and localized (ensuring that clearing log records also clears the objects returned by `get_records()`), but it does need some investigation into the internal data structure and a good grasp of how log messages flow through the capture mechanism. For someone familiar with pytest's fixture/lifecycle design, and after a brief orientation in the codebase, this is a bugfix likely solvable in 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major concerns; the issue is clear, can be solved and validated with reasonable confidence, and test coverage described is adequate. The setup is perfectly reasonable for use in a coding benchmark focused on bugfixing and internal API consistency. The environment details are not strictly necessary for the fix, but may aid in replicating setup if required.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-10081": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified, assuming context knowledge about how pytest integrates with unittest.TestCase and the significance of unittest.skip. The text clearly describes a test scenario where tearDown methods are being executed for skipped classes when running pytest with --pdb, and states that this is not the intended behavior, clarifying the expectation that setUp/tearDown should NOT be called for such skipped classes. It compares it to a previous issue (#7215) that covered the function level. It does not give step-by-step instructions or affected code lines, but the description is sufficient for an experienced engineer familiar with pytest's test case lifecycles to infer what is needed.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires understanding and potentially updating how pytest determines whether to run tearDown for unittest.TestCase subclasses with a class-level skip under --pdb. The fix likely involves checking the skip status at the class rather than just the function/case level. This is a targeted change in an existing (and relevant) part of the test runner logic, requiring some investigation but is not a large-scale rewrite or multi-file modification. Locating the correct spot and formulating the check requires familiarity with the code and Python unittest, but is not an extensive or esoteric fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues. The sample is suitable: the bug and required behavior are described, and the engineer is provided enough context to create a reproducible test and a focused code change. It tests knowledge of Python's unittest/pytest integration and method lifecycle control.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5262": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified and gives enough context for a competent developer, familiar with Python and pytest, to understand what is wrong: _pytest.capture.EncodedFile reports a mode ('rb+') that includes 'b' (binary), but its write method only works with text (not bytes). The problem is triggered by external code (youtube-dl) checking sys.stdout.mode for 'b' and acting accordingly, which leads to an exception. The issue text identifies the location (EncodedFile in _pytest.capture) and clearly states that mode is reported incorrectly and should not include a 'b'. While it would help to have more detail (like an example traceback or a precise API example), there is a sensible main interpretation: change .mode to not include 'b', even when the underlying buffer does. The scope and expected fix are therefore sufficiently clear, if not absolutely exhaustive.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is very specific: it involves changing the reported .mode property of EncodedFile so it doesn't include 'b', while the underlying buffer does. This requires understanding the Python IO model and possibly a quick look at the usage of .mode, but is likely to be localized to a single spot with unit tests possible. The actual code change (removing 'b' from the mode string) is trivial once the problem is identified. For an engineer familiar with pytest's capture code and Python's file protocols, this should take less than an hour, though discovering all details could take a bit of investigation in the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant additional issues are evident. The issue is isolated, the surrounding context is clear, and there is a simple change required. The only minor concern could be unforeseen edge-cases about .mode contract, but these are addressed by related tests. This makes the sample appropriate for the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5631": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description, while understandable for someone familiar with mocking and pytest internals, does not state exactly what line or function needs to be changed and assumes knowledge about the meaning of 'patchings', 'sentinels', and the specific check that is causing the problem. However, it clearly identifies the problem domain\u2014comparing numpy-like arrays with sentinel objects using 'in', which results in incorrect behavior due to array semantics. The problem can be sensibly interpreted: the equality comparison in a membership test returns an array of booleans, causing a ValueError in numpy, so the membership/sentinel check logic should be more robust. A competent engineer can determine that the identity check is more appropriate. However, details such as filenames, precise code locations, or explicit reproduction steps are omitted. Therefore, there are some gaps but a sensible solution path is clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is conceptually simple\u2014understanding why the membership check is failing with numpy arrays, and switching from equality to identity check to avoid incorrect comparisons. Implementing the fix requires identifying where the problematic 'in' check occurs and replacing it with an identity-based check. The actual code change is quite minimal, but understanding why the fix solves the problem requires some familiarity with how numpy arrays overload __eq__ and how this interacts with membership tests. Thus, while the change itself is quick to implement, it requires intermediate understanding and a careful reasoning step, making it likely to take 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues outside of the slight lack of precise code pointers or full reproduction steps, which could slightly slow an engineer but are not likely to cause a failure to solve the problem. The issue is representative of a real debugging-and-fix workflow and thus suitable for a coding ability benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5787": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description identifies the observed problem: exception traces are incomplete when running tests using pytest-xdist (distributed testing), only showing the last exception rather than the entire chained trace. It includes the user's pytest and pytest-xdist versions, so the context is clear. However, it does not specify exactly where in the codebase the problem occurs, or what the solution should look like; it's up to the engineer to identify the relevant serialization/deserialization code, and understand how chained exceptions should be represented in the output. Overall, it's clear what a correct outcome looks like\u2014all relevant chained exceptions should be serialized and displayed\u2014but the engineer must fill in the details of how to achieve it.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding of pytest's internal representation of exceptions, the distinction between local and distributed (xdist) execution, and the (de-)serialization process for test reports, including how chained exceptions are represented and transported. The patch affects several functions, introduces new serialization/deserialization helpers, and adds/changing tests to verify chained exception support. It involves editing multiple files, understanding the test infrastructure, and non-trivial reasoning about exception object graphs. For an experienced developer, this is likely a 1-4 hour task, as it requires both reading and rewriting some core logic and updating or adding substantial tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"This issue is suitable for the coding ability benchmark: the problem is reasonably specified (even if not completely precise about the code location), has clear acceptance criteria (proper serialization of chained exceptions), and the tests in the patch demonstrate that the fix is verifiable. The proposed changes are illustrative of practical debugging, serialization concepts, and test-driven development in a real-world open source codebase. There are no major reasons not to use this issue as a benchmark sample.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5809": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified and provides clear, actionable information. It describes a concrete error (HTTP 400 from bpaste.net when using 'lexer=python3'), gives an exact code reference (src/_pytest/pastebin.py), and explains the underlying cause. The desired behavior is also unambiguous: instead of 'lexer=python3', it should use 'lexer=text' since the output is arbitrary text, not Python code. No additional context or clarification is needed to make a successful fix; an engineer has all needed details to implement and test the solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change that requires a bit of thought. The fix involves identifying where the 'lexer' parameter is set and updating its value, based on understanding the purpose of the output and how the pastebin expects its input. There is a minor decision in interpreting why 'text' is preferable, but the implementation is straightforward and affects just a limited code area.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The issue text alone gives enough context for someone unfamiliar with the project to find and address the problem, and the scope is well-bounded. The only consideration is being careful about test coverage to ensure the change does not affect other functionality, but the test patch provided ensures this.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-5840": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is quite vague and lacks context. It only states that after upgrading from version 5.1.1 to 5.1.2, an ImportError occurs while loading 'conftest' due to 'the path was converted to lower case'. There are no reproduction steps, no error traceback, or even an explicit mention of which code paths or config settings are relevant. The reader is left to infer that this is related to path casing on Windows, but specifics are missing. There is not enough information about how to reproduce the problem, which APIs or functions are involved, or what a correct fix should look like. The successful solution demonstrated in the diff requires insight into both the file system casing issues and the pytest internals, but the issue description by itself barely hints at what is required. Therefore, there is considerable ambiguity, and it's not clear what a successful solution looks like without additional context.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Despite the brevity of the change in the PR, the problem is nontrivial due to the ambiguity of the issue and the need to understand how path normalization, Python path handling, and plugin management interact in the pytest codebase. A developer would need to investigate the flow of conftest loading, the effect of different OS filesystems, and how paths are managed between versions. This investigation, code tracing, and validation can take a few hours, even if the final code diff is not large.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"The main issue is that the sample\u2019s initial report is too vague for use as a benchmark of coding ability based solely on the description provided. A candidate using this sample in the setup described would have a very hard time inferring the correct fix, since key details are missing: there is no error message, no clear minimal reproduction, and the relationship between path lowercasing and ImportError is unexplained.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6197": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description identifies a regression introduced in pytest 5.2.3 where pytest is collecting (i.e., attempting to import) every __init__.py file under the current directory, which can cause problems on non-Windows platforms if those __init__.py files are platform-specific. The core of the problem is clear: pytest's collection logic needs to be modified to avoid unnecessarily importing certain __init__.py files. However, there are some details left to the engineer to interpret: for example, exactly when and why __init__.py files should not be imported (is it only platform-specific ones? All of them unless certain conditions are met?). The lack of concrete filenames, config examples, or specific reproduction steps means the engineer must infer some context about how pytest collection works and what the desired behavior is. Nonetheless, the issue is clear enough for a knowledgeable engineer familiar with pytest's collection mechanism to propose a rational fix. Therefore, it's not fully well-specified (0), but fits as 'some blanks, but sensible interpretation' (1).\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The fix itself is not trivial\u2014a careful engineer must trace through the pytest collection mechanism, especially how and when __init__.py files are considered for collection/import. The problem concerns core behavior that could affect many workflows and platforms, so a robust solution must be constructed and validated via appropriate test changes. The patches alter the behavior of class Module and the method determining when __init__.py is collected, and update several tests to confirm the right modules are collected or skipped. This scope (requiring both logic changes in the collector and new or altered tests validating nuanced behavior) suggests a 1-4 hour effort for an experienced engineer, rather than a quick fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No blockers for use in a benchmark. The issue's context and expected outcome are clear enough for a sophisticated user to propose a solution. Some platform-specific knowledge and understanding of pytest's internals help, but nothing prevents meaningful problem-solving.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-6202": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides a precise technical diagnosis of the bug: a problematic replacement in a particular function is causing incorrect test output headline formatting. The files and concrete lines implicated in the bug are cited, the faulty line of code is pinpointed, and the desired fix ('replace it with return s') is stated. Though the reporter notes they are unsure of the original intention for that replacement, the severity and site of the user-facing bug are clear. An experienced engineer, given this description, can confidently locate the broken logic, deduce its irrelevance to the intended fix, and implement the requested change. There is sufficient detail and context for a meaningful, focused solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a relatively simple bug fix: it requires tracing the string processing for test headlines, removing a single unnecessary string replacement, and ensuring output is now correct. The primary effort is in navigating the codebase to confirm the source of the bug and effects of the change. The lines in question are called out explicitly, and reproducing and verifying the fix is straightforward with the provided context. The change is to a single line and has a minimal blast radius, so this task would likely take an engineer less than an hour after initial codebase familiarization.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No significant issues identified beyond what is covered. The requirement only involves code changes and test updates pertaining directly to the bug described, and the sample adheres to the intended evaluation setup for coding tasks.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7205": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the specific warning (BytesWarning) that occurs during pytest's --setup-show output when handling a bytes parameter. The user points out that the cause is implicit use of str() on a bytes object, and requests using a safe representation (e.g., saferepr) instead. The context (pytest's output code path for displaying cached parameters) is clear. No references to specific lines, but the problem and a likely solution are unambiguous for an experienced engineer familiar with pytest's code structure.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires identifying where pytest outputs the cached_param in setup output (likely in a small number of code locations), and changing the output so that saferepr is used instead of str(). This is a straightforward code change, possibly involving an import and a replacement of one function call, but may require running the test suite to ensure clean output and no side effects. This is typical of a \\\"small change that requires a bit of thought.\\\"\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain. The problem is well-specified, directly actionable, and correspondingly reflected in the test patch. The test ensures that bytes values are output correctly in the setup-show output, so regression is detected. This makes the sample suitable for coding ability benchmarks as described.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7236": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified. The reporter points out that when using pytest with the --pdb flag, unittest.TestCase.tearDown is executed even on skipped tests, which is not the expected behavior. The expectation is that skipped tests should not run tearDown, matching prior behavior (pytest 5.4.1). The issue provides version comparisons and states the desired effect clearly: skipped tests, even when --pdb is enabled, should skip tearDown/setUp. However, the issue does not specify in detail where the code lives or exactly how skipping is implemented\u2014some knowledge of pytest/unittest internals is required to understand the context. Still, an experienced engineer could infer what's needed from this description with reasonable confidence.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The code patch modifies conditional logic related to skipping in a handful of functions and introduces a small utility to encapsulate skip behavior. The complexity lies mostly in tracking where skip logic is respected, rather than in writing lots of new logic or designing a large feature. For someone familiar with the codebase, this should be a sub-hour change (once the problem is understood), so I rate it as 'small change that requires a bit of thought'.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other blocking issues; the test patch that accompanies the fix checks the requirements appropriately. This sample is suitable for benchmarking coding ability regarding logic around conditional test skipping.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7324": "{\"q1_1_is_well_specified\":3,\"q1_2_explanation\":\"The issue description, as provided, is extremely vague. It only states: 'Pytest crashes the interpreter on debug build for 3.8+'. No further information is given on how or why it crashes, what error messages are generated, what part of pytest is involved, under what conditions the crash occurs, or any reproduction steps. The only concrete information is that the related behavior improvement is discussed elsewhere (in bpo-40870), but that cannot be referenced according to the instructions. With this level of detail, it is nearly impossible to understand what needs to be fixed, or even what functionality is at fault. There is no filename, function, input example, or error message included. As such, the issue is not specified well enough to attempt a meaningful solution purely from this text.\",\"q2_1_difficulty\":3,\"q2_2_explanation\":\"Because the original issue description is highly underspecified, an engineer would likely spend an inordinate amount of time just trying to figure out what the precise problem is\u2014this would require reproducing in a debug build of Python 3.8+, investigating why an interpreter crash happens using a large codebase, and teasing out the offending part of the code. Even granted extra clarification (as per instructions), the patch touches parser logic, AST representation, and deals with reserved keywords/identifiers not usually allowed in Python, which is a tricky problem to identify and resolve. The absence of any concrete symptoms in the issue would make this a highly esoteric and difficult-to-reproduce issue that would require substantial research and code comprehension to solve.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"The sample should not be used as a benchmark for evaluating coding ability, because the issue description is far too vague and gives almost no usable context. Benchmark participants would be guessing at the root cause and would not have a fair or realistic starting point. This does not evaluate coding ability but rather the ability to guess or reverse-engineer missing requirements. The test also does not check the ability to interpret tests or write code to spec, as the 'spec' is nearly absent.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7490": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is reasonably well-specified: it describes a regression in pytest behavior from version 5.x to 6.x, specifically regarding dynamically adding an xfail marker to a test during runtime via request.node.add_marker(). It explains what the expected and actual behaviors are, and references that the new behavior (since 6.0.0rc0) no longer treats such failures as expected xfails but reports them as failures. However, the description doesn't point to specific locations in the code nor provide a minimal example right in the issue text (though such examples are likely present in the PR's test changes). It also doesn't explicitly say what the expected fix should look like implementation-wise. Nonetheless, an experienced developer familiar with pytest should be able to reasonably understand the intent: restore the previous behavior where dynamically adding xfail at runtime causes the test to be treated as xfailed if it fails.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"I rated this a 2 (1-4 hours). Although the code changes themselves are relatively contained, the issue requires a solid understanding of pytest's internal test running and marker-handling mechanism. One must also understand the test lifecycle and how dynamic markers interact with result reporting, which is non-trivial and may require time inspecting the internals and verifying behavior with tests. Debugging when and how markers are evaluated and debugging the lifecycle hook timing could take additional time.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues with using this sample. The only note is that this issue relates to fairly advanced usage of the pytest plugin API and test internals, so it may not be representative of more beginner-level tasks. Otherwise, the regression and solution scope are clear.\",\"q2_5_confidence\":5}"
    },
    {
        "pytest-dev__pytest-7521": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description points to a change in the behavior of pytest's capfd.readouterr(), specifically that it now converts '\\\\r' to '\\\\n' in output. The reporter is observing this as a regression and notes it is causing test failures. However, the issue does not specify what the correct, expected behavior should be, nor does it provide a minimal reproducible example or further technical details. The described context and regression are clear, but there is ambiguity about the intended behavior of capfd.readouterr() with respect to newline handling, especially for those unfamiliar with pytest's capture system. The specifics of how to handle different newline conventions or why this matters are not made explicit. Therefore, while a sensible interpretation of the issue is possible (restore the original newline handling), there is substantial room for ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The underlying issue is to correctly preserve newline characters in output captured by pytest's capfd fixture. The patch itself is a very small change to the file handling for output capture\u2014specifically, passing the newline='' parameter when creating a TextIOWrapper. For an engineer familiar with Python I/O and pytest's capture system, identifying the problem, tracking down the line of code responsible, and applying the fix would require some investigation, but would likely fall within a 15 min - 1 hour window. The main difficulty is understanding how Python file objects handle newlines and how pytest's capfd interacts with them.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional major issues with using this sample for evaluating coding ability. While the issue text itself lacks detail about the exact intended behavior, the accompanying test from the gold patch illustrates the requirements, suggesting that it can serve as a clear evaluation once the requirements are clarified. However, candidates working only from the issue may need to make reasonable assumptions about the correct newline behavior.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10297": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well specified. It explicitly asks for adding support for the 'store_cv_values' boolean flag to RidgeClassifierCV in line with the rest of the documentation, as RidgeCV already has this support. There is a clear motivation (the flag is documented, but not available to RidgeClassifierCV), and the requirements are to ensure 'store_cv_values' behaves as described in existing documentation for the similar class. However, there are some blanks to fill in: it's not entirely clear if there are any edge cases or additional details to consider beyond matching RidgeCV's interface and behavior and ensuring that the associated attribute gets created correctly. The mention of the attribute's documented behavior and shape does help specify test expectations. Overall, an engineer could make a sensible, well-justified attempt at a fix, but might need to verify by analogy with the existing RidgeCV behavior.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires updating an estimator (RidgeClassifierCV) to support an additional flag, updating docstrings, and ensuring that the flag's behavior matches the described and existing behavior in RidgeCV. It entails passing the flag through the initializer, handling storage as appropriate, and possibly minimal code adjustment for computation and assignment to cv_values_. It also requires adding or updating tests. For an engineer familiar with scikit-learn's code structure, this is a small-to-moderate codebase change that should take between 15 minutes and 1 hour to understand and implement.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues present. The sample is appropriate: the problem is focused, the requirements are clear by reference to documented API behavior and existing similar class, and success is checkable via appropriately constructed tests. The absence of edge-case details is handled reasonably by following the RidgeCV precedent. Some code exploration is needed but not beyond what is typical in feature-parity fixes.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-10908": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified. It clearly describes the problematic behavior (get_feature_names raising NotFittedError when a custom vocabulary is provided) and the expected behavior (get_feature_names should work if a custom vocabulary is provided, even if fit was not called). Specific references are made to the CountVectorizer class, the vocabulary parameter, and the get_feature_names method. There is a clear high-level description of what is wrong and what should be changed. The engineer tasked with resolving this would know to modify get_feature_names so that it behaves consistently with transform. No external clarification is required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small, localized behavioral adjustment in an established API surface, requiring only a minor change to how attributes are initialized or checked in get_feature_names. The fix is conceptually straightforward (ensure vocabulary is set or initialized so get_feature_names does not raise in the described scenario), and should take well under an hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other significant issues. The setup matches typical use of the library and the expectation of scikit-learn users. The patch is small, targets the described problem directly, and test coverage is expanded appropriately. The issue is suitable for coding ability evaluation, focusing on correct and robust handling of feature initialization.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-12585": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified. The main problem is described: the clone function fails when a parameter is a type (class of an estimator) instead of an instance, and the desired fix is explicitly suggested (an additional check using isinstance). The context is sklearn's base.py clone function. However, the reporter mentions uncertainty about possible side effects elsewhere, so while the immediate code change is clear, complete verification may need extra thought. The specific version is also given. Thus, there are some blanks and a minor interpretative step (whether this change has knock-on effects), but it's clear what's intended for a minimal solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue requires a small modification to a conditional in a single function and understanding of estimator cloning behavior in scikit-learn. An engineer with some familiarity with scikit-learn's base estimator framework and Python's type system should be able to identify, implement, and test a fix in under an hour. It is not a totally trivial syntax-only change (because the engineer needs to consider edge cases and the impact of handling types vs. instances), but the scope of code to be touched is minimal.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"The only minor ambiguity is possible side effects, but this is typical and expected for this class of issue. Otherwise, as a benchmark problem, it is fine: the issue text communicates both the problem and the proposed area of change, and writing the fix requires some\u2014but not overly deep\u2014domain knowledge. No blockers for benchmark usage.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13135": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well specified: it indicates that KBinsDiscretizer with strategy='kmeans' can fail because the computed 'bin_edges' can be unsorted when cluster centers are not sorted. This causes np.digitize to fail. The issue is founded in implementation behavior and gives some indication of when the problem appears (e.g., large n_bins). As an engineer, I can infer that the fix involves ensuring that the bin edges are sorted before being passed to np.digitize. However, the issue does not specify exactly how the sorting should be performed or if there might be edge cases with duplicate or close centers, so there remains a little room for interpretation. The filename (KBinsDiscretizer), the context (bin_edges, kmeans), and the nature of failure (unsorted bins break digitize) are clear. Therefore, there are some small blanks to fill, but the requirement is clear enough for a solution attempt.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change consists of ensuring cluster centers are sorted before calculating bin_edges. The fix is a small and local code adjustment (inserting a sort call), and does not require refactoring multiple files or extensive redesign. The engineer has to understand the bug (unsorted centers lead to incorrect bin_edges) and know where to insert the fix. An experienced developer could find the relevant code and apply the fix in less than an hour. The accompanying tests just add further checks rather than rewriting test infrastructure, supporting the low degree of effort required.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. The sample accurately represents a concrete, well-understood bug with an obvious place for implementation (after kmeans and before bin_edges are constructed). The test changes are directly targeted and not ambiguous.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13142": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is clear about the bug: when n_init > 1 in GaussianMixture, the results of fit_predict(X) and predict(X) differ, but they should be the same. The underlying expectation is that fit_predict(X) should return the same labels as fit(X).predict(X) for all parameter settings, and this is not happening, specifically when n_init > 1. The rationale for why this is a problem is briefly explained, and a relevant test isn't catching the bug because of its parameter settings. The affected functionality (fit_predict vs. predict in GaussianMixture and its test) is directly named, and the expected invariant is concrete. However, the precise implementation bug and desired fix are not given, and some construction of the minimal fix/test is left to the implementer. The issue is clear enough for an experienced engineer to act upon and validate with proper tests, but some minor design choices (e.g., how to ensure consistency) would be judgment calls.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires identifying why fit_predict and predict diverge when n_init>1 and ensuring that the label assignments are consistent\u2014all within a limited surface area (the EM-loop, handling of results within fit_predict, etc.). It requires familiarity with the logic of GaussianMixture and understanding of its API, but is localized to a single method, and the code edit was modest. While it is slightly tricky in that the engineer should recognize why a final E-step is necessary, it is not a multi-file or complex architectural change.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present. Version information is supplied, but the problem is not version-dependent, and the sample fits the requirements for use in a benchmark for coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13328": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and clear in stating the current problem and expected behavior. It explicitly says that fitting HuberRegressor with boolean arrays as predictors (X) raises a TypeError, whereas other regressors like LinearRegression internally convert boolean arrays to float. Therefore, the solution should ensure HuberRegressor handles boolean input for X the same way (by converting boolean to float internally). The exact file and likely function (fit in HuberRegressor in sklearn/linear_model/huber.py) where the change should be made is clear from context. An experienced engineer could proceed confidently from this issue description alone.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a small change that requires understanding the codebase's data validation and conversion strategy. The fix likely involves specifying an additional dtype parameter to handle boolean inputs in the check_X_y function call within HuberRegressor.fit. While it does require some thought to ensure consistency with LinearRegression, the change is localized and straightforward, making this a 15 min - 1 hour task for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. This is a focused, reproducible bug with a clear path to success and corresponding test coverage. The test patch adds coverage for the new case, so both diagnosis and verification are cleanly supported.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-13779": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well-specified: it identifies a specific failure in the Voting estimator's fit method when sample weights are passed and one estimator is set to None. The description provides sufficient context to suspect where the error occurs (i.e., lack of a None check when iterating over estimators in sample_weight support). However, it doesn't reference exact code locations or provide a reproducible snippet or stack trace, so the developer must know the Voting estimator's internals and guess the nature of the problem. There's a sensible interpretation of what is required: ensure that estimators set to None in the ensemble are skipped when handling sample_weight. However, there is some blank to fill in (how None estimators are handled elsewhere in fit, expected error raised, etc.), so it fits solidly as a '1', not a perfect '0'.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue is a straightforward bug fix. The cause of failure is described, and for someone familiar with ensemble methods and the sklearn codebase, the reasonable solution is to skip estimators that are None when supporting sample_weight. The code change is small (a conditional continue in a loop), and the right area to patch is obvious to someone who knows the convention for handling None estimators in scikit-learn. The engineer would need brief investigation to understand how estimators are stored and iterated, but this would likely take 15 minutes up to an hour total. Test coverage can be extended simply to cover the bug fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues stand out. The issue is amenable to a test-and-code evaluation and, despite not giving a reproducible snippet, describes a specific and actionable bug in a standard pattern. The scenario fits the benchmark's requirements.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14053": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is concise but gives key information: when export_text is called on a decision tree with only one feature, it raises an IndexError due to a 'list index out of range' bug. It specifies which function is affected (export_text) and under what conditions (single feature in the tree); this is enough for an experienced engineer to reproduce and investigate the bug. However, it doesn't specify what the expected output should look like, nor does it describe the root cause, feature_names handling, or the desired test results. Still, it is sensible to expect that the function should not crash in this scenario and should return a correct readable output. This leaves a small gap for interpretation but is generally actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is localized and narrowly scoped: it's a bug in a specific function (export_text) that occurs only with single-feature trees. The code changes needed are likely small, focused on safe indexing or handling feature_names appropriately. While some investigation is needed to reproduce the error and understand why the IndexError occurs, a fix (e.g., checking for undefined features) is a minor logical correction rather than a thorough rewrite. Diagnostic and test updates are straightforward. An engineer familiar with Python and the codebase should be able to resolve it in less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues with this sample. Reproducibility is ensured by the clear trigger condition (single feature) and explicit mention of the error type and affected function. The issue is suitable for coding ability evaluation, as it requires diagnosing, correcting a bug, and optionally, updating or adding a test for single-feature cases. No blockers or requirements for clarifications are apparent.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14087": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear indication of the error: IndexError is thrown when using LogisticRegressionCV with refit=False in the context of estimating a regularization parameter via cross-validation. However, it does not provide the actual error traceback, specific code examples that reproduce the error, or explicit details about which line(s) in the code are affected. The description does mention that the error only occurs when refit=False, giving a strong clue as to where to investigate in the codebase. With knowledge of scikit-learn's API and the indicated parameters, an experienced engineer could reasonably interpret what is required for a solution, but there is some ambiguity about the precise nature of the bug without the traceback or a reproducible code snippet.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires understanding the code path taken by LogisticRegressionCV when refit=False and why an IndexError is triggered in that scenario. The engineer needs to review the code, understand the intent and shapes of arrays being indexed, and introduce appropriate checks or modifications for cases where attributes (like l1_ratio_) should not be set. The patch itself is a small, targeted change, but requires understanding the relationships between multiple parameters (like penalty, multi_class, refit) and how they impact the fit method. Implementing and validating the fix would likely take 15m-1h for someone familiar with the codebase as it focuses on a specific logical case.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are present besides the mildly underspecified nature of the bug report. The test patch demonstrates the desired fixed behavior, so the combination of issue and test gives a concrete goal for patch authors in a benchmarking context.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14629": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue provides a reasonably clear description of the bug: when using cross_val_predict(method='predict_proba') with MultiOutputClassifier, AttributeError occurs due to incorrect access of the classes_ attribute. The user pinpoints the probable problematic usage and suggests the correct access pattern (mo_clf.estimators_[i].classes_). However, the issue does not specify test cases or exact error messages, and leaves it partially to the reader to infer the connection between cross_val_predict and MultiOutputClassifier, and the exact location/context of failure. Nevertheless, an experienced developer familiar with scikit-learn can sensibly interpret what is required for a successful solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix involves overriding the fit() method in MultiOutputClassifier to set the classes_ attribute correctly based on the underlying estimators. This is a relatively small change and localized to one class. Understanding and implementing this fix, including writing tests, would take 15 min to 1 hour for an experienced developer who is familiarizing themselves with the relevant parts of the scikit-learn codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues were found that should prevent this sample from being used for evaluating coding ability. The only slight downside is that the issue doesn't provide sample input/output or stack traces, but the description and references are sufficient for a knowledgeable engineer to proceed.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14710": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified but requires some interpretation. The problem is described: HistGradientBoostingClassifier fails with string targets when early stopping is on, due to a mismatch between encoded/integer y_true and string y_pred used by the scorer function. The issue further includes a clear code snippet showing a suggested fix and highlights the lines in sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py to focus on. However, a fully precise specification would clarify exactly where the encoding of targets should happen in the codebase for all relevant scorer usages. Still, the provided information makes it possible for a competent engineer familiar with scikit-learn to resolve the issue without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This is a targeted bug fix in a well-defined area of the code. The required change is small (a handful of lines where y_true is mapped back to class labels before being passed to the scorer). Understanding the interaction between early stopping logic, encoded classes, and the scoring API requires moderate familiarity but is not complex for someone versed in scikit-learn internals. The patch is contained to a single function, with no structural or major cross-file changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no other major issues that would prevent this sample from being used for evaluating coding ability in this benchmark. The bug, expected behavior, and relevant code location are all sufficiently identified. The only minor concern is that a true newcomer to scikit-learn internals might need some time to find the classes_/label encoding tie-in, but this is within scope for such benchmarks.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-14894": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a clear account of the problem: a ZeroDivisionError occurs in the _sparse_fit method for SVM when support_vectors_ is empty and input data is sparse. This points precisely to the offending area of code and the scenario under which it fails. However, while it provides enough information for someone familiar with the sklearn codebase to locate and grasp the cause (empty support vectors with sparse data), the description does not explicitly dictate the expected behavior or how to handle the case. It leaves the choice of correct handling (e.g., what output or object state is correct if support_vectors_ is empty) up to the developer, even though the error class and location are specified. Thus, there are minimal blanks to fill but a clear, sensible path to a solution.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The bug is localized: it describes a crash in _sparse_fit when support_vectors_ is empty due to a division by zero. Once the relevant function is found, the core of the fix is to guard against the zero case, which is simple and requires a small code change. It does not require editing multiple files, just the SVM base code and potentially a test. The bulk of work is identifying the proper check and adding a defensive branch, which should not take longer than an hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The bug is sufficiently described, has a clear reproduction case (even if not in the main issue text, it's reasoned from the description), and the fix is contained and testable. The real-world consequence (ZeroDivisionError) is directly described, and no ambiguous project-wide implications are noted. All relevant information is in the scope of the sample.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25747": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description gives a concise, reproducible scenario: using a FeatureUnion (from sklearn) with pandas DataFrames and a custom transformer that aggregates data leads to an error, whereas using the default numpy output works. Although the exact error message, the precise FeatureUnion setup, and a minimal code snippet to reproduce are missing, an experienced engineer familiar with sklearn's set_output API and transformers could reasonably infer the context. The core expectation is to make FeatureUnion + custom transformer + pandas set_output work without error, specifically around how DataFrame indices are handled when combining results. There are blanks to fill (requirement to preserve/not override DataFrame indices upon wrapping), but the problem is sensibly interpretable and actionable given the context provided.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is small and focuses on a conditional in the utility that wraps numpy/pandas containers: it should not override the index if the input is already a DataFrame. While hunting down the root cause and verifying that the change doesn't break anything else takes some thought, the patch is minimal and local. Writing/adjusting appropriate tests adds a bit more work, but the scope is on the level of a single function's logic and not a large refactor.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers or suitability issues. Main caution: A full reproducible example is not provided, but for the purposes of benchmarking ability to implement a focused, comprehensible bugfix with the high-level requirements, this is fine.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25931": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is well-specified and provides a clear explanation of the problem: when an IsolationForest is fitted using a pandas DataFrame with feature names, a warning is incorrectly triggered if the 'contamination' parameter is non-default, because the estimator internally calls predict/score_samples and loses feature name information. The description accurately outlines the code path triggering the problem, the context in which it happens (non-default contamination, DataFrame, call to fit), as well as the expected behavior (should not trigger a warning if the DataFrame with feature names is used). Furthermore, a precise reference to the relevant source file and line is given. An engineer would have all the necessary information to reproduce and understand the issue, and to decide what constitutes a successful PR (prevent the spurious warning while preserving input feature names).\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix is conceptually straightforward once the problem is understood: avoid stripping feature names or repeating input validation when not needed. The bulk of the work is understanding the need for a private scoring function that skips re-validation, then wiring it into the correct place in the fit method. The change touches one main function and involves refactoring, documentation updates, and minor test modifications. For an engineer familiar with the scikit-learn codebase, this would likely be a 15-60 minute fix.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No further issues are present. The issue is well-defined, the surface area for the fix is limited, and the risk of unexpected side effects is minimal. Test coverage is added in the test patch, which is also straightforward. This is a suitable candidate for benchmark evaluation of coding ability.\",\"q2_5_confidence\":5}"
    },
    {
        "scikit-learn__scikit-learn-25973": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is fairly well-specified: it identifies the precise behavior that is not working ('unable to pass splits to SequentialFeatureSelector'), refers to the interface and equivalency to existing scikit-learn APIs (\\\"passing splits from the cross validator fails\\\"), and references that the class should accept iterables of splits for its 'cv' parameter. It might lack specifics about the traceback or error encountered and does not provide a code example, but for an engineer familiar with scikit-learn, it is clear what needs support: SequentialFeatureSelector should accept and handle the same types for the `cv` parameter as other estimators \u2013 specifically, split iterators/generators. There are some minor blanks to fill in (the exact error or expected return values), but it's clear what enhancement or bugfix is required.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The difficulty is best categorized as a '15 min \u2013 1 hour' change for a reasonably experienced engineer. The engineer would need to familiarize themselves with how 'cv' is handled in other parts of the scikit-learn codebase, especially in model selection utilities and similar feature selectors. The actual patch simply involves using 'check_cv' to ensure the 'cv' variable can be a generator or split object. The code change is small but not trivial, as it requires finding the correct location (in the fit method), updating method signatures, and validating that functionality via tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues identified: the issue is appropriate for evaluating coding ability. The main challenge is integration with established conventions in the sklearn codebase, not in ambiguous requirements or missing context.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10323": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is moderately well-specified: it states that using the literalinclude directive with `prepend` (or `append`) leads to incorrect (unexpected) code indentation because leading whitespace is stripped. The context, project (Sphinx documentation with code examples), and environment are provided. However, the issue does not provide concrete examples or expected/actual output, and it is necessary to infer both the desired behavior and standard use for literalinclude/prepend. An experienced engineer can reasonably interpret what is required (i.e., that prepended and appended code in literalinclude should have indent matching the included code), but will need to fill in some blanks and possibly make implementation decisions. Thus, the prompt is not fully unambiguous but gives sufficient information for a focused fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change involves adjusting the ordering of filters applied to lines in the LiteralIncludeReader, requiring an understanding of how dedenting and code inclusion/prepend work. There is some complexity due to sequence-dependent transformations, but changes are limited to a well-scoped section and don't require fundamental rewrites or broad architectural changes. An experienced developer familiar with the codebase should be able to do this in 15-60 minutes, after understanding the relevant function.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues were identified that block this sample from being usable. The problem is framed in a real-world documentation/code context and solvable using reasonable domain inference for Sphinx extensions. While more examples would help, enough information is present for the task.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-10673": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly well-specified: it focused on allowing the inclusion of 'genindex', 'modindex', and 'search' in Sphinx toctree directives without triggering warnings/errors for non-existing documents. The motivation is explained, and a specific snippet of toctree syntax is supplied that should work without errors or warnings. However, the technical background (specifically, what Sphinx \\\"generated documents\\\" are and how their handling is different from ordinary documents) is assumed. This requires the implementer to understand how Sphinx constructs these indices and why they are currently omitted with errors. A reasonably experienced engineer, after looking into Sphinx internals, will be able to map the request to code, but the root cause and the actual internal fix are not immediately specified. No file names or classes/functions are given, but the direction is concrete enough for a meaningful and testable solution.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"The patch touches several source files and is not a simple change: it requires knowledge of Sphinx's handling of toctree, generated document names, and the way toctree directives are parsed and resolved. The fix updates code in multiple places to account for generated indices (genindex, modindex, search) as valid toctree entries, which involves understanding both Sphinx\u2019s environment and how labels work internally. The test coverage requires creating new test roots and checking that the indices appear correctly, which implies some non-trivial investigation and understanding of both the codebase and its testing setup. For someone familiar with Sphinx internals, this is an intermediate fix, but it is unlikely to be resolved in under an hour without prior experience with this subsystem. Estimating 1-4 hours for investigation, implementation, and validation.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no additional issues that would make this sample unsuitable for evaluating coding ability. The issue is specific, actionable, and associated with a concrete desired behavior. The test patch confirms that verifying the solution is straightforward. There is some assumed domain knowledge (about Sphinx's generated documents), but the requirement is clear enough for someone familiar with the domain.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7440": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is vague and lacks precision regarding the exact problem. It simply says \\\"glossary duplicate term with a different case\\\" and points to a line in a file, without explaining the expected behavior, error messages, or steps to reproduce. It also references an external Travis CI job, but the content of this job is not revealed. There is no concrete description of what is broken, only the implication that duplicate glossary terms differing only in case is a problem. An engineer could infer that the glossary should distinguish between terms like 'term' and 'TERM', but the description does not clearly spell this out or specify whether duplicate entries are being merged, failing, or causing a build issue. Therefore, there is significant ambiguity requiring interpretation, making it a level 2 ('vague/ambiguous') issue specification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The patch demonstrates that only a small modification (removing .lower()) is needed in two locations, which is a straightforward code change for someone familiar with the codebase. The scope is limited to a couple of lines, and updating the test to include an upper-case term follows directly. Once the bug is understood, the code change is quick to implement, fitting into the 15min-1hr range.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Despite the vague issue description, it is possible\u2014by inferring reasonable intent\u2014to construct a meaningful benchmark problem once the gist is established (i.e., glossary terms should respect case and not treat 'term' and 'TERM' as duplicates). However, the biggest weakness is that participants are likely to be confused or make incorrect assumptions due to lack of explicit requirements or examples of the problem's effects. If clarity is improved, this would be a fine benchmark sample. No other major issues noted.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7462": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides a clear summary of the problem: Sphinx raises an 'IndexError: pop from empty list' when handling an empty tuple type annotation as per mypy's notation. The error is reproducible in a known environment and relates specifically to a corner case involving type annotation parsing. However, the description does not precisely specify what the correct output should be for the empty tuple annotation (e.g., how it should be rendered or displayed), nor is there a detailed trace or example input/output beyond the error and context. An experienced engineer can deduce the likely expectation based on the context, but there are some gaps to fill in. Additional info such as a minimal code snippet that triggers the error, or desired output, would make it even clearer. Nonetheless, it is interpretable and actionable without ambiguity.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix affects the logic in two locations where AST 'Tuple' nodes are unparsed. The solution involves a small refactor: handle the empty-tuple case separately to avoid popping an item from an empty list and to ensure proper rendering (return '()'). The code change is concise and localized. An engineer familiar with Sphinx and/or how type annotations are parsed can implement and test this within 15-60 minutes, including validation against the test suite. It requires some domain understanding but no extensive algorithmic effort or large-scale code rewrite.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no external dependencies or ambiguous requirements that would block this issue from being used as a coding benchmark. The absence of a minimal input/output example is not prohibitive, as the patch and tests clarify intent. The type of reasoning required (handling a syntactic edge case) is reasonable for evaluation.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-7985": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is vague and leaves significant ambiguity about the precise requirements. The main statement, 'linkcheck currently doesn't check local (internal) links, but this would be useful,' is brief with no details about how local links are defined, what 'checking' a local link should entail, or how they should be treated compared to external links. There are no concrete examples, acceptance criteria, or mention of what behavior is desired, e.g., what constitutes a valid or broken local link. The reader must infer intent and fill in gaps. While an experienced engineer could draw reasonable conclusions (e.g., that 'linkcheck' should attempt to verify the existence or status of local files referenced in links), the specification is not explicit or detailed, and interpretation may vary.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"While the problem ultimately requires enhancing linkcheck to detect and handle local file links (and possibly to report their existence or lack thereof), the patch indicates the change involves adding logic to identify and check local URIs within an existing code path. This is not a trivial fix but is likely small in code footprint and does not appear to require major refactoring or novel algorithms. Some code investigation and understanding of the codebase's flow is necessary, but for someone familiar with Sphinx, this should be achievable in 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers or reasons to exclude this issue for evaluating coding ability, aside from the previously noted lack of concrete specification in the issue text. While more details or explicit examples in the GitHub issue would boost clarity, the modifications and desired test outcomes can be reasonably deduced by an engineer with moderate familiarity with static site generators.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8269": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is mostly well-specified: it describes a specific bug where the linkcheck command in Sphinx reports 'anchor not found' when, in fact, the true source of error is an HTTP error from the server (like 404 or 500). The expected behavior is that the tool should report the HTTP error instead of (or before) reporting a missing anchor. The environment is specified, and from the description, it is clear what is going wrong and roughly what needs to be changed\u2014namely, that error status codes from the server should cause the command to fail/report appropriately. It is not 100% prescriptive about the exact phrasing or format of the error, but for an experienced engineer, it is clear enough what functionality is expected.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change involves a small, targeted modification in the code\u2014ensuring that HTTP error statuses cause an appropriate exception/reporting, rather than continuing to check for anchors (which is pointless if the page cannot be loaded). The code patch consists mainly of a single line to raise errors when the response is not OK. The associated test requires setting up a simple local server and adding a test to validate behavior, which, while not entirely trivial, is a standard testing approach. Estimated time would be between 15 minutes and 1 hour for someone familiar with the codebase.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues with this sample. The problem and fix are clear, and the task requires a precise modification plus a test. The test setup (local HTTP server, checking output) is relatively standard and not unusual for this context. No other factors would disqualify this issue from use in the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-8551": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is mostly well-specified: it clearly describes buggy warning behavior in Sphinx when using :type: and :rtype: in documentation fields, explaining that implicit cross-references do ambiguous lookups compared to explicit xref roles. The expected behavior is stated\u2014no warnings, and correct resolution of types to a specific module/class. However, it requires inference about the underlying mechanism, such as the distinction between lookup strategies for different field types, and how Sphinx resolves cross-references. The context assumes familiarity with Sphinx internals but gives enough detail for an informed engineer to attempt a solution. There are some small blanks: e.g., it doesn\u2019t specify exact code locations or test cases, and assumes the engineer understands Sphinx's docfield/context system and what a warning in this context means.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue likely takes 1-4 hours for an experienced engineer. While the fix involves a relatively brief code change once the problem is found, correctly identifying the lookup behavior difference, finding where implicit versus explicit xrefs are resolved, and updating the cross-reference context is non-trivial and requires knowledge of Sphinx internals. The engineer would need to trace the docfield and reference resolution code, understand how environment context is managed, and ensure their change doesn't break existing functionality. There may also be some trial-and-error to get the module/class scoping correct and tests passing.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major concerns beyond those discussed. The issue has a well-structured test confirming correct reference resolution and context, and would be a reasonable benchmark of backend Sphinx/domain handling knowledge. As usual, the need for Sphinx internals familiarity is the biggest challenge, but this is in-scope for a skills benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sphinx-doc__sphinx-9711": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it clearly describes a bug in version checking for Sphinx extensions within the 'needs_extensions' check, explaining the mistake of comparing version strings lexicographically instead of numerically (e.g., '0.6' > '0.10' as strings, but 0.6 < 0.10 as versions). The expected behavior is specified using a concrete example: sphinx-gallery 0.10.0 should be accepted when 0.6 is the minimum required, clarifying the direction needed for a fix. The context and module where this takes place are explicit. For an experienced developer with access to the codebase, requirements, and test suite, the issue provides sufficient context to implement and validate a fix primarily in the extension version checking logic.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This issue requires a change that is contained within a single function responsible for checking version requirements. The main change is to replace string comparison with proper version-aware comparison, which can be accomplished by using a standard version parsing/comparison library (such as 'packaging.version'). This is a well-understood change pattern; although it requires choosing the proper tool and potentially handling exceptions for invalid versions, it is otherwise straightforward for an experienced engineer. The test changes are minimal and require updating or adding test cases to ensure proper functioning. Total estimated time is 15-60 minutes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues remain unaddressed. The description is clear, relevant tests can be written, and dependencies (version comparison packages) are industry standard. The only caveat is ensuring consistency for all possible version formats, but this is a normal consideration for this kind of issue.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13372": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides some context, including the actual error (UnboundLocalError in evalf) and a probable solution (raising NotImplementedError in certain cases in the 'elif' clauses that define reprec and imprec). While it references a specific code path and describes that changing the order of Mul args triggered the problem, it does not specify the exact input or scenario that causes the failure. However, it gives a reasonably concrete direction: check elif clauses and add else: raise NotImplementedError. This leaves some blanks (the exact lines, the context in which the error occurs), but for an experienced engineer with access to the code, it's possible to fill in these gaps and reach a sensible solution. Thus, it is not fully specified, but not ambiguous enough to be unusable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix as reflected in the gold patch involves adding else clauses to raise NotImplementedError in two places, following the guidance given in the issue text. Locating the precise places to add these checks requires navigating to the correct function (evalf in sympy/core/evalf.py) and understanding a bit of the surrounding logic, but this does not require a deep or time-consuming rewrite. For someone familiar with the codebase, this should take 15 minutes to 1 hour, accounting for investigation, edit, and verification.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional issues are apparent. The issue is reasonably well-specified for an engineer familiar with the codebase, and the required code/test changes are straightforward. While more detail in the issue text would have been helpful, the engineer receiving this would have little difficulty inferring the fix.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13480": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is somewhat vague and lacks specific details. It states that .subs on coth(log(tan(x))) errors for certain integral values and lists some failing integers, but does not specify what the error is (exception? incorrect output?), what the correct behavior should be, or in what context .subs is being used (e.g., what should be substituted, and for which values of x?). There are no code samples, stack traces, or explicit statements about expected/correct results. An engineer familiar with the codebase could attempt to interpret and localize the problem, but would need to make assumptions about both the error and the desired result. This leaves considerable ambiguity about what the successful fix should achieve and how it should behave in edge cases.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Despite the vague specification, the actual code change is quite small\u2014a conditional fix related to the handling of particular values within the coth function. A developer experienced with the codebase could likely diagnose and resolve the error within 15 minutes to an hour, especially factoring in debugging and ensuring correct mathematical behavior for these function evaluations. The patch and corresponding test additions confirm that the fix is localized and not especially complex, even though it requires understanding some nuances of symbolic math.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"While the sample is usable for coding evaluation benchmarks, it is important to note that the issue description does not provide any actual example code or guidance on the nature of the error (exception, wrong value, etc.). The patch and test fill in these blanks, but real candidates would need to infer much of this themselves. The ambiguity may partially test debugging and inferential skills rather than just direct implementation ability.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-13877": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is quite vague and does not directly specify a concrete problem or expected behavior. It only suggests a suspicion about the applicability of the Bareiss algorithm for non-integer matrices, particularly when symbolic entries are involved. There is no specific example given, no clear bug description (beyond a title referencing Invalid NaN comparison), and no guidance about the desired behavior or fix. An engineer would need to investigate what happens with determinant calculation on symbolic matrices, deduce if and how Bareiss fails, and figure out how to address it. While there is a meaningful technical pointer provided (Bareiss may be inappropriate for symbolic matrices), the problem definition and expected outcome are left open-ended.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue appears to require at least a few hours of work for an experienced engineer. They would need to investigate how the determinant is calculated using the Bareiss method when symbolic entries are present, understand why NaN comparisons become invalid, and devise a fix that is mathematically sound. The solution might entail understanding and modifying the core determinant computation, introducing a new zero-check function suitable for symbolic input, and adding or updating relevant tests. This likely requires knowledge of symbolic algebra, the numerical pitfalls of typical algorithms, and investigation into current code behavior.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"While the issue is vague, it is possible for an engineer familiar with matrix algorithms and symbolic computation to infer a meaningful direction for a solution given the technical hint. However, the lack of an explicit minimal example in the issue description means it requires higher baseline knowledge and investigation skills. The sample is still usable but not ideal for a benchmark that aims for clarity in the problem statement.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-16792": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue is reasonably well specified. It describes a failure mode for the 'autowrap' tool with the Cython backend in sympy's code generation utilities. Specifically, when a function has array arguments that do not appear in the final output expression, the generated code misidentifies these array arguments as scalars in the C function signature. The description includes hints about what goes wrong (C signatures are wrong, affects interfacing with external libraries), and gives a concrete scenario and cause (problem in how codegen identifies array arguments if they are unused in the expr). However, it does not specify exactly where in the code the bug is, or a completely minimal reproducible example in the codebase \u2013 a sensible engineer could still implement a fix based on this, but might need to decide on specific implementation or edge cases. It's not as exhaustive as the best bug reports. Thus, I score it as 1.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is relatively focused and isolated to argument handling in the code generation paths. The code patch only affects the handling and metadata of arguments of code-generated routines in codegen.py. The engineer will need to understand how arguments are identified and passed in sympy's codegen utilities and ensure that array arguments are always treated with correct metadata, even when not referenced in the final output expression. This task requires familiarity with the codegen framework but doesn't require deeper algorithmic complexity or re-architecting significant code; it is one clear bug fix that might take a bit of tracing, but is not a substantial rewrite. Estimated developer effort: 15min\u20131h.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues detected. The issue matches the type of coding ability we want to assess, especially around understanding codegen, argument handling, and being careful with edge cases around unused parameters. The only minor challenge would be if someone was totally unfamiliar with the code generation flow, but this is suitable for a technical benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17139": "{\"q1_1_is_well_specified\":3,\"q1_2_explanation\":\"The issue description is extremely minimal \u2013 it only states: \\\"simplify(cos(x)**I): Invalid comparison of complex I (fu.py)\\\". This leaves a lot to be desired in terms of context. There is no description of the current or expected behavior, what \u201cinvalid comparison\u201d refers to, or any reproduction steps. It mentions fu.py, implying the location of relevant code, but otherwise it is not clear what code path or scenario is broken. An engineer coming in cold would not know exactly what is required, why it's a problem, or what a correct fix should look like, without digging into the code or seeking clarification. Therefore, this issue is very underspecified and almost impossible to act on directly as described.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The gold patch and test coverage reveal that, once the problem is understood, the code change is relatively minor \u2013 a simple guard checking if the exponent is real is added, and a few tests for complex exponents are introduced. An experienced developer, assuming the problem is clarified, could implement this in 15-60 minutes. However, reaching the right understanding would require much more effort without extra context.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"The issue does not provide any input/output examples, error traces, nor does it explain why this is an issue or what the user expects as a fix. The real-world usability of this issue as an evaluation sample is low \u2013 without access to discussions or related PRs, the candidate could easily misinterpret the requirements. This undermines the validity of testing coding ability using this issue sample.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17318": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description has some blanks to fill in but admits a reasonable interpretation. It states that sqrtdenest raises IndexError if an expression cannot be denested, and that such expressions should be returned unchanged. The objective is to modify sqrtdenest so that, upon failure to denest, it returns the input as-is rather than throwing IndexError. There is a mention of \\\"fixed for sqrtdenest\\\" and that now the expression will be returned unchanged if it cannot be denested, confirming the desired behavior. However, it is not very explicit about in which circumstances the failure occurs or specific examples. The absence of concrete steps to reproduce and lack of explicit input/output behavior make some details left up to the implementer, but the requirement itself is clear and actionable.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The problem is localized to ensuring sqrtdenest (or a helper function) does not raise IndexError in a specific failure mode, but returns the original expression instead. An experienced engineer may need to read the relevant code paths and adapt exception handling or add pre-condition checks to meet this requirement. The diff confirms that the change is relatively minor, involving a simple conditional check on the arguments to avoid the error. Thus, the change is relatively contained and should take no more than an hour (likely less) for someone who understands the code.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major issues that would preclude use for assessing coding ability. This issue is suitable for evaluating error-handling and behavior correctness in the absence of full explicit requirements, which is a common real-world task.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17630": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides specific information on the bug: when performing block matrix multiplication with blocks containing ZeroMatrix, multiplying once succeeds, but multiplying twice raises an exception. It suspects the intermediate result contains zeros as Zero instead of ZeroMatrix, referencing SymPy's internal matrix types. While there is some ambiguity (e.g., no explicit example code, stack trace, or affected methods/classes), the problem is clear enough: matrix block multiplication does not consistently handle ZeroMatrix, leading to exceptions. A sensible interpretation is to ensure block multiplication consistently produces ZeroMatrix blocks as results and to debug why the wrong type appears. The issue is not perfectly specified, but it provides enough concrete context for an experienced engineer to make progress without further clarification.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix requires understanding how block matrices, ZeroMatrix, and multiplication logic interact internally in SymPy. This involves reading relevant SymPy matrix expression and block handling code, reproducing the issue (the problem description is clear enough for this), and determining why and where a plain Zero is produced instead of ZeroMatrix during nested multiplications. The patch demonstrates the fix is rather localized and small (adding a short conditional in matexpr.py), and the associated tests are straightforward. An experienced engineer could likely diagnose and patch this within one hour after familiarizing themselves with the relevant matrix expression handling. Some knowledge of SymPy's structure is beneficial, but the problem and fix scope are contained. Thus, I rate it 1.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional concerns. While the issue itself could benefit from including a minimal code example and the actual exception message, the information is enough for an engineer to work from and for a benchmark. The tests in the patch provide clear pass/fail criteria. This is suitable for coding ability evaluation, particularly in debugging and understanding underlying type consistency in a codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-17655": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue is well-specified: it states clearly that multiplying a geometry.Point by a number should work the same regardless of the order of multiplication. This clearly implies the need for implementing the __rmul__ method for the Point class, so that 'scalar * Point' works the same as 'Point * scalar'. The intent, expected behavior, and required change are obvious to anyone familiar with Python operator overloading. No filenames or class names are given, but the context (geometry.Point) is enough to immediately locate the relevant code for anyone familiarizing themselves with the project.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"This change is small and clearly defined (implement the __rmul__ method), requiring minimal investigation of the relevant class and its methods. The fix is simple for an experienced engineer: define one method delegating to the already-existing __mul__. Testing is straightforward by checking both multiplication orders. This should take at most 1 hour including validation and test-checking.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues or blockers are apparent: the issue is clear, the solution is straightforward, and the type of testing required is obvious based on the description. The issue is suitable for inclusion in the benchmark.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-18211": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is fairly clear: when using solveset (or, by context, as_set()), a NotImplementedError is raised in some unsolved cases, but it would be better if a ConditionSet were returned, as exemplified in the issue text. The example provided gives a specific scenario: ConditionSet(n, Eq(n*cos(n) - 3*sin(n), 0), Reals) is expected for such cases. The description is a bit brief, requiring some minimal understanding of how unsolvable equations should fallback to a ConditionSet. However, the main requirement is clear enough for an experienced engineer: intercept NotImplementedError and return ConditionSet instead. The referenced methods and expected output are concrete. Thus, there is a reasonable, sensible interpretation of what needs to change, though some blanks about other edge cases remain.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The required code change is targeted: it requires catching NotImplementedError in a specific part of the code and returning a constructed ConditionSet instead. This is not a trivial one-line fix (since it requires knowing where in the symbolic logic flow to apply the change and importing the right symbols), but it is also not a large change\u2014just a few lines, with associated tests. An engineer already familiar with the sympy codebase could implement this in well under an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues identified: the question of what to do when solveset cannot solve directly is clearly illustrated, and test changes in the provided patch support the correct behavior. The sample is well-suited for coding benchmarks that assess understanding of exception handling and symbolic logic.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20428": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified. It clearly points to a problem with the internal representation of zero polynomials resulting from clear_denoms(), specifically describing unexpected behavior where Poly prints as zero but is_zero is False and other methods like terms_gcd fail. The root cause is also pinpointed\u2014the DMP representation retains a leading zero coefficient where it should have an empty list. However, it doesn't name the affected functions explicitly, nor does it specify the exact place to patch. Still, an experienced engineer familiar with SymPy would reasonably infer which classes and code paths are relevant and what the desired behavior is (that all ways of producing the zero polynomial in the EX domain should result in the canonical internal representation). The objective is clear: ensure that zero polynomials are represented correctly internally so that methods work consistently. Some underspecification remains about edge cases and the full list of affected methods, but there is a sensible interpretation for a fix.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Although the bug is subtle and requires an understanding of the domain (symbolic computation in Python, internal polynomial representation), the code change is relatively small (a one-line fix to the internal __bool__ method in the expression domain). The task is not just trivial, as it requires reading and understanding the cause, but for someone with context on the codebase it likely takes on the order of 30 minutes to 1 hour, including updating or adding tests.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers for using this as a benchmark issue. The issue is domain-specific and could be challenging for those unfamiliar with symbolic math libraries, but it is still well within the skills expected of a qualified contributor. The description, gold patch, and test coverage are all appropriately aligned for benchmarking coding ability, assuming some background in polynomials and domains.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20438": "{\"q1_1_is_well_specified\":3,\"q1_2_explanation\":\"The issue description provided is extremely vague: \\\"is_subset gives wrong results\\\". There are no details about what inputs cause wrong results, what the expected output should be, what class or method is directly implicated, or even what the correct behavior is. The only context is a reference to a code comment in a merged PR, but per the setup, we are only to use the main issue text. Without knowledge of what the correct behavior is or any examples of failing or expected cases, it would not be feasible for an engineer to make a meaningful attempt at a solution from just this text.\",\"q2_1_difficulty\":3,\"q2_2_explanation\":\"If we adjusted the difficulty to assume the problem is clarified to its intended scope (i.e., fixing incorrect results in is_subset for ProductSet/FiniteSet etc.), the fix itself still spans multiple files, involves nuanced understanding of set relationships in SymPy, and requires careful test changes. However, given only the original issue text, the task becomes almost impossible, since it lacks any specification of the bug. Even with clarification, the need to traverse the codebase to find where is_subset is defined, how it's dispatched, what special cases are missing, and to add the proper logic, makes this a multiple-file, multi-hour fix, leaning toward the upper end of complexity, especially for someone not already familiar with this part of the code.\",\"q2_3_other_issues\":1,\"q2_4_other_notes\":\"The main issue is that the issue description is far too vague to use in a benchmark: without examples, expected output or reference to a use-case or specific context, it is impossible to know what constitutes a correct fix. This means participants would not be able to purely rely on reasoning skills or code navigation, but would essentially need to guess or rely on private knowledge. Thus, this is not suitable as a standalone coding benchmark. If we were to use regression or behavioral tests written after the fact, this could show the correct behavior, but the purpose here is to test reasoning from just the issue text.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-20590": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified. It describes a behavioral regression: in version 1.6.2, Symbol instances did not have a __dict__, but in 1.7 they do, likely due to a change with __slots__. The user suspects a parent class stopped defining __slots__. The desired behavior is that Symbol instances should not have a __dict__ attribute, which aligns with Python convention for using __slots__. However, it is not explicit which class(es) to modify, which packages/files are relevant, or whether this should be enforced across all subclasses. Still, there is a sensible and concrete interpretation, and an engineer familiar with Python and the codebase should be able to identify the source of the regression and correct the class definition to restore the __slots__ behavior.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Once the problem is understood, the fix is straightforward and limited to a small code change \u2014 ensuring the relevant class (or mixin) has __slots__ defined. Understanding the root cause may require a quick look through the inheritance tree, but for an experienced engineer, this should take less than an hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major blockers: the issue has a clear regression and its fix is localizable. Ambiguity does exist around which class should be fixed (especially if unfamiliar with the class hierarchy), but the code and test patches serve to clarify this for benchmark purposes.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-21379": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides some concrete details (unexpected PolynomialError upon substitution in specific expressions involving hyperbolic functions, piecewise arguments, and assumptions on realness). However, it does not include a minimal reproducible example, nor precise code snippets, and the observed behavior is piecemeal (\\\"the error goes away if...\\\", etc.). As an engineer, I can reasonably infer that a bug occurs in SymPy when using subs with some combination of cosh, tanh, exp, real symbol assumptions, division, and piecewise functions. The goal is to prevent a PolynomialError from being raised in these cases. A sensible fix could be to guard PolynomialError or handle such cases more robustly. Yet, there is still some interpretation left: the exact desired behavior is not specified besides \\\"don't error\\\" (e.g., what should be returned or how exceptions should be handled). Overall, the issue is reasonably specific, although the solution boundary is not fully explicit, so I rate this as level 1: well enough specified for a meaningful solution, but requiring some interpretation.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The fix as shown in the gold patch is relatively contained: it adds a try/except block to handle PolynomialError when computing gcd and falls back to a default value. This would likely be understood and implemented in 15 minutes to 1 hour by an experienced engineer after tracing the call stack and reproducing the error. It requires familiarity with the relevant code and SymPy's exception model, but not deep refactoring or broad code changes.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are present. The issue is well-suited to evaluating code comprehension and the ability to handle exceptions or design robust behavior under edge cases. While the bug scenario is somewhat esoteric, the test coverage added provides a concrete basis to confirm if a solution is correct.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-22714": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue describes a specific error encountered when using sp.S on a Point2D expression within an evaluate(False) context in SymPy. It also notes that the error does not occur without evaluate(False) or with evaluate=False passed directly to sp.S. While the description clearly identifies the problem scenario and the unwanted error, it does not specify the precise cause, expected internal behavior, or the desired final outcome beyond not raising the error. However, an experienced developer could reasonably infer that the solution is to ensure Point2D/Point does not raise an error in this specific evaluate context, likely relating to handling of imaginary parts in coordinates. Thus, the problem is mostly well-identified, with some interpretation needed, but is sensible enough for a meaningful solution attempt.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"The issue appears to be a relatively small bug fix: the patch is a one-line change in the Point class, adjusting how imaginary coordinates are checked. Understanding the problem likely involves reproducing the bug, tracing through the Point constructor and how it interprets numbers with imaginary parts, and then updating the specific condition. The core logic is encapsulated in a small part of the code and there are clear pointers (usage of evaluate context, error message) in the issue. An experienced engineer could probably find, fix, and test this within 15 minutes to 1 hour.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No other major issues are apparent. The scenario is reproduciable based on the description and the fix is tightly scoped. This sample appears suitable for a coding benchmark as it requires understanding both a subtle bug and its correct context-specific handling in the codebase.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23824": "{\"q1_1_is_well_specified\":0,\"q1_2_explanation\":\"The issue description is precise and actionable. It states that kahane_simplify() is intended to simplify products of gamma matrices, using identities for contracted indices (e.g., \u03b3^\u03bc \u03b3_\u03bc = 4 I\u2084), and leading uncontracted gamma matrices should be preserved in order. The description identifies the bug: leading uncontracted gamma matrices are inserted in reverse order when restoring them to the expression. Clear input/output examples are given: both \u03b3^\u03bc \u03b3_\u03bc \u03b3^\u03c1 \u03b3^\u03c3 and \u03b3^\u03c1 \u03b3^\u03c3 \u03b3^\u03bc \u03b3_\u03bc should simplify to 4 \u03b3^\u03c1 \u03b3^\u03c3 (not 4 \u03b3^\u03c3 \u03b3^\u03c1). The specific root cause is described: the loop that re-inserts the matrices is backward. An experienced engineer could confidently attempt a fix with this information. The relevant function is named, and the manipulation steps are clear.\",\"q2_1_difficulty\":1,\"q2_2_explanation\":\"Identifying the bug and correcting the insertion order in kahane_simplify() is a targeted change in a specific function. The core logic fix is straightforward (adjusting how the uncontracted gamma matrices are reinserted), though the engineer does require understanding of the mathematical context and the specifics of the function's manipulation of terms. For someone familiar with the codebase, this is a task that would likely take between 15 minutes and an hour. There is minimal code to change, but care must be taken to ensure the fix doesn't inadvertently break other matrix orderings.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"There are no major blockers to using this issue for evaluation. The description is clear, scoped, and testable. While some familiarity with the mathematics of gamma matrices helps, the main coding task is algorithmic and clear from the description and examples given. The test patch demonstrates that the input/output examples are checked. This sample is appropriate for benchmarking coding ability in a specialized scientific codebase context.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-23950": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description provides enough detail for a reasonably knowledgeable contributor to deduce the core problem. It points out that the Contains.as_set method currently returns Contains itself (a boolean), which is not a set, and this causes failures elsewhere because Contains does not have as_relational. The desired behavior can be inferred: as_set should return the set argument contained in a Contains expression. Although a specific expected output example is not provided, the problem is clear and there's a sensible interpretation: change the implementation of as_set in Contains to return the second argument (the set), rather than raising NotImplementedError or returning self. The link to the code and the context of why the change is needed is enough, but lacks extensive context or justification/explanation of broader impacts. Still, for an experienced engineer, this description enables a clear solution path.\",\"q2_1_difficulty\":0,\"q2_2_explanation\":\"This is a <15 min fix for an experienced engineer who is familiar with the codebase: the required change is to update the as_set method of Contains so that it returns the set provided as its second argument. There's no refactoring required, just a simple method override. The task is straightforward after reading the issue description.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No major issues are present that would make this issue unsuitable for coding benchmarks. The task is directly actionable and tests a basic understanding of class structure and return value semantics in Python, as well as minimal knowledge of the sympy codebase. The lack of a concise expected output in the issue is the only mild drawback.\",\"q2_5_confidence\":5}"
    },
    {
        "sympy__sympy-24066": "{\"q1_1_is_well_specified\":2,\"q1_2_explanation\":\"The issue description is very terse: \\\"SI._collect_factor_and_dimension() cannot properly detect that exponent is dimensionless.\\\" It names the method involved and part of the problem, but gives no context, examples, or explanation of how or when the detection fails, or what behavior is expected. Reading only the issue text, an engineer is left to guess: what is meant by \\\"exponent\\\" here; what input causes the failure; what does \\\"properly detect\\\" mean; what is the expected versus actual output? The lack of detail leaves considerable ambiguity. While an experienced developer in this domain might deduce something, the instructions are much less complete than the examples of well-specified or even moderately-specified issues provided above. Without seeing the associated code or tests, it is unclear how to proceed specifically. Thus, it falls closest to 'vague and ambiguous', though it does mention the relevant method and idea of dimensionlessness.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"Due to the lack of specificity in the issue text, the engineer would need to invest a good amount of time understanding the context and diving into relevant code, and manually determine what \\\"exponent\\\" refers to and how the method should operate. This likely involves digging into SI unit systems, dimension handling logic, understanding both what the current implementation does and what the correct definition would be, then writing and verifying changes. While the actual code fix as shown in the patch is not massively complex, the time spent on analysis and validation likely pushes this into the 1-4 hour bracket, especially without a clear, concise statement of the problem.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"Although the patch and tests clarify the needed fix much more, the issue description alone is not ideal for a coding benchmark\u2014lacking concrete examples, expected/actual outputs, or guidance on what inputs should be handled. It relies on substantial (and possibly domain-specific) inference.\\n\\nHowever, it does not have any critical blockers (such as security, privacy, or copyright problems), and could still be salvageable if the intent is to measure not only code-writing but also inference and investigation ability.\",\"q2_5_confidence\":4}"
    },
    {
        "sympy__sympy-24443": "{\"q1_1_is_well_specified\":1,\"q1_2_explanation\":\"The issue description is moderately well-specified: it clearly states that _check_homomorphism is broken for PermutationGroups, pinpointing the problem to the _image() function and the handling of inverted generators. It identifies the relevant code (sympy/combinatorics/homomorphisms.py, lines 336-337) and what specifically is going wrong (the 'in gens' test fails for inverted generators). The suggestion that the whole thing could be simplified implies freedom in the approach as long as the core bug (handling inverted generators) is fixed. However, the description assumes some familiarity with the domain, and does not give an example input that breaks, the expected correct behavior, or a specification of what constitutes 'correct' for this internal function. Nevertheless, with access to the codebase and tests, an experienced developer can infer the fix and the core requirement.\",\"q2_1_difficulty\":2,\"q2_2_explanation\":\"This issue requires understanding the sympy/combinatorics/homomorphisms.py internals, the behavior of PermutationGroups, and the relation between generators and their images under homomorphisms. The fix involves refactoring the logic for handling how relators and generators are mapped during homomorphism checks, which is nontrivial and requires solid domain understanding and careful editing of a medium-complexity function. It is more than a small bugfix, involves logic rework, and ensuring tests pass for subtle group theory behaviors, placing it in the 1-4 hour range for an experienced engineer.\",\"q2_3_other_issues\":0,\"q2_4_other_notes\":\"No additional major issues are apparent. While the problem isn't described with a worked example, the blame is clearly enough localized that a developer with basic context about sympy's combinatorics can attempt a well-motivated fix. The test patch demonstrates that new behavior (a test involving DihedralGroup(3)) is covered.\",\"q2_5_confidence\":5}"
    }
]