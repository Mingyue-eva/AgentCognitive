[
    {
        "astropy__astropy-13033": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when trying to remove a required column from a TimeSeries object, the error message is misleading and confusing. The issue provides: (1) A clear description of expected vs actual behavior, (2) Detailed step-by-step reproduction instructions including specific code to create a TimeSeries with custom required columns and attempt to remove one, (3) The exact misleading error message currently shown, (4) What the error message should convey instead (that required columns are missing). The reproduction steps are precise enough that an engineer can immediately understand the problem and know exactly what needs to be fixed - improving the error message formatting in the _check_required_columns method to properly handle multiple required columns.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution involves modifying the error message formatting in the _check_required_columns method of astropy/timeseries/core.py. The fix adds a helper function to properly format column names (handling both single columns and lists) and updates the error message to use this formatting. This is a localized change that requires understanding the existing error handling logic and improving the string formatting, but doesn't require deep architectural changes or extensive codebase knowledge. The main work is figuring out how to properly format the column names in the error message, which is straightforward once you locate the relevant code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, reproducible steps, and a straightforward solution involving error message improvement. The issue is suitable for evaluating coding ability as it tests understanding of error handling, string formatting, and codebase navigation skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-13977": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some blanks to fill in. The core problem is clearly described: `Quantity.__array_ufunc__()` should return `NotImplemented` instead of raising `ValueError` when inputs are incompatible, specifically to allow duck typing and reflected operations to work properly. The issue provides a concrete example with `DuckArray` that demonstrates the problem, shows the exact error traceback, and references the NumPy documentation about when `NotImplemented` should be returned. However, some implementation details are left ambiguous - it's not entirely clear what constitutes \"incompatible inputs\" beyond the specific case shown, and the exact conditions under which `NotImplemented` should be returned versus raising an exception need to be inferred. The issue author mentions they're \"more than happy to open a PR\" which suggests they have a clear idea of the solution, but the specifics of when to return `NotImplemented` vs raise exceptions would need to be determined by examining similar patterns in the codebase.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding the numpy ufunc protocol, the current error handling in `Quantity.__array_ufunc__()`, and implementing proper exception handling that returns `NotImplemented` in appropriate cases while still raising exceptions when needed. The solution involves wrapping the existing logic in a try-catch block and determining when to return `NotImplemented` versus re-raising exceptions based on the types of inputs/outputs. Looking at the patch, it adds about 20 lines of code with sophisticated logic to check if other objects in the operation chain have custom `__array_ufunc__` implementations. This requires understanding the numpy ufunc dispatch mechanism and astropy's unit conversion system. The engineer would need time to understand when returning `NotImplemented` is appropriate versus when genuine errors should still be raised.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-structured issue that would make a good benchmark sample. The problem is clearly motivated by real-world duck typing needs, the error is reproducible, and the solution requires understanding both the numpy ufunc protocol and astropy's internal architecture. The test cases in the patch are comprehensive and test various scenarios with different duck types. An engineer would need to understand the difference between cases where NotImplemented should be returned (when other objects can handle the operation) versus when exceptions should still be raised (when there are genuine type/unit incompatibilities that no object can resolve).",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14096": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. The problem is described with a concrete example: when subclassing SkyCoord and adding a property that accesses a non-existent attribute, the error message incorrectly reports that the property itself is missing rather than the actual missing attribute. The issue provides a specific reproduction case with a custom_coord subclass that has a 'prop' property accessing 'self.random_attr'. The expected vs actual behavior is clearly explained - the error should mention 'random_attr' is missing, not 'prop'. The traceback shows the exact error location in sky_coordinate.py line 600 in the __getattr__ method.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that requires 15 minutes to 1 hour. The issue is in the __getattr__ method of SkyCoord class where it raises a generic AttributeError instead of letting Python's normal attribute access mechanism handle it properly. Looking at the gold patch, the solution is simple: replace the custom AttributeError with a call to __getattribute__ which will provide the correct exception message. The fix involves understanding how Python's attribute access works and changing just a few lines in one method. An experienced engineer familiar with Python's descriptor protocol would quickly identify this as an attribute access delegation issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the test case validates the expected behavior. This is a good example for a coding benchmark as it tests understanding of Python's attribute access mechanism and proper error handling in subclassing scenarios.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14182": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the `header_rows` parameter works with `format=\"ascii.fixed_width\"` but fails with `format=\"ascii.rst\"`, producing a specific TypeError. The user provides a concrete example with QTable containing \"wave\" and \"response\" columns, shows the exact error traceback indicating that `RST.__init__()` doesn't accept the `header_rows` keyword argument, and explains the desired functionality (supporting multiple header rows in RST output for documentation purposes). The issue includes sufficient context about what currently works, what fails, and what the expected behavior should be.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The error message clearly points to the problem: the RST class doesn't accept the header_rows parameter in its __init__ method. Looking at the solution, it requires: (1) Adding header_rows parameter to RST.__init__(), (2) Modifying the write method to handle multiple header rows by using the correct index for the separator line, and (3) Updating the read method to set the correct start_line based on header row count. These are relatively straightforward changes that involve understanding how the existing FixedWidth parent class works and adapting the RST subclass to support the same header_rows functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is well-scoped, and the test case demonstrates both reading and writing functionality. This is a good benchmark issue that tests understanding of inheritance, parameter passing, and ASCII table formatting logic.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14309": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) Clear context about when the problem occurs - cron tests in HENDRICS started failing with identify_format, (2) Complete stack trace showing the exact error: IndexError: tuple index out of range in astropy/io/fits/connect.py line 72, (3) Explicit reproduction steps with the exact function call: identify_format(\"write\", Table, \"bububu.ecsv\", None, [], {}), (4) Expected vs actual behavior - the function should handle the case when args is empty but instead tries to access args[0], (5) Reference to the specific commit that likely introduced the issue. The root cause is clear: the is_fits function assumes args[0] exists but when calling identify_format with an empty args list, this causes an IndexError.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-isolated to a single function (is_fits in astropy/io/fits/connect.py) and the problem is clear from the stack trace. Looking at the gold patch confirms this assessment - it's a simple logic fix that changes the control flow to return early when checking file extensions rather than falling through to check args[0]. The fix involves moving a return statement and removing an if condition, which is a small, focused change that requires minimal thought once you understand the control flow issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with clear reproduction steps and a straightforward fix. The issue demonstrates good software engineering practices with proper error reporting and context. The test patch also shows appropriate regression testing to prevent this issue from recurring.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14365": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly states the problem: astropy's QDP table format reader incorrectly assumes QDP commands must be uppercase, when the actual QDP format is case-insensitive. The issue provides a concrete reproduction case (creating a file with \"read serr 1 2\" instead of \"READ SERR 1 2\"), shows the exact error message that occurs, demonstrates that the native QDP tool handles lowercase commands fine, and specifies the expected behavior (the table should load successfully with errors included). The problem is focused and unambiguous - make the QDP parser case-insensitive for commands.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The solution involves making regular expression matching case-insensitive, which is a straightforward change. Looking at the actual fix, it required adding re.IGNORECASE flag to one regex compilation and changing one string comparison to be case-insensitive. An experienced engineer would need some time to locate the QDP parsing code in astropy's codebase (likely in astropy/io/ascii/qdp.py), understand how the command parsing works, and identify where case sensitivity is enforced. But once found, the changes are minimal and the solution approach is obvious.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, has a straightforward reproduction case, and the solution involves a focused change to parsing logic. The test changes also appropriately verify both uppercase and lowercase variants work correctly. The issue demonstrates good software engineering practices: it provides version information, clear steps to reproduce, expected vs actual behavior, and shows the problem occurs in a realistic scenario (hand-created QDP files).",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14508": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: Card may use unnecessarily long string representations of floats causing comment truncation. It includes a concrete reproduction case with specific values (keyword 'ESO IFM CL RADIUS', value 0.009125, comment '[m] radius arround actuator to avoid'), shows the exact problematic behavior (repr() and str() outputs), demonstrates the warning message, and explains the expected behavior. The issue gives enough context about FITS card formatting limits and shows both the working original card and the broken recreated version. An engineer would clearly understand they need to modify float formatting in Card to use shorter representations while maintaining precision.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min to 1 hour fix. The problem is clearly localized to float formatting in the Card class. Looking at the patch, the solution involves simplifying the `_format_float` function from complex `.16G` formatting with exponent normalization to simply using `str(value)` which produces more compact representations. The core logic change is straightforward - replace the complex formatting with a simpler approach. The main time would be spent understanding the FITS card format constraints, locating the relevant function, and testing that the shorter representations don't break precision requirements. The actual code change is quite minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution area is clear, and the test cases are straightforward. The issue provides good reproduction steps and expected outcomes.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14995": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is generally well-specified with some minor gaps. The problem is clearly described: mask propagation fails in v5.3 when one operand doesn't have a mask, specifically with `handle_mask=np.bitwise_or`, causing a TypeError when trying to perform bitwise OR between an integer and NoneType. The expected behavior is also clear - the existing mask should be copied to the output when one operand lacks a mask. The reproduction steps are detailed enough to understand the issue, mentioning specific operations like multiplying masked/unmasked NDDataRef instances. However, there are some minor gaps: the exact code to reproduce the issue isn't provided (just described), and the internal implementation details aren't fully explained. But an experienced engineer familiar with the codebase could reasonably infer what needs to be fixed based on the error description and expected behavior.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively simple fix that would take 15 minutes to 1 hour. The issue is in the `_arithmetic_mask` method in `astropy/nddata/mixins/ndarithmetic.py`, where the condition `elif operand is None:` should be `elif operand.mask is None:`. This is essentially a one-line fix that corrects the logic for handling cases where an operand exists but doesn't have a mask. The error message clearly points to the problem (bitwise OR between int and NoneType), and the fix involves understanding the distinction between a missing operand versus an operand with no mask. An experienced engineer would need some time to familiarize themselves with the NDData arithmetic system and understand how mask propagation works, but once they locate the relevant code, the fix is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the reproduction case is clear, and the solution is testable. The provided test cases in the patch demonstrate various scenarios for mask arithmetic operations, which would help verify that a solution works correctly. This appears to be a suitable sample for evaluating coding ability as it requires understanding the codebase structure, debugging skills, and logical reasoning about edge cases in mask operations.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-7336": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the `units.quantity_input` decorator fails when used on constructors that have a return type annotation of `-> None`. The issue provides a specific reproducer explaining exactly how to trigger the bug, shows the complete error traceback with the AttributeError: 'NoneType' object has no attribute 'to', identifies the root cause (the decorator tries to call `.to()` on None), provides a workaround (omitting the return type hint), and even suggests a potential fix (checking for None explicitly). All the information needed to understand and solve this issue is present in the description.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. The problem is clearly identified in the error traceback - line 225 in decorators.py calls `return_.to(wrapped_signature.return_annotation)` when `return_annotation` is None, causing the AttributeError. The solution is simple: modify the condition on line 224 from checking only `inspect.Signature.empty` to also exclude `None`. This requires changing one line of code from `if wrapped_signature.return_annotation is not inspect.Signature.empty:` to `if wrapped_signature.return_annotation not in (inspect.Signature.empty, None):`. The fix is minimal, well-contained, and the logic is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean, well-defined bug with a simple fix. The issue description provides all necessary information including reproduction steps, error details, and suggested solution approach. The test patch shows appropriate test coverage for the fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7606": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear about what needs to be fixed. The problem is described step-by-step: 1) A unit is created from string \"asdf\" with parse_strict=\"silent\", 2) When comparing this unit to None using ==, it should return False but instead raises a TypeError, 3) The complete stack trace is provided showing the error occurs in astropy/units/core.py at line 1701 in the __eq__ method of UnrecognizedUnit class. The expected behavior is explicitly stated (should return False), and the current broken behavior is demonstrated with a clear error traceback. An experienced engineer would understand they need to modify the __eq__ method to handle None comparisons gracefully rather than raising an exception.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is localized to the __eq__ method in the UnrecognizedUnit class (line 1701 in core.py). The solution requires adding error handling around the Unit(other, parse_strict='silent') call to catch TypeError when other is None, and returning an appropriate value (NotImplemented rather than False, as shown in the patch). The fix involves understanding Python's comparison protocol and adding a simple try-except block. While it requires some thought about the correct return value (NotImplemented vs False), the change itself is small and straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly stated, the location of the bug is evident from the stack trace, and the expected behavior is unambiguous. The solution involves standard Python exception handling patterns that any experienced engineer should be familiar with.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7671": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear problem statement - the `minversion` function fails when comparing versions with \"dev\" suffixes due to a TypeError in LooseVersion, (2) A complete reproducible example with full traceback showing the exact error, (3) Root cause analysis identifying the underlying bug in LooseVersion and referencing the Python bug report, (4) Specific examples demonstrating when the comparison works vs fails (e.g., \"1.14\" vs \"1.14dev\" works, but \"1.14.3\" vs \"1.14dev\" fails), (5) A suggested alternative (pkg_resources.parse_version) that handles this correctly. The issue clearly indicates that the solution should handle version strings with \"dev\" suffixes without raising TypeErrors, making it entirely clear what constitutes a successful fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-defined and localized to the `minversion` function in `astropy/utils/introspection.py`. The solution involves preprocessing the version string to extract only the numeric portion before passing it to LooseVersion, which is exactly what the patch does using a regex from PEP440. An experienced engineer would need to: (1) Understand the LooseVersion limitation, (2) Research version string formats (PEP440), (3) Implement regex-based preprocessing to extract the numeric part. The actual code change is small (about 8 lines) and the logic is straightforward, but it requires some thought about version string parsing standards.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution approach is evident from the issue description, and the test changes are minimal and appropriate. This is a good benchmark sample that tests understanding of version comparison edge cases and regex-based string preprocessing.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-10554": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It provides a clear error message (\"ORDER BY position 4 is not in select list\") and describes the specific scenario where the problem occurs: when creating a union queryset with ordering and then performing certain operations. The description includes a concrete stack trace showing the ProgrammingError that occurs when Django tries to execute the SQL query. However, the issue description lacks a minimal reproducible example with actual model definitions and the exact sequence of operations that triggers the bug. The description mentions \"Dimension\" models and specific operations like values_list('id', flat=True) and union with order_by, but doesn't provide the complete code to reproduce the issue. An experienced Django developer familiar with querysets and SQL compilation would understand this is about ORDER BY clauses referencing columns not in the SELECT list, which is a common SQL constraint violation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The problem involves understanding Django's ORM query compilation, specifically how ORDER BY clauses are handled in union querysets. The solution requires modifying the SQL compiler logic in django/db/models/sql/compiler.py to handle cases where ORDER BY references columns not in the SELECT list. An engineer would need to: 1) Understand the Django ORM's query compilation process, 2) Identify why the ORDER BY clause is referencing a column position that doesn't exist in the SELECT list, 3) Implement logic to automatically add missing columns to the SELECT clause when they're used in ORDER BY, 4) Ensure the fix works correctly with union querysets. The patch shows this requires changes to both the compiler logic and adding a new method to the Query class, which is substantial but not extremely complex.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate Django ORM bug with a clear error condition and the test patch provides good coverage of the fix. While the issue description could be more detailed with a complete reproducible example, it provides sufficient context about the SQL error and the operations that trigger it for an experienced Django developer to understand and fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11087": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. The core problem is clear: Django's .delete() operation is fetching unnecessary fields during cascade deletion, which causes UnicodeDecodeError when those fields contain invalid data. The issue identifies two specific problems: (1) mysqlclient-python behavior differences between Python versions, and (2) Django fetching fields like text_log_error.line that aren't needed for deletion (not primary keys). The reporter specifically states that \"fixing issue (2)\" about unnecessary field fetching \"would presumably also improve .delete() performance.\" The stack trace clearly shows the error occurs during cascade deletion when Django tries to fetch all fields from related objects. However, the exact implementation details of how to optimize the field selection are left for the engineer to determine.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The solution requires: (1) Understanding Django's ORM deletion mechanism and cascade behavior, (2) Analyzing the deletion.py collector code to identify where unnecessary fields are being selected, (3) Implementing logic to determine which fields are actually needed (referenced fields, primary keys) vs which can be deferred, (4) Adding signal listener detection to ensure compatibility when deletion signals are connected, (5) Writing comprehensive tests. The patch shows this involves modifying the collect() method in deletion.py, adding helper methods, and creating new test models and test cases. This requires solid understanding of Django's internals, ORM relationships, and query optimization - definitely more than a simple fix but not an extremely esoteric issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-documented with a clear reproduction case, specific error traces, and identified root causes. The solution space is reasonably bounded - optimize field selection during deletion while maintaining compatibility. The test patch shows good verification of the optimization working correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11265": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The problem is clearly described: using exclude() on a queryset with an annotated FilteredRelation causes a FieldError. The issue provides a concrete example from Django's test suite, showing that .filter(book_alice__isnull=False) works but .exclude(book_alice__isnull=False) fails with a specific error traceback. The error message and stack trace clearly point to the split_exclude function in django/db/models/sql/query.py as the root cause - specifically that a new query is created without the filtered relations data from the original query. While the exact implementation details aren't specified, an experienced Django developer would understand that the issue is about preserving FilteredRelation annotations when creating subqueries for exclude operations.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because it requires understanding Django's ORM internals, specifically the query construction and filtering mechanisms. The developer needs to: 1) Understand how FilteredRelation works and how it's stored in queries, 2) Trace through the split_exclude method to see why filtered relations are lost, 3) Understand the trim_start method and how it optimizes joins, 4) Recognize that the solution involves both preserving _filtered_relations in the subquery AND preventing trimming of filtered relation joins. The actual code changes are small (3-4 lines total), but require deep understanding of Django's query optimization logic and the interaction between filtered relations and join trimming.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the test case is clear and appropriate, and the solution addresses a legitimate bug in Django's ORM. The issue provides sufficient context for an experienced developer to understand and solve the problem.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11299": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a Django bug where CheckConstraints with combined OR and AND clauses incorrectly include fully qualified field names (table.field) in the generated SQL. The issue provides: (1) A concrete example with a TestConstraint model having field_1 and flag fields, (2) The exact constraint logic \"either flag is true and field_1 is not null, or flag is false\", (3) The specific error message \"malformed database schema (app_testconstraint) - no such column: new__app_testconstraint.field_1\", (4) The root cause explanation about AND clauses using Col while OR clauses use SimpleCol, and (5) The expected correct output format \"CHECK ((field_1 IS NOT NULL AND flag = 1) OR flag = 0)\" without table prefixes. The problem, reproduction steps, error symptoms, and desired solution are all clearly articulated.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. The issue requires understanding Django's query building internals, specifically how CheckConstraints are rendered and the difference between Col and SimpleCol usage in OR vs AND clauses. The actual fix shown is a one-line change adding a `simple_col` parameter to a recursive call in `_add_q()` method, but arriving at this solution requires: (1) Understanding Django's ORM query building architecture, (2) Tracing through the constraint rendering process to identify where table qualifiers are incorrectly added, (3) Understanding the relationship between Col and SimpleCol classes, and (4) Recognizing that the `simple_col` parameter needs to be propagated through recursive calls. While the code change is minimal, the debugging and investigation process to identify the root cause would take several hours for someone not intimately familiar with Django's query internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with clear reproduction steps, specific error conditions, and a targeted solution. The issue is database-specific (SQLite migrations) but the underlying problem affects Django's query generation logic. The solution and test cases demonstrate the fix works correctly. This would make an excellent benchmark sample as it tests understanding of Django ORM internals, debugging skills, and the ability to trace through complex codebases to find minimal but correct solutions.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11532": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some minor gaps to fill in. The problem is clearly described: when the system hostname contains non-ASCII characters (like \"\u6b63\u5b97\") and an email encoding is set to a non-Unicode encoding like \"iso-8859-1\", Django crashes with a UnicodeEncodeError when generating the Message-ID header. The issue provides specific reproduction steps, references exact line numbers in the codebase (django/core/mail/message.py#L260), includes a detailed traceback, and suggests a clear fix direction (convert domain names to punycode). The test case mentioned (tests/mail/tests.py#L368) gives additional context. While an engineer would need to understand punycode encoding and explore the codebase structure to implement the solution properly, the core requirements are clear enough for a meaningful solution attempt.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) The engineer needs to understand Unicode/punycode encoding concepts and how they apply to email headers, (2) Multiple files need modification across different Django modules (utils, message, validators, html), (3) A new punycode utility function must be created and integrated consistently across the codebase, (4) The solution involves replacing multiple instances of manual IDNA encoding with the new centralized function, and (5) Understanding the email message generation flow and header encoding constraints requires some domain knowledge. While the core concept is straightforward, the implementation requires careful coordination across multiple modules and thorough testing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is technically sound, the solution is well-defined, and the test case effectively verifies the fix. This is a good benchmark sample that tests both Unicode handling knowledge and the ability to implement consistent changes across multiple modules in a large codebase.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11551": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) A clear description of the problem - admin.E108 error being raised incorrectly on fields accessible only via instance after upgrading Django versions, (2) Specific reproduction scenario with model \"Thing\" having \"number\" and \"order\" fields, (3) Root cause analysis pointing to specific commit 47016adbf54b54143d4cf052eeb29fc72d27e6b1, (4) Detailed explanation of why the hasattr(model, item) check is problematic for PositionField, (5) A comprehensive truth table showing expected vs actual behavior across different scenarios, and (6) Complete proposed solution code for the _check_list_display_item function. An experienced engineer would have everything needed to understand and fix this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. While the issue description is detailed and complex, the actual code change is straightforward - replacing the hasattr check with a try/except structure in the _check_list_display_item function in django/contrib/admin/checks.py. The issue provides the exact solution code, and the logic change involves reorganizing existing validation logic rather than implementing new functionality. An experienced engineer familiar with Django would need minimal time to understand the problem and implement the fix, though some time would be needed to understand the nuanced validation logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample as it tests understanding of Django's admin validation logic, exception handling patterns, and the ability to reason about edge cases in field access mechanisms. The issue tests both code comprehension and the ability to implement a logical fix based on detailed requirements.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11734": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with a clear error message and specific test scenario described. The reporter explains they added a test with Number objects and Item subqueries, shows what works (checking for matches) versus what fails (exclusions), and provides the specific error. However, some details are missing - the exact models being used (Number, Item), their relationships, and the specific code that reproduces the issue. An experienced engineer familiar with Django ORM could likely understand this is about OuterRef handling in exclude operations and subqueries, but would need to infer some details about the model structure and write test code to reproduce the problem.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a very complex Django ORM issue that requires deep understanding of Django's query system. The solution spans multiple core files (fields, lookups, and query handling), involves removing methods, modifying lookup preparation logic, and adding special handling for OuterRef in exclude operations. The fix touches low-level ORM internals and requires understanding how Django processes subqueries, outer references, and exclusions. An experienced engineer would need substantial research time to understand the query compilation process, identify why OuterRef behaves differently in exclude vs include contexts, and develop a solution that doesn't break existing functionality. The patch shows this affects core Django functionality with changes that could have wide-reaching implications.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is technical but well-defined within Django ORM scope, the test patch provides clear verification of the fix, and while complex, this represents a legitimate Django bug that an experienced engineer could reasonably be expected to solve given sufficient time and familiarity with the codebase.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11740": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The user clearly describes the problem: when changing a UUIDField to a ForeignKey pointing to a model in another app, Django's makemigrations doesn't automatically create the necessary dependency on the target app, leading to a \"Related model cannot be resolved\" error. The user provides specific context (Django 2.2, PostgreSQL, two apps testapp1 and testapp2, specific model structure with UUIDs as primary keys), includes the exact error message, and even mentions they have a minimal test that reproduces the issue. However, there are some details that could be clearer - the exact model definitions, the precise migration steps, and the specific field transformation. An experienced Django developer would likely understand this is about migration dependency detection in the autodetector system, but some interpretation is required about the exact implementation details.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's migration autodetector system, specifically how dependencies are calculated when fields are altered. The solution involves modifying the generate_altered_fields method in django/db/migrations/autodetector.py to detect when a field is being changed to a ForeignKey and add appropriate dependencies. While the core change is relatively small (adding dependency detection logic), it requires: 1) Understanding Django's migration system architecture, 2) Locating the right place in the autodetector code, 3) Understanding how dependency calculation works for foreign keys, 4) Implementing the logic to detect this specific case of field alteration, and 5) Writing appropriate tests. The patch shows this touches the core migration logic and requires knowledge of Django's internal field relationship handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample because it tests understanding of Django's migration system, dependency management, and field relationships. The issue is specific enough to have a clear correct solution, and the test case verifies the fix works properly. The problem is realistic and represents the kind of issue Django developers might encounter in real projects.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11815": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when using an Enum object as a default value for a CharField, Django's migration generator uses the enum's value instead of its name, which causes issues with translated values. The issue provides a concrete reproduction case with Status.GOOD enum, shows the problematic migration output (Status('Good')), explains why this fails (translation changes break the lookup), and specifies exactly what the solution should look like (Status['GOOD'] using bracket notation to reference by name). The description includes all necessary context about gettext_lazy translation and the specific error that occurs.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution involves modifying the EnumSerializer.serialize() method in django/db/migrations/serializer.py to change from using the enum's value with constructor syntax to using bracket notation with the enum's name. The change is relatively small (about 5 lines modified) but requires understanding how Django's migration serialization works and knowing that enum.name provides the member name while enum.value provides the value. An experienced engineer would need some time to locate the right serializer class and understand the existing serialization logic, but the actual code change is straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with a clear reproduction case and expected solution. The provided test cases adequately cover the fix by testing various enum types (TextEnum, TextTranslatedEnum, BinaryEnum, IntEnum) and verifying the serialization format changes from constructor syntax to bracket notation. The issue is realistic and represents a genuine problem that Django developers might encounter when using enums with translations.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11964": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that when using TextChoices/IntegerChoices with Django model fields, the value returned by the getter has an enum type instead of the expected primitive type (str/int). The issue provides a concrete reproduction scenario with MyChoice.FIRST_CHOICE and shows the exact test failure where str(my_object.my_str_value) returns 'MyChoice.FIRST_CHOICE' instead of 'first'. The expected behavior is clearly stated: the field should always return a plain string/integer rather than an enum instance. The problem statement also explains the practical implications (issues when communicating with external APIs). The gold patch confirms this understanding by adding a __str__ method to the Choices class that returns str(self.value).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly defined and the solution is straightforward - adding a __str__ method to the Choices class in django/db/models/enums.py that returns str(self.value). An experienced engineer would need to: 1) Understand that the problem is with string representation of enum values, 2) Locate the Choices class in the enums module, 3) Add the __str__ method override. The gold patch shows this is only 6 lines of actual code change. While it requires understanding Django's enum implementation, the fix itself is simple and localized to one method in one file.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly described, the expected behavior is well-defined, and the solution is straightforward. The test patch also provides good validation that the fix works correctly across different enum types. This is a good sample for evaluating coding ability as it tests understanding of Python's enum behavior and Django's model field system.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12155": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The problem: admindoc fails when rendering view docstrings that begin immediately on the first line without an initial blank line, (2) The specific error message that occurs, (3) The exact location of the problematic code in the trim_docstring function, (4) The root cause: the indentation calculation includes the first line which has 0 indentation, and (5) A specific proposed solution to skip the first line in the indentation calculation. The issue provides enough detail for an experienced engineer to understand both the problem and implement a fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic code in trim_docstring and provides the exact solution. However, the actual implementation in the gold patch shows a more comprehensive approach - replacing the custom trim_docstring function entirely with Python's built-in inspect.cleandoc function, which properly handles docstring formatting. This requires understanding that cleandoc is the standard solution for this type of problem and making the decision to replace custom code with a well-tested library function. It involves editing multiple files (utils.py and views.py) but the changes are straightforward once the approach is decided.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution approach is clear, and the test demonstrates the fix works correctly. An engineer should be able to solve this effectively with just the issue description.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12262": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a bug in Django's template tag system where keyword-only arguments with defaults cause TemplateSyntaxError when used in templates. The description provides specific examples: a `hello` tag with a `greeting` parameter defaulting to \"hello\", and demonstrates the exact error messages that occur (\"'hello' received unexpected keyword argument 'greeting'\"). It also specifies what the correct behavior should be, including the proper error message for duplicate keyword arguments (\"'hi' received multiple values for keyword argument 'greeting'\" instead of \"unexpected keyword argument\"). The issue mentions that both simple_tag and inclusion_tag are affected, and that the bug exists since version 2.0. With this level of detail, an experienced engineer can clearly understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is a simple one-line change in django/template/library.py, line 264, changing `param not in params` to `param not in params and param not in kwonly`. The bug is in the parse_bits function where it incorrectly validates keyword arguments - it only checks if the parameter is in the regular params list, but fails to check if it's in the keyword-only parameters (kwonly). An experienced engineer familiar with Django's template system would need some time to locate the right function and understand the parameter validation logic, but once they understand how parse_bits works and see that kwonly parameters are being excluded from the validation check, the fix is straightforward. The test cases show this is specifically about keyword-only parameters with defaults, which helps narrow down the scope.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, specific examples, and a focused scope. The issue is in Django's template tag system which is a well-documented part of the framework. The test cases provided comprehensively cover the scenarios described in the issue, including both simple_tag and inclusion_tag variants, and test both the fix for allowing keyword-only arguments and the proper error messaging for duplicate arguments.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12325": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the core problem: Django's MTI (Model Table Inheritance) setup gets confused when there are multiple OneToOneField references, and the order of field declaration matters when it shouldn't. The issue provides a specific error message and explains the unexpected behavior. However, it lacks a concrete example showing the problematic code structure - we don't see the actual model definitions that cause this issue. An experienced engineer could reasonably infer what needs to be fixed (the order dependency should be eliminated and the parent_link=True marker should be properly respected), but having a minimal reproducible example would make it clearer.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's model metaclass system and inheritance mechanics. The engineer needs to familiarize themselves with how Django processes OneToOneField declarations during model creation, particularly in the context of multi-table inheritance. Looking at the gold patch, the solution involves changes to both django/db/models/base.py and django/db/models/options.py - the fix requires understanding the parent_link detection logic and modifying how Django identifies parent link fields. The core logic change is relatively small (adding a parent_link check in one place and removing validation code in another), but understanding where and why these changes are needed requires substantial knowledge of Django's internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability. While the description could be more precise with concrete examples, an experienced engineer should be able to understand the problem and work toward a solution. The fix demonstrates understanding of Django's model inheritance system and metaclass behavior, which are important software engineering concepts. The test changes also show that a complete solution requires updating existing tests to reflect the new behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12663": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It clearly describes a regression where SimpleLazyObject usage in querysets with nested subquery annotations fails. The issue provides concrete models (A, B, C with User), a reproduction scenario involving creating a Subquery, wrapping a User in SimpleLazyObject, annotating with the subquery, and filtering by the lazy user. A complete stack trace is provided showing the TypeError occurs in the Django ORM when trying to convert SimpleLazyObject to int. However, the exact test code to reproduce the issue isn't provided - an engineer would need to write the reproduction case based on the description. The key insight is that this worked before commit 35431298226165986ad07e91f9d3aca721ff38ec, giving clear context about when the regression was introduced.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours of work. The engineer needs to: (1) Understand Django's ORM internals, specifically how SimpleLazyObject interacts with field preparation and query building, (2) Analyze the stack trace to identify the root cause in get_prep_value trying to convert SimpleLazyObject to int, (3) Research the mentioned commit to understand what changed, (4) Write reproduction code based on the description, (5) Understand that the solution involves modifying the output_field property in Query class to handle cases where select items have a 'target' attribute instead of just 'field'. The fix itself is small (3-line change) but requires substantial understanding of Django's query system and the relationship between Subquery, annotations, and field preparation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's ORM system, has a clear regression context, and the provided patches show both the fix and comprehensive test coverage. The issue demonstrates good software engineering practices with proper models, reproduction steps, and stack trace.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12708": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is well-specified with some minor gaps that require reasonable interpretation. It clearly identifies the problem (migration crashes when deleting index_together with existing unique_together), provides the exact error location (django/db/backends/base/schema.py, line 378, _delete_composed_index()), the specific error message (ValueError: Found wrong number (2) of constraints), and explains the root cause (finding both _uniq and _idx constraints). The concrete use case of refactoring to use Django 1.11's Options.indexes feature provides good context. However, an engineer would need to infer some implementation details about how to properly distinguish between index and unique constraints when deleting composed indexes.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the exact location of the problem (line 378 in _delete_composed_index()) and the root cause (finding both unique and index constraints). The solution involves a small, targeted change: modifying the call to _delete_composed_index() to specify that only index constraints should be considered when deleting index_together (adding 'unique': False to the constraint specification). This requires understanding Django's schema management but is a straightforward fix once the problem location and cause are identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement, specific error location, and straightforward solution. The issue is suitable for benchmarking coding ability as it tests understanding of Django's migration system and database schema management.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12754": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: when creating a model subclass and moving a field from the parent to the child in the same migration, Django's migration autodetector generates operations in the wrong order, causing a FieldError during migration. The issue includes: (1) Clear reproduction steps starting with a Readable model with a title field, (2) The exact changes needed (remove title from Readable, add Book subclass with title), (3) The specific error message that occurs, (4) The root cause (operation ordering in migrations), and (5) A clear solution direction (the operations should be reordered). The technical context about Django's migration autodetector and the specific error about field clashing provides sufficient detail for an experienced engineer to understand and solve the problem.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour problem because it requires: (1) Understanding Django's migration autodetector system and how it determines operation dependencies, (2) Analyzing the specific case where field removal and model creation with inheritance interact, (3) Implementing dependency logic to ensure field removal happens before model creation when there are naming conflicts, and (4) Working with Django's complex migration framework. The solution involves modifying the generate_created_models method in the autodetector to add dependencies on base field removals, which requires understanding the migration state system and dependency resolution. While the code change is relatively small (about 10 lines), the conceptual understanding and testing required make this a substantial task.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly defined, requires meaningful Django framework knowledge, has a specific technical solution, and includes comprehensive test coverage. The problem domain (Django migrations) is well-established and the solution can be objectively verified through the provided tests.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12858": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some gaps that require interpretation. The core problem is clearly described: Django's model checking incorrectly raises error E015 for ordering that uses valid lookups (like `__isnull`) instead of transforms. The issue provides a concrete example with `supply__product__parent__isnull` ordering that works in practice but fails the system check. However, some details need to be inferred: the specific distinction between \"lookups\" and \"transforms\" in Django's context, and the exact location in the codebase where the validation logic resides. The mention of issue #29408 provides helpful context about when this regression was introduced. While an experienced Django developer could reasonably interpret what needs to be fixed - allowing lookups in addition to transforms in the ordering validation - the exact implementation details would require code exploration.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a focused bug fix that should take 15 minutes to 1 hour for an experienced engineer. The gold patch shows a simple 3-line change to django/db/models/base.py in the _check_ordering method, adding a condition to check for lookups in addition to transforms. The logic is straightforward: modify the existing validation to accept both transforms and lookups. Most of the time would be spent understanding Django's field lookup/transform system and locating the correct validation code, but the actual fix is minimal and doesn't require complex logic or extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is a well-contained Django framework bug with a clear reproduction case and straightforward fix. The test case provided confirms the expected behavior. This would make a good benchmark question for testing Django knowledge and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13012": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clearly described: when a constant expression (like Value(3)) is wrapped in an ExpressionWrapper, Django incorrectly includes it in the GROUP BY clause, causing a PostgreSQL error. The issue provides a specific error message, describes the current behavior vs expected behavior, and mentions that direct annotation with Value(3) works correctly. However, there are some gaps: the exact implementation of the helper function \"execQuery\" isn't provided, and while the problem is clear, the specific technical solution approach isn't obvious without understanding Django's ORM internals. An experienced engineer would need to investigate how ExpressionWrapper handles GROUP BY clauses and understand why constants should be excluded from grouping.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the provided patch, the solution is conceptually simple - adding a get_group_by_cols method to ExpressionWrapper that delegates to the wrapped expression. However, arriving at this solution requires: (1) Understanding Django's ORM query compilation process and how GROUP BY clauses are generated, (2) Investigating why ExpressionWrapper doesn't properly handle constant expressions in GROUP BY contexts, (3) Understanding the difference between expressions that should and shouldn't be grouped by, (4) Writing appropriate tests to verify the fix works for both constant and non-constant expressions. The actual code change is small (3 lines), but diagnosing the root cause and understanding the ORM's expression system would take substantial investigation time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This issue is suitable for the benchmark. It has a clear problem statement with a specific error case, requires understanding of Django's ORM internals, and has a focused solution. The test case provided shows good coverage of both the constant expression case (should return empty GROUP BY) and non-constant case (should delegate to wrapped expression). The issue tests both problem-solving skills and framework-specific knowledge.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13028": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It provides clear reproduction steps with specific model definitions (ProductMetaDataType and ProductMetaData), shows the exact error traceback with the line that fails, and demonstrates that renaming the 'filterable' field fixes the issue. The problem is clearly a naming conflict where Django's internal 'filterable' attribute check is incorrectly being applied to user model instances that happen to have a field with that name. The expected behavior is that filtering should work normally regardless of field names in user models.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-isolated to Django's check_filterable method in query.py. The problem is straightforward: the method needs to distinguish between Django expression objects (which legitimately use 'filterable') and regular model instances (which might coincidentally have a 'filterable' field). The fix requires adding a simple condition to check if the object has 'resolve_expression' method before applying the filterable check. This is a small, targeted change that doesn't require deep architectural understanding.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly defined, has a focused scope, and the solution requires understanding Django's query system without being overly complex. The fix is elegant and targeted, making it suitable for evaluating an engineer's ability to debug framework-level issues and implement appropriate solutions.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13112": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some blanks to fill in. The core problem is clear: makemigrations crashes in Django 3.1b1 when dealing with a ForeignKey reference to a model in an app with mixed-case naming (\"DJ_RegLogin\" vs \"dj_reglogin\"). The error message is specific and provides the exact ValueError being thrown. The user describes their setup adequately - they have an app called \"DJ_RegLogin\" registered in INSTALLED_APPS, with models including a Category and Content model where Content has a ForeignKey to Category. However, some details are missing like the exact model definitions and how the ForeignKey is declared, but these can be reasonably inferred from the context and error message. The issue works in Django 3.0 but fails in 3.1b1, indicating a regression. An experienced engineer could understand this is about case sensitivity in app name resolution for lazy ForeignKey references.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's migration system, model field resolution, and lazy reference handling. The engineer needs to: 1) Understand how Django resolves lazy ForeignKey references during migrations, 2) Identify where the case conversion is happening incorrectly (the patch shows it's in django/db/models/fields/related.py in the deconstruct method), 3) Understand that when a reference includes an app label (contains '.'), only the model name should be lowercased, not the entire string, 4) Implement the logic to split on '.' and handle each part appropriately. While the actual code change is small (about 6 lines), finding the right location in Django's codebase and understanding the migration/model reference system requires substantial investigation time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear reproduction case and specific error. The test case demonstrates the fix works for mixed-case app labels with ForeignKey and ManyToManyField relationships. The issue represents a genuine regression between Django versions that would be suitable for evaluating debugging and Django framework knowledge.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem where temporal subtraction (end minus start) between DateTimeFields fails with a FieldError about mixed types requiring output_field specification. The user expects this to work without needing ExpressionWrapper. While the issue doesn't explicitly state \"make this work without ExpressionWrapper\" in the title, the context makes it clear that the goal is to enable temporal subtraction to automatically infer the correct output type (DurationField) without manual specification. The error message and expected behavior are clearly described, though some implementation details are left for the engineer to determine.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue because it requires: (1) Understanding Django's expression system architecture, particularly how CombinedExpression, TemporalSubtraction, and DurationExpression work together, (2) Analyzing the current flow where type checking happens in as_sql() method but needs to move to resolve_expression() for proper type inference, (3) Implementing a solution that moves the temporal/duration type detection logic from as_sql() to resolve_expression() method, and (4) Ensuring the change works across different database backends with varying native duration support. The patch shows this involves restructuring significant logic in the CombinedExpression class and requires understanding of Django's ORM internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. The problem is clearly defined with a specific error message, the expected behavior is reasonable, and the solution requires good understanding of Django ORM internals without being overly esoteric. The test changes show comprehensive coverage of the fix across different temporal field types.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13297": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear information for a solution. It describes the specific problem (SimpleLazyObject causing database parameter binding errors), the context where it occurs (TemplateView kwargs in Django 3.1), the exact error message, and demonstrates understanding of the root cause. The user provides a concrete example with their OfferView class using get_object_or_404 with a slug from kwargs. While the user doesn't provide the exact code, they give enough detail about the URLconf pattern ('/offers/<slug:offer_slug>/') and the method (get_context_data pulling offer_slug from kwargs) to understand what's happening. An experienced engineer could identify that this relates to the deprecation warning wrapper for URL kwargs and the use of SimpleLazyObject.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve. While the symptom is clear, an engineer would need to: (1) Understand Django's TemplateView implementation and how URL kwargs are processed, (2) Identify the _wrap_url_kwargs_with_deprecation_warning function that wraps kwargs with SimpleLazyObject, (3) Understand the difference between SimpleLazyObject and the lazy() function with type preservation, (4) Realize that SimpleLazyObject doesn't preserve the original type information needed for database operations, and (5) Implement the fix using lazy() with type information. This requires knowledge of Django's lazy evaluation mechanisms and debugging through the framework code to find the root cause.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-described, the solution is targeted and clean, and the test case appropriately verifies that kwargs values can be used in database operations without type conversion. This is a good benchmark sample that tests knowledge of Django internals, lazy evaluation, and debugging skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13406": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is reasonably well-specified. It clearly describes a problem with Django's queryset pickling functionality where queries using values()/values_list() with annotations return model instances instead of dictionaries when recreated from pickled queries. The issue provides a clear reproduction scenario: create a queryset with values('material').annotate(total_price=Sum('price')), pickle the query, recreate it, and observe that it returns model instances instead of dictionaries. The expected behavior (list of dictionaries) vs actual behavior (model instances with broken state) is clearly stated. However, there are some implementation details left to the engineer to figure out, such as exactly which part of the Django ORM code needs to be modified and how to properly restore the _iterable_class attribute during query reconstruction.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is quite straightforward: adding just 2 lines of code in the query setter to check if the query has values_select and set the appropriate _iterable_class to ValuesIterable. The core issue is that when a query is pickled and unpickled, the _iterable_class attribute (which determines how results are returned) is not properly restored. An experienced engineer familiar with Django's ORM would need to understand how querysets work, identify that the _iterable_class is the key attribute that's missing, and implement the simple conditional check. The fix requires understanding Django's queryset internals but the actual code change is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-documented with a clear reproduction case, the solution is targeted and minimal, and the test cases demonstrate the fix works for both values() and values_list() scenarios. This makes it a good benchmark sample for evaluating understanding of object serialization and ORM internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13449": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) Clear reproduction steps with specific Django version (3.0.7) and database (SQLite), (2) Exact model definition requirements (DateField 'modified', FloatField 'data', DecimalField 'amount' with specific parameters), (3) Precise QuerySet construction using Lag('amount', 7) with window function, (4) Complete error traceback showing \"OperationalError: near 'OVER': syntax error\", (5) The actual generated SQL query that's malformed, (6) Root cause analysis identifying that CAST() ends prematurely after LAG instead of wrapping the entire window expression, (7) Evidence that it works with FloatField but fails with DecimalField, and (8) A working workaround using output_field=FloatField(). An experienced engineer has all the information needed to understand this is a SQL generation bug specific to DecimalField in window functions on SQLite.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours because: (1) Understanding Django's ORM expression system and how window functions generate SQL takes time, (2) The solution involves creating a SQLite-specific method (as_sqlite) that handles DecimalField casting differently, (3) Need to understand the SQLiteNumericMixin inheritance pattern and how database backend-specific SQL generation works, (4) Must modify the Window class in django/db/models/expressions.py, which is core ORM functionality, (5) The fix requires careful handling of expression copying and field type manipulation to avoid side effects, (6) Writing comprehensive tests for the new functionality. While the root cause is clear from the issue, implementing the proper fix requires deep understanding of Django's database abstraction layers.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is database-specific (SQLite), involves complex ORM internals, and requires understanding of SQL generation patterns. The solution demonstrates good software engineering practices with proper inheritance, defensive copying, and comprehensive test coverage. The bug affects a specific but important use case (window functions with DecimalFields), making it a realistic scenario developers might encounter.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13568": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly explains the problem: when a custom User model uses a non-unique CharField for the username field but enforces uniqueness through a UniqueConstraint in Meta.constraints, Django's system check incorrectly reports auth.E003 error. The issue provides specific context about why this approach is preferred (avoiding extra implicit indexes on PostgreSQL), identifies the exact error message, and suggests the solution direction - extending the system check to look for USERNAME_FIELD in Model._meta.constraints. The technical details are concrete and actionable.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The solution requires understanding Django's auth checks system and modifying the existing logic in django/contrib/auth/checks.py. Looking at the patch, it's a small, focused change - adding a condition to check for UniqueConstraints alongside the existing unique field check. The logic is straightforward: if the field isn't unique but there's a total unique constraint on that field, don't trigger the error. An experienced engineer familiar with Django would need time to locate the relevant code, understand the constraint system, and write the additional condition, but it's not a complex algorithmic change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix in Django's system checks that has clear requirements, a focused solution area, and straightforward test cases. The issue represents a realistic software engineering task that tests understanding of Django's internals without being overly complex.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13807": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) A clear problem statement - loaddata crashes on SQLite when table names are SQL keywords like \"order\", (2) A concrete reproduction scenario - defining a Django model called Order and running manage.py loaddata, (3) The exact error message and stack trace showing the syntax error, (4) Root cause analysis pointing to the specific file (django/db/backends/sqlite3/base.py) and function (check_constraints) at line 327, (5) The exact problematic code showing unquoted table names in PRAGMA statements, and (6) Affected Django versions (3.1.0, 3.1.2). An engineer would clearly understand that the solution requires adding proper quoting around table names in the PRAGMA SQL statements.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue clearly identifies the root cause (missing quotes around table names in PRAGMA statements) and the exact location (line 327 in check_constraints function). The solution involves replacing direct string formatting with proper name quoting using self.ops.quote_name() method. While an engineer needs to understand Django's database operations and find the appropriate quoting method, the core problem and location are explicitly provided. The fix involves modifying a few lines in one function to wrap table names with quote_name() calls.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample with clear problem description, specific error reproduction, identified root cause, and a straightforward solution that tests knowledge of proper SQL identifier quoting in database backends.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13810": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The reporter provides a clear reproduction case with a specific GitHub repository, describes the exact error symptoms (TypeError about HttpResponse in await expression), and identifies the root cause in django/core/handlers/base.py#L58. They explain that the MiddlewareNotUsed exception leaves the handler variable \"poisoned\" by overwriting it with adapt_method_mode() output, which affects subsequent middleware processing. The technical explanation about async/sync middleware interaction is detailed enough for an experienced Django developer to understand the problem. However, there are some gaps - the exact fix mechanism isn't specified, and one would need to understand Django's middleware loading system to implement the solution properly.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours because: (1) Understanding the problem requires familiarity with Django's middleware system, async/sync handling, and the MiddlewareNotUsed exception flow. (2) The root cause analysis involves tracing through middleware loading logic in base.py to understand how handler adaptation affects the middleware chain. (3) The solution, while small in terms of lines changed (moving handler assignment into else block), requires careful consideration of the middleware loading flow and potential edge cases. (4) Writing appropriate tests requires understanding both sync and async middleware behavior. The fix itself is relatively small but requires substantial understanding of Django internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-documented with a reproduction case, the solution space is clearly defined (fixing handler variable assignment in middleware loading), and the test patch demonstrates the expected behavior. The issue is suitable for benchmarking coding ability as it tests understanding of framework internals, async/sync patterns, and careful code analysis skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14017": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly demonstrates the problem with a concrete example showing that `Exists(...) & Q(...)` works but `Q(...) & Exists(...)` raises a TypeError. The issue includes the exact error traceback pointing to line 73 in query_utils.py where the TypeError is raised. The expected behavior is clearly stated - the & (and |) operators should be commutative on Q-Exists pairs. The issue even provides a hint about the likely cause (\"missing definition of __rand__ somewhere\"). An experienced engineer would have enough information to understand that the _combine method in the Q class needs to be modified to handle Exists objects, not just Q objects.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides a clear error traceback pointing to the exact location (line 73 in query_utils.py). The solution involves modifying the _combine method to accept not just Q objects but also objects with a 'conditional' attribute (like Exists). The actual code change is just one line - changing the isinstance check to also allow objects with conditional=True. While an engineer needs to understand Django's query system and the concept of conditional expressions, the fix itself is relatively straightforward once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear reproduction case, specific error location, and straightforward fix. The test cases provided in the patch confirm the expected behavior for all combinations of Q and Exists objects with both & and | operators. This makes it an excellent benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14140": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Q objects with a single child are treated differently during deconstruct, and this special-case handling breaks when the child isn't a 2-item tuple (like when using Exists objects). The issue provides a concrete example showing the TypeError that occurs when calling Q(Exists(...)).deconstruct(), including the exact traceback and error message. It also mentions a specific patch (PR #14126) and discusses two potential approaches to fix it - removing the special case entirely or keeping it with explicit validation. The technical details about how deconstruct currently works for single vs multiple children Q objects are clearly explained, making it straightforward for an engineer to understand both the current behavior and what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is localized to the deconstruct method in django/db/models/query_utils.py, and the solution shown in the patch is quite simple - removing the special case handling for single-child Q objects (about 8 lines removed, 4 lines added). An experienced engineer would need some time to understand the Django Q object architecture and the purpose of the deconstruct method, but once familiar with the codebase, the actual fix is straightforward. The problem is well-isolated and doesn't require deep architectural changes or extensive research into complex Django internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. The issue description is clear, the problem is reproducible, and the solution is well-defined. The test cases properly verify that the fix works for boolean expressions in Q objects while maintaining backward compatibility. This would be a good benchmark sample for evaluating coding ability as it tests understanding of object deconstruction, special case handling, and Django ORM internals.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14238": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: when using a custom subclass of BigAutoField or SmallAutoField as DEFAULT_AUTO_FIELD, Django throws a ValueError claiming it must subclass AutoField, even though it does through inheritance. The issue includes a complete stack trace showing the exact error location (django/db/models/options.py line 246), the specific error message, and even suggests the root cause and solution location (AutoFieldMeta.__subclasscheck__ method). The user provides a concrete example with MyBigAutoField subclassing BigAutoField, making it clear what behavior is expected vs. what's happening.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location (AutoFieldMeta.__subclasscheck__) and even suggests the solution approach. Looking at the patch, it's a one-line change from 'subclass in self._subclasses' to 'issubclass(subclass, self._subclasses)'. The problem is that the current implementation only checks for exact membership rather than inheritance relationships. An experienced Django developer could quickly understand that the metaclass needs to properly handle subclass checking for inheritance hierarchies. The fix requires minimal code understanding and is conceptually straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described, has a focused solution, and the provided tests comprehensively verify both the basic functionality and the specific edge case that was broken. The solution requires understanding Python's inheritance model and Django's metaclass system, making it a good test of both Python fundamentals and Django knowledge.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14351": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It clearly describes a Django ORM problem where Q object OR operations create subqueries that return multiple columns instead of just the primary key, causing a \"subquery must return only one column\" database error. The reproduction steps are detailed and show the exact error scenario. However, there are some gaps: the issue doesn't clearly explain what PropertyGroup.objects.agent_groups() does, and YourModel is a placeholder. Despite these details, an experienced Django developer would understand this is about Django's query generation logic for lookups with querysets, and the core problem is clear - the ORM is including too many fields in subqueries when using __in with querysets in OR conditions.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires deep understanding of Django's ORM internals, specifically the lookups system and query compilation. The engineer needs to understand how Q objects work, how the In lookup processes querysets, and why OR operations affect field selection differently. Looking at the gold patch, the solution involves adding a get_group_by_cols method to the In lookup class that properly handles queryset field selection by clearing the select clause and adding only the primary key field. This requires knowledge of Django's query compilation process, the difference between has_select_fields and regular querysets, and how group by columns are handled. While the actual code change is relatively small (about 10 lines), identifying the root cause and the correct place to fix it requires substantial investigation into Django's ORM codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate Django ORM bug with a clear reproduction case and a proper fix. The test case validates the solution appropriately, and the issue represents a real-world scenario that Django developers could encounter. The problem requires good understanding of Django internals but is representative of the kind of complex ORM issues that experienced developers should be able to debug and fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14493": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when ManifestStaticFilesStorage is subclassed with max_post_process_passes set to 0, running collectstatic fails with an UnboundLocalError. The issue provides the complete stack trace, points to the exact line of code causing the problem (storage.py line 246-257), and explains the root cause - the variable 'substitutions' is only set if the loop is entered at least once, but when max_post_process_passes is 0, the loop never executes, leaving 'substitutions' undefined when it's referenced later. The issue even provides a direct link to the problematic code on GitHub. The motivation for setting max_post_process_passes to 0 is also explained with a reference to another ticket. This gives an engineer everything needed to understand and fix the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The problem is clearly identified - an UnboundLocalError due to an uninitialized variable. The solution is simple: initialize the 'substitutions' variable before the loop that may or may not execute. Looking at the provided patch, the fix is literally one line of code: adding 'substitutions = False' before the for loop. An experienced engineer would quickly understand the issue from the stack trace and code reference, locate the problematic code, and implement the simple initialization fix. The only time needed would be to familiarize themselves with the codebase structure and understand the context around the post_process method.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear problem statement, obvious solution, and simple test case. The issue demonstrates a classic programming error (uninitialized variable) in a real-world codebase, making it an excellent benchmark sample for evaluating basic debugging and problem-solving skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14580": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear reproduction scenario with specific model setup requirements (custom TextField subclass, abstract base model, mixin class, and concrete model), identifies the exact problem (missing 'from django.db import models' import in generated migration), shows the specific error message with file location and line details, and clearly states the expected vs actual behavior. The issue pinpoints that the problem is likely in django.db.migrations.writer module. An experienced engineer has all the information needed to understand that Django's migration generator is not including the necessary import statement when serializing models.Model references.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location (django.db.migrations.writer) and the specific symptom (missing import for models.Model). Looking at the gold patch, the fix is a simple one-line change in the TypeSerializer class - adding the import statement to the special case for models.Model. The hardest part would be familiarizing oneself with the migration serialization code to locate where models.Model is handled, but the error message and issue description provide good guidance. The actual code change is minimal and straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements and a straightforward solution. The test case provided in the patch is also simple and directly validates the fix. This would be a good benchmark sample as it tests understanding of Django's migration system and code serialization.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14672": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly explains the problem: the `make_hashable` call is missing on `through_fields` in `ManyToManyRel`, which causes a TypeError when `through_fields` is a list and needs to be hashed. The issue provides excellent context about how the identity property works, what `make_hashable` does, and why it's needed. It includes a detailed minimal reproduction case with specific model setup instructions and shows the exact stack trace with the error \"TypeError: unhashable type: 'list'\". The solution is explicitly stated: \"Add missing make_hashable call on self.through_fields in ManyToManyRel.\" The issue also explains the specific context where this fails (proxy models with their additional checks). All the information needed to understand and fix the problem is provided.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified and the solution is explicitly stated in the issue description. Looking at the gold patch, it's literally a one-line change: wrapping `self.through_fields` with `make_hashable()` in the `identity` property of `ManyToManyRel`. An experienced engineer would need to: 1) Locate the `ManyToManyRel` class in `django/db/models/fields/reverse_related.py`, 2) Find the `identity` property, 3) Add the `make_hashable` wrapper around `self.through_fields`, and 4) Verify the fix works with the reproduction case. The most time would be spent understanding Django's codebase structure and running tests to confirm the fix, but the actual code change is trivial.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample. The issue is clearly described, has a specific reproduction case, includes the exact error message, and has a well-defined solution. The fix is straightforward but requires understanding Django's model relationship system and the purpose of the make_hashable utility function. The test patch shows good coverage by verifying that ManyToManyRel objects with list-based through_fields can be properly hashed and compared, which is exactly what the fix addresses.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14787": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: method_decorator() should preserve wrapper assignments, specifically that when a function is passed to a decorator, it becomes a partial object without expected function attributes like __name__, __module__, etc. The user describes a specific use case with a logger decorator that tries to access __name__ and fails with AttributeError. While the description doesn't provide complete code examples showing the exact setup, there's enough information to understand that the issue is about functools.partial objects not having the standard function attributes that decorators expect. The solution direction is also hinted at in the title - \"preserve wrapper assignments\" suggests using functools.wraps or similar mechanisms. An experienced engineer familiar with Python decorators would understand this is about ensuring that when method_decorator creates partial objects, those objects should maintain the original function's metadata attributes.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the actual solution, this is a relatively straightforward fix that takes 15 minutes to 1 hour. The solution involves wrapping the partial object with functools.wraps to preserve the original method's attributes. The change is minimal - just one line modification in django/utils/decorators.py where `bound_method = partial(method.__get__(self, type(self)))` becomes `bound_method = wraps(method)(partial(method.__get__(self, type(self))))`. An experienced engineer would need to: 1) Understand that the issue is about missing function attributes on partial objects, 2) Recognize this is a common decorator pattern problem solved by functools.wraps, 3) Locate the method_decorator implementation, 4) Apply the wraps solution. The concept is well-known in Python decorator patterns, and the fix is a standard idiom for preserving function metadata.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark issue. It tests understanding of Python decorators, functools.partial behavior, function metadata preservation, and Django's method_decorator utility. The issue is realistic and represents a common problem developers face when working with decorators. The solution requires knowledge of functools.wraps and understanding how method binding works in Python classes. The test case also provides good validation that the fix works correctly by checking that decorated methods preserve their __name__ and __module__ attributes.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15104": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "This issue is extremely well-specified. The reporter provides: (1) A clear problem description with the exact error message and stack trace, (2) The root cause analysis - a custom ForeignKey field that hardcodes its 'to' argument and removes it in deconstruct(), (3) A minimal reproduction case showing how to recreate the issue, (4) The exact location of the problematic code in django/db/migrations/autodetector.py line 99, (5) A specific proposed solution changing \"del deconstruction[2]['to']\" to \"deconstruction[2].pop('to', None)\". The issue includes both the symptom (KeyError during test execution) and the technical root cause, making it completely clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward 15-60 minute fix. The issue reporter has already identified the exact problematic line and provided the solution. The fix involves changing one line from \"del deconstruction[2]['to']\" to \"deconstruction[2].pop('to', None)\" to handle cases where the 'to' key might not exist in the dictionary. An experienced engineer would need minimal time to: (1) understand the issue from the clear description, (2) locate the file django/db/migrations/autodetector.py, (3) find line 99 in the only_relation_agnostic_fields method, (4) make the one-line change, and (5) verify the fix works. The test case is also provided in the patch, showing exactly what behavior should be tested.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample because it represents a real-world edge case in Django's migration system, has a clear technical explanation, includes a minimal reproduction case, and tests a developer's ability to understand Django's internals and make defensive programming improvements. The issue demonstrates good debugging skills from the reporter and the solution is both simple and robust.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The reporter provides a clear reproduction scenario with specific Django models (Foo, Bar, Baz, Qux with their relationships), concrete steps to reproduce the AssertionError, and the exact location where it fails (Query.change_aliases in django/db/models/sql/query.py line 854). They also provide technical analysis of the root cause involving alias conflicts during QuerySet union operations and suggest a potential solution approach. However, the issue description lacks the actual model definitions and complete reproduction code, requiring an engineer to infer the exact model structure from the description. The core problem is clear: qs1 | qs2 works but qs2 | qs1 raises AssertionError, indicating a non-commutative behavior in QuerySet unions that shouldn't exist.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Deep understanding of Django's ORM internals, specifically the Query.combine() method and alias management system, (2) Analyzing the complex alias collision problem during QuerySet unions where sequential aliases (T4, T5) conflict with newly generated ones, (3) Implementing the solution that involves modifying Query.bump_prefix() to accept an exclude parameter and calling it strategically in Query.combine(), (4) The fix touches multiple methods and requires careful consideration of edge cases. While the code changes are not massive (~30 lines), the conceptual complexity of Django's query system and the need to ensure the fix doesn't break other functionality makes this a substantial engineering task requiring significant research and testing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is a legitimate Django ORM bug with a clear reproduction path and the provided solution correctly addresses the root cause of alias conflicts during QuerySet combinations. The test case demonstrates the fix works for the reported scenario.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15277": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when creating a Value('test') instance, the CharField.__init__ method unconditionally adds a MaxLengthValidator even when max_length is None, which causes a TypeError when the validator's clean method is called. The issue provides specific technical details including: 1) The exact location of the problematic code in Value._resolve_output_field(), 2) The specific error that occurs (TypeError: '>' not supported between instances of 'int' and 'NoneType'), 3) Performance measurements showing the current impact, 4) The exact proposed solution with code snippet showing the conditional check for max_length, and 5) Precedent from BinaryField.__init__ that follows the same pattern. The reproduction steps are clear and the expected outcome is unambiguous.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. The issue description provides the exact location of the problem (CharField.__init__ in django/db/models/fields/__init__.py), the precise line causing the issue, and even includes the exact code change needed. The solution involves adding a simple conditional check before appending the MaxLengthValidator. An experienced engineer would need minimal time to understand the Django field system, locate the CharField.__init__ method, and implement the one-line conditional check. The main time would be spent running tests to ensure the change doesn't break anything, but the actual code modification is trivial.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with clear reproduction steps, the solution is straightforward, and the provided test case adequately verifies that the fix works by ensuring Value fields can be cleaned without throwing exceptions. The issue demonstrates good software engineering practices by showing performance measurements and referencing existing precedent in the codebase.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15280": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. The author provides a clear reproduction scenario involving User and Profile models with specific relationships (one-to-one), describes the exact test setup with prefetch_related configurations, and shows the problematic SQL queries being executed. They demonstrate the core problem: when accessing user.profile.user.kind, an unexpected database query occurs even though the data should already be prefetched. The issue includes concrete evidence like the deferred fields output ({'kind'}) and explains that the problem also occurs with ForeignKey relationships. However, there are some details missing - the exact model definitions, the complete test code, and the precise prefetch configuration syntax. An experienced engineer could reasonably infer the missing details from the Django ORM context and implement a solution, but would need to make some assumptions about the setup.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour problem because it requires deep understanding of Django's ORM internals, specifically the prefetch_related mechanism and deferred field handling. The engineer needs to: 1) Understand how Django manages object caching and field deferral across related objects, 2) Trace through the prefetch_related code to identify where the deferred field state is incorrectly inherited, 3) Identify the specific location in related_descriptors.py where the bug occurs, and 4) Implement the fix that checks if a field is already cached before overwriting it. Looking at the solution, it's a small but non-trivial change that requires understanding the interaction between prefetching and field caching. The fix involves adding a condition to check `self.field.is_cached(rel_obj)` before setting the relationship, which requires knowledge of Django's caching mechanisms.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's ORM system, has a clear reproduction case, and the test validates the fix effectively. The issue demonstrates good technical communication with specific examples and error output.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15315": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Field.__hash__() changes value when assigned to a model class, breaking its use in dictionaries. The issue provides a concrete example (CharField with max_length=200 used as dict key), explains the root cause (changes from PR #31750), and suggests a specific solution (revert the __hash__ change). The behavior is unambiguous - hash values should remain immutable once created, and the current implementation violates this principle.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified (hash method includes mutable model information), the location is specific (Field.__hash__ in django/db/models/fields/__init__.py), and the solution is straightforward (remove model-dependent parts from hash calculation). An experienced engineer would need minimal time to understand the issue, locate the problematic code, and implement the fix by simplifying the hash method to only use immutable properties like creation_counter.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined bug with a clear solution. The issue provides good context about why hash immutability matters and gives a concrete scenario where the current behavior fails. The proposed fix aligns with Python's hash contract and the provided test adequately verifies the expected behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15375": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear evidence of the problem. The author demonstrates the exact scenario that triggers the bug: using Django's ORM to annotate a queryset with F('id') and then aggregate with Sum('id', default=0). They provide the complete error traceback showing an OperationalError with \"syntax error\" near \"FROM\", and crucially show the malformed SQL query generated: 'SELECT FROM (SELECT \"core_book\".\"id\" AS \"idx\", COALESCE(SUM(\"core_book\".\"id\"), ?) AS \"id__sum\" FROM \"core_book\") subquery'. The missing column list after SELECT clearly indicates the bug. The issue includes specific Django version (4.0.1), database (SQLite), and shows that the same query works without the default parameter but fails when default=0 is added. This provides enough information for an engineer to reproduce the issue and understand that the problem is in how Django generates SQL when combining annotations with aggregations that have default values.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue because it requires understanding Django's ORM internals, specifically how aggregations with default values are processed and how they interact with annotations. The engineer needs to: 1) Reproduce the issue and understand the SQL generation problem, 2) Trace through Django's aggregation code to find where the is_summary attribute is lost when wrapping with Coalesce, 3) Understand that the missing is_summary attribute causes incorrect SQL generation in subqueries, and 4) Implement the fix to preserve this attribute. The actual code change is small (3 lines), but diagnosing the root cause in Django's complex ORM requires significant investigation and understanding of how summary aggregations work in subqueries.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-documented, the fix is targeted and testable, and the tests provided verify both the specific bug case and edge cases. This is a good benchmark sample as it tests understanding of ORM internals and SQL generation logic.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15380": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified. It provides a clear reproduction scenario: renaming a model class from `MyModel` to `MyModel2` and updating one of its fields, then running `python manage.py makemigrations`. The specific error is clearly shown with a complete traceback pointing to line 823 in `autodetector.py` where a KeyError occurs for `('test_one', 'mymodel')`. The issue indicates this is a regression from a specific commit. While some details could be clearer (like the exact field changes made), there's enough information to understand the problem: the migration autodetector fails when both a model and its field are renamed simultaneously. The traceback provides the exact location and type of error, making it possible to investigate and fix the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides a clear traceback pointing to the exact line causing the problem (line 823 in autodetector.py). Looking at the gold patch, the fix is indeed simple - it's a one-line change where `old_model_name` should be replaced with `model_name` when looking up the new model state. The error occurs because the code is using the old model name to look up something in the new state, but after a model rename, the new state contains the new model name. An experienced engineer familiar with Django's migration system could identify this logical error relatively quickly by examining the traceback and understanding that the code is incorrectly using the old model name when it should use the new one.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the reproduction steps are clear, and the fix is straightforward. The test patch shows that a proper test case can be written to verify the solution works correctly when both model and field renaming happen together. This is a good benchmark sample as it tests understanding of Django's migration autodetector logic and the ability to debug from a clear error traceback.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15503": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It provides specific details including: (1) The exact problem - JSONField has_key lookup with numerical keys fails on SQLite but works on PostgreSQL, (2) Clear reproduction case with a model having JSONField data, test records with {\"foo\": \"bar\"} and {\"1111\": \"bar\"}, and queries using data__has_key for both \"foo\" and \"1111\", (3) Expected vs actual behavior - the query for \"1111\" should return 1 result but returns 0, (4) Complete environment details (Django 4.0.3, Python 3.9.6, SQLite versions), and (5) Evidence that it's database-specific (works on PostgreSQL, fails on SQLite). The title also mentions this affects MySQL and Oracle, making the scope clear. An experienced engineer has enough information to understand this is a database backend compatibility issue with JSON key lookups for numeric keys.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: (1) It requires understanding Django's JSONField implementation and database-specific SQL generation, (2) The engineer needs to investigate how different databases handle JSON path lookups, particularly how numeric keys are treated differently across PostgreSQL, SQLite, MySQL, and Oracle, (3) The solution involves modifying the HasKeyLookup class and creating a new HasKeyOrArrayIndex class, affecting multiple methods across the django/db/models/fields/json.py file, (4) It requires understanding the distinction between object keys and array indices in JSON path compilation, and (5) The fix touches several related lookup classes and methods, requiring careful consideration of backward compatibility. While not extremely complex, it's more than a simple bug fix and requires substantial domain knowledge of Django's ORM and database differences.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the test case provides good coverage for the fix, and the solution is testable. The issue represents a realistic database compatibility problem that Django developers would encounter.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15525": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: loaddata fails on non-default database when natural keys use foreign keys. The issue includes a complete, minimal reproducible example with specific models (Author and Book), the exact command that triggers the error, the full stack trace, the model definitions showing natural_key methods and dependencies, and the JSON fixture data. The user clearly explains what they expected (both author and book to be created in the 'other' database) versus what actually happened (DoesNotExist exception). All the necessary context is provided to understand and reproduce the issue without any ambiguity.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15min-1hour fix. The issue is in Django's serialization system where natural_key() method calls are made before the object's database state is properly set. The stack trace clearly points to the problem area in base.py line 332. The fix is conceptually simple: create the model instance, set its database state, then call natural_key(). Looking at the provided patch, it's just a 3-line change that extracts object creation, sets obj._state.db = db, then calls natural_key() on the properly configured object. An experienced Django developer familiar with the ORM and serialization system could identify and implement this fix within an hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained Django ORM bug with a clear reproduction case, obvious symptoms, and a targeted fix. The issue demonstrates good software engineering practices with minimal reproducible examples and comprehensive error reporting.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15695": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: RenameIndex() crashes when an unnamed index is moving backward and forward, specifically with the error \"relation 'new_pony_test_idx' already exists\" on PostgreSQL. The description outlines a specific scenario where the backwards migration is run, then the forwards migration, causing a crash. While some implementation details could be clearer (like what \"unnamed index for unique_together\" specifically means), there's enough context to understand that the problem is related to index name handling during migration operations. The expected behavior is implied - the operation should not crash when re-applied. Looking at the gold patch confirms this interpretation: the fix is a simple check to avoid altering when the index name hasn't changed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is straightforward once you understand Django migrations and the RenameIndex operation. The problem is clearly isolated to the RenameIndex operation in the migrations system. The gold patch shows this is a simple 3-line fix: adding a check to return early if the old and new index names are the same. An experienced engineer familiar with Django would need to: 1) Locate the RenameIndex operation code in django/db/migrations/operations/, 2) Understand the crash happens because the operation tries to rename an index to the same name, 3) Add a simple conditional check. The logic is straightforward and doesn't require complex algorithmic thinking or extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. It's a real-world problem with a clear, testable solution. The test patch shows exactly how to verify the fix works correctly. The problem domain (Django migrations) is well-established and the expected behavior is reasonable. An engineer would be able to implement and test this fix independently based on the issue description.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15732": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some Django knowledge to fully understand. The problem is clear: removing a unique_together constraint on a field that already has its own unique constraint fails because the migration code finds multiple unique constraints and expects only one. The user provides specific details including the PostgreSQL indexes present and the constraint that caused the issue (unique_together = (('id',),) on a primary key). However, an engineer would need to infer some details about Django's migration system and how it handles constraint removal. The expected behavior (successful removal of the unique_together constraint while preserving the primary key constraint) is reasonably clear from context.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour problem because it requires: (1) Understanding Django's migration system and schema editor internals, (2) Analyzing how the _delete_composed_index method works and why it's failing when multiple constraints exist, (3) Figuring out how to distinguish between different types of unique constraints (primary key vs unique_together), and (4) Implementing logic to specifically target the unique_together constraint while leaving other unique constraints intact. The solution involves modifying multiple methods in the schema backend and requires understanding Django's constraint naming conventions. While not extremely complex, it requires substantial familiarity with Django's ORM internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate Django bug that requires understanding the framework's migration system, but the problem is concrete and the expected behavior is clear. The issue provides sufficient context including database state and error conditions.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15814": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is exceptionally well-specified. It provides: (1) Clear model setup with CustomModel, ProxyCustomModel (proxy), and AnotherModel with ForeignKey, (2) Exact reproducible code showing the crash: AnotherModel.objects.select_related(\"custom\").only(\"custom__name\").all(), (3) Complete stack trace with ValueError: 'id' is not in list, (4) Specific file and line number in Django's codebase (django/db/models/sql/query.py line 745), (5) A concrete proposed solution suggesting to use cur_model._meta.concrete_model._meta. The issue clearly explains the problem occurs when using QuerySet.only() after select_related() with proxy models, making it trivial to understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue provides the exact location of the problem (line 745 in query.py) and even suggests the solution. The root cause is that proxy models need to use their concrete model's metadata when building queries. The actual fix is just adding one line: cur_model = cur_model._meta.concrete_model. An experienced Django developer would quickly understand that proxy models don't have their own database table and need to reference their concrete model's metadata. The main time would be spent understanding the QuerySet internals and writing a test case.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, has a focused solution, and tests a specific Django ORM feature (proxy model handling in QuerySet operations). It requires understanding Django's model meta system and QuerySet internals, which are important skills for Django developers. The fix is surgical and the test case is straightforward to write.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15930": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: Django's Case expression crashes when used with `~Q(pk__in=[])` (negation of an empty Q object), producing invalid SQL with a missing condition in the CASE WHEN clause. The error message shows the exact SQL syntax error and the developer explains their expectation that all rows should be annotated with True since `~Q(pk__in=[])` should match everything. However, some details require inference: the specific Django ORM query structure isn't fully shown (we have to piece together that it involves annotating User objects with a Case expression), and the developer doesn't explicitly state the root cause (that empty Q objects compile to empty strings which are invalid in CASE WHEN contexts). An experienced Django developer could reasonably interpret this as needing to handle the case where Q object compilation results in empty conditions within Case expressions.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution is quite straightforward once you understand the problem: when a condition compiles to an empty string in a Case expression, replace it with a condition that always evaluates to True. The patch shows this is just a 5-line addition with a simple if-statement check. An experienced Django developer would quickly identify that the issue is in the Case.as_sql() method in expressions.py, and the fix involves detecting empty condition_sql and replacing it with Value(True). The main time would be spent understanding Django's expression compilation system and writing/running tests, but the actual code change is minimal and the logic is clear.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-isolated bug fix in Django's ORM expression handling that demonstrates good software engineering practices. The issue has a clear reproduction case, specific error message, and the solution is focused and minimal. The test case provided shows the expected behavior clearly. This would be a good benchmark sample for testing understanding of ORM internals and edge case handling.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15957": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some Django ORM knowledge to fully understand. The problem is clearly stated: using Prefetch objects with sliced querysets (e.g., queryset[:3]) raises an AssertionError \"Cannot filter a query once a slice has been taken.\" The user wants to prefetch a limited number of related objects per category for display purposes. The expected behavior is that each Category should have up to three posts in the example_posts attribute. While the issue doesn't explicitly state technical implementation details, an experienced Django developer would understand this involves the prefetch_related mechanism and QuerySet slicing. The use case is common and well-motivated - displaying categories with a few example posts without fetching all related objects.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a complex issue requiring 4+ hours to solve properly. The solution involves deep understanding of Django's ORM internals, specifically how prefetch_related works with QuerySet filtering and slicing. From the patch, we can see it requires: 1) Creating a new utility function _filter_prefetch_queryset that handles sliced querysets, 2) Understanding QuerySet internals like low_mark, high_mark, and query compilation, 3) Using advanced SQL concepts like Window functions and RowNumber, 4) Modifying multiple descriptor classes (ForwardManyToOneDescriptor and related), 5) Handling edge cases around database routing and query limits. The solution spans multiple files and requires knowledge of Django's query compilation process, SQL window functions, and the intricate relationship between Python slicing and SQL LIMIT/OFFSET. This is clearly an advanced ORM feature that would require substantial research into Django's source code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined within Django's ORM context, the test cases are comprehensive covering multiple relationship types (M2M forward/reverse, ForeignKey reverse), and the solution addresses a legitimate limitation. An experienced engineer familiar with Django should be able to tackle this given sufficient time to research the ORM internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15973": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The user describes a clear scenario: they have three Django apps (fonte, variavel, fonte_variavel) with specific models and relationships. They provide a detailed error traceback showing \"AttributeError: 'str' object has no attribute '_meta'\" occurring during migration when the through model is in a different app. They also mention that putting everything in the same models.py file works. While some implementation details about the exact model definitions are not provided, the core problem is clear: Django's migration system fails when a ManyToManyField's through model is defined in a separate app. The error message and stack trace provide enough context to understand that the issue is in the migration autodetector code where it's trying to access the _meta attribute on a string rather than a model class.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's migration system internals, specifically the autodetector code. The engineer needs to: 1) Understand how Django resolves model references across apps during migration generation, 2) Trace through the migration autodetector code to find where the string-to-model resolution is failing, 3) Identify that the issue is in _get_dependencies_for_foreign_key where remote_field_model is being passed instead of field.remote_field.through to resolve_relation, 4) Make the fix and ensure it doesn't break other functionality. The patch shows it's ultimately a one-line change, but finding the root cause requires navigating Django's complex migration system and understanding the difference between model references and actual model objects.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug in Django's migration system with a clear reproduction case and specific error. The issue demonstrates a real problem that developers would encounter, and the solution requires meaningful debugging skills and understanding of Django internals. The test case shows the exact scenario that needs to work correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16032": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It provides a clear, step-by-step reproduction case that explains exactly what should happen: 1) Filter Book records with >400 pages, 2) Call annotate() to add a constant field, 3) Call alias() to add another constant field, 4) Use that QuerySet in Publisher.objects.filter(book__in=...) with values(\"name\"), 5) Assert specific expected publisher names. The issue also provides the exact error message that occurs: \"django.db.utils.OperationalError: sub-select returns 10 columns - expected 1\". This gives a clear understanding that the problem is with the SQL generation when using alias() after annotate() in subqueries - the subquery is returning too many columns instead of just the primary key needed for the __in lookup.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's ORM internals, specifically how QuerySet.alias() and annotate() interact with subquery generation for __in lookups. The engineer needs to understand the SQL generation pipeline, how select fields are managed in queries, and how the has_select_fields property affects query compilation. Looking at the patch, the solution involves modifying related_lookups.py to use set_values() instead of clear_select_clause() + add_fields(), and updating query.py to properly track has_select_fields as an attribute rather than a computed property. This requires navigating multiple files and understanding the intricate relationships between Django's query building components.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. The reproduction steps are clear and testable, the expected behavior is explicitly stated, and the error message provides a concrete failure condition. The solution requires understanding of ORM internals but the problem scope is well-defined. The test case provided in the reproduction steps matches exactly with the test added in the PR, confirming the issue description accurately captures the problem.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16136": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when a Django class-based view only implements an async post method, GET requests that should return a 405 Method Not Allowed response instead crash with \"TypeError: object HttpResponseNotAllowed can't be used in 'await' expression\". The issue provides a complete stack trace showing the error occurs in asgiref/sync.py when trying to await an HttpResponseNotAllowed object. The reproduction steps are clear: create a fresh Django 4.1.1 project, define a view with only an async post method, route it to \"/demo\", and make a GET request. The expected behavior (405 response) vs actual behavior (TypeError crash) is explicitly stated. The technical context is sufficient - it's clear this is about Django's handling of async views and method not allowed responses.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly isolated to the http_method_not_allowed method in Django's generic base view. The problem is straightforward: when a view is async (view_is_async=True), the method returns an HttpResponseNotAllowed object directly, but the async handling expects a coroutine. The solution shown in the patch is elegant and small - it checks if the view is async and wraps the response in an async function if needed. An experienced engineer familiar with Django's async handling could identify this pattern quickly. The fix involves modifying only one method in one file (django/views/generic/base.py) with about 10 lines of code. The logic is not complex - it's a conditional check and wrapper function.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean, well-defined bug with a clear reproduction case and solution. The issue demonstrates good understanding of Django's async view handling and the test case appropriately verifies the fix works for both sync and async views. The sample would work well for evaluating coding ability as it tests understanding of async/await patterns, Django's view system, and defensive programming practices.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16429": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The specific error \"TypeError: can't subtract offset-naive and offset-aware datetimes\", (2) The exact conditions that trigger it (USE_TZ=True and >1 month interval), (3) A reproducible test case (capture current time with timezone.now(), subtract 31 days, call timesince()), (4) The expected behavior (\"1 month\"), (5) The root cause analysis pointing to the pivot instantiation at lines 93-100 in django/utils/timesince.py, (6) A specific proposed fix (adding \"0, d.tzinfo\" arguments to the datetime.datetime call). The issue references a specific commit hash and provides a direct link to the problematic code. An experienced engineer would have all the information needed to understand and fix this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location (django/utils/timesince.py lines 93-100) and suggests the exact fix needed (adding tzinfo=d.tzinfo to the datetime constructor). The actual code change is minimal - just adding one parameter to a datetime constructor. The main time would be spent understanding the timezone handling logic and verifying the fix works correctly, but the issue provides clear guidance on both the problem and solution approach.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent example for a coding benchmark - it has a clear problem statement, specific error conditions, a targeted fix location, and the solution can be verified through the provided test case. The issue demonstrates good software engineering practices by including commit references, code links, and a proposed solution approach.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16454": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Django management commands use CommandParser (a subclass of ArgumentParser) with extra arguments for error formatting, but these arguments aren't copied to subparsers created via add_subparsers().add_parser(). This results in stack traces instead of user-friendly error messages for subparser errors. The issue provides a concrete example with a \"cheeses\" command that has a \"create\" subcommand requiring a \"name\" argument. It shows the expected behavior (usage message for missing subcommand) versus the actual behavior (stack trace for missing subcommand arguments). The root cause is identified: subparser actions need to copy relevant arguments from the parent CommandParser. The solution direction is clear: ensure add_subparsers() copies the necessary arguments to constructed subparsers.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location (CommandParser.add_subparsers()) and the root cause (arguments not being copied to subparsers). The solution involves overriding add_subparsers() in CommandParser to pass the called_from_command_line argument to subparser instances using functools.partial. While it requires understanding Django's management command architecture and argparse subparsers, the actual code change is small and straightforward - just adding a method that ensures proper argument propagation. The gold patch confirms this with only ~10 lines of code added.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with clear examples, the solution approach is evident from the issue description, and the fix is localized to a specific method in Django's management command framework. The tests also provide good coverage for both Django CommandParser and vanilla ArgumentParser subparsers, ensuring the solution works correctly in different scenarios.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16485": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Django's floatformat filter crashes when processing the literal '0.00' with a precision argument of 0, raising a \"ValueError: valid range for prec is [1, MAX_PREC]\". The issue provides specific test cases (both string '0.00' and Decimal('0.00')) and the exact error message. An experienced engineer would understand that the floatformat function needs to handle the edge case where precision is 0 and the input value has trailing zeros. The gold patch confirms this interpretation - it's a simple one-character fix changing \"p < 0\" to \"p <= 0\" in the condition, allowing precision of 0 to be handled properly.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix that should take less than 15 minutes. The issue description clearly points to the floatformat function, and the error message indicates a boundary condition problem with precision values. An engineer would quickly locate the floatformat function in django/template/defaultfilters.py, identify that the condition \"p < 0\" should include \"p == 0\" to handle zero precision, and make the one-character change from \"p < 0\" to \"p <= 0\". The solution requires minimal understanding of the codebase and is a straightforward boundary condition fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. This is an ideal benchmark sample - the problem is clearly defined, the solution is straightforward but not immediately obvious from the description alone, and it tests the ability to understand error messages and fix boundary conditions. The test cases provided in the patch are also clear and would properly validate the solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16569": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the add_fields() method in Django formsets fails when index is None under specific conditions (can_delete=True and can_delete_extra=False). The issue provides the exact error message \"TypeError: '<' not supported between instances of 'NoneType' and 'int'\", identifies the problematic line of code (line 493 in django/forms/formsets.py), explains why it occurs (index is compared to initial_form_count without checking if index is None), and even provides the exact fix needed. The reproduction steps are detailed and specific, making it clear what conditions trigger the bug and how to test for it.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue description pinpoints the exact problematic line of code and provides the solution. The fix involves adding a simple null check (index is not None) to an existing conditional statement. An experienced engineer would need minimal time to understand the Django formsets code structure, locate the specific line mentioned, understand the logic flow, and implement the suggested fix. The solution is a one-line change that adds proper null checking to prevent the TypeError.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. This is an ideal sample for a coding benchmark as it has a clear problem statement, specific reproduction steps, a well-defined solution, and tests that verify the fix. The issue demonstrates understanding of conditional logic, null safety, and Python type comparisons - all valuable skills for software engineering.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16667": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: SelectDateWidget crashes with an OverflowError when extremely large integers are provided for the year parameter. The issue provides a detailed reproduction scenario with specific parameter values (my_date_day=1, my_date_month=1, my_date_year=1234567821345678), shows the exact error traceback, identifies the problematic code location (django/forms/widgets.py line 1160 in value_from_datadict method), and explains the root cause (datetime.date(int(y), int(m), int(d)) throws OverflowError when large integers exceed sys.maxsize). The issue even includes a Python example demonstrating the overflow condition. A developer would clearly understand they need to add error handling around the datetime.date() constructor call to catch OverflowError exceptions.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The problem is clearly identified (OverflowError in datetime.date constructor), the location is pinpointed (value_from_datadict method in widgets.py), and the solution is relatively simple: wrap the datetime.date() call in a try-catch block to handle OverflowError. Looking at the provided patch, the fix is just 2 lines of code - adding an except OverflowError clause that returns \"0-0-0\". The main time would be spent understanding the existing code flow and determining the appropriate fallback value, but the core issue and fix approach are straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. This is a well-contained security/robustness fix with a clear problem statement, obvious solution approach, and minimal scope. The patch shows it's a simple exception handling addition that maintains backward compatibility.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16950": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear scenario: Django Admin with inlines involving UUIDField default values is not working correctly. The reporter describes a specific setup with an abstract base model containing a BigAutoField primary key and a UUIDField defaulting to uuid.uuid4, a concrete Thing model inheriting from it, and a SubThing model with a foreign key to Thing. The problem occurs when trying to add a SubThing inline while creating/editing a Thing - it triggers an exception where the Thing model's \"id\" is being set to null. While the description could be more technical about the exact mechanics, it provides enough context about the models, the admin setup, and the specific failure scenario that an experienced Django developer could reasonably understand what needs to be fixed. The external link to dpaste.com isn't accessible but the core issue is clear from the description.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a Django forms/admin issue that requires understanding Django's inline formset mechanics, specifically how default values are handled for foreign key relationships in the context of admin inlines. Looking at the solution, it involves modifying the add_fields method in django/forms/models.py with a conditional check that considers whether the target field is the parent model's primary key and whether form data is provided. This requires: (1) Understanding Django's formset internals and how inline forms work, (2) Understanding the relationship between UUIDField defaults and foreign key handling, (3) Debugging why the current logic incorrectly nullifies values, and (4) Implementing a precise conditional fix that doesn't break other functionality. The solution touches core Django ORM/forms functionality and requires careful consideration of edge cases, making it a solid 1-4 hour task for an experienced engineer.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained Django bug with a specific, testable solution. The issue is clearly scoped to Django's admin inline functionality with UUIDField defaults, and the provided test cases demonstrate both the problem and the expected behavior. The solution involves modifying core Django formset logic, which is appropriate for a coding benchmark as it tests understanding of Django internals, debugging skills, and the ability to implement precise conditional logic without breaking existing functionality.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-20859": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It provides: 1) A clear bug summary stating that adding a legend to a SubFigure doesn't work, 2) Specific code reproduction steps (create figure, get subfigure, call subfig.subplots(), plot with label, call subfig.legend()), 3) The exact error traceback showing \"TypeError: Legend needs either Axes or Figure as parent\", 4) Clear expected outcome (visible legend should appear), 5) Even suggests the specific fix needed - changing line 437 to check against FigureBase instead of Figure, with a direct link to the problematic code. The issue includes all necessary technical details like matplotlib version, OS, backend, and Python version. An experienced engineer would have everything needed to understand and fix this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because: 1) The exact problem location and solution are already identified in the issue description (change Figure to FigureBase in legend.py line 437), 2) The fix involves changing only a few lines in a single file - updating import statement and isinstance check, 3) The error message clearly points to the problematic code in legend.py __init__ method, 4) The solution requires minimal code understanding - just recognizing that SubFigure inherits from FigureBase, not Figure directly, 5) While some time is needed to understand the inheritance hierarchy and verify the fix works correctly, the actual implementation is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, reproducible, has a specific solution path, and includes a test case that verifies the fix. The issue description provides excellent guidance without being overly prescriptive, making it suitable for evaluating coding ability while being fair to the implementer.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-22719": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is well-specified with some minor gaps. The problem is clearly described: a deprecation warning is being triggered when calling ax.plot([], []) on axes with categorical units, which shouldn't happen since empty data structures don't actually contain numeric values that would trigger the deprecated behavior. The issue provides clear reproduction code, full tracebacks, expected outcome, and demonstrates that the suggested fix (using convert_units) doesn't work. However, there are some implementation details left to the engineer - specifically where in the codebase the fix should be applied and the exact conditional logic needed. The core requirement is clear: prevent the deprecation warning for empty data structures while maintaining it for actual numeric data.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The gold patch shows it's a simple 2-line change adding size checks (values.size and data.size) to existing conditionals in lib/matplotlib/category.py. The traceback clearly points to the problematic code in category.py line 62, making it easy to locate. The fix logic is straightforward: only trigger the deprecation warning if there's actual data to convert. An experienced engineer familiar with the codebase could identify the issue location, understand that empty arrays have size 0, and implement the fix within an hour. The test is also simple - just verify no warning is emitted for empty data.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear reproduction steps, good test coverage, and a targeted solution. The issue demonstrates good software engineering practices with detailed error reporting and attempted debugging that helps identify the root cause.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-23299": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug summary stating that `matplotlib.get_backend()` removes all figures from `Gcf` if the first figure was created in an `rc_context`. The reproduction steps are detailed and specific: 1) Enter an rc_context block, 2) Create a figure inside that block, 3) Record the Gcf state, 4) Call get_backend(), 5) Record state again, 6) Assert they should be the same. The actual outcome shows concrete evidence with an AssertionError demonstrating that the figures dictionary goes from containing one figure to being empty. The expected outcome is clear - figures should not disappear from Gcf. The consequences are also explained (e.g., plt.close() won't work). This gives an engineer everything needed to understand the problem and work toward a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the bug reproduction is straightforward, understanding why get_backend() affects figures created in rc_context requires deeper investigation into matplotlib's internals. The engineer needs to understand the relationship between rc_context, backend switching, and figure management in Gcf. Looking at the solution, it involves modifying the rc_context function to exclude 'backend' from the parameters that get reset, which requires understanding how rc_context works internally and why backend changes affect figure state. The fix itself is small (a few lines), but discovering the root cause and understanding the implications requires substantial investigation of the codebase and the interaction between different matplotlib subsystems.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly reproducible, well-documented, and represents a real bug that affects user workflows. The solution requires understanding matplotlib's internal architecture while being focused enough to implement and test. The test case provided in the patch is straightforward and directly validates the fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-23476": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when a figure is unpickled on M1 Mac, its DPI value doubles each time. The issue provides detailed reproduction steps including specific code structure (import statements, helper function for pickle/unpickle, main loop with 32 iterations), shows the actual problematic output with DPI values doubling from 200.0 to astronomical numbers before causing an OverflowError, and specifies the expected outcome (constant DPI of 200.0). The environment details are comprehensive (MacOSX backend, Matplotlib 3.5.2, M1 Mac platform). The bug is platform-specific and reproducible, making it clear what needs to be fixed: preserve the original DPI value during unpickling operations.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution, this is a relatively straightforward fix that takes 15 minutes to 1 hour. The solution involves adding just 3 lines of code to the __getstate__ method in figure.py to preserve the original DPI value before pickling. The key insight required is understanding that the DPI gets modified by device pixel ratio changes during display, but the original DPI should be preserved when pickling/unpickling. An experienced engineer familiar with Python's pickle mechanism and matplotlib's figure architecture could identify this issue by examining the __setstate__/__getstate__ methods and understanding how device pixel ratios affect DPI values. The fix is localized to a single method and doesn't require extensive code changes or complex logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is platform-specific but well-documented, the reproduction case is clear and comprehensive, and the fix requires understanding of both Python pickling mechanisms and matplotlib internals. The test case provided validates the fix properly by checking that DPI is preserved after unpickling. The issue demonstrates a real-world problem that developers might encounter and tests knowledge of object serialization, graphics library internals, and platform-specific behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24026": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: stackplot doesn't accept CN color aliases (like 'C0', 'C1', etc.) that other matplotlib functions support, throwing a ValueError when trying to use them. The issue provides a specific error message and traceback showing exactly where the problem occurs (in stackplot.py line 73 when calling axes.set_prop_cycle). The use case is clear - the user wants to maintain color consistency across different plot types. The expected behavior is also clear: stackplot should accept CN color aliases just like ax.plot() and matplotlib.patches.Rectangle() do. An experienced engineer would have all the information needed to understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-localized to the stackplot function, and the error traceback points directly to the problematic line (axes.set_prop_cycle). The solution involves understanding that CN aliases can't be used in prop_cycle, so the code needs to be restructured to resolve colors individually rather than setting the entire cycle. Looking at the patch, the fix is relatively straightforward: replace the set_prop_cycle approach with itertools.cycle for the colors parameter and use individual color resolution. This requires some understanding of matplotlib's color handling but doesn't involve complex algorithmic changes or extensive research.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution scope is reasonable, and the test changes show that existing functionality is preserved while the new capability (using CN colors) is added. This is a good benchmark sample as it tests understanding of matplotlib's color system and ability to refactor code to handle edge cases.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24149": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes that ax.bar raises a StopIteration exception when passed all-NaN data in matplotlib 3.6.1, whereas it worked in 3.6.0. The bug report provides: (1) A clear reproduction case with specific code (`ax.bar([np.nan], [np.nan])`), (2) The exact stack trace showing the StopIteration error originating from `cbook._safe_first_finite()`, (3) Expected behavior (should return a BarCollection with NaN values like in 3.6.0), (4) Additional context noting this likely relates to a recent fix for \"barplot being empty when first element is NaN\". The issue even specifies that NaN in x-data triggers the error while NaN in height doesn't, providing good boundary condition information. An experienced engineer would clearly understand they need to handle the StopIteration exception when no finite values are found.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The stack trace clearly points to the problematic function `_safe_first_finite()` in the `_convert_dx` method. The issue is that when all values are NaN, `_safe_first_finite()` raises StopIteration because it can't find any finite values. Looking at the gold patch, the solution is straightforward: catch the StopIteration exception and fall back to using the first element unconditionally via `cbook.safe_first_element()`. This requires adding just a few lines of exception handling code in two places within the same function. An experienced engineer familiar with the codebase could identify this pattern quickly and implement the fix without extensive research or major code restructuring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear reproduction steps, specific error conditions, and a focused solution area. The test case also demonstrates the expected behavior clearly.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24970": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It clearly describes the problem: NumPy 1.24 deprecation warnings occur when using matplotlib colormaps with empty uint8 arrays. The reproduction steps are provided (import matplotlib.pyplot, get a colormap via plt.get_cmap(), call it with an empty uint8 array), and the actual warnings are shown with specific line numbers in colors.py. The expected outcome is clear: \"No warnings.\" However, there are some gaps - the exact code to reproduce the issue isn't provided as a complete script, and the solution approach isn't specified. An engineer would need to understand why the warnings occur (out-of-bound integer conversions to uint8 arrays) and figure out how to fix the assignment operations in colors.py lines 730-732 to avoid triggering NumPy's new stricter behavior.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve. An engineer needs to: (1) Understand NumPy 1.24's new deprecation warnings about out-of-bound integer conversions, (2) Reproduce the issue by creating the specific scenario with empty uint8 arrays and colormaps, (3) Analyze the colormap indexing code in colors.py to understand why values 256, 257, 258 are being assigned to uint8 arrays, (4) Research the proper way to handle the casting to avoid warnings while maintaining the same behavior, (5) Implement the fix by restructuring the code to use np.errstate appropriately and ensure proper type conversions. The solution involves moving the np.errstate context and restructuring the float vs integer handling logic, which requires understanding both the mathematical operations and NumPy's type system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a solid benchmark issue. The problem is real and well-documented, the solution requires genuine debugging and understanding of NumPy's type system, and the fix is non-trivial but achievable. The test provided ensures the fix works correctly across different data types. An engineer would need to demonstrate understanding of deprecation warnings, NumPy array operations, and proper error handling.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25311": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly states the problem (unable to pickle figure with draggable legend), provides specific reproduction steps (create figure, add draggable legend, call pickle.dumps), shows the exact error message (TypeError: cannot pickle 'FigureCanvasQTAgg' object), and specifies the expected outcome (successful pickling without errors). The issue also mentions that the same problem occurs with draggable annotations, giving additional context. An experienced engineer would have enough information to understand that the problem is related to unpicklable references being stored in the draggable functionality and would be able to investigate the relevant code paths.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue because it requires: (1) Understanding matplotlib's architecture and how draggable functionality works, (2) Investigating why canvas objects can't be pickled and where they're being stored, (3) Finding the specific location in the DraggableBase class where the canvas reference is stored, (4) Designing a solution that maintains functionality while avoiding unpicklable references (using a property instead of direct attribute storage), and (5) Understanding the implications for the draggable behavior. The actual code change is small (removing one line, adding a property), but requires significant investigation and understanding of the codebase architecture. The solution is elegant but not immediately obvious without diving into the code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-suited benchmark issue. The problem statement is clear, the reproduction case is straightforward, and the solution requires genuine engineering insight rather than just following obvious patterns. The fix demonstrates understanding of Python object serialization, matplotlib's internal architecture, and property-based design patterns. The test changes also show good engineering practices by verifying the fix works and ensuring no canvas references remain in the pickle stream.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-25332": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug summary (unable to pickle figure after calling align_labels()), detailed reproduction steps with specific code (create figure with two subplots, define arrays, plot data, set labels, call fig.align_labels(), then attempt pickle.dumps(fig)), the exact error message (TypeError: cannot pickle 'weakref.ReferenceType' object), and the expected outcome (figure should pickle successfully). The reproduction steps are concrete enough that any engineer could reproduce the exact problem and understand what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding the internal workings of matplotlib's align_labels() functionality and how it interacts with Python's pickle module. The engineer needs to: 1) Understand that align_labels() creates weak references that cannot be pickled, 2) Locate the Grouper class in matplotlib/cbook.py where the weak references are stored, 3) Implement __getstate__ and __setstate__ methods to handle pickle serialization by converting weak references to strong references during pickling and back to weak references during unpickling. This requires knowledge of Python's pickle protocol, weak references, and matplotlib's internal architecture, but doesn't require extensive code changes (only ~15 lines added).",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for the benchmark. The problem is clearly defined, the solution involves core Python concepts (pickling, weak references) that are important for software engineers to understand, and the fix is focused and testable. The test patch appropriately adds align_ylabels() to the existing pickle test to ensure the bug doesn't regress.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-25479": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but requires some interpretation. The user describes a clear problem: they created a colormap with internal name 'some_cmap_name', registered it under key 'my_cmap_name', but when using plt.set_cmap('my_cmap_name'), matplotlib looks up the internal name instead of the registration key, causing a ValueError. The user provides a complete error traceback and identifies the root cause - matplotlib maintains two separate name-lookup tables and the internal name must match the registration name. However, the exact solution approach isn't explicitly stated, requiring the engineer to determine whether to fix the registration process, the lookup process, or both. The issue provides enough context to understand what needs to be fixed, but leaves room for interpretation on the implementation details.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. Looking at the provided patches, the solution requires understanding matplotlib's colormap registration and lookup system across multiple files (cm.py and colors.py). The engineer needs to: 1) Understand how colormap registration works in the register() method, 2) Identify that when registering a colormap with a different name, the internal .name attribute should be updated to match, 3) Modify the __eq__ method in colors.py to not compare names when determining colormap equality, and 4) Update tests accordingly. While the core changes are relatively small (adding a few lines), it requires understanding the interaction between different parts of the codebase and making coordinated changes across multiple files. The debugging process to identify the exact root cause and appropriate fix points would take substantial time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly reproducible with the provided code example, the error message is informative, and the expected behavior is reasonable. The issue represents a legitimate bug in matplotlib's colormap handling system that would benefit from being fixed. The test case provided in the patch demonstrates the fix works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-26291": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It provides a specific bug report with a complete error traceback showing an AttributeError when trying to create inset axes. The user describes exactly what they're trying to do (following the matplotlib demo), provides the specific function call that fails (`inset_axes(ax, width=1.3, height=0.9)`), shows the complete error stacktrace pointing to the exact line where the failure occurs (line 399 in offsetbox.py where `self.figure._get_renderer()` is called on a NoneType object), and clearly states the expected outcome (a small inset box in the upper-right corner). The reproduction case is straightforward and the problem is precisely identified in the code.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward bug fix that should take 15 minutes to 1 hour. The error traceback clearly points to the exact issue: in the `__call__` method of AnchoredLocatorBase, the code tries to call `self.figure._get_renderer()` but `self.figure` is None. Looking at the provided patch, the solution is simple - add a null check for renderer and get it from `ax.figure._get_renderer()` instead. The fix is only 2 lines of code and involves understanding the control flow of the inset axes creation. An experienced engineer familiar with matplotlib would quickly identify that the renderer parameter is None and needs to be obtained from the axes figure instead of the offsetbox figure.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a straightforward bug fix with a clear reproduction case and a simple solution. The test case also makes sense - it reproduces the exact scenario that triggers the bug (using bbox_inches=\"tight\" when saving a figure with inset axes).",
            "q2_5_confidence": 5
        }
    },
    {
        "psf__requests-1724": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear reproduction case showing that using a unicode string for the method parameter (u'POST') causes a UnicodeDecodeError, while using a regular string ('POST') works fine. The issue includes a complete stack trace pointing to the exact problem location, identifies the root cause (sessions.py:313 using method.upper() on unicode), and provides specific version information (requests 1.2.3, Python 2.7.2). The author even provides their hypothesis about what's happening - that u'POST' is \"infecting the header with unicode when it should be a string.\" This gives an engineer everything needed to understand the problem and implement a fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. The problem is clearly identified - unicode method names need to be converted to strings before processing. Looking at the gold patch, the solution is a simple 1-line addition: `method = builtin_str(method)` before the existing `req = Request(method = method.upper(), ...)` call. The fix requires: (1) importing builtin_str from .compat, (2) adding the conversion line. An experienced engineer would quickly identify this as a Python 2/3 compatibility issue and implement the string conversion. The test case is also straightforward - just verify that using u'POST' doesn't crash.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described with a minimal reproduction case, the fix is simple but requires understanding of Python 2/3 unicode handling, and it tests an important edge case. The problem is realistic and the solution demonstrates good engineering practices.",
            "q2_5_confidence": 5
        }
    },
    {
        "psf__requests-5414": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The current behavior - a UnicodeError is raised when requesting \"http://.example.com\", (2) The expected behavior - an InvalidURL exception should be raised instead with the message \"URL has an invalid label.\", (3) Exact reproduction steps with code, (4) Reference to existing similar handling for URLs starting with \"*\", and (5) Complete system information. The issue text provides all necessary context including the specific error message, expected exception type and message, and a clear example. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is extremely simple - just modifying one line in requests/models.py to change `elif host.startswith(u'*'):` to `elif host.startswith((u'*', u'.')):`. The engineer needs to: (1) Locate the existing UnicodeError handling code mentioned in the issue, (2) Find where URLs starting with \"*\" are already being caught and raising InvalidURL, (3) Extend that condition to also catch URLs starting with \".\". The logic is straightforward - it's extending existing pattern matching rather than implementing new functionality. Most time would be spent familiarizing with the codebase structure to locate the right file and function.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue description is crystal clear, the solution is non-trivial enough to require some investigation but simple enough to implement quickly, and the test cases clearly validate the fix. The issue demonstrates good software engineering practices - extending existing error handling patterns rather than creating entirely new code paths.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-3151": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear MCVE (Minimal Complete Verifiable Example) that demonstrates the problem, explains the expected behavior based on documentation, shows the actual error message, and even explains why the current behavior contradicts the documented behavior. The user clearly explains that identical coordinate dimensions should be ignored according to the documentation, but the current implementation still requires them to be monotonic. An experienced engineer would have no difficulty understanding what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The issue is straightforward once you understand the problem: the monotonicity check is being applied to all dimensions instead of just the concatenation dimensions. Looking at the patch, the fix involves changing one loop from iterating over `concatenated.dims` to iterating over `concat_dims`, and removing one conditional check. The logic is simple and the change is minimal. An experienced engineer familiar with the codebase would quickly identify that the issue is in the combine_by_coords function and that the monotonicity check scope needs to be narrowed.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described with a proper MCVE, the problem is well-defined, the fix is straightforward but requires understanding the code logic, and the test case properly validates the fix. The issue demonstrates good software engineering practices in both problem reporting and solution implementation.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-3677": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: while the top-level merge() function can merge a Dataset and DataArray, the Dataset.merge() method fails when trying to merge with a DataArray. The issue includes a specific error traceback showing that the problem occurs because DataArray objects don't have an 'items' attribute, which is expected in the coerce_pandas_values function. The expected behavior is clearly defined - the Dataset.merge() method should work the same way as the top-level merge() function. A test case is implicitly described (merging a Dataset with variable \"a\" and DataArray \"b\"), making it clear what the solution should accomplish.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it's a simple one-line change that converts a DataArray to a Dataset before processing. The error traceback clearly points to the root cause - the code expects objects with an 'items' method, but DataArray doesn't have one. The solution is straightforward: convert the DataArray to a Dataset using to_dataset() method before passing it to the merge processing functions. An experienced engineer would need some time to trace through the code and understand the merge flow, but the fix itself is minimal and the approach is obvious once you understand that DataArrays need to be converted to Datasets for the existing merge logic to work.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for a coding benchmark. The issue is clearly described, the solution is well-scoped, and the test case provided in the patch validates the exact functionality described in the issue. The fix demonstrates understanding of the xarray API and object model, requiring the engineer to recognize that DataArrays can be converted to Datasets to work with existing merge infrastructure.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-4094": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some filling in of blanks. The description provides a clear MCVE (Minimal Complete Verifiable Example) with specific steps: create a one-dimensional DataArray with values 0,1,2 along coordinate \"x\", build a Dataset with variables \"a\" and \"b\", call to_stacked_array('y', sample_dims=['x']), then call to_unstacked_dataset('y'). The expected behavior (lossless round-trip) and actual error (MergeError about conflicting values) are clearly stated. However, the issue description lacks the actual executable code sample - it describes what to do but doesn't provide the specific code that reproduces the problem. An experienced engineer would need to construct the exact reproduction case from the description, but the steps are clear enough to do so unambiguously.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue is in the to_unstacked_dataset method in xarray/core/dataarray.py. Looking at the gold patch, the fix is a single line change: adding `drop=True` to the `.sel()` call before `.squeeze(drop=True)`. The problem occurs because when selecting from a single-dimension variable, the coordinate becomes a scalar but isn't properly dropped, leading to conflicts when merging. An experienced engineer would need to: 1) Reproduce the issue (5-10 minutes), 2) Trace through the to_unstacked_dataset method to understand where the conflict arises (10-20 minutes), 3) Identify that the coordinate needs to be dropped during selection (10-15 minutes), 4) Implement and test the fix (5-10 minutes). The debugging process is straightforward since the error message points to a merge conflict, and the method is relatively short and focused.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - it tests understanding of xarray's coordinate handling, debugging skills for merge conflicts, and knowledge of the sel() method's drop parameter. The fix is small but requires proper understanding of the underlying issue with coordinate handling in single-dimension cases.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-4695": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear reproduction case with specific steps: create two DataArray objects, one with dimensions ['dim1', 'dim2'] and another with ['dim1', 'method'], then call .loc with dictionary indexing. The problem is clearly stated - using \"method\" as a dimension name causes a ValueError about invalid fill method when it should work just like any other dimension name. The issue includes the exact error message and explains that dimension names should be irrelevant. The reproduction steps are concrete and the expected vs actual behavior is clearly described.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is quite simple - changing from `self.data_array.sel(**key)` to `self.data_array.sel(key)` in the dataarray.py file. The issue is that when using `**key`, the dictionary keys (including \"method\") are passed as keyword arguments to the sel() method, and \"method\" happens to be a parameter name in sel() that expects specific values. By passing the dictionary directly, it avoids this naming collision. The fix requires understanding the problem (dimension name collision with method parameters) and making a one-line change, which is straightforward once identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-described, the reproduction case is clear, and the solution is a focused code change. The test case confirms the fix works for the specific scenario described. This is a good example of a clearly defined bug with a clean solution that would be suitable for a coding benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-6599": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The reporter clearly describes the problem: `polyval` with timedelta64 coordinates produces wrong results in the latest version compared to version 2022.3.0. They provide a specific scenario involving timestamps at 10-second intervals and polynomial coefficients, showing that results changed from correct values (around 4.4-4.7 million) to ridiculously large numbers (around 1\u00d710\u00b3\u2070). However, the issue description lacks the actual minimal reproducible code example - they mention creating \"six timestamps\" and \"5\u00d73 array of polynomial coefficients\" but don't show the exact code. Despite this, the core problem is clear: there's a regression in polyval behavior with timedelta64 data types, and an experienced engineer could reasonably construct a minimal example and identify the root cause from the description provided.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min to 1 hour fix. Looking at the gold patch, the solution is quite straightforward: the issue was in the `_ensure_numeric` function in `computation.py` where timedelta64 data (dtype.kind == \"m\") was being handled incorrectly. The original code lumped together datetime (\"M\") and timedelta (\"m\") types with `if x.dtype.kind in \"mM\"`, but timedeltas need different handling than datetimes. The fix simply separates these cases and converts timedeltas to float with `x.astype(float)`. This is a small, focused change that requires understanding the difference between datetime and timedelta handling, but once the root cause is identified in the `_ensure_numeric` function, the fix is relatively obvious and involves only a few lines of code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a clear regression with a well-defined expected behavior. The solution involves a focused change to data type handling logic, and the test case provided in the patch shows exactly what should be tested. An engineer could reasonably create a minimal example from the description and identify the root cause.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-6721": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: accessing the `.chunks` property on a zarr-backed xarray dataset loads the entire array into memory instead of just inspecting metadata. The expected behavior is also stated - it should simply inspect the encoding attribute without loading data. However, there are some missing details: no minimal reproducible example is provided (marked as \"No response\"), and the specific zarr URL mentioned may not be accessible to all developers. The stack trace provides valuable debugging information showing the call chain from `Dataset.chunks` through to zarr operations. While an engineer could reasonably understand what needs to be fixed from this description, they would need to create their own test case or find alternative zarr datasets to reproduce the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it's a one-line change in `xarray/core/common.py` line 2026, changing `v.data` to `v._data`. The issue is that `v.data` triggers the property getter which loads the data into memory, while `v._data` directly accesses the underlying data object without triggering data loading. The stack trace clearly points to the problematic line in `get_chunksizes()` function. An experienced engineer familiar with the codebase would quickly identify that the issue is accessing `.data` instead of `._data` when checking for chunk attributes. The fix is conceptually simple once you understand the difference between these two attributes in xarray's Variable class.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is straightforward, and the test case provided in the patch adequately verifies the fix by using an InaccessibleVariableDataStore to ensure data loading doesn't occur when accessing chunks.",
            "q2_5_confidence": 4
        }
    },
    {
        "pylint-dev__pylint-6386": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the short option `-v` for verbose mode expects an argument when it shouldn't, while the long option `--verbose` works correctly without an argument. The issue provides a specific error message (\"argument --verbose/-v: expected one argument\"), shows the exact command that fails (`pylint mytest.py -v`), and states the expected behavior (\"Similar behaviour to the long option\"). The bug report includes version information and demonstrates a clear understanding of what should happen versus what actually happens. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patches, this is a relatively straightforward configuration issue. The fix involves: 1) Adding metavar parameter support to the argument configuration system, 2) Setting metavar to empty string for verbose option to prevent argument expectation, 3) Adding the short `-v` flag to the preprocessing options mapping, and 4) Updating the preprocessing logic to handle short flags starting with `-`. While it touches multiple files, each change is small and focused. The core issue is that the argument parser was configured to expect a value for the verbose option, which needed to be corrected by setting an empty metavar. An experienced engineer familiar with argparse and the codebase structure could identify and implement this fix within 15 minutes to 1 hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is clear from the issue description, and the test case validates that the fix works correctly. This would be a good benchmark sample as it tests understanding of command-line argument parsing, configuration systems, and requires making coordinated changes across multiple files while maintaining backward compatibility.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-6903": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: pylint crashes when run with --jobs=0 in Kubernetes environments because the _query_cpu() function returns 0, causing multiprocessing.Pool to fail with \"Number of processes must be at least 1\". The issue provides: (1) exact reproduction steps with the command used, (2) complete stack trace showing the crash location, (3) detailed explanation of the root cause (CPU quota=-1, period=100000, shares=2, leading to 0 after integer division by 1024), (4) specific environment details (Ubuntu 20.04, Kubernetes v1.18.6, Python 3.9.12, pylint>2.14.0), and (5) a clear suggested solution (\"max(computed, 1)\" fallback). The expected behavior is also clearly stated: pylint should not crash and should use at least 1 worker when the computed count is 0.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. The issue clearly identifies the problematic function (_query_cpu() in pylint/lint/run.py), the exact cause (returning 0 in Kubernetes environments), and suggests a simple solution. Looking at the gold patch confirms this - it's just a 6-line addition that checks if avail_cpu == 0 and sets it to 1. The fix requires minimal code understanding beyond locating the function and adding the safety check. The test is more complex but still manageable within an hour, involving mocking file operations to simulate the Kubernetes environment conditions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described, has a straightforward solution, and tests a developer's ability to implement defensive programming practices and handle edge cases in system resource detection. The problem domain (CPU detection in containerized environments) is realistic and the solution requires understanding both the immediate fix and proper testing of the edge case.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-8898": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the `bad-names-rgxs` option in pylint splits on commas without considering that commas can be part of valid regex syntax (like quantifiers `{1,3}`). The issue provides a concrete example that demonstrates the crash: setting `bad-name-rgxs` to `(foo{1,3})` causes pylint to fail with a clear traceback showing the regex parsing error. The expected behavior is also clearly stated: any valid regular expression should be expressible in this option, with potential escaping mechanisms if needed. The bug report includes all necessary details including pylint version, environment, and a complete stack trace that pinpoints the exact location of the failure in the code (`_regexp_csv_transfomer` function in `/config/argument.py`).",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours of work. The engineer needs to: (1) Understand how the current CSV parsing works in `_csv_transformer`, (2) Design a new regex-aware CSV parser that can handle commas within quantifiers like `{1,3}`, (3) Implement the `_check_regexp_csv` function with proper brace tracking logic, (4) Update the imports and exports in the utils module, and (5) Write comprehensive tests. The solution involves creating a stateful parser that tracks open/close braces to determine when commas are part of regex syntax vs. delimiters. While the concept is straightforward, the implementation requires careful handling of edge cases and thorough testing with various regex patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, reproducible, and the solution space is well-constrained. The provided patches show a clean, targeted fix that addresses the core issue without over-engineering. The test cases cover the key scenarios including the specific bug case mentioned in the issue. This is a good benchmark problem as it requires understanding both the existing codebase architecture and implementing a non-trivial parsing algorithm.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10051": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: `caplog.get_records()` becomes decoupled from actual caplog records when `caplog.clear()` is called, resulting in frozen records that don't get cleared or updated. The description provides specific technical details including links to the exact code lines causing the issue (lines 699 and 345 in logging.py), explains the root cause (records list gets replaced rather than cleared), and includes a detailed reproductive example with expected vs actual behavior. The environment details are also provided. An experienced engineer would have all the information needed to understand and fix this bug.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the root cause: in caplog.clear(), the handler.reset() method replaces the records list instead of clearing it, which breaks the reference used by get_records(). The solution requires adding a new clear() method to the handler that clears the existing list rather than replacing it, and updating caplog.clear() to use this new method. This involves minimal code changes (adding one method and changing one method call) but requires understanding the object reference issue and implementing the correct fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined bug with a clear reproduction case and straightforward technical solution. The issue provides excellent context including code references and detailed behavior description, making it suitable for a coding benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10081": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when running pytest with the --pdb flag, the tearDown() method of unittest.TestCase classes decorated with @unittest.skip is being executed, causing errors when it shouldn't be. The issue provides a complete minimal reproduction case with the exact file content (test_repro_skip_class.py), shows the expected behavior (clean skip without tearDown execution), demonstrates the actual problematic behavior (tearDown executed with NameError), and provides complete environment details. The reporter also references a related issue (#7215) for function-level skips, making it clear this is specifically about class-level skips. The expected solution is implicit but clear: tearDown should not be executed for skipped test classes when using --pdb, just like it works for function-level skips.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is in the unittest.py file where the logic for postponing tearDown when using --pdb only checks if the test method is skipped, but doesn't check if the entire test class is skipped. The gold patch shows this is a small, targeted change: adding a check for class-level skipping by examining both _is_skipped(self.obj) and _is_skipped(self.parent.obj). The fix requires understanding the existing skip detection logic and extending it to consider parent class skips, but it's a straightforward logical extension of existing code rather than a complex architectural change. The test changes are also simple - duplicating an existing test pattern for class-level decorators.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described with a perfect minimal reproduction case, the expected vs actual behavior is unambiguous, and the solution requires understanding pytest's unittest integration and skip handling logic. An engineer would need to locate the relevant code in src/_pytest/unittest.py, understand how the existing skip detection works for methods, and extend it to also check parent class skip status. The fix is small but requires proper understanding of the codebase structure and unittest/pytest interaction patterns.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5262": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: `_pytest.capture.EncodedFile` incorrectly advertises a mode that includes 'b' (binary), which causes youtube-dl to write bytes to it, but the `write()` method only accepts strings. The issue provides a complete reproduction case with specific steps, error output, and explains the root cause - that youtube-dl checks for 'b' in `out.mode` to decide whether to write bytes or strings. The expected behavior is clear: the mode should not include 'b' since the EncodedFile expects string input. The provided patches confirm this understanding - the solution is to override the `mode` property to remove 'b' from the underlying buffer's mode.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-understood from the issue description and the solution is straightforward: add a `mode` property to the `EncodedFile` class that returns the underlying buffer's mode with 'b' removed. This requires understanding the interaction between pytest's capture mechanism and external libraries that check file modes, but once that's clear, the implementation is simple - just a few lines of code adding a property method. The test is also trivial to write.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described with a complete reproduction case, the problem has a well-defined root cause, and the solution is focused and testable. The interaction between pytest's capture system and external libraries provides good insight into how different components interact in real-world scenarios.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5631": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear problem description - test collection fails when using unittest.mock.patch with numpy arrays as the 'new' value, (2) A complete error traceback showing exactly where the failure occurs in the code (/usr/local/lib/python3.6/dist-packages/_pytest/compat.py:94), (3) The specific error message \"ValueError: The truth value of an array with more than one element is ambiguous\", (4) Context about when the regression was introduced (pytest 3.1.3 to 3.6.0), (5) The root cause analysis pointing to a specific commit and explaining that \"p.new in sentinels\" produces an array of booleans instead of a single boolean when p.new is an ndarray. This gives an engineer everything needed to understand and fix the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly identified in the error traceback at line 94 in compat.py where \"p.new in sentinels\" fails for numpy arrays. The solution requires changing the equality comparison from \"in\" to identity comparison using \"is\" to avoid triggering numpy's __eq__ method. Looking at the gold patch, it's a focused change to one function (num_mock_patch_args) that replaces the problematic list comprehension with explicit identity checks. An experienced engineer would quickly identify this as a classic numpy comparison issue and implement the fix relatively quickly once they understand the codebase structure.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is clear from the error description, and the test case appropriately validates the fix. This is a good benchmark sample that tests understanding of Python object comparison semantics and numpy behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5787": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly demonstrates the problem: when pytest is run with xdist (parallel execution), chained exceptions are not properly serialized and only the final exception in the chain is reported, whereas running pytest normally shows the complete nested traceback. The issue provides concrete examples with code showing the expected behavior (normal pytest output with full exception chains) versus the actual behavior (xdist output with only final exceptions). The problem is in exception serialization during distributed test execution, and the expected solution is that exception serialization should include chained exceptions so that the same detailed traceback information is preserved when using xdist.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty because: (1) The engineer needs to understand pytest's report serialization system and how xdist communicates test results between workers and the main process, (2) They need to understand Python's exception chaining mechanism (__cause__ and __context__ attributes), (3) The solution requires substantial changes to the serialization/deserialization logic in _pytest/reports.py, including adding support for ExceptionChainRepr objects which wasn't previously handled, (4) The fix involves modifying multiple functions (_to_json, _from_json) and adding new serialization logic for chained exceptions, and (5) Comprehensive testing is needed to ensure the fix works for both explicit chaining (with 'from') and implicit chaining (without 'from'). While the problem is well-defined, implementing the solution requires deep understanding of pytest internals and careful handling of complex object serialization.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution scope is reasonable, and the test cases provided in the PR verify that the fix works correctly for both types of exception chaining. This would be a good benchmark problem for evaluating understanding of serialization, exception handling, and working with complex codebases.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5809": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) the exact location of the problematic code with a GitHub link to lines 68-73 in src/_pytest/pastebin.py, (2) a clear description of the HTTP 400 error that occurs, (3) a reproducible test case with attached data.txt file, (4) the root cause (lexer=python3 causing issues with certain content), and (5) a specific proposed solution (change lexer to \"text\"). The rationale is also clear - pytest output is arbitrary text, not Python code, so \"text\" lexer is more appropriate. An engineer would have all the information needed to implement this fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward 15 minute to 1 hour fix. The issue clearly identifies the exact problem location and the solution involves changing a single parameter value from \"python3\" to \"text\" in the create_new_paste function. Looking at the gold patch, it's literally a few lines of code modification removing the conditional logic for lexer selection and hardcoding it to \"text\". An experienced engineer would need minimal time to understand the codebase context, locate the relevant function, and make this simple change. The test update is also trivial - just updating the expected lexer value.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal benchmark sample - the problem is clearly defined, the solution is straightforward but requires some understanding of the context, and the fix can be verified through testing. The issue provides good technical detail without being overly complex.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5840": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides some key information but lacks completeness. It clearly states that upgrading from pytest 5.1.1 to 5.1.2 caused an ImportError due to \"path was converted to lower case\" and shows the specific error message. However, it doesn't explain the root cause or provide enough context about the case sensitivity issue on Windows file systems. The error message and version information give a starting point, but an engineer would need to investigate further to understand that this is related to case-insensitive file system handling and conftest.py loading. The issue mentions \"windows import folder casing issues\" in the title which provides additional context, but the description itself is somewhat brief.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding case-insensitive file system behavior on Windows, investigating how pytest handles conftest.py loading, and implementing a proper solution using Path.resolve() instead of the custom unique_path function. The solution involves changes across multiple files (config/__init__.py and pathlib.py), removing the unique_path function entirely, and updating the conftest loading mechanism to use pathlib.Path.resolve() for proper case-insensitive path resolution. While not extremely complex, it requires understanding of file system nuances and careful refactoring to avoid breaking existing functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking as it represents a real-world problem with file system case sensitivity that requires both problem-solving skills and knowledge of Python path handling. The solution involves both removing problematic code and implementing a better approach using standard library functions.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6197": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a regression in pytest 5.2.3 where the tool now tries to collect/import random __init__.py files under the current directory that it shouldn't. The problem is demonstrated with a concrete reproduction case: creating a directory 'foobar' with an __init__.py that contains 'assert False', and showing that pytest 5.2.2 works correctly while 5.2.3 fails by trying to import this file. The issue includes the exact error output, showing the collection failure at foobar/__init__.py. This provides clear requirements for a solution: prevent pytest from inappropriately collecting __init__.py files that are not intended to be test modules.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the provided gold patch, this is a 1-4 hour fix. The solution involves understanding pytest's collection mechanism and making targeted changes to multiple files (src/_pytest/python.py primarily, plus test files). The fix removes the __init__.py special handling in the Module class constructor, modifies the obj property to avoid premature object mounting, and removes a call to _mount_obj_if_needed in the collect method. While the actual code changes are not massive (removing ~20 lines, modifying a few others), understanding the collection logic, identifying the root cause of the regression, and ensuring the fix doesn't break other functionality would require significant investigation into pytest's internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is clearly described, the reproduction case is straightforward, and the expected behavior (that pytest should not collect arbitrary __init__.py files) is well-understood. The issue would make for a good benchmark sample as it tests understanding of collection mechanisms in testing frameworks and requires careful analysis of when and how modules should be imported.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6202": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear reproduction case with exact steps (create bug.py with specific content, run pytest), shows the expected vs actual output, and even includes detailed source code analysis tracing the problem to a specific line of code (`return s.replace(\".[\", \"[\")` in src/_pytest/python.py). The issue demonstrates that when a parameterized test has a parameter containing \"..[\", the test identifier in failure reports incorrectly shows \"[.[]\" instead of \"[..[\". The reporter has done excellent detective work, pinpointing the exact problematic line and suggesting the fix should be `return s` instead of the current replacement logic.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue description provides exceptional detail, including the exact line of code causing the problem and the suggested fix. The actual code change is trivial (removing one line that does string replacement), but an engineer would need some time to: (1) understand the pytest codebase structure and locate the relevant file, (2) verify the issue by reproducing it, (3) understand why the problematic line exists and ensure removing it doesn't break other functionality, and (4) run tests to confirm the fix works. The detective work is already done by the issue reporter, making this a straightforward fix once the context is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent example for a coding benchmark - the problem is clearly defined, the solution is straightforward but requires understanding the codebase, and the test patch validates the fix appropriately. The issue demonstrates good software engineering practices with detailed analysis and clear reproduction steps.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7205": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides clear reproduction steps (running Python 3.8.2 with pytest 5.4.1, using `python3 -bb -m pytest --setup-show` with a parametrized fixture containing bytes), shows the exact error and stack trace with the specific line causing the problem (`tw.write(\"[{}]\".format(fixturedef.cached_param))` in `src/_pytest/setuponly.py:69`), explains the expected vs actual behavior, and even suggests a potential solution (\"Shouldn't that be using `saferepr` or something rather than (implicitly) `str()`?\"). The problem is clear: when displaying fixture setup information, pytest is calling `str()` on bytes objects which triggers a BytesWarning when bytes warnings are enabled.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic line of code (line 69 in setuponly.py) and suggests the solution approach (using saferepr). The actual fix is straightforward: import saferepr and replace the direct string formatting with saferepr formatting. Looking at the gold patch confirms this - it's a simple 2-line change: adding an import and replacing `str()` formatting with `saferepr()`. The main time would be spent understanding how saferepr works and determining the appropriate maxsize parameter, but this is a well-established pattern in pytest codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, straightforward implementation, and good test coverage. The issue provides excellent reproduction steps and the solution follows established patterns in the pytest codebase.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7236": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug report with: (1) A specific scenario - unittest.TestCase with @unittest.skip decorator, (2) Expected behavior - skipped tests should not invoke tearDown, (3) Actual behavior - tearDown is called when using --pdb flag, (4) A complete minimal reproduction case showing the NameError when tearDown tries to access undefined 'xxx', (5) Version information showing this is a regression between pytest 5.4.1 and 5.4.2, and (6) Clear output demonstrating the difference between normal pytest run (works correctly) vs pytest --pdb (breaks). The problem statement is unambiguous: tearDown should not be executed for skipped tests, even when --pdb is used.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours because: (1) Understanding the codebase - need to familiarize with pytest's unittest integration, fixture handling, and PDB integration, (2) Root cause analysis - need to trace through the code to understand why --pdb flag changes the behavior for skipped tests, (3) The solution involves multiple files and requires understanding of pytest's internal architecture, specifically how unittest fixtures are handled and when tearDown is called, (4) Looking at the patch, it involves creating a new helper function _is_skipped() and modifying logic in multiple places, indicating this isn't a trivial one-line fix, (5) The issue involves interaction between different pytest subsystems (unittest support, PDB support, fixture management) which requires broader understanding of the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified with a complete reproduction case, the problem is well-defined (tearDown shouldn't run for skipped tests with --pdb), and it represents a realistic debugging scenario that developers encounter. The solution requires understanding pytest internals but doesn't involve overly esoteric knowledge. The test case provided in the patch also gives clear validation criteria.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7324": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the problem: Expression.compile with \"False\" crashes Python 3.8+ debug builds with a specific assertion failure in compiler_nameop. The error message and assertion are very specific, pointing to the exact problem - the compiler cannot handle \"False\", \"True\", and \"None\" as identifiers. However, the issue doesn't explicitly state what Expression.compile is or where it's located in the codebase, though from context it appears to be part of pytest's mark expression parsing. An experienced engineer could reasonably infer that the solution involves making these reserved Python keywords usable as identifiers in pytest's expression language, likely by prefixing them or transforming them in some way. The gold patch confirms this interpretation - it adds a prefix to identifiers to avoid conflicts with Python reserved words.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-localized to the expression parsing logic in pytest's mark system. The assertion failure clearly points to the issue: certain identifiers (\"False\", \"True\", \"None\") cannot be used because they conflict with Python reserved words. The solution is conceptually straightforward - add a prefix to identifiers during AST generation and remove it during evaluation. The actual implementation involves modifying two functions (adding prefix in not_expr() and removing it in MatcherAdapter.__getitem__()) and adding a constant. While it requires understanding the expression parsing flow, the changes are small and focused. An experienced engineer familiar with AST manipulation could implement this relatively quickly once they understand the codebase structure.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is technical but clear, has a focused scope, and tests both problem diagnosis skills and knowledge of Python AST manipulation. The solution requires understanding the interaction between pytest's expression language and Python's compiler constraints, which is a reasonable test of coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7490": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides a clear problem description, specific reproduction steps with exact code examples, version information, and demonstrates the behavioral difference between pytest 5.x and 6.0 with actual command outputs. An engineer would have everything needed to understand that dynamically added xfail markers using `request.node.add_marker(pytest.mark.xfail())` should cause failing tests to be treated as XFAIL rather than regular failures, but this behavior was broken in pytest 6.0. The issue includes a minimal test case in `test_foo.py` that can be used for reproduction and verification.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding pytest's internal marker and test execution lifecycle. The engineer needs to: 1) Understand how pytest processes xfail markers during different test phases, 2) Identify why dynamically added markers aren't being recognized in version 6.0, 3) Determine the correct place in the test execution flow to re-evaluate xfail marks (after test execution but before reporting), and 4) Modify the skipping.py logic to handle dynamic markers. The fix involves changes to multiple functions in pytest_runtest_setup, pytest_runtest_call, and understanding the interaction between marker evaluation and test result reporting. While not requiring >100 lines of changes, it demands solid understanding of pytest internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. This is a well-isolated behavioral regression with clear expected behavior, good reproduction steps, and the solution involves core pytest functionality that would be valuable to test in a coding benchmark. The issue demonstrates understanding of test frameworks, marker systems, and debugging behavioral changes between versions.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7521": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a regression in pytest 6.0.0rc1 where capfd.readouterr() is converting carriage returns (\\r) to newlines (\\n). The problem is demonstrated with concrete examples: (1) a real-world test case from borgbackup showing a progress indicator that writes \"  0%\\r\" but capfd captures \"  0%\\n\", and (2) a minimal reproducible test case that prints \"Greetings from DOS\" with end='\\r' but capfd.readouterr() returns the string ending with '\\n'. The issue includes complete test output showing the exact assertion failures, environment details, and package versions. The expected behavior (preserve original line endings) versus actual behavior (convert \\r to \\n) is crystal clear.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The gold patch shows it's a single-line change adding newline=\"\" parameter to TextIOWrapper in src/_pytest/capture.py. The issue is in the capture mechanism where Python's TextIOWrapper by default performs newline translation. An experienced engineer would: (1) recognize this as a newline handling issue in capture logic, (2) locate the capture.py file, (3) find the TextIOWrapper initialization, and (4) add the newline=\"\" parameter to preserve original line endings. The fix requires understanding Python's text I/O newline handling but doesn't involve complex logic or multiple files.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the problem is clearly described with reproducible test cases, the solution is well-defined (preserve original line endings in captured output), and it tests knowledge of Python's text I/O behavior and pytest's capture mechanisms. The issue includes complete context and demonstrates both the problem and expected behavior clearly.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10297": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes that RidgeClassifierCV is missing the `store_cv_values` parameter that should be available based on the documentation, which mentions a `cv_values_` attribute that depends on this parameter. The issue provides specific steps to reproduce the problem, shows the exact error message (TypeError about unexpected keyword argument), and even includes the contradictory documentation that claims the parameter should exist. The expected behavior is clear: the parameter should be accepted and should enable storing cross-validation values in the cv_values_ attribute, just like in the related RidgeCV class.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. Looking at the provided patch, the solution involves: (1) Adding the store_cv_values parameter to the RidgeClassifierCV.__init__ method signature with default False, (2) Passing this parameter to the parent class constructor, and (3) Updating the docstring to document the parameter properly. The issue is straightforward because RidgeClassifierCV inherits from _BaseRidgeCV which already supports store_cv_values - it just wasn't exposed in the child class interface. An experienced engineer familiar with the codebase could identify this pattern quickly by looking at the related RidgeCV class and understanding the inheritance hierarchy.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean, well-defined problem with a straightforward solution. The issue provides clear reproduction steps, expected vs actual behavior, and the inconsistency between documentation and implementation is obvious. The solution follows standard object-oriented patterns and doesn't require deep domain knowledge.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10908": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. The author provides a concrete scenario where CountVectorizer with a vocabulary parameter should allow get_feature_names() to work without prior fitting, just like transform() does. The issue includes specific examples showing the inconsistent behavior: transform() works with a provided vocabulary without fitting, but get_feature_names() raises NotFittedError. The expected behavior is clearly stated - get_feature_names() should not raise NotFittedError when vocabulary is provided. The gold patch confirms this interpretation by adding validation logic before the existing check.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is straightforward: adding a check in get_feature_names() to validate vocabulary before checking if the vectorizer is fitted. The solution requires understanding the existing code structure and adding a few lines to call _validate_vocabulary() when vocabulary_ attribute doesn't exist. An experienced engineer would need time to locate the relevant files (sklearn/feature_extraction/text.py), understand the existing validation patterns, and implement the fix. The patch shows it's a minimal change - just 3 lines added to the get_feature_names() method.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-defined, the solution is straightforward, and the test changes demonstrate the expected behavior clearly. This is a good benchmark sample for evaluating coding ability as it requires understanding existing patterns in the codebase and applying consistent behavior across methods.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-12585": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the `clone` function in sklearn fails when parameters are estimator types (classes) rather than instances. The issue provides a specific reproduction case, shows the exact error traceback, explains the expected behavior, and even suggests a specific fix with the exact line number and code change needed. The user explains their use case (storing wrapped estimators as classes) and provides enough technical detail including the specific error location in `sklearn.base.py` line 62. An experienced engineer would have all the information needed to understand and solve this problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue provides the exact location of the problem (line 51 in base.py) and suggests the specific fix needed: adding `or isinstance(estimator, type)` to the conditional check. The solution is a simple one-line change to handle the case where an estimator parameter is a type/class rather than an instance. An experienced engineer would need minimal time to understand the problem, verify the suggested fix makes sense, and implement it. The corresponding test is also straightforward - just verify that clone works with type parameters.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear reproduction steps, expected behavior, and a reasonable solution. The issue demonstrates good software engineering practices by providing a minimal reproduction case and suggesting a specific fix. The test case is also straightforward and directly validates the fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13135": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: KBinsDiscretizer with strategy='kmeans' fails because the computed bin edges are not sorted, which causes np.digitize to raise a ValueError. The issue provides a specific reproduction case with exact input data ([0, 0.5, 2, 3, 9, 10]), clear parameters (n_bins=5, strategy='kmeans', encode='ordinal'), and shows the complete error traceback pointing to line 255 in _discretization.py where np.digitize fails due to unsorted bins. The expected result (no error) is clear, and the actual error message explicitly states \"bins must be monotonically increasing or decreasing\". An experienced engineer would understand they need to ensure bin_edges are sorted before being passed to np.digitize.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified in the error traceback and the solution is straightforward: ensure the centers/bin_edges are sorted before use. Looking at the gold patch, the fix is literally just adding `centers.sort()` after computing the k-means centers. An experienced engineer would quickly identify that the k-means algorithm can return unsorted cluster centers, and the fix requires just one line of code. The main time would be spent understanding the KBinsDiscretizer workflow and locating the exact spot in the code where sorting is needed, which shouldn't take more than an hour given the clear error traceback.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the problem is clearly defined, easily reproducible, and the fix is surgical and well-contained. The test patch appropriately adds a test case for the specific scenario that was failing (5 bins), ensuring the fix works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13142": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when n_init>1 is specified in GaussianMixture, fit_predict(X) and predict(X) return different results, but they should be consistent. The issue provides concrete steps to reproduce the problem with specific parameters (n_components=5, n_init=5, dataset shape (1000,5)), shows the expected vs actual behavior with error output demonstrating 88.6% mismatch, and explains that the existing unit test doesn't catch this because it doesn't set n_init. The problem statement is unambiguous - the two methods should return identical label arrays but currently don't when n_init>1.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it's a simple code reorganization in the fit_predict method in sklearn/mixture/base.py. The fix involves moving the final e-step (which ensures consistency between fit_predict and predict) to occur after setting the best parameters, rather than before. This is a small logical change that requires understanding the flow but doesn't involve substantial code rewriting. The solution shows moving just 5 lines of code (the e-step call) to a different position in the same function. An experienced engineer familiar with the codebase could identify this timing issue and implement the fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with clear reproduction steps and expected behavior. The issue has good test coverage as shown in the test patches, and the solution is straightforward to verify. The problem is specific to the GaussianMixture implementation and doesn't involve complex algorithmic changes or dependencies on external systems.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13328": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: HuberRegressor fails with a TypeError when given boolean input data, while it should work by internally converting boolean arrays to float (like LinearRegressor does). The issue provides a complete reproduction scenario with make_regression, shows the exact error traceback pointing to line 93 in huber.py where the numpy boolean negative operator issue occurs, and states the expected behavior clearly. The error message itself is very specific: \"The numpy boolean negative, the `-` operator, is not supported, use the `~` operator or the logical_not function instead.\" This gives a clear indication of what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The problem is clearly identified in the error traceback - the issue is that boolean arrays are not being converted to float before being passed to the optimization routine. Looking at the gold patch, the solution is simply adding dtype=[np.float64, np.float32] to the check_X_y call in the fit method. This ensures boolean inputs get converted to float automatically. An experienced engineer would: 1) See the error points to boolean negative operator issue, 2) Recognize this is a data type conversion problem, 3) Look at how other sklearn regressors handle this (like LinearRegression), 4) Add the dtype parameter to check_X_y. The fix is minimal (one line change) and follows standard sklearn patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with clear reproduction steps, clear expected behavior, and a straightforward fix. The test patch appropriately adds a test case to verify boolean input works correctly. The issue is realistic and represents the type of data type handling bugs that commonly occur in ML libraries.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13779": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly states the problem (VotingEstimator fails when weights are passed and an estimator is None), provides the root cause (no check for None estimators in sample_weight support), gives detailed reproduction steps using specific components (Iris dataset, VotingClassifier with logistic regression and random forest), describes the expected behavior (skip missing estimator and complete successfully), and includes the exact error message (AttributeError: 'NoneType' object has no attribute 'fit'). An experienced engineer would have all the information needed to understand and solve this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is very simple: adding a 2-line None check (if step is None: continue) before processing estimators in the sample_weight validation loop. An experienced engineer would need some time to familiarize themselves with the VotingEstimator code structure and locate the relevant fit method, but once they understand where the sample_weight validation occurs, the fix is straightforward. The issue is well-isolated to a specific code path and doesn't require complex logic or extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean, well-defined bug with a clear reproduction case and straightforward solution. The issue description provides sufficient context, the problem is isolated, and the fix doesn't require extensive domain knowledge beyond understanding the VotingEstimator architecture. The test case provided also properly validates the fix by checking that weights can be passed when an estimator is set to None.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14053": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It states that `export_text` returns an `IndexError` when there is a single feature, provides clear steps to reproduce (loading Iris dataset, extracting first column, fitting DecisionTreeClassifier, calling export_text with feature_names), and shows the actual error result. The description gives enough context to understand that this is a bug in the `export_text` function in scikit-learn's decision tree module when handling trees with only one feature. An experienced engineer would be able to identify that this is likely an indexing issue when the function tries to access feature names for nodes that don't have features (leaf nodes).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is quite straightforward: it's a simple conditional check to handle the case where tree_.feature[i] is TREE_UNDEFINED (indicating a leaf node). The fix involves modifying just 2 lines in the export_text function in sklearn/tree/export.py to add a conditional that checks if the feature index is TREE_UNDEFINED before trying to access it in the feature_names list. An experienced engineer familiar with decision tree structures would quickly recognize this as a common edge case where leaf nodes don't have associated features, and the fix requires minimal code change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear reproduction steps, a specific error message, and a straightforward solution. The test patch also properly validates the fix by testing the edge case with single-feature trees.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14087": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides clear steps to reproduce the problem: seed NumPy with 29, create a 1000\u00d73 feature matrix from standard normal distribution, create random coefficients and intercept, define binary target as sign of intercept + X @ coefficients, then initialize LogisticRegressionCV with specific parameters (cv=5, solver='saga', tol=1e-2, refit=False) and call fit(X, y). The expected result is clear (should fit without errors), the actual result shows a specific IndexError with full traceback pointing to line 2194 in logistic.py, and version information is provided. An experienced engineer has all the information needed to reproduce and understand the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a 15 min - 1 hour fix. The solution involves 3 small changes: (1) changing self.multi_class to multi_class on line 2173, (2) adding a conditional check for elasticnet penalty before accessing l1_ratios_, and (3) appending None when penalty is not elasticnet. These are straightforward fixes once you understand the IndexError is caused by incorrect variable references and missing conditional logic. The error traceback clearly points to the problematic line, making debugging relatively quick for an experienced engineer.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, reproducible with clear steps, has a focused solution, and the test coverage shows comprehensive validation of the fix across different penalty types and multi_class settings. This is a good benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14629": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem: AttributeError when using cross_val_predict(method='predict_proba') with MultiOutputClassifier. The issue description pinpoints the root cause - the code tries to access estimator.classes_ on a MultiOutputClassifier, but this attribute doesn't exist. The author correctly identifies that MultiOutputClassifier stores classes in estimator.estimators_[i].classes_ instead. The issue includes clear reproduction steps (synthetic multilabel classification with LinearDiscriminantAnalysis wrapped in MultiOutputClassifier), expected vs actual results, and even points to the specific line in the codebase causing the problem. An experienced engineer would have enough information to understand both the problem and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution is straightforward: add a classes_ attribute to MultiOutputClassifier that aggregates the classes from individual estimators. Looking at the gold patch, the fix involves overriding the fit method to set self.classes_ = [estimator.classes_ for estimator in self.estimators_]. This requires understanding the MultiOutputClassifier structure and how it relates to cross_val_predict's expectations, but the code change itself is minimal and the logic is clear from the issue description.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described with specific technical details, the problem is well-scoped, and the solution requires understanding both the MultiOutputClassifier implementation and the cross-validation prediction workflow. The test patch also shows that a comprehensive test was added to verify the fix works for multiple estimator types.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14710": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: HistGradientBoostingClassifier fails when using string targets with early stopping because the scorer receives integer-encoded y_true but original string y_pred values, causing a type mismatch. The issue includes a complete reproduction case with specific steps (create 100 samples, 10 features, string targets 'x' and 'y', enable early stopping with n_iter_no_change=10), shows the exact error traceback pointing to the specific line in gradient_boosting.py where the issue occurs, and even provides a potential resolution with a diff showing exactly what needs to be changed. The root cause is clearly identified: y_true needs to be decoded back to original string classes before being passed to the scorer.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-understood with a clear root cause and solution path. The fix involves adding a few lines of code to decode integer-encoded targets back to their original string representation before scoring. The solution requires checking if the estimator is a classifier and then using self.classes_[y.astype(int)] to decode the targets. While it requires understanding the relationship between encoded and original targets in scikit-learn classifiers, the actual implementation is straightforward and involves minimal code changes in a single method (_check_early_stopping_scorer).",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, good test coverage in the provided test patch, and a straightforward solution. The issue demonstrates good software engineering practices with detailed reproduction steps and error analysis.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14894": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified. It clearly identifies the problem (ZeroDivisionError in _sparse_fit when support_vectors_ is empty), provides the exact error location with stack trace, and states the expected behavior. However, the reproduction steps are described conceptually rather than providing actual executable code, requiring the engineer to fill in some details about creating the specific test case that triggers empty support vectors.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The error location is clearly identified in the stack trace (line 302 in _sparse_fit), and the problem is straightforward: division by zero when n_SV is 0. The solution requires adding a simple conditional check to handle the edge case of empty support vectors. An experienced engineer would quickly identify that dual_coef_indices.size / n_class causes the error when n_class is involved in creating an empty scenario, and the fix is a basic if-else structure to create an empty sparse matrix instead.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained to a specific function, has a clear reproduction scenario, and the expected behavior is reasonable. The fix is localized and doesn't require extensive changes to the codebase architecture.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-25747": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: FeatureUnion fails when using pandas transform output with transformers that aggregate data, while it works fine with default numpy output. The issue includes detailed steps to reproduce the problem with a specific scenario (custom transformer that groups by date and sums values in a FeatureUnion), shows the exact error message with full stack trace pointing to the root cause in `_wrap_in_pandas_container` function, and clearly states the expected behavior (no error should be thrown). The error message reveals the core issue: a length mismatch when trying to set the DataFrame index (4 elements vs 96 elements), which occurs because the aggregating transformer changes the number of rows but the wrapping logic tries to preserve the original input's index. The versions section provides complete environment details. An experienced engineer would have sufficient information to understand that the problem is in the pandas output wrapping logic in sklearn/utils/_set_output.py.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The error stack trace clearly points to the problem in `_wrap_in_pandas_container` function where it tries to set `data_to_wrap.index = index` on line 59. The issue occurs when the transformer changes the number of rows (aggregation reduces 96 hourly records to 4 daily records), but the wrapping logic attempts to use the original input's index. Looking at the provided patch, the solution is simple: just remove the 2-line block that sets the index when `data_to_wrap` is already a DataFrame. The logic is straightforward - if the transformer already returns a DataFrame with its own index, don't override it with the original input's index. This requires understanding the code flow and the specific edge case with aggregating transformers, but the actual code change is minimal and the problem location is clearly identified by the stack trace.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the error is reproducible, the stack trace points to the exact location, and the solution requires understanding sklearn's output wrapping mechanism. This is a good example of a well-specified bug report that would be suitable for evaluating coding ability, as it requires understanding the interaction between pandas DataFrames, sklearn's transform output system, and the specific case where aggregating transformers change the shape of data.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25931": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug description: IsolationForest incorrectly warns about invalid feature names when fitted with a DataFrame and contamination != \"auto\". The issue includes: (1) Clear reproduction steps with specific code, (2) Expected vs actual behavior, (3) Root cause analysis pointing to line 337 in _iforest.py where predict is called on training data, (4) Complete version information, and (5) Technical explanation that the warning occurs because the estimator calls predict internally to determine offset_ parameters. An experienced engineer would have all necessary information to understand and fix this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-understood: the problem occurs because score_samples() performs input validation that strips feature names, but when contamination != \"auto\", the fit() method calls score_samples() internally. The solution involves creating a private _score_samples() method that bypasses input validation to preserve feature names. While it requires understanding the codebase structure and the validation pipeline, the actual code changes are minimal - extracting existing logic into a private method and updating one method call. The fix is straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - the bug is clearly described, reproducible, and the solution requires understanding both the immediate symptom and the underlying cause. The fix demonstrates knowledge of scikit-learn's validation patterns and internal method organization. The test case is also well-designed to prevent regression.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25973": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The expected behavior - that SequentialFeatureSelector should accept an iterable of splits as documented, (2) The actual behavior - it fails with an IndexError when passed splits from LeaveOneGroupOut, (3) Concrete steps to reproduce the issue including the specific cross-validator (LeaveOneGroupOut) and parameters used, (4) The exact error traceback showing the failure occurs in _aggregate_score_dicts with an IndexError: list index out of range. The user provides sufficient context about having done similar things with other scikit-learn classes, and the error message gives a clear indication of where the problem occurs in the validation code.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves: (1) Importing check_cv and is_classifier from sklearn modules, (2) Adding a single line to validate and normalize the cv parameter using check_cv(), (3) Passing the validated cv parameter to the helper method instead of using self.cv directly. The core issue is that SequentialFeatureSelector was not properly validating/normalizing the cv parameter like other scikit-learn classes do. The fix is straightforward once you understand that check_cv() is the standard way to handle cv parameters in scikit-learn - it converts generators/iterables to the proper format expected by cross_val_score. An experienced engineer familiar with scikit-learn patterns would likely recognize this quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is clean and follows established patterns in the codebase, and the test case properly verifies the fix. This is a good example of a well-specified bug report with a focused solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-10323": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified but has some gaps that require interpretation. The core problem is clear: when using Sphinx's literalinclude directive with prepend/append options, the indentation is not preserved correctly, causing misaligned code blocks. The user provides a specific example with pom.xml and Maven plugin configuration, explaining that the opening <plugin> tag loses its indentation while nested elements remain indented, creating inconsistent formatting. They also mention a workaround using dedent that produces warnings. However, some details are missing - the exact structure of the pom.xml file, the precise expected vs actual output formatting, and the specific Sphinx configuration. Despite these gaps, an experienced engineer familiar with Sphinx directives could reasonably understand that the issue involves the order of operations in text processing filters and work toward a solution that preserves original indentation when prepend/append modifiers are used.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution involves reordering the filters in the LiteralIncludeReader.read() method in sphinx/directives/code.py. The fix moves dedent_filter to execute before prepend_filter and append_filter rather than after them. This is a small, focused change that requires understanding the filter pipeline and recognizing that dedent should happen before prepend/append to preserve proper indentation. An experienced engineer would need to: 1) locate the literalinclude implementation, 2) understand the filter chain, 3) recognize that the order matters for indentation preservation, and 4) reorder the filters appropriately. The change is only 3 lines moved in the filter list, but requires some thought about text processing order and indentation handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. The problem is in core Sphinx functionality (literalinclude directive), the reproduction case is clear enough, and the expected behavior is reasonable. The provided test patch demonstrates that the fix can be properly validated with automated tests. While the issue description could be more precise about exact formatting expectations, it provides sufficient context for an engineer to understand and solve the indentation preservation problem.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-10673": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: users want to include automatically generated pages (genindex, modindex, search) in their toctree directive, but Sphinx currently generates warnings about \"nonexisting documents\". The issue provides specific examples of the warnings, shows the exact directive syntax that should work, and explains the expected behavior. The problem statement is concrete - eliminate the warnings when referencing these special generated documents in toctree directives. There's no ambiguity about what constitutes a successful solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Sphinx's internal architecture, specifically how toctree directives work and how generated documents are handled. The solution involves modifying multiple files (sphinx/directives/other.py, sphinx/environment/adapters/toctree.py, sphinx/environment/collectors/toctree.py) and understanding the relationship between found_docs and generated_docnames. An engineer would need to: 1) Understand how Sphinx tracks documents vs generated pages, 2) Identify where the validation occurs in toctree parsing, 3) Modify the logic to treat generated documents as valid references, and 4) Ensure the generated pages are properly resolved in the toctree. This requires architectural knowledge of Sphinx's document processing pipeline.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution approach is clear from the patch (treating generated documents as valid toctree entries), and the test coverage demonstrates the expected behavior. This is a good benchmark issue as it tests understanding of a complex codebase architecture while having a clear success criteria.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-7440": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some minor blanks to fill in. The error message clearly shows a \"duplicate term description\" warning for MySQL/mysql terms. The expected behavior is explicitly stated: \"MySQL\" and \"mysql\" should be recognized as distinct glossary entries. The reproduction steps are clear. However, an engineer would need to investigate the Sphinx codebase to understand how glossary terms are processed and where case-sensitivity logic needs to be modified. The specific files and functions aren't mentioned, requiring some exploration of the codebase.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is conceptually simple: glossary terms are being converted to lowercase, causing case-different terms to be treated as duplicates. An experienced engineer would need to: 1) Locate the glossary processing code in Sphinx (likely in std domain), 2) Find where terms are being lowercased, 3) Remove the lowercasing logic. The actual code changes are minimal (2 small modifications in one file), but require some time to navigate the Sphinx codebase and understand the glossary processing flow.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clear, reproducible, and has a straightforward solution. The test case appropriately validates that case-sensitive glossary terms work correctly. This is a good benchmark sample as it tests understanding of domain-specific code (Sphinx documentation generation) and debugging skills to locate where case conversion happens.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7462": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear what is required for a successful solution. The bug report provides: (1) A clear description of the problem - an IndexError occurs when parsing empty tuple type annotations like `Tuple[()]`, (2) A specific error message with stack trace pointing to `sphinx/domains/python.py`, line 112, (3) A minimal reproducible example showing the exact code that triggers the error, (4) Expected behavior - the documentation should build without errors, (5) Environment details including versions. The core issue is that the `unparse` function in the Python domain calls `result.pop()` on an empty list when handling empty tuples, which is exactly what the error message indicates. This gives a developer everything needed to understand and fix the problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly localized to the `unparse` function in `sphinx/domains/python.py` where `result.pop()` is called on an empty list when processing empty tuples. The fix requires checking if the tuple has elements before attempting to pop, and handling the empty tuple case separately. Looking at the gold patch confirms this - it's a straightforward conditional check with about 10 lines of code changes across 2 files. An experienced developer would quickly identify the root cause from the stack trace and implement the fix without extensive research or major refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear reproduction case and straightforward fix. The issue description provides sufficient context and the expected behavior is obvious. The test cases in the patch confirm the fix works correctly for both empty tuples and regular tuples.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-7985": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clearly stated: linkcheck currently doesn't check local (internal) links, but it should. The user provides a concrete reproduction case showing that external broken links are caught but internal broken links (like \"doesntexist\" target) are not flagged. The expected behavior is also clear - both external and internal broken links should be reported. However, there are some implementation details left unspecified, such as exactly how internal links should be validated (file existence vs target existence), what constitutes a \"local link\", and how to handle edge cases. An experienced engineer would need to make reasonable decisions about these details based on the Sphinx documentation system context.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because: 1) It requires understanding the existing linkcheck.py codebase and how URI parsing/checking works, 2) The solution involves modifying the core link checking logic to distinguish between different types of local links (file paths vs other schemes), 3) It requires implementing file existence checking and integrating with existing ignore patterns, 4) The patch shows substantial logic changes including regex pattern matching, file system operations, and control flow modifications, 5) Proper testing requires understanding Sphinx's test framework and adding appropriate test cases. While not extremely complex, it's more than a simple fix and requires thoughtful design decisions about how to handle various edge cases in link validation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-scoped within the linkcheck functionality, has a clear reproduction case, and the expected behavior is reasonable. The provided patches show this is a legitimate enhancement that improves the tool's functionality without breaking existing behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-8269": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when linkcheck_anchors is True, Sphinx's linkcheck command incorrectly reports \"Anchor not found\" for URLs that return HTTP error codes (like 404, 500), instead of reporting the actual HTTP error. The issue provides a concrete reproduction case with a specific URL (https://google.com/test.txt#test), shows the actual vs expected output, and includes environment details. The expected behavior is clearly defined - it should report the HTTP status error (e.g., \"404 Client Error: Not Found\") rather than the misleading \"Anchor not found\" message. The bug report is complete with reproduction steps, actual behavior, expected behavior, and environment information.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. Looking at the gold patch, the solution is simply adding `response.raise_for_status()` after the HTTP request in sphinx/builders/linkcheck.py. This is a one-line change that makes the code properly handle HTTP errors before attempting to check for anchors. An experienced engineer familiar with the codebase would need to: 1) Locate the linkcheck.py file and understand the check_uri function, 2) Identify where HTTP requests are made when checking anchors, 3) Recognize that the code should check for HTTP errors before proceeding to anchor validation, 4) Add the raise_for_status() call. The concept is simple - HTTP errors should be caught and reported before attempting anchor checking. The implementation is also simple - just one line of code using a standard requests library method.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined bug with a clear, minimal solution. The issue description provides sufficient context, the reproduction case is straightforward, and the expected behavior is unambiguous. The fix is localized to a single function and doesn't require complex architectural changes or deep domain knowledge beyond basic understanding of HTTP requests and error handling.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-8551": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some domain knowledge to fully understand. The bug description clearly states the problem: `:type:` and `:rtype:` fields in Sphinx documentation are creating ambiguous cross-reference warnings when they shouldn't, and are resolving to incorrect targets. The \"To Reproduce\" section provides a concrete scenario with two classes both named \"A\" in different modules (mod.A and mod.submod.A), showing that unqualified references to \"A\" from within mod.submod should resolve to mod.submod.A but instead trigger warnings and resolve incorrectly to mod.A. The expected behavior is clearly stated: no warnings should occur and the types should resolve to mod.submod.A. However, understanding this issue requires familiarity with Sphinx's cross-referencing system, module contexts, and how implicit vs explicit xrefs work. An experienced engineer could reasonably interpret what needs to be fixed: the lookup mechanism for implicit xrefs created by info fields needs to respect the current module context like explicit xref roles do.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Sphinx's complex cross-referencing and domain system. The engineer needs to: 1) Understand how Sphinx's Python domain handles cross-references and module contexts, 2) Identify where info fields like :type: and :rtype: create implicit cross-references differently from explicit ones, 3) Figure out how to pass the current module context (py:module and py:class) to the cross-reference creation process. Looking at the patches, the solution involves modifying two files in the core Sphinx codebase: sphinx/domains/python.py and sphinx/util/docfields.py. The changes are relatively small (adding context information) but finding the right place to make these changes and understanding the interaction between different parts of the system requires significant domain expertise and investigation time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix in a specific domain (Sphinx documentation system) with clear reproduction steps, expected behavior, and a focused solution. The test patch shows comprehensive validation of the fix, testing that the cross-references contain the correct module and class context information. This would be a good benchmark sample for testing understanding of complex software systems and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-9711": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that the `needs_extensions` function in Sphinx performs string-based version comparisons instead of proper semantic version comparisons, causing issues when comparing versions like \"0.6\" vs \"0.10\" (where string comparison incorrectly considers \"0.6\" > \"0.10\"). The issue provides a clear reproduction case with the mplcursors repository and sphinx-gallery versions, shows the exact error message, and states the expected behavior that \"sphinx-gallery 0.10.0 should be accepted if 0.6 is the minimum specified.\" The problem is in the sphinx/extension.py file's verify_needs_extensions function, and the solution direction is clear: replace string comparison with proper version comparison.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The problem is well-understood (string vs semantic version comparison), the location is clear (verify_needs_extensions function in sphinx/extension.py), and the solution is straightforward: import a proper version comparison library (like packaging.version) and replace the string comparison logic. The code change is small and localized, requiring only modifications to one function to use Version() objects instead of string comparisons, with appropriate error handling for invalid version strings. An experienced engineer familiar with Python packaging and version comparison would implement this quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of semantic versioning concepts, proper use of packaging libraries, and error handling. The issue is realistic, well-documented, and has a clear technical solution that can be objectively verified through the provided test cases.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13372": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides:\n1. A clear reproduction case: creating Mul(Max(0, y), x, evaluate=False).evalf() fails while x*Max(0, y).evalf() works\n2. A complete error traceback showing the UnboundLocalError with 'reprec' referenced before assignment\n3. The specific file and line where the error occurs (sympy/core/evalf.py)\n4. A proposed solution: adding \"else: raise NotImplementedError\" clauses for the elif statements that define reprec and imprec\n5. Context about when this was discovered (after changing Mul args order in PR #13059)\n\nThe issue clearly identifies what's broken (UnboundLocalError), why it happens (reprec/imprec not defined in all code paths), and suggests a specific fix. An experienced engineer would have all the information needed to understand and fix this bug.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour to complete. The issue provides the exact error location, a clear reproduction case, and even suggests the correct solution. The fix involves adding two simple \"else: raise NotImplementedError\" clauses in the evalf.py file where reprec and imprec variables are conditionally assigned. The engineer would need to:\n1. Understand the error traceback (5-10 minutes)\n2. Locate the problematic code in evalf.py (5 minutes)\n3. Add the missing else clauses (2-3 minutes)\n4. Test the fix (10-15 minutes)\nThe code change is minimal (adding 4 lines total) and the logic is straightforward - ensuring variables are always defined or explicitly raising NotImplementedError.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement, obvious reproduction steps, and a straightforward solution. The test case also validates that the specific failing case now works correctly. This would be an excellent sample for evaluating coding ability as it tests understanding of variable scope, error handling, and debugging skills while being achievable in a reasonable timeframe.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13480": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, reproducible example: importing SymPy, defining x as a Symbol, creating the expression e = coth(log(tan(x))), and substituting x with 2 causes a NameError because 'cotm' is not defined. The error message shows the exact file and line (hyperbolic.py, line 590) where the problem occurs, and even lists specific values that trigger the error (2, 3, 5, 6, 8, 9, 11, 12, 13, 15, 18, etc.). This is a straightforward bug report with all necessary information to reproduce and understand the problem. Looking at the provided patch, it's clear this is simply a typo where 'cotm' should be 'cothm' - the variable was defined as 'cothm' but referenced incorrectly as 'cotm'.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix that should take less than 15 minutes. The issue is a simple typo in the variable name - 'cotm' should be 'cothm'. The error message directly points to the problematic line (line 590 in hyperbolic.py), and any engineer can quickly see that 'cothm' is defined a few lines above but 'cotm' is used in the conditional check. The patch confirms this - it's literally a one-character change from 'cotm' to 'cothm'. No research, algorithm design, or complex debugging is required - just identifying and fixing a typo.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for a coding benchmark. The issue is crystal clear, easily reproducible, and tests basic debugging skills and attention to detail. The fix is simple enough that the main challenge is correctly identifying the typo, but not so trivial that it's meaningless. The test cases provided also help verify that the fix works correctly for the specific mathematical expressions mentioned.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13877": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The user clearly demonstrates a reproducible bug: computing determinants of n\u00d7n matrices with entries i + a*j leads to a \"Invalid NaN comparison\" error for n=6, and returns unexpected results like 'nan' for n=5. The traceback provides a clear path through the code showing where the error occurs. However, there are some gaps: the user mentions that f(5) returns 'nan' but doesn't explain why this is problematic, and the suggestion about Bareiss algorithm being \"only valid for integer matrices\" is presented as a question rather than a definitive statement. An experienced engineer would need to investigate whether this is indeed the root cause and determine the appropriate fix, but the core problem (crash with TypeError) and reproduction steps are clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding the mathematical context (Bareiss algorithm for determinant computation), analyzing why NaN comparisons are occurring in symbolic computation, and identifying the appropriate pivot selection strategy. The solution involves modifying the _eval_det_bareiss method to use a different zero-checking function (_is_zero_after_expand_mul) and switching from a simple pivot finder to _find_reasonable_pivot. This requires domain knowledge of both the codebase structure and the underlying mathematics, plus testing to ensure the fix doesn't break other functionality. While not requiring >100 lines of changes, it's a non-trivial algorithmic modification that needs careful consideration.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a solid benchmark issue. The problem is mathematically well-defined with clear reproduction steps, the error is unambiguous (TypeError with specific traceback), and the solution requires meaningful algorithmic understanding rather than just syntactic fixes. The test cases verify the fix works for the specific matrix pattern mentioned in the issue.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-16792": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: autowrap with cython backend fails when array arguments don't appear in the wrapped expression. The issue includes a concrete example showing the problem (MatrixSymbol x with expression 1.0), the exact error message (\"TypeError: only size-1 arrays can be converted to Python scalars\"), and even shows the incorrect C function signature generated (\"double autofunc(double x)\" instead of \"double autofunc(double *x)\"). The reporter also demonstrates that when the expression actually uses x (e.g., x[0,0]), the function works correctly. This gives an engineer everything needed to understand what's wrong and what the correct behavior should be.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is clearly described, fixing it requires understanding the codegen system in SymPy, specifically how argument sequences are processed and how metadata for array dimensions is handled. The solution involves modifying the routine() method in codegen.py to properly handle cases where array arguments appear in the argument sequence but not in the expression itself. An engineer would need to: 1) Understand the existing codegen flow, 2) Identify why array symbols not appearing in expressions lose their dimension metadata, 3) Implement logic to preserve array type information for all arguments in the sequence regardless of expression usage. The fix touches core codegen logic and requires careful consideration of edge cases.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is domain-specific (code generation for mathematical expressions) but well-contained. The problem description is thorough with clear examples, expected vs actual behavior, and even hints at the root cause. The test case provided in the patch is straightforward and directly validates the fix. An engineer working on this would have a clear success criterion: ensure that array arguments maintain their proper C type signatures even when not referenced in the expression.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17139": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, reproducible test case: calling `simplify(cos(x)**I)` where `I` is the imaginary unit. The issue includes the complete traceback showing exactly where the error occurs (in `fu.py` line 504 with the comparison `if (rv.exp < 0) == True:`). The error message \"Invalid comparison of complex I\" clearly indicates the problem: the code is trying to compare a complex number (I) with 0, which is not allowed in SymPy. The expected behavior is also clear - the function should return a simplified expression rather than throwing a TypeError. The traceback provides the exact file path and line number where the issue occurs, making it straightforward to locate and understand the problematic code.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-localized to a specific line in fu.py where a comparison is being made without checking if the exponent is real. The traceback points directly to the problematic code at line 504: `if (rv.exp < 0) == True:`. The solution shown in the patch is simple and elegant: add a check `if not rv.exp.is_real: return rv` before attempting the comparison. This requires understanding that complex numbers cannot be compared with `<` operator, and that SymPy has built-in methods like `is_real` to check number properties. An experienced engineer familiar with SymPy would quickly identify this as a type checking issue and implement the appropriate guard clause. The fix is minimal (2 lines of code) and follows a common pattern in mathematical libraries of checking number properties before performing operations.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clear, the reproduction case is simple, the error is well-defined, and the solution requires understanding both Python's type system and SymPy's symbolic math concepts. The fix demonstrates good defensive programming practices that are important in mathematical software. The test cases provided verify that the fix works for various complex exponent scenarios.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17318": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear reproduction case showing the exact expression that causes the IndexError, (2) The complete stack trace showing the error occurs in _split_gcd function with \"tuple index out of range\", (3) The expected behavior - that expressions that cannot be denested should be returned unchanged rather than raising an error, (4) The exact file and function where the error occurs (sympy/simplify/radsimp.py, line 1068, in _split_gcd function), and (5) A clear description that this is about the sqrtdenest function in sympy. An experienced engineer would have all the information needed to understand the problem and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly identified in the stack trace as an IndexError in _split_gcd when accessing a[0] of an empty tuple. Looking at the patch, the solution involves adding a condition to check that squared arguments are both rational AND positive before calling split_surds. The fix is localized to one function (_sqrt_match) and involves adding a simple validation check. An experienced engineer familiar with the codebase could identify that the issue occurs when sqargs is empty, understand that the split_surds function expects certain conditions to be met, and add the appropriate validation relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is straightforward, and the test cases provide clear validation criteria. The issue involves mathematical computation (symbolic math with complex numbers) but the core problem is a simple bounds-checking issue.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17630": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It provides: (1) Clear reproduction steps with specific code examples showing how to create a BlockMatrix with symbolic matrix 'a' and zero matrix 'z', (2) Detailed error traceback showing the exact AttributeError: 'Zero' object has no attribute 'cols', (3) Identification of the root cause - that zeros in the result are 'Zero' objects instead of 'ZeroMatrix' objects, (4) Specific functions/methods involved (_blockmul, block_collapse, colblocksizes), and (5) Environment details (Python 3.7.4, sympy 1.4). The problem is clearly defined: when multiplying three BlockMatrix objects containing ZeroMatrix blocks, an AttributeError occurs because Zero objects don't have a 'cols' attribute that the code expects from matrix-like objects.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue because: (1) The bug requires understanding SymPy's matrix expression system and how BlockMatrix multiplication works internally, (2) The root cause analysis shows that matrix operations are incorrectly producing Zero scalar objects instead of ZeroMatrix objects, requiring investigation of matrix expression handling, (3) The fix involves modifying the _postprocessor function in matexpr.py to properly handle MatAdd operations, which requires understanding the matrix expression evaluation pipeline, (4) While the actual code change is small (2 lines), identifying where and how to fix it requires substantial investigation into the codebase architecture and matrix expression processing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with clear reproduction steps, the solution involves core SymPy functionality that would be valuable to test, and the fix requires genuine debugging and understanding of the codebase rather than trivial changes.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17655": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: multiplying a Point by a scalar works in one direction (point * scalar) but fails in the other direction (scalar * point). The issue includes a complete error traceback showing exactly what goes wrong, demonstrates the expected behavior with a working example, and clearly states that \"both lines give the same result\" should be the expected outcome. The problem is a classic case of missing reverse multiplication operator (__rmul__) in Python, which is a well-understood concept in Python operator overloading.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward Python operator overloading issue that should take 15-60 minutes to solve. The problem is that the Point class has __mul__ defined but not __rmul__. An experienced engineer would quickly recognize this pattern from the error message and the fact that point*scalar works but scalar*point doesn't. The solution is simply adding a __rmul__ method that delegates to __mul__, which is a 4-line addition. The main time would be spent understanding the codebase structure and running tests to verify the fix works correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. This is a well-contained problem with a clear solution that doesn't require deep domain knowledge or complex algorithmic thinking. The test coverage appears adequate and the fix is minimal and safe.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-18211": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It describes a specific problem where `solveset` raises `NotImplementedError` when trying to solve the equation n\u22c5cos(n) \u2212 3\u22c5sin(n) = 0, but it should instead return a `ConditionSet` representing all real solutions. The issue provides a concrete example equation, explains the current incorrect behavior (raising NotImplementedError), and clearly states the expected behavior (returning a ConditionSet for unsolvable equations). The problem description is technical but precise, making it clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that takes 15 minutes to 1 hour. The solution involves wrapping the existing `solve_univariate_inequality` call in a try-catch block and returning a `ConditionSet` when a `NotImplementedError` is raised. The patch shows only about 8 lines of actual code changes in a single function (`_eval_as_set` in `sympy/core/relational.py`), plus imports. An experienced engineer familiar with the codebase would need to understand the existing flow, identify where the exception occurs, and implement the fallback logic - which is conceptually simple but requires some familiarity with SymPy's set operations and error handling patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined problem with a clear solution approach. The issue involves standard exception handling patterns and the fix is localized to a single method. The test cases provided also clearly validate the expected behavior, making it suitable for benchmark evaluation.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-20428": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: clear_denoms() on a complicated constant polynomial that simplifies to zero returns a polynomial that prints as Poly(0, x, domain='EX') but doesn't behave consistently like a zero polynomial. The issue provides specific examples of the inconsistent behavior: is_zero attribute is False, terms_gcd() triggers an IndexError, and primitive() behavior differs between versions. The root cause is identified: the problematic polynomial's internal representation is DMP([EX(0)], EX, None) instead of DMP([], EX, None) for a proper zero polynomial. The issue gives enough detail including specific error traces, function names (clear_denoms, terms_gcd, primitive), file paths, and the exact nature of the malformed internal representation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "The fix is actually quite simple - the patch shows it's just a one-line change in the __bool__ method of expressiondomain.py, changing \"return f.ex != 0\" to \"return not f.ex.is_zero\". This addresses the core issue where the polynomial wasn't being correctly identified as zero due to the boolean evaluation method. While understanding the problem requires some familiarity with SymPy's polynomial internals and the DMP representation, the actual solution is straightforward once the root cause is identified. An experienced engineer familiar with the codebase could implement this fix within 15-60 minutes after understanding the issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-crafted issue for a benchmark. The problem is clearly defined with specific reproducible examples, the root cause is identified, and the solution is targeted and minimal. The test case provided shows the exact polynomial expression that triggers the bug, making it easy to verify the fix. This type of issue tests understanding of internal representations, boolean logic, and edge case handling in mathematical software.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-20438": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear reproduction case with specific steps: create a finite set a = {1, 2}, create its Cartesian product b, and create an equivalent finite set c with the same elements. The problem is demonstrated through concrete examples showing that b.is_subset(c) and c.is_subset(b) both return True (which should indicate equality), but attempting to simplify the equality between b and c fails with an AttributeError. The issue also mentions that rewriting b as a FiniteSet yields an incorrect two-element set {1, 2} instead of the expected four-element set. While some technical details about the internal workings might need investigation, the core problem is clear: there are inconsistencies in set operations involving ProductSet and FiniteSet, particularly with is_subset behavior and equality simplification.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This appears to be a 1-4 hour fix based on several factors: (1) The issue involves multiple interconnected problems across different parts of the codebase - set comparison handlers, relational operations, and ProductSet behavior. (2) Looking at the gold patch, the solution requires changes to multiple files: sympy/core/relational.py, sympy/sets/handlers/comparison.py, and sympy/sets/handlers/issubset.py. (3) The fixes involve understanding the interaction between different set types and implementing proper type checking and dispatch methods. (4) The engineer needs to understand how SymPy's multiple dispatch system works and how different set operations should behave. (5) The AttributeError suggests missing method implementations that need to be properly handled. While not extremely complex, this requires solid understanding of the codebase architecture and careful implementation across multiple components.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. It provides clear reproduction steps, demonstrates specific failing behaviors, and has a well-defined scope. The problem involves fundamental set operations that should behave consistently, making it a good test of both debugging skills and understanding of mathematical abstractions in code. The solution requires changes to multiple related files but doesn't involve overly esoteric domain knowledge.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20590": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is generally well-specified but requires some domain knowledge to fully understand. The reporter clearly describes that Symbol instances in SymPy version 1.6.2 had no `__dict__` attribute (raising AttributeError when accessed) and only had `__slots__` defined as a tuple containing 'name'. However, in version 1.7, `__dict__` exists and returns an empty dict. The reporter suspects this is a bug because `__slots__` is meant to prevent `__dict__` creation for memory efficiency. While the core problem is clear, an engineer would need to understand Python's `__slots__` mechanism and investigate the class hierarchy to determine which parent class accidentally stopped defining `__slots__`. The solution direction is hinted at but requires investigation of the inheritance chain to identify where `__slots__` definition was lost.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The solution involves adding `__slots__ = ()` to the Printable class, which is a mixin used by Symbol and other classes. An experienced engineer would need to: 1) Understand that `__slots__` in a parent class prevents `__dict__` creation, 2) Investigate the Symbol class inheritance hierarchy to find which parent class lacks `__slots__`, 3) Add the empty `__slots__ = ()` to the appropriate mixin class. The actual code change is minimal (just one line), but requires understanding Python's `__slots__` mechanism and some investigation of the class hierarchy. The patch shows this is exactly what was done - adding `__slots__ = ()` to the Printable class.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of Python's object model, specifically `__slots__`, and requires some investigation skills to trace through class inheritance. The issue is concrete with a clear acceptance criteria (Symbol instances should not have `__dict__`), and the solution is verifiable through the provided test case.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-21379": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproducible example with exact steps to trigger the bug, the specific error message, version information, and detailed observations about when the error occurs vs. doesn't occur. The user clearly explains the expected behavior (substitution should work) vs. actual behavior (PolynomialError is raised). An experienced engineer would have enough information to understand the problem and work toward a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue would take 1-4 hours to solve. While the reproduction case is clear, debugging why the PolynomialError occurs specifically with real symbol assumptions in this complex expression involving exp/sinh/Piecewise would require significant investigation. The engineer would need to trace through the sympy codebase to understand where the error originates (in the gcd computation during modular arithmetic operations), understand why it happens with real assumptions but not without, and implement an appropriate fix. The solution involves understanding sympy's internal polynomial handling and adding proper error handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-documented bug report with clear reproduction steps and expected vs actual behavior. The mathematical nature of the problem and the specific sympy domain knowledge required make it a good test case for coding ability in the context of symbolic mathematics libraries.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-22714": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear information for a successful solution. It describes a specific problem: when using `with sp.evaluate(False):`, calling `sp.S('Point2D(Integer(1),Integer(2))')` raises a ValueError about imaginary coordinates, even though the coordinates are clearly real integers (1 and 2). The issue provides a complete, reproducible example with exact steps to trigger the bug, includes the full error traceback showing the call stack, and demonstrates that the same code works fine outside the evaluate(False) context or when using evaluate=False as a parameter. The problem is clearly about incorrect behavior in the Point2D constructor when automatic evaluation is disabled globally, and the expected behavior is that it should work the same way as when evaluation is enabled or when evaluate=False is passed directly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it's a one-line change in the Point geometry class that modifies how imaginary coordinate checking is performed. The issue is in sympy/geometry/point.py where the condition `any(a.is_number and im(a) for a in coords)` needs to be changed to `any(a.is_number and im(a).is_zero is False for a in coords)`. The problem occurs because when evaluation is disabled, `im(a)` returns an unevaluated expression rather than 0, so the truthy check fails incorrectly. The fix requires understanding that with evaluate=False, expressions don't automatically simplify, so you need to explicitly check if the imaginary part is non-zero rather than just checking if it's truthy. This requires some thought about SymPy's evaluation system but is conceptually straightforward once you understand the root cause.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear reproduction case, obvious expected behavior, and a surgical fix that doesn't affect other functionality. The test case is also straightforward and directly validates the fix. This would make an excellent benchmark sample for evaluating debugging and code comprehension skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23824": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly explains that the kahane_simplify() function in sympy.physics.hep.gamma_matrices has a bug where leading uncontracted gamma matrices are reversed in order. The issue provides a concrete example showing the problem: \u03b3^\u03bc \u03b3_\u03bc \u03b3^\u03c1 \u03b3^\u03c3 correctly simplifies to 4*GammaMatrix(rho)*GammaMatrix(sigma), but when the contracted pair is moved to the end (\u03b3^\u03c1 \u03b3^\u03c3 \u03b3^\u03bc \u03b3_\u03bc), it incorrectly produces 4*GammaMatrix(sigma)*GammaMatrix(rho) instead of maintaining the original order. The issue even identifies the root cause - that in kahane_simplify(), leading matrices are removed and then reinserted, but the insertion loop runs backward. The expected behavior is clearly defined: both expressions should simplify to 4 \u03b3^\u03c1 \u03b3^\u03c3 maintaining the order of \u03c1 and \u03c3.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue description explicitly identifies the root cause - the insertion loop for leading matrices runs backward in the kahane_simplify() function. The gold patch shows this is indeed a simple fix: replacing a backward loop with a list comprehension that maintains proper order. The fix changes only 2 lines of code in one file. An experienced engineer would need some time to understand the gamma matrix mathematics and trace through the code logic, but once the root cause is identified (which the issue helpfully provides), the implementation is straightforward. The test additions are also minimal, adding just a few assertions to verify the fix works correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample. The issue is clearly described with concrete examples, the problem is well-isolated to a specific function, and the fix requires understanding both the mathematical domain (gamma matrices) and the code structure. The provided test case clearly demonstrates the bug and would effectively validate any solution. The issue strikes a good balance between being specific enough to solve while requiring some domain knowledge and code comprehension skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23950": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Contains.as_set() incorrectly returns the Contains object itself instead of the actual set. The issue provides a concrete example showing that Contains(x, Reals).as_set() should return the set (Reals) rather than the Contains expression. It also includes a detailed error traceback showing how this bug manifests when using Contains in Piecewise expressions, specifically the AttributeError when trying to call as_relational() on the Contains object. The expected behavior is clear: as_set() should return the set argument (args[1]) from the Contains expression, not the Contains object itself.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that requires minimal time. The problem is clearly identified in the issue description, and looking at the gold patch confirms this is a simple one-line change. The Contains.as_set() method currently raises NotImplementedError, but should instead return self.args[1] (the set argument). An experienced engineer would need to: 1) Understand that Contains expressions have args where args[1] is the set, 2) Recognize that as_set() should return the actual set object, and 3) Make the simple change from raising NotImplementedError to returning self.args[1]. This requires understanding the basic structure of SymPy expressions but no deep domain knowledge or complex implementation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement, obvious solution, and straightforward implementation. The issue provides good context about why this matters (integration with Piecewise expressions) and the test patch shows the expected behavior clearly.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-24066": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear reproduction case with specific steps: import exponential function and SI units, create an expression for second/(farad*ohm), verify it's dimensionless, then add 100 to exp() of that expression. The expected behavior is implicit but reasonable - the exponential of a dimensionless quantity should itself be dimensionless, so the final expression (100 + exp(dimensionless)) should also be dimensionless. The error message clearly shows what's happening: \"exp(second/(farad*ohm))\" is being reported as having dimensions when it should be Dimension(1). While some details about the expected fix aren't explicitly stated, an experienced engineer can reasonably infer that SI._collect_factor_and_dimension() needs to properly handle exponentials of dimensionless quantities.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a focused change to the _collect_factor_and_dimension method in unitsystem.py. The fix involves modifying how Function instances are handled - specifically checking if dimensions are dimensionless and converting them to Dimension(1) appropriately. The change is only a few lines and targets a specific function. An experienced engineer familiar with the codebase would need to understand the dimension system, locate the problematic function, and implement the logic to properly handle dimensionless quantities in function arguments. This requires some understanding of the physics units system but is not a massive rewrite.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-contained within the physics units system, has a clear reproduction case, and the test patch shows exactly what behavior should be expected. The issue demonstrates a logical bug in dimension handling that has a focused solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-24443": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear reproduction case: importing the combinatorics package, creating the dihedral group of degree 3, and calling the `homomorphism` function with its own generators as both domain and image. It specifies the expected behavior (should return a valid homomorphism object) versus the actual error (ValueError: The given images do not define a homomorphism). The issue points to the specific problematic code location in `_image()` function and identifies that the problem occurs when `r[i]` is an inverted generator and the `in gens` test fails. However, some details are left for the engineer to figure out, such as exactly how to simplify the implementation as suggested in the final line.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires understanding the mathematical concepts of group homomorphisms and how they are implemented in SymPy's combinatorics module. The engineer needs to: 1) Understand the problem with inverted generators failing the `in gens` test, 2) Analyze the existing complex `_image()` function logic with dual indexing (i and j), 3) Redesign the function to handle generator mapping more reliably using array_form and symbol mapping, and 4) Simplify the implementation significantly (the patch shows a complete rewrite of the function from ~40 lines to ~7 lines). This involves substantial refactoring of core logic but is contained to one function, making it a 1-4 hour task.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for the benchmark. It has a clear reproduction case, specific error message, and the test patch shows exactly what behavior should be verified. The problem is isolated to one function and has a mathematically precise expected outcome.",
            "q2_5_confidence": 4
        }
    }
]