[
    {
        "astropy__astropy-13033": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is somewhat underspecified but has a reasonable interpretation. It describes that TimeSeries objects with required columns produce misleading exceptions when those columns are removed, but it doesn't provide specific examples of the misleading messages or the exact scenario that triggers them. The expected behavior is clear (informative exception about missing required columns), but the actual problematic behavior is only described as \"confusing\" without specifics. However, looking at the patches, it becomes clear this is about error message formatting when multiple required columns are involved - the issue is that when there are multiple required columns, the error message formatting is incorrect (showing individual strings instead of properly formatted lists). An experienced engineer could reasonably infer this is about improving error message clarity and formatting.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minutes to 1 hour fix. The solution involves adding a helper function `as_scalar_or_list_str` to format error messages properly and modifying one error message string in the `_check_required_columns` method. The patch shows this is a small, focused change - adding 7 lines for the helper function and modifying 4 lines for the error message formatting. This requires understanding the existing error handling logic and improving string formatting, which is straightforward once you locate the relevant code. The most time would be spent understanding the codebase structure and locating the right method to modify.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix that improves error message clarity. The solution is focused and doesn't require extensive domain knowledge beyond understanding the TimeSeries class structure. The test case clearly validates the fix by checking the exact error message format.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-13977": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The author clearly states that `Quantity.__array_ufunc__()` should return `NotImplemented` instead of raising `ValueError` when inputs are incompatible, which would allow `__radd__` to be called instead. They mention they're implementing a duck type of `astropy.units.Quantity` and want reflected arithmetic operators to work properly. While the specific technical context around numpy's `__array_ufunc__` protocol and duck typing requires domain knowledge, the core request is clear: change error-raising behavior to return `NotImplemented` to enable fallback to reflected operators. The reference to numpy docs about returning `NotImplemented` for unimplemented operations provides additional context. An experienced engineer familiar with Python's data model and numpy would understand what needs to be done, though they'd need to figure out the specific implementation details.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Understanding Python's `__array_ufunc__` protocol and when to return `NotImplemented` vs raising exceptions, (2) Understanding the existing `Quantity.__array_ufunc__` implementation and the `converters_and_unit` function, (3) Determining the right conditions under which to catch exceptions and return `NotImplemented` instead, and (4) Writing comprehensive tests for the new behavior with duck types. The actual code change involves wrapping existing logic in try-catch blocks and adding logic to detect when other types might handle the operation, but getting the conditions right requires careful thought. The patch shows this is more complex than a simple change - it involves restructuring the method flow and adding logic to check if inputs/outputs have custom `__array_ufunc__` implementations.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability. It tests understanding of Python's data model, numpy protocols, exception handling, and the ability to modify existing complex code while maintaining backward compatibility. The extensive test suite in the patch demonstrates that a proper solution requires thinking about various edge cases and duck type scenarios.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14096": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: when subclassing SkyCoord and creating a custom property that tries to access a non-existent attribute, the error message incorrectly reports that the property doesn't exist rather than the actual missing attribute. The issue provides a concrete scenario where a custom property `prop` tries to access `random_attr` (which doesn't exist), but the error says `prop` doesn't exist instead of `random_attr`. While the description doesn't provide a complete minimal reproducible example, an experienced developer can understand that this is about incorrect AttributeError messages in the `__getattr__` method of SkyCoord. The solution direction becomes clearer when looking at the patch, which shows the issue is in the `__getattr__` method's error handling.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is localized to the `__getattr__` method in the SkyCoord class. The current implementation raises a custom AttributeError with the original attribute name, but this masks the real source of AttributeErrors that occur within properties of subclasses. The solution is elegant and simple: instead of raising a custom AttributeError, call `__getattribute__` which will raise the correct exception with the proper attribute name. This requires understanding Python's attribute access mechanism and the difference between `__getattr__` and `__getattribute__`, but once understood, the fix is straightforward - essentially replacing a few lines of error handling code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with a clear solution that tests a specific aspect of Python's attribute access mechanism. The issue is focused on error handling behavior rather than complex algorithmic changes, making it suitable for evaluating understanding of Python's object model and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14182": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core request is clear: add support for a `header_rows` argument to the RestructuredText output format, similar to the fixed_width format. However, the description lacks important details: (1) What exactly should `header_rows` contain and how should it be used? (2) What should the output look like when multiple header rows are specified? (3) How should this interact with existing RST table formatting? The mention of \"names and units\" provides a hint about the intended use case, and referencing the fixed_width format gives a point of comparison, but an engineer would need to investigate the existing fixed_width implementation to understand the expected behavior. While there's a sensible interpretation of what's required, some details would need to be inferred from studying the codebase.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task. An engineer would need to: (1) Study the existing RST format implementation and understand how it currently works, (2) Examine the fixed_width format to understand how `header_rows` is implemented there, (3) Modify the RST class constructor to accept the `header_rows` parameter, (4) Update the write method to properly handle multiple header rows in RST format, (5) Modify the read method to correctly parse tables with multiple header rows, (6) Handle the data start line calculation based on the number of header rows. The solution involves understanding table formatting, modifying multiple methods across the class, and ensuring proper integration with the existing FixedWidth base class. While not extremely complex, it requires thoughtful implementation to maintain RST formatting standards.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This appears to be a legitimate feature enhancement request that's suitable for the benchmark. The issue is from a well-maintained scientific Python library (astropy), and the solution involves extending existing functionality in a clean way. The test patch shows comprehensive testing including round-trip functionality, which indicates the feature can be properly validated. The code changes are focused and don't appear to introduce breaking changes to existing functionality.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14309": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides some key details but requires interpretation. It mentions an IndexError in identify_format, points to a specific commit that changed behavior, and explains the behavioral change (previously returned None, now executes isinstance(args[0], ...)). However, the connection between \"identify_format\" and the actual code location in \"is_fits\" function isn't immediately clear from the description alone. An experienced engineer would need to trace through the codebase to understand that identify_format likely calls is_fits. The core problem is identifiable: when there are no arguments in args[0] but the code tries to access it, causing an IndexError. While some detective work is needed, there's enough information for a sensible interpretation of what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is essentially a logic error where the code was restructured but didn't account for the case where args might be empty. Looking at the patch, the fix is quite simple - just ensuring that the function returns early with the result of the filepath extension check rather than falling through to isinstance check when no args are present. The main time would be spent understanding the codebase structure, tracing the call from identify_format to is_fits, and reproducing the error. The actual code change is minimal - just restructuring the conditional logic to avoid the IndexError.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for the benchmark. While the issue description could be clearer about the exact function and location, the core problem is understandable and the solution is straightforward once located. The provided test case would effectively validate whether a candidate solution correctly handles the edge case of empty arguments.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14365": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear about what needs to be fixed. It describes that the ascii.qdp Table format currently assumes QDP commands are upper case (like \"READ SERR 1 2\") but QDP itself is case insensitive and should support lowercase commands (like \"read serr 1 2\"). The problem statement is concrete with a specific example, the expected behavior is clearly stated (the file should read into a Table with errors rather than crashing), and the context about many QDP files being created by hand makes the motivation clear. An experienced engineer would understand that they need to modify the QDP parser to handle case-insensitive commands.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that requires adding case-insensitive regex matching. Looking at the provided solution, it involves two simple changes: (1) adding re.IGNORECASE flag to the regex compilation in _line_type_re, and (2) changing the string comparison from `v == \"NO\"` to `v.upper() == \"NO\"`. These are minimal code changes that an experienced engineer familiar with regex and basic string operations could implement in 15-60 minutes after understanding the codebase structure. The main time would be spent locating the relevant parsing code in the astropy.io.ascii.qdp module.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-suited benchmark issue. The problem is clearly defined, the solution is straightforward but requires understanding the codebase, and the test changes demonstrate how to verify the fix works correctly. The issue tests both understanding of regex flags and string comparison handling, which are fundamental programming skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14508": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear about what needs to be fixed. It identifies a specific problem in `io.fits.Card._format_float()` function where float values are being represented with unnecessarily long strings (e.g., `0.009124999999999999` instead of `0.009125`), causing comment truncation in FITS cards. The issue provides the exact location of the problematic code (lines 1300-1302 in card.py), explains the root cause (using `f\"{value:.16G}\"` formatting), and suggests a specific solution approach (try `str(value)` first, then fall back to manual formatting if it exceeds 20 characters). The expected behavior is clearly stated: \"Being able to create any valid FITS Card via `io.fits.Card`\". An experienced engineer would have sufficient information to implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic function (`_format_float`) and provides the exact location and cause. The solution involves modifying a single function to use `str(value)` first before falling back to the existing formatting logic. Looking at the gold patch, the fix is straightforward: replace the complex formatting logic with a simpler approach that uses Python's default string representation, only applying truncation when the result exceeds 20 characters. The logic is contained within one function and doesn't require extensive changes across multiple files. An experienced engineer familiar with the codebase could implement this relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution approach is clear, and the test cases provided in the test patch demonstrate the expected behavior comprehensively. The issue is suitable for evaluating coding ability as it requires understanding floating-point representation, string formatting, and implementing a cleaner solution while maintaining backward compatibility.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14995": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides sufficient information to understand the problem, though some details need to be inferred. The core issue is clear: in astropy v5.3, NDDataRef mask propagation fails when one operand doesn't have a mask, specifically when using `handle_mask=np.bitwise_or`. The error occurs because the code tries to perform bitwise operations between an integer and None. The expected behavior is also clearly stated - when one operand lacks a mask, the existing mask should be copied to the output (as it worked in v5.2). While the description doesn't provide a specific code example demonstrating the failure, the problem statement is concrete enough for an experienced engineer to understand what needs to be fixed and how to reproduce the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a simple one-line fix in the `_arithmetic_mask` method of `astropy/nddata/mixins/ndarithmetic.py`. The bug is in line 523 where `elif operand is None:` should be `elif operand.mask is None:`. This is a straightforward logical error where the code was checking if the operand object itself is None rather than checking if the operand's mask attribute is None. An experienced engineer familiar with the codebase could identify this issue within 15-60 minutes by: 1) Understanding the mask propagation logic, 2) Tracing through the conditional statements, 3) Realizing the incorrect condition causes the wrong branch to execute when an operand has no mask. The fix requires minimal code change and the logic is relatively straightforward once the issue is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-suited benchmark sample. The issue has a clear problem statement, specific expected behavior, and the solution involves logical reasoning about conditional statements in mask handling code. The accompanying test cases are comprehensive and would effectively validate whether a candidate's solution correctly handles all the edge cases (masked * unmasked, unmasked * masked, etc.). The fix demonstrates good software engineering practices and would test a candidate's ability to understand object-oriented code, debug logical errors in conditional statements, and work with numpy operations.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-7336": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: the `units.quantity_input` decorator fails when used on constructors that have a type hint return annotation of `-> None`. The issue describes that `None` has no `.to()` attribute, which causes an exception. The user provides a clear workaround (removing the return type hint) and suggests a possible fix (explicitly checking for None return values). However, the issue doesn't provide a complete minimal reproducible example showing the exact error, the specific decorator being used, or the exact exception message. An experienced engineer would need to deduce that this relates to astropy's units system and understand how the decorator processes return annotations, but the general requirement is clear enough to work with.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that should take 15 minutes to 1 hour. Looking at the provided solution, it's a simple one-line change in the decorator logic: changing the condition from checking if the return annotation is not empty to also excluding None. The fix involves modifying a single conditional statement in `astropy/units/decorators.py` from `if wrapped_signature.return_annotation is not inspect.Signature.empty:` to `if wrapped_signature.return_annotation not in (inspect.Signature.empty, None):`. An experienced engineer would need to: 1) Locate the quantity_input decorator code, 2) Understand how it processes return annotations, 3) Identify that None return annotations are being treated like other return types and having `.to()` called on them, and 4) Add the simple check to exclude None. The logic is straightforward once you understand the problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. The issue is clear enough to solve, represents a real bug that would be encountered in practice, and the solution is focused and testable. The test changes show that the fix is properly validated with a test case for the None return annotation scenario.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-7606": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It describes a specific problem: comparing an unrecognized unit to None raises a TypeError instead of returning False. The expected behavior is clearly stated - the comparison should return False rather than raising an exception. The issue title is descriptive and the problem statement is concise but complete. An experienced engineer would understand exactly what needs to be fixed: the equality comparison behavior for UnrecognizedUnit objects when compared to None should be modified to handle this case gracefully by returning False instead of raising a TypeError.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. Looking at the gold patch, the solution involves modifying the __eq__ method in the UnrecognizedUnit class to handle exceptions gracefully by returning NotImplemented instead of letting the TypeError propagate. The fix is straightforward: wrap the Unit() call in a try-except block and return NotImplemented when exceptions occur. This follows Python's comparison protocol correctly. The change is localized to one method and doesn't require extensive research or understanding of complex domain logic - just basic Python comparison semantics and exception handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear expected behavior, appropriate test coverage in the test patch, and a straightforward solution. The issue demonstrates good software engineering practices with a clear problem statement and the fix follows Python idioms correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7671": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides enough context to understand the core problem. It mentions that minversion is failing due to a LooseVersion bug, references a specific Python bug report, and hints that certain version string formats are problematic. While it doesn't provide specific examples of failing cases or detailed reproduction steps, an experienced engineer could reasonably infer that the issue involves version string parsing problems in the minversion function. The mention of \"without the '.3' it doesn't fail\" and the reference to pkg_resources.parse_version gives clues about the nature of the problem - likely related to how certain version string formats are handled.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours because: (1) The engineer needs to research the referenced Python bug in LooseVersion to understand the specific failure cases, (2) They need to locate and understand the minversion function in the codebase (astropy/utils/introspection.py), (3) They need to design a solution that handles problematic version strings while maintaining backwards compatibility, (4) The solution involves implementing regex parsing based on PEP440 standards, which requires some research and careful implementation, (5) They need to write appropriate tests to verify the fix works for various version string formats. While not extremely complex, it requires research, understanding of version parsing standards, and careful implementation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability. It tests the engineer's ability to research external bugs, understand version parsing standards, implement regex solutions, and handle edge cases in existing code. The solution requires both technical knowledge and problem-solving skills, making it a good test of coding competency.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-10554": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is extremely vague and lacks crucial details. The title mentions \"Union queryset with ordering breaks on ordering with derived querysets\" but provides no concrete examples, error messages, or clear description of the problem. The description only contains two sentences: a reference to another issue and a vague statement about \"evaluating the queryset instead of creating a new queryset.\" There's no code example showing what's broken, no expected vs actual behavior, and no way for an engineer to understand what specific functionality needs to be implemented. While the patches reveal this is about Django ORM union operations with ordering, an engineer working only from the issue description would have no clear direction on what to fix or how to reproduce the problem.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the actual patches, this is a moderately complex issue involving Django's SQL compilation layer. The solution requires: 1) Understanding Django's ORM internals, specifically the SQL compiler and query classes, 2) Modifying the get_order_by method in the compiler to handle cases where ORDER BY terms don't match columns in the result set, 3) Adding a new method add_select_col to the Query class, 4) The fix involves adding columns used in ORDER BY clauses to the selected columns when they're missing. This requires understanding SQL generation, queryset mechanics, and Django's internal architecture. While not the most complex database issue, it requires solid knowledge of Django internals and careful consideration of edge cases with union querysets.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "This sample has a critical flaw for benchmark use: the issue description is so vague that it's essentially unusable without the patches. An engineer would need to reverse-engineer what the problem is from the test cases, which defeats the purpose of testing their ability to understand requirements and implement solutions. The issue text provides no actionable information about what functionality is broken or what the expected behavior should be. This makes it unsuitable for a benchmark that relies on engineers understanding the problem from the issue description alone.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11087": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The main problem is clearly described: Django's .delete() method is fetching unnecessary fields during cascade deletion, which causes UnicodeDecodeError when those fields contain invalid unicode data. The issue specifically identifies that the text_log_error.line field is not needed for the delete operation but is being fetched anyway. The request is to \"optimize .delete() to use only required fields.\" While the issue description provides good context about the Unicode problem they encountered, the core optimization request is clear enough for an experienced Django developer to understand what needs to be done - modify the deletion logic to only select fields that are actually required for the delete operation (like primary keys and foreign key references).",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a complex issue requiring deep understanding of Django's ORM deletion system. The solution involves modifying the core deletion.py module to detect which fields are actually referenced during cascade deletion operations and only select those fields. The engineer needs to: 1) Understand Django's deletion collector system and how it handles cascading deletes, 2) Identify which fields are referenced by foreign key relationships, 3) Implement logic to defer non-referenced fields using .only(), 4) Handle edge cases like signal listeners and select_related queries, 5) Use introspection of model metadata to determine candidate relations. The final patch shows this requires adding helper methods, importing additional modules (itertools.chain), and carefully modifying the collect() method with sophisticated relationship analysis. This is clearly a 4+ hour task requiring substantial Django ORM expertise.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-suited for a coding benchmark as it requires genuine understanding of Django's internals and careful implementation of an optimization that maintains correctness while improving performance. The test case is also well-designed to verify the optimization works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11265": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: using exclude() on a queryset with an annotated FilteredRelation raises a FieldError. The reporter identifies the likely problematic function (split_exclude) and mentions that a new query is created without extra data from the original query. While the description is brief and doesn't include a minimal reproducible example, there's enough information to understand what's happening - the FilteredRelation annotation is being lost when exclude() creates a subquery. The test patch confirms this interpretation by showing exactly what should work: Author.objects.annotate(book_alice=FilteredRelation(...)).exclude(book_alice__isnull=False) should execute without error.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The gold patch shows two small, focused changes: (1) Adding query._filtered_relations = self._filtered_relations in split_exclude() to preserve filtered relations when creating subqueries, and (2) Modifying the trim_start() logic to avoid trimming joins from filtered relations. The fix requires understanding how Django's ORM handles subqueries and filtered relations, but the changes themselves are minimal and surgical. An experienced engineer familiar with Django's query system could identify and implement this solution relatively quickly once they understand the root cause.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for the benchmark. It represents a real-world Django ORM problem that requires understanding of query construction internals. The test case is simple and clear, making it easy to verify if a solution works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11299": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: CheckConstraint with OR operator generates incorrect SQL on SQLite and Oracle by including fully qualified field names. It explains that this happens with combinations of OR and AND clauses, causing migration failures when tables are dropped/swapped. The description identifies the root cause (AND clauses using Col vs OR clauses using SimpleCol) and provides enough context about the failure mode (malformed schema exception). However, some technical details about Django's internal SQL generation mechanics and the exact conditions triggering the bug would need to be inferred from codebase exploration. An experienced Django developer could reasonably understand what needs fixing and develop a solution approach.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django's ORM query building internals, specifically how CheckConstraints generate SQL and the difference between Col and SimpleCol usage; (2) The developer needs to trace through the query building process to understand when and why fully qualified names are incorrectly included; (3) Looking at the patch, the fix is a single line change passing the simple_col parameter through recursive calls, but finding this requires navigating complex ORM code; (4) The issue involves database-specific behavior (SQLite/Oracle) and migration mechanics, requiring broader Django knowledge; (5) Writing appropriate tests requires understanding CheckConstraint testing patterns and creating scenarios with OR/AND combinations.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-scoped to Django's ORM, the solution is targeted and minimal, and the test coverage appears comprehensive. The issue represents a legitimate bug with clear reproduction conditions and validation criteria. The technical complexity is appropriate for evaluating intermediate Django/ORM development skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11532": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: when a computer hostname contains non-ASCII characters (like \"\u6b63\u5b97\") and the email encoding is set to a non-Unicode encoding (like iso-8859-1), Python crashes when trying to convert headers including the Message-ID header to that encoding. The issue specifically states that \"Django should be handling the encoding of the message properly\" and suggests a fix: \"have django.core.mail.utils or django.core.mail.message convert domain name to punycode before using\". While the general direction is clear, an engineer would need to understand Django's mail system architecture and identify all the places where domain names need punycode conversion. The solution involves multiple files (message.py, utils.py, validators.py, encoding.py, html.py) which suggests this is more than a simple one-line fix, but the core requirement is understandable.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the provided patches, this is a 1-4 hour fix that involves: 1) Understanding Django's mail system and how it handles domain names, 2) Creating a new punycode utility function in django.utils.encoding, 3) Identifying all locations where domain encoding occurs (mail/message.py, mail/utils.py, validators.py, html.py), 4) Replacing manual idna encoding with the centralized punycode function, 5) Writing appropriate tests. The solution touches 5 different files and requires understanding of internationalized domain names, email standards, and Django's architecture. While not extremely complex, it requires systematic identification of all encoding points and careful testing to ensure non-ASCII domains work correctly across the framework.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate bug fix that addresses a real problem with internationalized domain names in Django's email system. The solution is testable and the requirements are clear enough for an experienced engineer to implement successfully.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11551": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "This issue is exceptionally well-specified. The description provides: (1) Clear context about what broke (admin.E108 error after Django upgrade from 2.0.7 to 2.2.1), (2) Specific root cause analysis identifying the problematic commit and the exact issue with hasattr(model, item) check in _check_list_display_item, (3) Detailed explanation of why PositionField's descriptor causes the problem, and (4) Precise correct logic specification stating the exact order of checks that should be performed and when E108 vs E109 errors should be raised. The issue even identifies the specific function (_check_list_display_item) that needs modification. An experienced engineer has all the information needed to implement a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because it requires: (1) Understanding Django's admin validation system and how ModelAdmin.list_display works, (2) Comprehending the interaction between Python descriptors and hasattr() behavior, (3) Carefully restructuring the validation logic to follow the specified order (try get_field first, then getattr, with proper error handling), and (4) Understanding the difference between E108 and E109 errors. While the issue description is clear, the solution involves non-trivial logic changes to error handling flow and requires understanding of Django internals and Python descriptor protocol.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample with clear requirements, well-defined success criteria, and appropriate test coverage. The issue demonstrates a real-world Django problem with a concrete solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11734": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in but provides a sensible interpretation of what needs to be fixed. It clearly states that using exclude() or ~Q() with OuterRef inside an Exists annotation causes queries to crash, while filter() works correctly. The core problem is well-defined: there's a specific failure case involving OuterRef in exclude operations that needs to be resolved. However, the description lacks concrete examples demonstrating the crash, specific error messages, or a minimal reproducible case. An experienced engineer would need to infer the exact scenarios and create test cases to reproduce the issue, but the general direction is clear - fix the handling of OuterRef in exclude/~Q operations to work like it does in filter operations.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix based on several factors: (1) The issue involves Django's complex ORM query system, requiring understanding of how OuterRef, exclude operations, and query compilation interact. (2) The solution spans multiple files (fields/__init__.py, related_lookups.py, sql/query.py) indicating the fix requires understanding relationships between different ORM components. (3) The changes involve subtle logic around expression handling and query splitting - the split_exclude method modification shows this requires understanding Django's internal query processing. (4) An engineer would need time to reproduce the issue, trace through the ORM code to understand where OuterRef handling differs between filter and exclude, and implement a fix that doesn't break existing functionality. While not extremely complex, it's more than a simple bug fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is specific enough to be solvable, the test case provided shows clear expected behavior, and the fix appears to be well-contained within Django's ORM system. This would be a good benchmark sample for testing understanding of complex framework internals and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11740": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem: when changing a UUIDField to a ForeignKey that references another app (testapp2), Django's migration autodetector fails to create the necessary dependency on testapp2 in the generated migration. While the description is somewhat brief and lacks concrete code examples showing the exact models involved, the core problem is clear enough for an experienced Django developer to understand and work with. The issue mentions specific Django components (migrations, UUIDField, ForeignKey) and provides context about the expected vs actual behavior. An engineer familiar with Django's migration system would be able to identify this as a dependency detection issue in the autodetector and work towards a solution, even though they'd need to fill in some details about the exact implementation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) It involves understanding Django's complex migration autodetector system in django/db/migrations/autodetector.py, which requires domain expertise; (2) The engineer needs to trace through the generate_altered_fields method to understand how field changes are detected and dependencies are calculated; (3) The solution involves identifying that foreign key dependencies are not being properly detected when altering fields (as opposed to creating new fields), and adding the missing dependency detection logic; (4) While the actual code change is relatively small (adding a few lines), finding the right place to make the change and understanding the existing dependency detection patterns requires substantial investigation of the codebase; (5) The engineer also needs to understand Django's migration dependency system to implement the fix correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This sample is well-suited for the benchmark. The issue describes a legitimate bug in Django's migration system, the problem is technical and specific enough to test coding ability, and the solution requires understanding of Django's internals without being overly esoteric. The test case provided in the patch clearly validates the fix and would be suitable for checking whether a solution works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11815": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is well-specified and clearly articulates the problem. It explains that Django migrations use Enum values instead of names when serializing Enum objects as CharField defaults. The issue provides a concrete example of the problem: when translated values change based on active language, old migrations can fail because the Enum no longer recognizes the translated string. The solution requirement is also clearly stated - migrations should reference Enum members by name (e.g., Status['GOOD']) instead of by value. The description provides sufficient context about Django's migration system and Enum handling to understand both the current problematic behavior and the desired fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a focused change that requires understanding Django's migration serialization system, specifically the EnumSerializer class. The fix involves modifying how Enum objects are serialized in migrations - changing from using the enum value to using the enum name with bracket notation. Looking at the provided patch, the solution is relatively straightforward: replace the current serialization logic that uses `self.value.value` with `self.value.name` and update the format string accordingly. The change is localized to the EnumSerializer.serialize() method in django/db/migrations/serializer.py. While it requires understanding the Django codebase structure and enum serialization, the actual implementation is a small, targeted modification.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem statement is clear, the solution approach is well-defined, and the scope is appropriately limited. The issue represents a legitimate bug in Django's migration system that has a clean, backwards-compatible solution. The test cases demonstrate the expected behavior changes comprehensively.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11964": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with a clear example showing the problem. It describes that TextChoices/IntegerChoices fields return enum values instead of their underlying values when converted to strings, causing inconsistency between fresh and retrieved instances. The example with MyChoice and MyObject clearly demonstrates the issue. However, there are some minor gaps - it doesn't explicitly show the exact output that demonstrates the problem, and the phrase \"especially when communicating to an external API\" could be more specific about what exactly goes wrong. But an experienced engineer can reasonably infer that the __str__ method needs to return the enum's value rather than its default string representation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that takes 15 minutes to 1 hour. The issue clearly identifies that the problem is with string conversion of enum values, and the solution is simply overriding the __str__ method in the Choices class to return str(self.value). An experienced engineer would quickly understand that Django's enum implementation needs to return the underlying value when converted to string for consistency. The fix involves adding just a few lines of code to django/db/models/enums.py, making it a small, focused change that requires minimal research into the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-contained, has a clear solution, and the test case appropriately validates that str(enum_instance) returns the same value as str(enum_value) across different enum types. This is a good benchmark sample.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12155": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Django's admindoc fails when docstrings don't have an empty first line because the trim_docstring function incorrectly calculates indentation by including the first line (which has 0 indentation). The issue provides the exact problematic code line, explains why it fails, and even suggests a specific fix by modifying the lines slice from 'lines' to 'lines[1:]'. The description identifies the specific function (trim_docstring), the exact line causing the issue, and provides a clear technical explanation of why the current implementation is incorrect.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the actual solution, this is a 15 min - 1 hour fix. The issue suggested changing lines to lines[1:] in the problematic line, but the actual solution was more elegant: replacing the custom trim_docstring function entirely with Python's built-in inspect.cleandoc function, which handles this scenario correctly. This requires understanding that cleandoc is the standard library solution for this exact problem, importing it, and updating the two places where trim_docstring was called. It's a small change requiring some thought about using the right standard library function rather than fixing the buggy custom implementation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - the problem is clearly described, the solution involves both understanding the technical issue and choosing the right approach (using standard library vs fixing custom code), and the test verifies that docstrings without leading line feeds don't cause stderr errors when parsed.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12262": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks but provides a sensible interpretation of what needs to be fixed. The core problem is clearly stated: \"Custom template tags raise TemplateSyntaxError when keyword-only arguments with defaults are provided.\" The description mentions that when creating simple tags with keyword arguments that have default values, it's not possible to supply any other variable, and that supplying a keyword argument a second time produces an incorrect error message. The same issue affects inclusion tags. While the description doesn't provide specific code examples or detailed reproduction steps, the problem is understandable in the context of Django's template tag system. Looking at the provided patches, it's clear this relates to the parse_bits function in django/template/library.py where keyword-only parameters with defaults weren't being handled correctly in the parameter validation logic.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The solution involves a simple one-line change in the parse_bits function, changing 'unhandled_kwargs' to 'kwonly' in the parameter validation condition. An experienced engineer familiar with Django's template system would need to: (1) understand how template tag parsing works in Django, (2) locate the parse_bits function, (3) identify that keyword-only parameters weren't being properly recognized in the validation logic, and (4) make the one-line fix. The bulk of the time would be spent understanding the codebase and template tag mechanics rather than writing complex code. The fix itself is straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's template system, and the solution is a targeted fix that doesn't require extensive changes across multiple components. The test cases provided demonstrate clear expected behavior for both simple tags and inclusion tags with keyword-only arguments.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12325": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue is quite vague and lacks concrete details. It mentions \"pk setup for MTI to parent get confused by multiple OneToOne references\" and talks about ordering mattering, but doesn't provide specific examples of the problem, expected vs actual behavior, error messages, or code snippets demonstrating the issue. The phrase \"First issue is that order seems to matter?\" suggests there may be multiple issues, but only one is mentioned. The reference to \"explicit parent_link marker\" gives a hint about Django's model inheritance, but the specific problem scenario is unclear. While the patch shows the solution involves filtering OneToOneField instances by parent_link=True, the original issue description doesn't clearly explain what behavior was broken or what the desired outcome should be.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) Understanding Django's Multi-Table Inheritance (MTI) and OneToOneField relationships requires domain expertise, (2) The vague issue description would require significant investigation to understand what's actually broken, (3) The solution involves understanding the model metaclass creation process in django/db/models/base.py, which is core Django internals, (4) You need to trace through how parent_links are identified and processed during model creation, (5) The fix touches multiple files (base.py and options.py) and involves both adding logic and removing validation. While the actual code change is relatively small, the research and understanding required to arrive at this solution would take several hours.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. While the issue description is vague, the test cases provide good coverage of the expected behavior, and the solution is technically sound. The problem domain (Django model inheritance) is well-established and the fix addresses a legitimate bug in the framework.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12663": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks crucial details. It states that \"Using SimpleLazyObject with a nested subquery annotation fails\" and mentions that prior to a specific commit it was possible to use SimpleLazyObject in a queryset, but doesn't provide: 1) A concrete example of the failing code, 2) The specific error message or failure mode, 3) What the expected behavior should be, 4) A minimal reproducible example. The description mentions \"Sorry for the somewhat arbitrary testcase, hopefully it's sufficient to repro this issue\" but no actual test case is provided in the issue text. Without seeing the actual failing code or error, it would be very difficult for an engineer to understand what specifically needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the solution patch, this appears to be a 1-4 hour fix. The actual code change is minimal (adding 2 lines in django/db/models/sql/query.py), but the difficulty comes from: 1) Understanding the Django ORM internals and how SimpleLazyObject interacts with query annotations, 2) Identifying that the issue is in the output_field property where select[0].field fails for SimpleLazyObject because it may not have a 'field' attribute but has a 'target' attribute instead, 3) Writing appropriate tests to verify the fix works correctly. The solution requires understanding Django's query system architecture and the difference between regular objects and SimpleLazyObject, which would take some research and debugging time.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The main issue is that the original GitHub issue description is severely lacking in detail. An engineer working from just this description would struggle to even reproduce the problem, let alone fix it. The issue mentions a commit hash and says there's a testcase, but provides neither the failing code nor the error. In a real benchmark scenario, this would be problematic because engineers wouldn't have enough information to start working on a solution. The test case in the patch helps clarify what the issue actually is, but that wouldn't be available in the benchmark setup.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12708": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: \"Migration crashes deleting an index_together if there is a unique_together on the same fields.\" The context is well-defined (Django 1.11.10, migration system), and the specific scenario is described (attempting to delete index_together on fields also in unique_together). The issue mentions it occurred while refactoring to use the new Options.indexes feature. While there are some gaps (no specific error message, no minimal reproduction code), an experienced Django developer would understand this is about the migration system incorrectly handling the deletion of index_together when unique_together exists on the same fields. The two points mentioned clarify the expected behavior: 1) deletion should be possible independently or made coherent, and 2) moving declaration shouldn't recreate indexes. The solution space is reasonably constrained to Django's schema migration handling.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the provided solution, this is a 1-4 hour fix. The actual code change is minimal (adding `'unique': False` to a dictionary parameter in alter_index_together method), but understanding the problem requires: 1) Familiarity with Django's migration system and schema operations, 2) Understanding the relationship between index_together and unique_together constraints, 3) Debugging why the deletion fails when both constraint types exist on the same fields, 4) Understanding that the issue is in the _delete_composed_index method not properly distinguishing between index and unique constraints. The test changes also require understanding Django's introspection system and constraint checking. An experienced engineer would need time to trace through the migration code, understand the constraint introspection logic, and identify that the deletion logic needs to specify it's only targeting non-unique indexes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained Django framework issue with a clear problem domain (database schema migrations) and a targeted solution. The issue doesn't require external dependencies or complex architectural changes. The test coverage demonstrates the fix works correctly and the problem is reproducible. The solution shows good engineering practice by being minimal and targeted.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12754": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in but provides a sensible interpretation of what's required. It clearly states that there's a FieldError when migrating a field to a new model subclass, and that makemigrations works but migrate fails due to a field name clash. The key insight is that reversing the order of migration operations makes it pass, indicating the auto-detector should use this correct order. However, the description lacks concrete examples of the problematic models/fields and doesn't provide the exact error message. The reference to \"#21890\" (an analogous issue) suggests there's a pattern, but without access to that issue, some context is missing. Despite these gaps, an experienced Django developer familiar with migrations would understand this is about dependency ordering in migration operations when moving fields between model inheritance hierarchies.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the patch, the solution involves modifying the migration autodetector logic in django/db/migrations/autodetector.py to handle dependency ordering when creating models that inherit from bases where fields have been removed. The engineer needs to: (1) Understand Django's migration system and autodetector logic, (2) Identify where the dependency calculation happens in generate_created_models(), (3) Implement logic to detect when a new model has fields with the same name as removed fields from its base model, (4) Add appropriate dependencies to ensure field removal happens before model creation. This requires deep understanding of Django internals, the migration dependency system, and careful analysis of model state differences. The patch shows ~10 lines of new code but in a complex area that requires significant research and testing to get right.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The test case clearly demonstrates the expected behavior and the patch is focused on a specific part of Django's migration system. While the issue description could be clearer with concrete examples, it's sufficient for an experienced Django developer to understand and solve.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12858": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue is vague and lacks critical details. It states that \"models.E015 is raised when ordering uses lookups that are not transforms\" and mentions it was fine until a specific PR (#29408), but provides insufficient context. The issue doesn't explain: (1) what models.E015 specifically means or when it should/shouldn't be raised, (2) what the actual problem behavior is versus expected behavior, (3) how the mentioned foreign key relationships relate to the ordering issue, or (4) provide a concrete example demonstrating the problem. While the gold patch shows the fix involves allowing lookups in addition to transforms in ordering validation, this intent is not clear from the issue description alone. An engineer would need to investigate the codebase extensively to understand what E015 represents, how ordering validation works, and why the current behavior is incorrect.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This would take 1-4 hours to solve. The engineer needs to: (1) understand Django's model checking system and what models.E015 represents, (2) investigate the ordering validation logic in django/db/models/base.py, (3) understand the difference between transforms and lookups in Django's ORM, (4) analyze the referenced PR #29408 to understand what changed, and (5) determine why lookups should be allowed in ordering alongside transforms. The actual code change is small (adding `and fld.get_lookup(part) is None` to a condition), but significant research is required to understand the problem domain and arrive at the correct solution. The fix touches core Django ORM functionality, requiring careful consideration of the implications.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The issue description is too vague for a meaningful coding benchmark. Without understanding what models.E015 represents, when it should be raised, or seeing a concrete example of the problematic behavior, an engineer cannot reasonably be expected to arrive at the correct solution. The issue essentially requires prior knowledge of Django internals or extensive codebase investigation just to understand the problem statement. For a coding benchmark, the issue should clearly describe the current behavior, expected behavior, and provide reproducible examples. This issue would be more suitable if it included a code example that triggers the unwanted E015 error and explained why that error is incorrect.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13012": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the problem: when an ExpressionWrapper contains a constant expression, Django incorrectly includes it in the GROUP BY clause for Postgres, which is invalid SQL. The issue mentions that unwrapped expressions work correctly. However, it lacks concrete examples showing the problematic SQL generation, specific Django version information, and a minimal reproducible code snippet. The description gives enough context to understand that this is about Django's ORM query generation behavior, specifically related to the ExpressionWrapper class and how it handles constant expressions in GROUP BY clauses. While more details would be helpful, an experienced Django developer could reasonably interpret what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a focused fix requiring 15 minutes to 1 hour. Looking at the provided patch, the solution involves adding a simple `get_group_by_cols()` method to the ExpressionWrapper class that delegates to the wrapped expression. This is a small, targeted change that requires understanding Django's expression system but doesn't involve complex logic or extensive code changes. An experienced engineer familiar with Django's ORM would need time to understand how GROUP BY column determination works in the expressions system, locate the ExpressionWrapper class, and implement the delegation pattern. The fix is conceptually straightforward once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean, well-scoped bug fix in Django's ORM with a clear problem statement and straightforward solution. The patch shows exactly what needs to be implemented, and the test cases provide good coverage for both constant and non-constant expressions within ExpressionWrapper.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13028": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some gaps. The user clearly describes the problem: having a model field named \"filterable\" causes NotSupportedError in querysets, provides concrete model code showing the setup, mentions the error occurs when using the field in \"query filters\", and shows that renaming fixes it. However, the description lacks the exact error message, specific query that fails, and complete stack trace. An experienced engineer could reasonably infer that the issue is with Django's filterable attribute check incorrectly being applied to model fields rather than just query expressions, and could work toward a solution that distinguishes between the two cases.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15min-1hour fix. The issue is in Django's query system incorrectly checking the \"filterable\" attribute on model fields when it should only check it on query expressions. The solution requires: 1) Understanding Django's query filtering mechanism and the check_filterable method, 2) Recognizing the fix needs to distinguish between model fields and query expressions using hasattr(expression, 'resolve_expression'), 3) Adding the condition to the existing check. The code change itself is minimal (3-4 lines) but requires some Django ORM knowledge to understand why the original logic was wrong.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-scoped to Django's query system, the solution is testable with the provided test case, and the issue represents a legitimate bug that should be fixed. The sample is suitable for benchmarking coding ability as it tests understanding of Django's ORM internals and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13112": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the problem: makemigrations crashes when dealing with ForeignKey references to models in apps with mixed-case names (like 'DJ_RegLogin'). The issue states that ForeignKey to Category references 'dj_reglogin.category' but the app is installed as 'DJ_RegLogin', and this worked in Django 3.0 but fails in 3.1b1. However, the description lacks some important details: (1) It doesn't show the actual error message or stack trace from the crash, (2) It doesn't provide the specific model definitions or INSTALLED_APPS configuration, (3) It doesn't show exactly how the ForeignKey is defined. Despite these gaps, an experienced Django developer could reasonably infer that this is about case sensitivity handling in foreign key references during migration generation, and the solution would likely involve preserving proper case handling for app labels while lowercasing only the model name part.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the gold patch, the solution is quite straightforward - it's a small change to the `deconstruct` method in `related.py` that properly handles case sensitivity when dealing with string-based foreign key references. The fix involves splitting the model reference on '.' and only lowercasing the model name part while preserving the app label case. An experienced Django developer familiar with the migrations system could identify that this is likely in the foreign key field's deconstruct method, locate the relevant code quickly, and implement the fix. The logic is not complex - it's essentially adding a conditional to handle dotted references differently from simple model names.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate Django bug with a clear technical solution. The test patch shows proper test coverage for the fix, testing mixed-case app labels with ForeignKey and ManyToManyField relationships. The issue represents a real-world scenario that Django developers might encounter.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in but provides a sensible interpretation of what's required. The core problem is clear: temporal subtraction (subtracting DateTimeField attributes) currently requires ExpressionWrapper with an explicit output_field, but it should work natively without this wrapper. The issue mentions \"triggers a FieldError about mixed types\" which indicates the current behavior, and states the goal is to \"support temporal subtraction natively without requiring ExpressionWrapper.\" However, the issue lacks specific examples of the failing code or detailed error messages, which would help clarify the exact scenarios that need to be fixed. An experienced engineer could reasonably infer that operations like F('completed') - F('assigned') should work directly without needing ExpressionWrapper(..., output_field=DurationField()).",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's ORM expression system, specifically how CombinedExpression handles different field types and when to apply temporal subtraction logic. The solution involves moving logic from as_sql() to resolve_expression() method and adding proper type checking. While the core concept is clear, it requires familiarity with Django's expression compilation system, understanding the difference between as_sql() and resolve_expression() phases, and ensuring the changes work across different database backends. The patch shows significant restructuring of the CombinedExpression class with about 40 lines changed, indicating this isn't a trivial fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-scoped to Django's ORM expression handling, the test cases clearly show the expected behavior (removing ExpressionWrapper requirements), and the solution is verifiable through the existing test suite. This is a good benchmark sample as it tests understanding of Django internals without being overly esoteric.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13297": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem: in Django 3.1, TemplateView.get_context_data()'s kwargs returns SimpleLazyObjects that cause crashes when filtering, specifically mentioning that kwargs.get returns a SimpleLazyObject for offer_slug that must be converted to string for get_object_or_404 to work. While the issue doesn't provide a complete minimal reproduction case or show the exact error message, it clearly identifies the root cause (SimpleLazyObject behavior change between Django 3.0 and 3.1) and the context where it occurs (TemplateView kwargs handling). An experienced engineer could reasonably understand that the solution involves modifying how SimpleLazyObjects are handled in the context data to ensure proper type conversion for downstream operations like get_object_or_404.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: 1) The engineer needs to understand Django's lazy object system and how SimpleLazyObject differs from the lazy() function, 2) They must locate the specific code in django/views/generic/base.py that wraps URL kwargs with SimpleLazyObject, 3) They need to understand the deprecation warning mechanism and why the change from SimpleLazyObject to lazy() with type preservation is necessary, 4) The solution involves understanding the subtle difference between these two lazy evaluation approaches and their impact on type checking/filtering operations. While not extremely complex, it requires knowledge of Django internals and careful consideration of backward compatibility.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined within Django's framework, the test case clearly validates the fix by ensuring filtering operations work correctly with the modified context data, and the solution demonstrates a clean approach to resolving type-related issues with lazy objects.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13406": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clearly described: when a Django queryset with values()/values_list() and annotate() is pickled and then recreated, it returns model instances instead of dictionaries, breaking the expected behavior. The issue explains that it should return dictionaries but instead returns models with broken internal state. However, there are some details that could be clearer - the exact steps to reproduce aren't provided as a minimal example, and the internal mechanics of why this happens aren't fully explained. An experienced engineer would need to understand Django's queryset pickling mechanism and how values() affects the iterable class to solve this properly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is quite simple: adding just 2 lines of code in the query setter to check if values_select exists and set the _iterable_class to ValuesIterable accordingly. The core insight needed is understanding that when a query is pickled and recreated, the queryset loses track of what iterable class it should use. An experienced Django developer familiar with the codebase would likely identify this pattern relatively quickly by examining how values() works and what gets lost during pickling/unpickling. The fix itself is minimal and elegant.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is focused, the solution is clean and minimal, and the test coverage appears comprehensive. This is a good example of a Django ORM edge case that requires understanding the framework internals but has a straightforward fix once identified.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13449": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that require interpretation. The core problem is clearly stated: \"Lag() with DecimalField crashes on SQLite\" due to incorrect CAST() placement in the generated SQL. The issue provides key contextual information including the Django version (3.0.7), database (SQLite), specific functionality (Window expressions with Lag on DecimalField), and mentions a workaround (using output_field=FloatField()). However, it lacks crucial details like the actual error message or SQL syntax that's being generated incorrectly. The phrase \"CAST() is applied only to the LAG() call rather than the entire OVER clause\" gives a strong hint about the solution direction, but an engineer would need to investigate the codebase to understand the exact SQL generation logic and determine the proper fix.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) Understanding the problem requires familiarity with Django's ORM, window functions, and SQLite-specific decimal handling; (2) The engineer needs to trace through Django's SQL generation code to understand how Window expressions and DecimalField casting interact; (3) The solution involves creating a specialized as_sqlite() method in the Window class that correctly handles DecimalField by converting to FloatField before SQL generation - this requires understanding Django's database backend architecture and the SQLiteNumericMixin pattern; (4) The fix touches core ORM functionality and requires careful consideration of edge cases. While not extremely complex, it's more than a simple bug fix and requires architectural understanding of Django's database abstraction layer.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is database-specific (SQLite) which is common in Django development, and the solution follows established Django patterns for handling database-specific behavior. The test coverage is appropriate and the fix is contained within the ORM layer.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13568": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some Django-specific knowledge to fully understand. The core problem is clear: the auth.E003 system check incorrectly flags USERNAME_FIELD as non-unique when it's covered by a UniqueConstraint instead of having unique=True directly on the field. The issue explains the motivation (avoiding extra implicit indexes on PostgreSQL) and suggests the solution direction (checking Model._meta.constraints). However, some blanks need to be filled in: understanding Django's authentication system checks, the difference between field-level unique=True vs UniqueConstraint, and how total_unique_constraints works. An experienced engineer familiar with Django would understand what needs to be implemented, but someone unfamiliar with Django's auth system might need to research these concepts.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because: (1) Understanding the problem requires familiarity with Django's authentication system checks, the difference between field.unique=True and UniqueConstraint, and why someone would prefer one over the other. (2) The solution requires locating the auth system check code in django/contrib/auth/checks.py and understanding the existing logic. (3) The implementation involves modifying the conditional logic to check both the field's unique attribute AND whether the field is covered by a UniqueConstraint in Model._meta.total_unique_constraints. (4) Writing comprehensive tests requires understanding the test structure and creating test cases for different scenarios. While the actual code change is small (adding an 'and not any(...)' clause), understanding the Django internals and getting the logic right takes significant time for someone not immediately familiar with this part of the framework.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This issue is suitable for the benchmark. It tests understanding of Django's authentication system, model constraints, system checks, and database optimization considerations. The solution is well-defined and testable, making it a good coding challenge for developers working with Django.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13807": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the root cause: missing backticks around table names in SQL statements when using PRAGMA commands in SQLite. The issue description provides: 1) Exact file location (django/db/backends/sqlite3/base.py line 327), 2) Specific function name (check_constraints), 3) Precise technical details about the problem (missing backticks around %s in SQL statements), 4) Code context showing the problematic lines, and 5) Affected Django versions (3.1.0, 3.1.2). The problem occurs when table names are SQL keywords, which would fail without proper quoting. An experienced engineer would clearly understand that the solution involves properly quoting table names using Django's quote_name method.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. The issue description clearly identifies the root cause and the exact locations that need to be fixed. The solution involves replacing direct string formatting with Django's self.ops.quote_name() method to properly quote table and column names. An experienced Django developer would recognize this as a standard SQL injection prevention pattern. The fix requires changes to only a few lines in one function, and the pattern is consistent across multiple similar statements in the same function.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample as it tests knowledge of SQL injection prevention, Django ORM internals, and SQLite-specific behavior. The issue is well-documented, the solution is clear, and the test case properly validates the fix by creating a model with SQL keywords as table and column names.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13810": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific technical problem with Django's middleware handling in ASGI context. The core issue is clearly stated: when MiddlewareNotUsed is raised, Django skips further processing but still overwrites the handler variable with adapt_method_mode() results, which \"poisons\" the middleware chain on subsequent passes. The issue provides concrete context (ASGI, django-debug-toolbar, custom synchronous middleware) and explains the problematic behavior (last middleware returns HttpResponse synchronously instead of a coroutine). However, there are some gaps - the exact reproduction steps aren't provided, and the specific middleware configurations that trigger this aren't detailed. Despite these gaps, an experienced engineer familiar with Django's middleware system could reasonably interpret what needs to be fixed based on the description of the handler variable pollution issue.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires understanding Django's complex middleware loading system, ASGI/async handling, and the specific interaction between MiddlewareNotUsed exceptions and handler adaptation. The solution involves modifying the load_middleware method in django/core/handlers/base.py to prevent handler variable overwriting when MiddlewareNotUsed is raised. While the actual code change is relatively small (introducing adapted_handler variable and conditional assignment), it requires deep understanding of Django's middleware architecture, async/sync compatibility, and the subtle timing of when handler variables should be updated. An engineer would need 1-4 hours to trace through the middleware loading logic, understand the ASGI context implications, design the fix, and write appropriate tests.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue and solution are technically sound and appropriate for a coding benchmark. The fix demonstrates important concepts around exception handling, variable scoping, and async/sync compatibility in web frameworks. The test patch shows proper async testing methodology. This would be a good sample for evaluating an engineer's ability to debug complex middleware interactions and implement clean solutions to subtle timing issues.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14017": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem with operator precedence/commutativity in Django's query system. It clearly states that `Exists(...) & Q(...)` works but `Q(...) & Exists(...)` raises a TypeError, and that the & and | operators should be commutative on Q-Exists pairs. The issue also provides a reasonable hypothesis about the cause (missing __rand__ definition). While the exact implementation details aren't specified, an experienced engineer familiar with Python's operator overloading and Django's query system would understand that this is about making the logical operators work bidirectionally between Q and Exists objects. The gold patch confirms this interpretation by modifying the _combine method in Q to accept objects with a 'conditional' attribute (which Exists has), not just Q instances.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-defined and the solution is relatively straightforward once you understand Python's operator overloading. The gold patch shows it's a simple one-line change in the _combine method of the Q class in django/db/models/query_utils.py - changing the isinstance check to also accept objects with a 'conditional' attribute. An experienced engineer would need some time to: 1) locate the relevant code in Django's query system, 2) understand how Q and Exists objects interact, 3) identify that the issue is in the _combine method's type checking, and 4) implement the fix to allow objects with conditional=True. The change itself is minimal but requires understanding the codebase structure and the relationship between Q and Exists objects.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is reasonable in scope for a coding benchmark, and the test cases provided adequately verify the fix. The issue demonstrates good software engineering principles around operator commutativity and type checking.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14140": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is vague and lacks essential context for understanding the problem. While it mentions that \"Q objects with 1 child are treated differently during deconstruct\" and this \"causes issues when deconstructing Q objects with a non-subscriptable child,\" it doesn't explain what Q objects are, what deconstruct means in this context, or provide concrete examples of the problematic behavior. The description references a patch URL but instructs not to follow external links, leaving the reader without crucial context. There's no error message, reproduction steps, or clear explanation of what the current vs. expected behavior should be. An engineer would need significant domain knowledge of Django's Q objects and ORM internals to understand what needs fixing.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the patch, this appears to be a 1-4 hour task. The actual code change is relatively simple (removing special case logic in the deconstruct method), but understanding the problem requires significant investigation. An engineer would need to: (1) understand Django's Q object system and how deconstruction works, (2) figure out what \"non-subscriptable child\" means and when this causes crashes, (3) analyze the current special-case logic for single-child Q objects, (4) understand the backward compatibility implications mentioned, and (5) write comprehensive tests. The solution involves removing about 10 lines and adding 4 lines, but the research and testing effort to ensure correctness would be substantial.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "Yes, there are several major issues that make this unsuitable for a coding benchmark: (1) The problem description is too vague and context-dependent - an engineer would need deep Django ORM knowledge to even understand what's being asked. (2) The issue mentions a patch URL for context but we're instructed not to follow external links, creating an impossible situation. (3) There's no reproduction case or concrete example of the crash/problem behavior. (4) The issue discusses backward compatibility concerns that require understanding undocumented Django internals. (5) Without the ability to see the referenced patch or ask clarifying questions, this becomes more of a Django archaeology exercise than a coding problem. This would test Django expertise more than general coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14238": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when specifying a custom AutoField subclass as DEFAULT_AUTO_FIELD, Django raises a ValueError claiming it must subclass AutoField because the subclass check excludes subclasses of BigAutoField and SmallAutoField. The issue provides the exact location of the problem (AutoFieldMeta.__subclasscheck__) and suggests the specific fix needed (allowing subclasses of those classes in the _subclasses property). The description is technical but precise, giving an experienced Django developer all the information needed to understand and fix the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location (AutoFieldMeta.__subclasscheck__ method) and the solution is straightforward: changing `subclass in self._subclasses` to `issubclass(subclass, self._subclasses)` in the __subclasscheck__ method. The fix is just a one-line change that replaces a membership check with a proper subclass check. An experienced Django developer would quickly understand that the current implementation only checks for exact class matches in _subclasses, but doesn't handle inheritance properly for subclasses of BigAutoField and SmallAutoField.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. The problem is well-defined, the solution is clear and testable, and it represents a realistic Django development scenario that would be suitable for evaluating coding ability. The accompanying tests properly verify that both direct subclasses and inheritance work correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14351": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem with Q object OR operations in Django that worked in version 2.2.5 but breaks in version 3.2. It mentions that when using `agent__property_groups__in` versus `agent__property_groups__id__in`, there's different behavior in how Q object aliases are set up, with the former causing `get_default_columns` to include every field while the latter includes only the id. This results in a \"subquery must return only one column\" error. While the issue could benefit from a concrete code example demonstrating the problem, there's enough technical detail about the specific Django ORM behavior change and the error condition to allow an experienced Django developer to understand what needs to be fixed. The provided patch and test also help clarify the expected solution scope.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a Django ORM internals issue that requires understanding how Q objects, lookups, and query compilation work. The solution involves adding a `get_group_by_cols` method to handle subquery column selection properly. An experienced engineer would need 1-4 hours to: (1) reproduce the issue and understand the difference in behavior between Django versions, (2) trace through the Django ORM code to understand how `get_default_columns` and query compilation works, (3) identify that the issue is in the `In` lookup class in `lookups.py`, and (4) implement the fix that ensures subqueries only return necessary columns. The patch shows it's not a trivial one-line fix but requires understanding Django's query internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability as it tests knowledge of Django ORM internals, debugging skills for version compatibility issues, and the ability to implement a targeted fix in a complex codebase. The provided test case gives clear validation criteria.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14493": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: ManifestStaticFilesStorage crashes when max_post_process_passes = 0 because the variable 'substitutions' is only initialized if the loop runs at least once. The issue points to specific code line (246-257 in storage.py) and explains the motivation for setting max_post_process_passes to 0. However, the exact crash behavior and error message aren't described, requiring the engineer to understand the code flow and identify that 'substitutions' would be undefined if the loop doesn't execute. The reference to Django ticket #21080 provides context but the core technical issue is understandable from the main description.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue is a classic uninitialized variable problem - 'substitutions' is only set inside a loop that may not execute when max_post_process_passes = 0. The solution is simple: initialize 'substitutions = False' before the loop. Looking at the gold patch confirms this - it's literally a one-line addition. An experienced engineer would quickly identify this pattern, understand the code flow, and implement the fix. The main time would be spent understanding the Django staticfiles system and ensuring the fix doesn't break existing functionality, but the actual coding is trivial.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is technical, has a clear correct solution, and the test patch appropriately verifies the fix by creating a storage class with max_post_process_passes = 0 and ensuring collectstatic runs without crashing. The issue requires understanding of Python variable scoping and Django's staticfiles system but doesn't require domain-specific knowledge beyond what a competent Python/Django developer should have.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14580": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is exceptionally well-specified. It clearly describes: (1) the specific bug - missing import statement in generated migrations, (2) the exact error that occurs - NameError for 'models' not being defined, (3) the expected vs actual behavior, (4) the specific scenario that triggers it - models using custom fields and mixins, (5) the suspected location of the bug in django.db.migrations.writer. The description provides all necessary information for an engineer to understand the problem, reproduce it, and implement a fix. The issue is concrete, specific, and actionable.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue is well-defined and the solution, as shown in the patch, is a simple one-line change adding the missing import to the special_cases list in the TypeSerializer class. An experienced engineer would need minimal time to: (1) understand that migrations are missing an import, (2) locate the serializer code that handles models.Model references, (3) add the required import to the serialization logic. The fix requires understanding Django's migration serialization system but the change itself is straightforward - just adding the import requirement to an existing list.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal benchmark sample - the problem is clearly described, the solution is focused and testable, and it represents a real-world bug that requires understanding of Django's internals. The issue has appropriate scope and complexity for evaluating coding ability.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14672": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some blanks to fill in. The core problem is clearly stated: there's a missing `make_hashable` call on `through_fields` in `ManyToManyRel` that causes issues with proxy models. The issue description identifies the specific location (ManyToManyRel class), explains the context (identity property in 3.2, hash derivation from identity tuple), and provides a clear solution direction. However, there are some gaps: it doesn't specify the exact file location of ManyToManyRel, the specific error symptoms are vague (\"it only fails on checking proxy model\"), and the connection between proxy models having different check counts and the hash issue isn't fully explained. An experienced engineer familiar with Django's codebase could reasonably infer these details and implement the fix based on the pattern described for limit_choices_to.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The solution is straightforward - adding a single `make_hashable()` call around `self.through_fields` in the ManyToManyRel.identity property. The patch shows it's literally a one-line change in django/db/models/fields/reverse_related.py. An experienced engineer would need some time to: 1) Locate the ManyToManyRel class in the Django codebase, 2) Find the identity property method, 3) Understand why through_fields needs to be hashable (similar to limit_choices_to), and 4) Apply the same pattern. The conceptual understanding is simple since the issue explicitly states the solution and references the existing pattern used for limit_choices_to.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-contained, has a clear technical solution, and the test case validates the fix appropriately by checking that hash values work correctly for inherited models with M2M fields using through_fields as lists.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14787": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in but provides a sensible interpretation of what needs to be fixed. The core problem is clearly stated: \"method_decorator() should preserve wrapper assignments\" and explains that \"the function that is passed to the decorator is a partial object and does not have any of the attributes expected from a function i.e. __name__, __module__ etc.\" While the issue doesn't explicitly mention Django or provide code examples showing the problem, an experienced engineer familiar with Python decorators would understand this is about preserving function metadata (like __name__, __module__) when wrapping functions. The title and description together provide enough context to understand that method_decorator() is not properly preserving these attributes when it creates partial objects, and the solution would involve using functools.wraps() or similar mechanisms to copy the metadata.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is a simple one-line change: wrapping the partial object with wraps(method) to preserve function attributes. An experienced engineer would quickly identify that the issue is about missing function metadata on partial objects and know that functools.wraps() is the standard solution. The change is minimal (adding wraps(method) around the partial call) and doesn't require substantial research or code restructuring. The main time would be spent understanding the existing method_decorator implementation and writing/running tests to verify the fix works correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-understood in the Python community (preserving function metadata in decorators), the solution is straightforward, and the test case clearly validates that the fix works by checking that __name__ and __module__ attributes are preserved when using method_decorator.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15104": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The author clearly describes the problem: a KeyError occurs with Django's migration autodetector when using a custom ForeignKey field that hardcodes its 'to' argument and removes it from deconstructed kwargs. They provide a specific solution suggestion: changing `del deconstruction[2]['to']` to `deconstruction[2].pop('to', None)` in the autodetector. However, there are some gaps - the exact location of this line isn't specified, and the full error traceback isn't provided. An experienced engineer would need to locate the relevant code in django/db/migrations/autodetector.py and understand the migration system, but the core problem and suggested fix are clear enough to work with.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue provides a very specific solution: changing a `del` statement to use `pop()` with a default value. The actual code change is minimal (one line), but it requires understanding Django's migration autodetector system and locating the correct file and method. An experienced engineer would need to: 1) Find the autodetector.py file, 2) Locate the method containing the problematic line, 3) Understand why the KeyError occurs when 'to' key doesn't exist, 4) Apply the suggested fix. The solution is straightforward once the location is found, and the test patch shows this affects the `only_relation_agnostic_fields` method.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample because it tests understanding of Django's migration system, debugging skills to locate the problematic code, and the ability to apply a defensive programming fix. The issue has a clear problem statement, a specific suggested solution, and the test case validates the fix works for custom ForeignKey fields that remove the 'to' parameter from their deconstruction.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is well-specified with some blanks to fill in. It provides a clear technical explanation of the bug (alias collisions during QuerySet OR operations causing AssertionError in Query.change_aliases), identifies the root cause (overlapping mappings like T4\u2192T5 and T5\u2192T6), and suggests a specific solution approach (ensuring new aliases don't intersect with existing ones by passing rhs.alias_map into Query.join/Query.table_alias). However, an engineer would need to work out the exact implementation details, such as how to modify the bump_prefix method and where exactly to call it in the combine method. The issue also requests documentation improvements that aren't fully specified.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Deep understanding of Django's ORM query system, particularly the Query class, alias management, and QuerySet combining logic; (2) Analyzing the complex interaction between Query.combine, Query.table_alias, and Query.change_aliases methods; (3) Implementing a solution that modifies the bump_prefix method to accept an exclude parameter and correctly calling it during combine operations; (4) Understanding the assertion logic and adding appropriate documentation. The fix involves modifying multiple methods across the Query class and requires careful consideration of edge cases in alias management. While not extremely complex, it's non-trivial and requires substantial domain knowledge of Django's internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is technical but well-defined, the solution approach is reasonable, and the test case provided in the patch demonstrates the issue clearly. This would make a good benchmark problem for testing understanding of complex ORM systems and alias management.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15277": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly identifies the problem: CharField.__init__ always adds a MaxLengthValidator even when max_length is None, which is unnecessary. The issue provides specific performance benchmarks (8.1 \u00b5s \u2192 6.96 \u00b5s \u2192 5.86 \u00b5s improvement) and explains the exact optimization goal. It also provides the specific proposed solution: mirroring BinaryField.__init__ by only appending the validator when max_length is set, with the exact code change needed. The context is clear - this relates to Django's Value._resolve_output_field() method and CharField class. An experienced engineer would understand exactly what needs to be changed and why.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward micro-optimization that requires minimal time. The issue clearly states the exact change needed: modify CharField.__init__ to conditionally add the MaxLengthValidator only when max_length is not None, following the pattern used in BinaryField.__init__. The change involves adding a simple if condition around one line of code in django/db/models/fields/__init__.py. An experienced engineer familiar with Django would understand the codebase structure and be able to locate the CharField class quickly. The solution is a 2-line change (adding if condition and indenting the existing validator append line), making this a 15-30 minute task including testing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample as it represents a realistic micro-optimization task that tests understanding of Django's field system, code reading comprehension, and ability to make targeted performance improvements. The issue is well-documented with clear performance metrics and a specific solution approach.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15280": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified but has some blanks to fill in. It clearly describes the problem: when using User.objects.only(\"email\") with prefetching Profile and a nested prefetch on Profile's User limited to only(\"kind\"), accessing the kind field still results in a database query despite get_deferred_fields() showing \"kind\" was deferred correctly. The issue provides concrete model examples (User and Profile classes) and explains that Django is incorrectly inheriting deferred fields from the outer User queryset. However, there are some ambiguities - the exact expected behavior isn't fully spelled out (should the nested prefetch override the deferred fields from the parent query?), and the phrase \"Is this a bug, and how might it be fixed?\" suggests uncertainty about whether this is intended behavior. An experienced engineer could reasonably interpret this as needing to ensure that nested prefetches don't inherit deferred field restrictions from parent querysets, but some clarification would be helpful.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty level issue. Looking at the provided patch, the solution involves understanding Django's ORM internals, specifically the prefetch mechanism in related_descriptors.py. The fix is relatively small (just a few lines checking if a field is cached before setting attributes), but requires deep understanding of Django's caching behavior, the relationship between deferred fields and prefetching, and how the ORM manages object relationships. An engineer would need to: 1) Understand the prefetch_related mechanism, 2) Debug why deferred fields from parent queries are affecting nested prefetches, 3) Identify that the issue is in get_prefetch_queryset where setattr is called unconditionally, and 4) Implement the caching check. This requires navigating Django's complex ORM codebase and understanding the interaction between multiple systems (deferred fields, prefetching, caching).",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's ORM, has a clear reproducible scenario, and the test case provided gives good guidance on expected behavior. The issue is technical enough to be a good coding challenge while remaining solvable with Django ORM knowledge.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15315": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some domain knowledge to fully understand. The problem is clearly stated: Field.__hash__() changes value when a field is assigned to a model class, which breaks the immutability principle for hash functions. The issue references a specific PR (#31750) that introduced the bug and provides context about why this is problematic (breaks use in dictionaries). However, to fully understand the solution, an engineer would need to: 1) Understand Django's model field system and how fields get assigned to model classes, 2) Know what the __hash__ method should return for immutability, 3) Understand the trade-offs mentioned regarding the equality fix from #31750. The suggestion to \"revert the __hash__ change from #31750\" provides clear direction, though the engineer would need to investigate what that change was and ensure reverting it doesn't break the original equality fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that takes 15-60 minutes. The solution involves simplifying the __hash__ method from a complex tuple that includes model metadata to just using the creation_counter. An experienced engineer would need to: 1) Understand the current __hash__ implementation and why it's problematic, 2) Recognize that creation_counter is an immutable property suitable for hashing, 3) Make the simple code change to return hash(self.creation_counter) instead of the complex tuple, 4) Write a test to verify the hash remains constant before and after field assignment. The actual code change is minimal (removing 4 lines, changing to 1 line), and the logic is straightforward once you understand that hash values must remain constant for an object's lifetime.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear problem statement and solution. The test case effectively demonstrates the issue and validates the fix. The change is backwards compatible and addresses a fundamental principle of hash function implementation in Python. An engineer with Django experience should be able to solve this efficiently.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15375": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly states that using aggregate() with a 'default' parameter after annotate() crashes in Django 4.0.1, and mentions this happens on both PostgreSQL and SQLite. The reporter also notes that the long form using Coalesce works as expected, which provides a helpful clue about the solution direction. However, the issue lacks crucial details like the actual error message/stack trace, specific code examples that reproduce the crash, and the exact expected behavior. While an experienced Django developer could likely infer what needs to be fixed (the default parameter handling in aggregates after annotation), the lack of concrete reproduction steps and error details means there's some interpretation required.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. Looking at the gold patch, the solution is quite simple - just 2 lines added to preserve the is_summary attribute when wrapping with Coalesce. The core issue is that when an aggregate with a default parameter is processed after annotation, the is_summary attribute gets lost during the Coalesce wrapping. An experienced Django developer familiar with the ORM internals would likely identify this quickly by examining the resolve_expression method in aggregates.py and understanding how the default parameter creates a Coalesce wrapper. The fix itself is trivial once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues that would prevent this from being used in the benchmark. While the issue description could be more detailed with error messages and reproduction code, it provides enough context for an experienced developer to understand and solve the problem. The solution is surgical and well-contained within Django's aggregation system.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15380": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The title clearly states the problem: \"Migration autodetector crashes when renaming a model and field in a single step.\" This gives a clear indication of what's happening - there's a crash in Django's migration autodetector when both a model and field are renamed simultaneously. The issue references a specific regression commit (aa4acc164d1247c0de515c959f7b09648b57dc42), which provides context about when the problem was introduced. However, the description lacks crucial details like the exact error message, stack trace, or reproduction steps. An experienced Django developer familiar with the migration system would understand this refers to the autodetector.py module and involves the logic for detecting field and model renames, but someone less familiar might need to investigate further to understand the specific crash scenario.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The actual solution is a simple one-line change in django/db/migrations/autodetector.py, changing 'old_model_name' to 'model_name' in line 827. The bug is in the generate_renamed_fields method where it incorrectly uses the old model name when looking up the new model state. An experienced Django developer would need some time to understand the migration autodetector logic, trace through the code to find where the crash occurs when both model and field are renamed, and identify that the wrong variable is being used. However, once the issue is located, the fix is straightforward. The test patch also shows this is a well-contained issue that can be verified with a relatively simple test case.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark candidate as it tests understanding of Django's migration system, debugging skills to locate the source of a crash, and the ability to write a simple but precise fix. The issue is realistic and represents the type of bug that occurs in real Django development.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15503": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: JSONField() lookups (has_key, has_keys, has_any_keys) fail to handle numeric keys on SQLite, MySQL, and Oracle databases, while working correctly on PostgreSQL. The description includes specific version information and confirms testing on both SQLite and PostgreSQL backends. However, there are some gaps that require interpretation: (1) The issue mentions \"numerical keys\" in the description but doesn't provide concrete examples of what numeric keys are being used or how they're failing, (2) While it states the lookups \"fail to find the keys,\" it doesn't specify the exact error behavior (e.g., does it return no results, throw an exception, etc.), (3) The issue mentions MySQL and Oracle in the title but only provides testing details for SQLite vs PostgreSQL. Despite these gaps, an experienced engineer could reasonably infer that this is about JSON path compilation treating numeric keys differently across database backends, and the solution would likely involve standardizing how numeric keys are handled in the JSON path compilation logic.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue for several reasons: (1) Database backend differences: Understanding how different database engines (SQLite, MySQL, Oracle vs PostgreSQL) handle JSON path queries requires familiarity with Django's database abstraction layer and JSON field implementations, (2) JSON path compilation logic: The solution involves modifying the compile_json_path_final_key method and understanding how Django compiles JSON paths for different database backends, (3) Multiple related lookups: The fix affects has_key, has_keys, has_any_keys, and related functionality like KeyTransformIsNull, requiring understanding of the inheritance hierarchy and lookup system, (4) Cross-backend compatibility: Ensuring the solution works across multiple database engines requires careful consideration and testing, (5) The patch shows this isn't a trivial fix - it involves creating a new HasKeyOrArrayIndex class, modifying multiple methods, and understanding the distinction between object keys and array indices in JSON path compilation. However, it's not extremely esoteric and doesn't require massive code changes (the patch is focused and under 100 lines), making it a solid intermediate-level difficulty.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with using this sample. The issue is well-scoped to Django's JSONField functionality, has a clear technical problem with database backend differences, and includes comprehensive tests that verify the fix works correctly. The solution requires good understanding of Django's ORM internals but is achievable for an experienced software engineer within the expected timeframe.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15525": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: \"loaddata fails on non-default database when natural keys uses foreign keys.\" The author includes concrete code examples showing the Author and Book models with their natural key implementations, and explains that the issue occurs specifically when using a second database (non-default). The description states that it \"works in the default database, but when I use a second database, then I get an exception.\" While the exact exception details aren't provided in the main issue text, the problem is well-defined enough for an experienced engineer to understand: the natural key deserialization process fails when operating on a non-default database. The models show the relationship structure (Book has a foreign key to Author, both define natural keys, and Book's natural key depends on Author's), which provides sufficient context to understand the data structure involved.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's serialization internals and the natural key resolution process across multiple databases. The solution involves modifying the `build_instance` function in `django/core/serializers/base.py` to properly set the database state on the model instance before calling `natural_key()`. The engineer needs to: 1) Understand how Django's loaddata command works with natural keys, 2) Trace through the serialization code to find where the database context gets lost, 3) Identify that the issue is in the `build_instance` function where a temporary model instance is created without proper database state, and 4) Implement the fix by setting `obj._state.db = db`. While the actual code change is small (3 lines), finding the root cause requires navigating Django's serialization framework and understanding the model state management system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained Django framework issue with a clear problem statement and specific reproduction scenario. The test demonstrates the exact use case (loading natural key fixtures on a non-default database), and the solution is focused and minimal. The issue would be suitable for evaluating an engineer's ability to debug Django internals and understand database state management.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15695": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides enough context for a sensible interpretation. It describes that RenameIndex() crashes when re-applied on an unnamed index that has moved backward and forward. The key insight is that \"re-applying RenameIndex() crashes\" suggests the operation should be idempotent but currently isn't. Looking at the gold patch confirms this interpretation - it adds a check to return early if old_index.name == self.new_name, making the operation a no-op when names match. While the description doesn't explicitly state the expected behavior (that re-applying should be safe), this is a reasonable interpretation given the context of database migrations needing to be re-runnable.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution is conceptually simple: add a guard clause to check if the old and new index names are the same and return early if so. The actual implementation is just 3 lines of code in the database_forwards method. An experienced engineer would need some time to understand the Django migrations system and locate the RenameIndex operation in django/db/migrations/operations/models.py, but once they understand that the issue is about making the operation idempotent, the fix is straightforward. The test patch also shows this is a focused change that doesn't require extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean example of a bug fix that requires understanding the codebase context but has a clear, focused solution. The issue is domain-specific to Django migrations but the core problem (making an operation idempotent) is a common software engineering pattern that most experienced developers would recognize.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15732": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem: there's an erroneous unique_together constraint on a model's primary key that cannot be dropped by a migration. The migration fails because it finds multiple unique constraints on the same column (both the primary key constraint and the unique_together constraint) when it expects only one. While the issue doesn't provide the exact model definition or migration code that's failing, there's enough information to understand the core problem - the migration system needs to be able to handle cases where multiple constraints exist on the same field(s) and specifically target the unique_together constraint for removal. The mention of PostgreSQL provides helpful context about the database backend. An experienced Django developer would understand this is about Django's migration system and schema operations.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: (1) It requires understanding Django's migration system and schema operations, particularly how unique_together constraints are handled. (2) The solution involves modifying the _delete_composed_index method to better handle constraint selection when multiple constraints exist on the same fields. (3) The fix requires adding logic to prioritize the unique_together constraint over other types of unique constraints (like primary keys). (4) It involves working with database schema operations and constraint naming conventions. (5) The solution spans multiple methods in the schema backend and requires understanding of how Django generates constraint names. While not extremely complex, it requires substantial knowledge of Django internals and careful consideration of edge cases.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This issue is suitable for the benchmark. It represents a real-world Django migration problem that requires understanding of ORM internals and database schema operations. The solution is testable through the migration system, and the tests provided verify the correct behavior. The issue is specific to Django but represents the kind of complex framework issue that experienced developers encounter.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15814": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. The reporter clearly describes: (1) the specific scenario - using QuerySet.only() after select_related() on proxy models causes a crash, (2) the environment details (Windows 10, Python 3.10, Django 4.0.5), (3) the exact location of the problem (django/db/models/sql/query.py line 745), and (4) a proposed solution (replacing `opts = cur_model._meta` with `opts = cur_model._meta.concrete_model._meta`). However, there are some blanks: the issue doesn't provide the actual error message/traceback, doesn't show a minimal code example that reproduces the problem, and doesn't explain why proxy models specifically cause this issue. Despite these gaps, an experienced Django developer could reasonably infer what's happening - proxy models inherit from concrete models but have their own _meta, and the query optimization code needs to work with the concrete model's metadata rather than the proxy's metadata. The proposed solution provides a strong hint about the intended fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This falls into the 15 min - 1 hour category. The issue provides the exact file and line number where the problem occurs, along with a specific proposed solution. An experienced Django developer would need some time to: (1) understand Django's proxy model system and how _meta works, (2) understand why using the concrete_model._meta is necessary in this context, (3) verify the proposed fix is correct and doesn't break other functionality, and (4) potentially write or understand the test case. The actual code change is minimal (adding one line), but requires domain knowledge about Django's ORM internals. The fix shown in the gold patch is exactly what the issue reporter suggested, which confirms this isn't a complex debugging exercise but rather applying a known solution.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is specific, the solution is targeted, and the test case validates the fix appropriately. This represents a good coding challenge that tests understanding of Django ORM internals and proxy models.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15930": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in but there is a sensible interpretation. The core problem is clearly stated: \"Case() crashes when used with ~Q(pk__in=[])\". The context explains that ~Q(pk__in=[]) is a sentinel value returned by application code, which helps understand why this is important. However, the issue lacks specific details like: what kind of crash occurs (error message), in what exact context this happens, or a minimal reproduction example. An experienced engineer would need to infer that this likely relates to Django ORM's Case/When expressions and that the crash probably occurs during SQL compilation when an empty condition needs to be handled. The test patch confirms this interpretation by showing a test case using Case/When with ~Q(pk__in=[]).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The solution is quite straightforward - adding a simple conditional check in the Case.as_sql method to handle empty condition strings by replacing them with a True predicate. The patch shows only 5 lines of code added with clear logic and comments. An experienced engineer would need some time to: understand Django's ORM expressions system, locate the relevant Case class in django/db/models/expressions.py, understand how SQL compilation works for Case expressions, and recognize that empty conditions need special handling. However, once the problem is understood, the fix is simple and localized.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix in a specific Django ORM component. The problem is reproducible, the solution is clean and well-documented in the code, and the test case clearly validates the fix. The issue represents a real-world scenario that application developers might encounter.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15957": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem: prefetch() objects don't work with sliced querysets in Django. While the issue text is somewhat brief, it provides a clear use case - wanting to display categories with a few example objects from each category efficiently. The problem statement is concrete enough that an experienced Django developer would understand what needs to be fixed. However, there are some implementation details that would need to be figured out, such as how exactly the slicing should interact with the prefetching mechanism and what the expected behavior should be when both are used together.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a complex issue requiring deep understanding of Django's ORM internals, specifically the prefetch mechanism and query slicing. The solution involves creating a new function `_filter_prefetch_queryset` that handles sliced querysets by using window functions with row numbering, partitioning, and complex SQL operations. The patch shows changes across multiple parts of the related_descriptors.py file, involving sophisticated query manipulation with Window functions, RowNumber, and conditional predicates. This requires substantial knowledge of Django's query compilation, database operations, and the interaction between different ORM features. The solution is non-trivial and would require significant research into Django's codebase to understand how prefetching works internally.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined within Django's ecosystem, the solution is testable, and the issue represents a legitimate limitation in Django's ORM that would be valuable to fix. The comprehensive test cases show various scenarios (m2m forward/reverse, foreignkey reverse, ordering) that validate the solution works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15973": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem with Django migrations when defining a many-to-many relationship with a \"through\" model in a separate app. The user provides clear context about three apps (\"fonte\", \"fonte_variavel\", \"variavel\") and explains that migration works when the FonteVariavelModel is in the same app but fails with AttributeError when it's in its own app. The error message \"AttributeError: 'str' object has no attribute '_meta'\" gives a concrete indication of what's failing. While some implementation details need to be figured out, the core problem is well-defined: the migration system is incorrectly handling cross-app through model references, treating a model reference as a string instead of resolving it to the actual model class. The gold patch shows this is indeed a targeted fix in the autodetector's dependency resolution logic.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) Understanding the problem requires familiarity with Django's migration system, specifically how it handles many-to-many relationships and through models across apps. (2) The engineer needs to trace through the migration autodetector code to understand how model dependencies are resolved. (3) The error indicates a type mismatch where a string is being used instead of a model reference, requiring investigation of the resolve_relation function and related code. (4) The fix itself is small (changing one parameter in a function call), but identifying the exact location and understanding why this specific change resolves the cross-app reference issue requires debugging skills and Django internals knowledge. (5) Writing appropriate tests to verify the fix works for cross-app scenarios adds additional complexity. The gold patch shows it's a targeted 1-line fix, but reaching that solution requires substantial investigation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is reproducible, the error message is specific, and the solution can be validated through the migration system. The issue represents a legitimate Django framework bug that would benefit from being fixed.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16032": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks sufficient detail for a clear solution. The title mentions \"__in doesn't clear selected fields on the RHS when QuerySet.alias() is used after annotate()\" but provides no concrete example, error message, expected vs actual behavior, or reproduction steps. While the title gives some technical context about QuerySet operations (alias() after annotate() affecting __in lookups), it's unclear what \"doesn't clear selected fields\" means in practice, what the expected behavior should be, or how this manifests as a problem. An experienced engineer would need to do significant investigation to understand the actual issue and determine what constitutes a correct fix.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a complex Django ORM issue requiring deep understanding of QuerySet internals. The patch shows changes to multiple files (related_lookups.py and query.py) involving intricate query compilation logic. The solution involves removing a clear_select_clause() call, changing add_fields() to set_values(), and converting has_select_fields from a property to a boolean attribute. Understanding why these specific changes fix the interaction between annotate(), alias(), and __in lookups would require substantial research into Django's query compilation process, field selection mechanisms, and subquery handling. The issue is quite esoteric and the fix touches core ORM functionality that requires careful analysis to avoid regressions.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The issue description is too minimal to be useful for benchmarking coding ability. Without a clear problem statement, reproduction case, or expected behavior description, even experienced engineers would struggle to understand what needs fixing. The tests provide some clues about the intended functionality (annotation_and_alias_filter_in_subquery scenarios), but relying on tests to infer the problem statement defeats the purpose of evaluating whether someone can solve an issue from its description alone.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16136": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: \"When defining a simple View subclass with only an async 'post' method, GET requests to this view cause an exception.\" The title also gives important context about the specific error: \"object HttpResponseNotAllowed can't be used in 'await' expression\". While the issue doesn't provide a complete minimal reproduction example or show the exact stack trace, there is enough information to understand the core problem. The issue occurs in Django's async view handling when HTTP methods that aren't implemented (like GET when only POST is defined) trigger the http_method_not_allowed handler, which returns an HttpResponseNotAllowed object. In an async context, this object can't be awaited, causing the exception. An experienced Django developer would be able to identify that this is likely an issue in the View base class's http_method_not_allowed method not properly handling async vs sync contexts. There are some blanks to fill in (like the exact view implementation that triggers this), but the core problem and expected behavior are clear enough for a meaningful solution attempt.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution patch, this is a relatively straightforward fix that takes 15 minutes to 1 hour. The problem is localized to a single method (http_method_not_allowed) in django/views/generic/base.py. The solution involves checking if the view is async (using the existing view_is_async attribute) and wrapping the HttpResponseNotAllowed response in an async function if needed. This requires understanding Django's async view pattern and how coroutines work, but the actual code change is small (about 10 lines) and follows a clear pattern that's likely used elsewhere in the Django codebase. An experienced engineer familiar with Django's async patterns could identify and implement this fix relatively quickly once they understand the problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue that tests a specific aspect of Django's async view handling. The solution is clean and follows Django's established patterns for handling async/sync differences. The test case provided is comprehensive and clearly validates the fix. This sample would be good for evaluating understanding of async/await patterns in Python and Django-specific knowledge about view handling.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16429": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It identifies: (1) the specific function `timesince()` that's failing, (2) the exact conditions when it fails (USE_TZ=True and >1 month interval), (3) the root cause (mismatch between offset-naive and offset-aware datetimes), (4) the specific location where the problem occurs (django/utils/timesince.py), and (5) a clear description of what needs to be fixed (the pivot datetime construction doesn't account for the input's tzinfo). The description even suggests the solution approach. An experienced engineer would have sufficient information to understand and fix this timezone-related bug.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location and cause. The solution involves adding a single parameter `tzinfo=d.tzinfo` to the datetime constructor in the pivot creation, which is a small, targeted change. While it requires understanding Django's timezone handling and datetime operations, the fix itself is straightforward once the problem is understood. The gold patch confirms this - it's literally a one-line addition to pass the tzinfo parameter.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-suited sample for the benchmark. The issue description is clear, the solution is focused and testable, and it represents a realistic bug that developers might encounter when working with timezone-aware datetime operations in Django. The test case properly validates the fix by extending existing tests to cover timezone-aware scenarios.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16454": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that Django's CommandParser class (a subclass of argparse.ArgumentParser) has custom error formatting arguments that aren't being passed through to subparsers created via add_subparsers(). This causes missing arguments to subparsers to result in stack traces instead of user-friendly error messages. The issue also provides a clear direction for the solution: ensuring that the subparser action copies relevant arguments to constructed subparsers. The problem statement is concrete and the expected behavior is clearly defined.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it's a relatively small change that adds an add_subparsers method to the CommandParser class. The solution uses functools.partial to wrap the parser_class with the called_from_command_line argument. While it requires understanding of Django's management command structure and argparse subparsers, the actual code change is minimal (about 10 lines) and the logic is straightforward. An experienced engineer familiar with the codebase could identify the issue location, understand the inheritance pattern, and implement the fix within an hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear problem statement, a focused solution area, and comprehensive test coverage. The provided tests demonstrate both the main use case and edge cases (vanilla argparse vs Django's CommandParser). The issue doesn't require extensive domain knowledge beyond understanding argparse and Django management commands, making it suitable for evaluating coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16485": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description \"floatformat() crashes on '0.00'\" is quite vague and lacks important details. While it identifies that there's a crash involving the floatformat() function when processing \"0.00\", it doesn't specify: (1) what arguments are being passed to floatformat() beyond the input value, (2) what the expected behavior should be instead of crashing, (3) what error/exception is being thrown, or (4) the context in which this occurs. From the test patch, we can infer this relates to Django's template filter and involves a precision argument of 0, but this context isn't provided in the issue description. An experienced engineer would need to investigate the codebase and potentially reproduce the crash to understand the full scope of the problem and determine the correct fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a simple one-character fix changing \"p < 0\" to \"p <= 0\" in the floatformat function. Once an engineer reproduces the crash and identifies the root cause, the actual code change is trivial. The main time would be spent understanding how the floatformat function works, reproducing the issue with the right parameters (input \"0.00\" with precision 0), and tracing through the logic to find where the condition needs adjustment. This investigation and fix would likely take 15 minutes to 1 hour for an experienced engineer familiar with the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "While the issue description is vague, the problem is still solvable through investigation, and the fix itself is straightforward. The test cases provide clear validation criteria. This represents a reasonable debugging challenge that tests an engineer's ability to reproduce issues, trace through code logic, and identify edge cases in conditional statements.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16569": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It clearly describes: (1) The exact problem: FormSet's add_fields() method fails when index=None and specific conditions are met (can_delete=True, can_delete_extra=False), (2) The root cause: index is compared to initial_form_count without checking for None first, causing a TypeError, (3) When this occurs: when calling add_fields() with index=None (e.g., via empty_form), (4) The exact fix needed: add an index-not-None check before the comparison, and (5) Even provides the specific code change needed in the condition. The issue includes the precise line of code that needs to be modified and exactly how to modify it. An experienced engineer could implement this fix immediately without any ambiguity.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a <15 minute fix because: (1) The issue provides the exact problem location and solution, (2) The fix is a simple one-line change adding an \"index is not None\" check to an existing condition, (3) No research or deep understanding of the codebase is required since the problem and solution are explicitly stated, (4) The change is trivial - just adding a null check to prevent a TypeError, (5) Looking at the gold patch confirms this is indeed a minimal change to a single condition in the add_fields method. An experienced engineer familiar with the codebase could implement this in under 15 minutes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is actually an excellent benchmark sample because: (1) The problem is clearly defined with specific reproduction conditions, (2) The solution is straightforward but requires understanding the logic flow, (3) The test verifies the fix works for the empty_form case, (4) It tests a real-world edge case that could easily be missed, (5) The issue demonstrates good software engineering practices by providing both the problem analysis and suggested fix. This sample would effectively test an engineer's ability to understand Django formsets, identify the null pointer issue, and implement a defensive programming solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16667": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It identifies the specific widget (SelectDateWidget), the exact method where the issue occurs (value_from_datadict), the root cause (user-controlled year, month, and day values are passed directly to datetime.date without overflow checks), and the specific error condition (integers larger than sys.maxsize cause OverflowError). The description provides enough technical detail for an experienced engineer to understand both the problem location and the nature of the vulnerability. From this description, it's clear that a solution needs to handle the OverflowError exception when creating datetime.date objects in the SelectDateWidget.value_from_datadict method.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward exception handling issue that requires 15 minutes to 1 hour to fix. The problem is clearly identified in a specific method (SelectDateWidget.value_from_datadict), and the solution involves adding a try-catch block to handle OverflowError exceptions. The fix shown in the patch is minimal - just adding two lines to catch the OverflowError and return a default value \"0-0-0\". An experienced engineer would need to: 1) Locate the SelectDateWidget.value_from_datadict method in django/forms/widgets.py, 2) Identify where datetime.date is called, 3) Add exception handling for OverflowError, and 4) Determine an appropriate fallback value. The solution doesn't require deep architectural changes or extensive research into the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-defined, the solution is straightforward, and the test cases clearly demonstrate the expected behavior. This would be a good sample for evaluating basic exception handling and defensive programming skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16950": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem with Django Admin inlines and UUIDField default values. The user provides clear context: they have an abstract base model with a UUIDField(default=uuid.uuid4), a concrete Thing model, a SubThing model with ForeignKey to Thing's UUIDField, and Thing is registered in admin with StackedInline for SubThing. The problem is that when creating a Thing with SubThing inline in admin, the UUIDField default isn't applied and the id ends up null. While the issue doesn't provide actual code examples of the models or detailed reproduction steps, an experienced Django developer would understand this is about form handling in admin inlines where default values for parent model fields aren't being properly set. The gold patch confirms this interpretation - it's about modifying the add_fields method in django/forms/models.py to handle default values correctly when form data is provided. There's a sensible interpretation of what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django's form system, specifically how ModelFormSets and InlineFormSets work, (2) The engineer needs to trace through the form creation process to understand when and why default values are being nulled out inappropriately, (3) The fix involves modifying the add_fields method in django/forms/models.py with conditional logic that considers whether the field is the parent model's primary key and whether form data is provided, (4) Understanding the relationship between parent and child models in inline formsets and how foreign key fields are handled requires domain knowledge of Django's ORM and forms, (5) The solution requires careful consideration of edge cases (as shown in the conditional logic in the patch). While not the most complex Django issue, it requires substantial understanding of the framework's internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate Django framework bug with a clear manifestation (UUIDs not getting default values in admin inlines). The test cases show the expected behavior and the fix is targeted and reasonable. This would be a good benchmark sample for testing Django expertise.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-20859": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The bug summary clearly states that \"Adding a legend to a SubFigure doesn't work\" and the reporter provides a specific technical hint about changing L437 to check against `FigureBase` instead of presumably `Figure`. However, the issue lacks a concrete code example showing the problem - the reporter mentions \"The example is of course a bit contrived\" but doesn't actually provide the example code that fails. An experienced engineer would likely understand that this is about matplotlib's legend functionality not working with SubFigure objects, and the hint about checking against `FigureBase` provides a clear direction for the solution. The expected outcome is clear (legends should work on subfigures), making this a sensible interpretation despite the missing example.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue reporter has essentially provided the solution - changing a type check from `Figure` to `FigureBase` at line 437. Looking at the actual patch, this is exactly what was done: the import was changed from `Figure` to `FigureBase`, the isinstance check was updated, and the error message was updated accordingly. This requires minimal code changes (about 5 lines) and is a straightforward type hierarchy fix. An experienced engineer familiar with the codebase would quickly understand that `SubFigure` likely inherits from `FigureBase` but not `Figure`, making this a simple inheritance/type checking issue. The main time would be spent understanding the codebase structure and writing a proper test.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear technical solution provided by the reporter. The test case is also straightforward and verifies the fix adequately. The issue is suitable for a coding benchmark as it tests understanding of object-oriented inheritance and type checking in a real codebase.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-22719": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The user clearly describes the problem (confusing deprecation warning for empty data with category units) and provides two acceptable outcomes: either continue producing artists with no data or provide a more accurate warning. However, the issue lacks a minimal reproducible example, which would make it easier to understand exactly when this warning occurs. The user mentions that \"exceptions are caught too broadly\" and references implementation details, but doesn't provide the specific code that triggers the warning. Despite these gaps, an experienced engineer could reasonably interpret this as needing to fix the deprecation warning logic to not trigger for empty data structures, which aligns with the actual solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves adding simple size checks (`values.size and` and `data.size and`) before issuing deprecation warnings in two locations in the category.py file. Once an engineer identifies that the issue is about preventing warnings for empty data, the fix is straightforward - just check if the data structure has any elements before proceeding with the warning logic. The main time would be spent understanding the codebase structure and locating where these warnings are generated, but the actual code changes are minimal and conceptually simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is legitimate, the solution is clean and focused, and the test case appropriately verifies that no deprecation warning is emitted for empty data. This is a good candidate for a coding benchmark as it tests understanding of edge cases and appropriate warning logic.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-23299": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that need to be inferred. The core problem is clearly stated: calling matplotlib.get_backend() removes figures from Gcf.figs when the first figure was created in an rc_context. The expected behavior is also clear - figures should not be removed. However, the issue description lacks crucial details like a minimal reproducible example showing exactly how to trigger the bug, and it doesn't explain the underlying mechanism causing this behavior. An experienced engineer would need to investigate the codebase to understand that get_backend() likely calls rc_context internally, and that rc_context's cleanup mechanism is incorrectly affecting the backend parameter and subsequently clearing figures. While solvable, it requires filling in these technical details through code investigation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires significant investigation to understand the root cause. The engineer needs to: (1) trace through the matplotlib codebase to understand how get_backend() interacts with rc_context, (2) understand the figure management system in Gcf, (3) identify that the backend parameter reset in rc_context is causing figure clearing, and (4) devise the solution to exclude the backend from being reset. The actual code change is small (3-4 lines), but discovering why this seemingly unrelated behavior occurs requires substantial debugging and understanding of matplotlib's internal architecture. The fix involves modifying the rc_context function to exclude the backend parameter from being reset, which is non-obvious without deep investigation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. While the bug description could be more detailed with a reproduction example, the core problem and expected behavior are sufficiently clear. The solution involves understanding matplotlib's internal architecture, which is appropriate for evaluating coding ability in a complex codebase. The test case provided validates the fix appropriately.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-23476": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description clearly states the problem: when a figure is unpickled, its DPI is doubled. It provides specific context (M1 Mac, matplotlib version 3.5.2, MacOSX backend) and mentions that repeated unpickling can cause OverflowError. However, there are some gaps that require interpretation: (1) The issue doesn't explain WHY the DPI gets doubled - looking at the solution patch, it's related to device pixel ratio changes that affect the internal DPI but shouldn't persist after unpickling. (2) The expected behavior isn't explicitly stated, though it's reasonable to infer that the DPI should remain unchanged after unpickling. (3) No reproduction steps are provided, though the problem statement is clear enough. An experienced engineer could reasonably interpret this as: \"The figure's DPI should be preserved at its original value when unpickling, not affected by any runtime device pixel ratio adjustments.\" The test patch confirms this interpretation by showing a figure with original DPI 42 that gets modified to 42*7 due to device pixel ratio, but should restore to 42 after unpickling.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve for several reasons: (1) Understanding the problem requires familiarity with matplotlib's figure serialization mechanism and how device pixel ratios work on different platforms, particularly M1 Macs. (2) The engineer needs to investigate the pickling/unpickling process in matplotlib's Figure class to understand why DPI values are being modified. (3) The solution involves understanding the relationship between _dpi, _original_dpi, and device pixel ratio changes that occur at runtime but shouldn't persist in pickled state. (4) The fix is relatively simple (3 lines in __getstate__ method) but requires understanding the underlying architecture. (5) Writing appropriate tests requires understanding the pickling mechanism and device pixel ratio simulation. This falls into the 1-4 hour range because it requires substantial investigation into matplotlib's internals and platform-specific behavior, but isn't extremely complex once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is platform-specific (M1 Mac) but the solution and test are general enough. The issue is well-suited for a coding benchmark as it tests understanding of object serialization, platform-specific behaviors, and debugging skills. The provided test patch gives clear success criteria.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-24026": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: stackplot() doesn't support CN color aliases (like \"C0\", \"C1\", etc.) that other matplotlib functions support, and it's changing the Axes cycler when colors are provided. The user wants consistent color handling across different plot types. However, the issue description doesn't explicitly state that stackplot is modifying the axes cycler inappropriately - this needs to be inferred from the title \"stackplot should not change Axes cycler\" combined with the usecase description. An experienced engineer familiar with matplotlib's color cycling system would understand that the problem is stackplot calling axes.set_prop_cycle() which permanently modifies the axes' color cycle, rather than just using colors for the current plot.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Understanding matplotlib's color cycling system and how axes.set_prop_cycle() works, (2) Understanding the difference between temporarily using colors vs permanently changing the axes cycler, (3) Analyzing the existing stackplot implementation to see where it modifies the cycler, (4) Designing a solution that handles both the case where colors are provided and where they should come from the default cycler, (5) Implementing the itertools-based solution that creates temporary color iterators instead of modifying the axes state. The fix involves understanding matplotlib's internal color management and requires thoughtful refactoring of the color handling logic, but doesn't require extensive code changes or deep architectural knowledge.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is solvable with the given information, the test patch appropriately validates the fix by testing CN color aliases, and the solution demonstrates good software engineering practices by not modifying shared state unnecessarily.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-24149": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes that ax.bar raises an exception when passed only NaN data in matplotlib 3.6.1, and mentions this breaks seaborn's histogram function. The core problem is evident: the function should handle all-NaN data gracefully rather than crashing. However, the issue lacks specific details like the exact exception type, a minimal reproducible example, or the expected behavior when all data is NaN. The mention of seaborn's \"phantom\" bar usage provides helpful context about a real-world impact. While an experienced engineer could reasonably infer that the function should not crash and should handle NaN data gracefully, some interpretation is required about the exact desired behavior.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This appears to be a 15 min - 1 hour fix. The patch shows it's essentially adding exception handling for StopIteration exceptions in two try-catch blocks within the _convert_dx function. The solution involves catching StopIteration (which occurs when _safe_first_finite finds no finite elements) and falling back to safe_first_element instead. This is a focused change requiring understanding of the existing exception handling pattern and adding a new exception case. While it requires some thought about the control flow and understanding why StopIteration occurs with all-NaN data, it's a relatively small, localized change that doesn't require substantial research or code restructuring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clear enough to work with, the solution is focused and testable, and it represents a realistic debugging scenario that tests exception handling and edge case management skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-24970": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks crucial details. While it mentions \"NumPy 1.24 deprecation warnings\" in the title and summary, it provides no specifics about what the warnings are, where they occur, or what deprecated functionality is being used. The \"Additional information\" section is marked as \"No response,\" which means critical context is missing. An engineer would need to either reproduce the warnings themselves by running matplotlib with NumPy 1.24, or guess at what deprecated NumPy features might be causing issues. The solution requires understanding both the specific deprecation warnings and how to fix them in the matplotlib codebase, but the issue provides insufficient information to identify either.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that involves moving a context manager (`np.errstate(invalid=\"ignore\")`) to wrap only the `astype(int)` conversion rather than the entire floating-point arithmetic block. The change is localized to a single function in the colors.py file and involves understanding that the deprecation warning is related to invalid value handling during type conversion. Once an engineer identifies the specific deprecation warning (likely about invalid values in astype operations), the fix is conceptually simple and requires minimal code changes. The main time would be spent in reproducing the warning and understanding the NumPy 1.24 changes, which should take 15 minutes to 1 hour for an experienced engineer.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "This issue has a significant problem for benchmark usage: the issue description is so vague that an engineer would likely need to spend considerable time just figuring out what the actual problem is. They would need to install NumPy 1.24, run matplotlib code, identify which specific deprecation warnings appear, and then trace those warnings back to the source code. This detective work is not really about coding ability but about debugging and investigation skills. Additionally, without seeing the actual deprecation warnings, there's no way to verify that a proposed solution addresses the right problem. The benchmark setup assumes the issue description provides enough information to understand the problem, but this case fails that assumption.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25311": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides a sensible interpretation. The user clearly states they cannot pickle a figure with a draggable legend and mentions the same error occurs for draggable annotations. While the issue lacks specific error messages, code examples, or steps to reproduce, the core problem is understandable: pickling fails when using draggable elements in matplotlib. An experienced engineer familiar with matplotlib would understand this relates to serialization issues with interactive components. The solution patch confirms this interpretation - it addresses pickling by converting a canvas attribute to a property to maintain picklability. Though more detail would be helpful, there's enough information to understand the problem and work toward a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task. The solution requires understanding matplotlib's architecture, particularly the relationship between figures, canvases, and interactive elements like draggable legends. The engineer needs to: 1) Reproduce the pickling issue with draggable legends, 2) Debug why pickling fails (likely canvas references preventing serialization), 3) Understand matplotlib's offsetbox.py and DraggableBase class structure, 4) Design a solution that maintains functionality while enabling pickling. The actual fix is elegant but non-trivial - converting a stored canvas attribute to a property that dynamically retrieves the canvas. This requires good understanding of Python properties, pickling mechanics, and matplotlib's internal design. While the code change is small, the investigation and design work makes this a moderate difficulty task.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is technically sound, the solution is well-designed, and the test coverage is appropriate. This represents a good benchmark problem that tests understanding of serialization, object-oriented design, and matplotlib internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25332": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is somewhat brief but provides the essential information needed. It clearly states that figures become unpicklable after calling align_labels(), which is a specific and reproducible problem. While it doesn't provide a minimal code example to reproduce the issue, the core problem is well-defined: the align_labels() method breaks the pickle serialization capability of matplotlib figures. An experienced engineer can reasonably infer that they need to investigate why align_labels() creates objects that aren't pickle-compatible and implement proper serialization support. The solution space is bounded - it's clearly a serialization issue that needs to be fixed in the align_labels functionality.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires moderate effort (1-4 hours) because: (1) Understanding the problem requires familiarity with both matplotlib's labeling system and Python's pickle mechanism, (2) The engineer needs to trace through the align_labels() implementation to identify what objects it creates that break pickling, (3) The solution involves implementing __getstate__ and __setstate__ methods for the Grouper class, which requires understanding how to properly handle weak references during serialization, (4) This involves converting between weak and strong references during pickle/unpickle operations, which is a non-trivial serialization pattern. The fix itself is relatively contained (adding two methods to one class) but requires solid understanding of Python's object serialization and weak reference mechanics.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-bounded technical problem with a clear acceptance criteria (figures should be picklable after calling align_labels). The test patch confirms this by adding align_ylabels() to the existing pickle test suite, which is a straightforward way to verify the fix works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25479": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some gaps that require interpretation. The core problem is clearly described: when a colormap is registered under one name but has a different internal name, pyplot functions fail because they look up the colormap's internal name rather than the registered name. The issue explains that cm.get_cmap('registered_name') works but pyplot uses cmap.name internally, creating inconsistency. However, the issue doesn't specify exactly what the expected behavior should be - should the colormap's internal name be updated to match the registered name, or should pyplot be changed to use the registered name? The provided patches clarify this should be resolved by updating the colormap's internal name to match the registered name during registration, but this isn't explicitly stated in the issue description itself. An experienced engineer could reasonably infer this solution approach from the problem description.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is conceptually straightforward once understood - there's a mismatch between registered names and internal colormap names. The solution involves a small change to the register method in cm.py to update the colormap's name attribute to match the registered name (about 5 lines of code). An experienced engineer would need some time to understand the colormap registration system and trace through the code to see where pyplot looks up colormaps by name, but the actual fix is minimal. The accompanying test changes are also simple. This doesn't require substantial research or rewriting of large code sections.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within matplotlib's colormap handling system, has a clear reproduction case, and the solution approach is logical. The test case provided shows exactly how to verify the fix works. This is a good benchmark sample.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-26291": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue is quite vague and missing critical information. The user mentions they cannot create inset axes following \"the first example on the website\" but doesn't provide: (1) the actual code they're trying to run, (2) the specific error message or traceback, (3) the example code they're referencing. They only mention expecting \"an empty box towards the top right\" but give no technical details about what's failing. While there's clearly some problem with inset_axes functionality, the lack of reproducible code and error details makes it very difficult to understand what exactly needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "While the actual fix is simple (just 2 lines of code), the difficulty lies in reproducing and understanding the issue from the vague description. An engineer would need to: (1) deduce that this relates to bbox_inches=\"tight\" parameter during figure saving, (2) understand the matplotlib rendering system enough to know when renderer can be None, (3) identify that the issue is in the __call__ method of inset_locator, and (4) figure out the appropriate way to get a renderer from the figure. This requires substantial investigation and understanding of matplotlib internals, likely taking 1-4 hours to fully understand and implement.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The main issue is that the problem description is so vague that it would be nearly impossible for someone to reproduce the bug and arrive at the correct solution without significant guesswork. The issue lacks the specific code example and error message needed to properly understand what's failing. Additionally, the solution requires deep knowledge of matplotlib's internal rendering system, which makes it unsuitable for a general coding benchmark as it tests domain-specific knowledge rather than general programming ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "psf__requests-1724": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem where unicode method names in Python 2.7.2 cause UnicodeDecodeError because the method.upper() call in sessions.py \"infects\" headers with unicode instead of native strings. While the exact error scenario and reproduction steps aren't detailed, the core problem is clear: unicode method names need to be converted to native strings before the .upper() call. The issue points to the specific location (sessions.py, method.upper() line) and provides enough context about the Python version and requests version. An experienced engineer could reasonably understand that the solution involves converting unicode method names to native strings before processing.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-localized to a specific line in sessions.py where method.upper() is called. The solution involves adding a single line to convert unicode to builtin_str before the method assignment. The fix requires minimal code change (adding one line: method = builtin_str(method)) and understanding the Python 2/3 compatibility issue with unicode strings. An experienced engineer familiar with the codebase could identify and implement this fix relatively quickly once they understand the unicode/string compatibility problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is specific to Python 2.7.2 compatibility which was a real concern when this issue was likely filed. The test case is straightforward and validates the fix. This is a good example of a compatibility bug with a clean, targeted solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "psf__requests-5414": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: attempting to access URLs like `http://.example.com` raises a `UnicodeError` instead of the expected `InvalidURL` exception. The issue provides a specific example URL that triggers the problem, references the expected behavior (raising `InvalidURL` as seen in line 401 of requests/models.py), and mentions a similar previous issue (#4168). The problem is concrete and testable - any engineer can reproduce it by trying to make a request to `http://.example.com` and observing the exception type. The expected solution is clear: catch the UnicodeError and convert it to an InvalidURL exception, maintaining consistency with the existing error handling patterns in the codebase.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue clearly points to the relevant code location (requests/models.py line 401 area), and the solution is simple: extend the existing condition that checks for URLs starting with '*' to also check for URLs starting with '.'. Looking at the patch, it's literally a one-character change - adding 'u'.' to the tuple in the startswith() check. The main time would be spent understanding the existing URL validation logic and writing appropriate tests, but the core fix is trivial once you locate the right spot in the code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal benchmark sample - the problem is clearly described, easily reproducible, has a definitive correct solution, and the fix can be objectively verified through the provided tests. The issue demonstrates good software engineering practices by referencing existing code patterns and similar previous issues.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-3151": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is reasonably well-specified but has some gaps. It clearly states the problem: combine_by_coords raises ValueError for non-monotonic coordinates even when those coordinates are identical across datasets and should be ignored according to documentation. The core problem is understandable - there's a discrepancy between documented behavior and actual implementation. However, the description could be clearer about exactly which coordinates are causing issues and provide a concrete example. The reference to documentation helps clarify expected behavior, and the version information is useful for context. While an engineer could work with this description, they would need to investigate the codebase to understand the specific implementation details and create appropriate test cases.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves a targeted change to the monotonic checking logic in combine.py - changing from checking all dimensions to only checking concat_dims (the dimensions that actually vary between datasets). The fix is conceptually straightforward once you understand that identical coordinates shouldn't be subject to monotonic requirements. An experienced engineer would need time to: 1) Understand the combine_by_coords function and locate the problematic monotonic checking code, 2) Understand what concat_dims represents vs all dimensions, 3) Make the simple but precise change from 'concatenated.dims' to 'concat_dims', 4) Write appropriate tests. The actual code change is minimal (just a few lines), but requires understanding the logic and ensuring the fix aligns with documented behavior.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement, reasonable solution complexity, and good test coverage. The issue represents a discrepancy between documented and actual behavior, which is a common and valuable type of software engineering problem. The fix requires understanding of the codebase logic but is not overly complex or esoteric.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-3677": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description clearly states the problem: the dataset's merge() method fails when trying to merge a DataArray, while the top-level merge() function works. However, it lacks specific details like error messages, expected behavior, or code examples. An experienced engineer would need to infer that the solution should make ds.merge(da) behave like xr.merge([ds, da]). The core requirement is clear but some implementation details need to be figured out.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively simple fix that takes 15 minutes to 1 hour. The solution involves adding just one line of code to handle DataArray inputs by converting them to Datasets. An experienced engineer would need to: 1) Understand that DataArrays need to be converted to Datasets before merging, 2) Locate the merge method in the Dataset class, 3) Add the type check and conversion. The patch shows this is exactly a one-line fix with isinstance() check and to_dataset() conversion.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained, the solution is straightforward, and the test case clearly validates the expected behavior. This would be a good benchmark sample as it tests understanding of object-oriented design patterns and type handling.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-4094": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides a sensible interpretation of what's required. The user states they need to \"stack a bunch of variables and later unstack them again\" but this \"doesn't work if the variables only have a single dimension.\" While the description lacks a concrete example showing the failure, the problem statement is clear enough: the to_unstacked_dataset method fails for single-dimension variables. The provided version information and context about stacking/unstacking operations in xarray gives sufficient background. An experienced engineer familiar with xarray would understand this refers to the data manipulation workflow where datasets are converted to stacked arrays and back. The test patch confirms this interpretation by showing a regression test for single-dimension arrays that should pass through the stack/unstack cycle unchanged.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves a very small change - adding a single parameter (drop=True) to a .sel() method call in the to_unstacked_dataset method. The issue appears to be that when selecting single-dimension variables, the dimension coordinate is preserved when it should be dropped, causing problems in the unstacking process. The fix is conceptually simple: ensure that dimension coordinates are properly dropped during selection. An engineer familiar with xarray's API would recognize this as a common pattern. The change is localized to one line in one method, and the logic is straightforward once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This issue appears well-suited for a coding benchmark. The problem is specific to xarray's data manipulation functionality, the solution is targeted and verifiable through the provided test case, and it represents a realistic bug that could occur in real-world usage. The test case clearly demonstrates the expected behavior: single-dimension arrays should be able to go through a stack/unstack cycle and remain identical to the original data.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-4695": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks but provides a sensible interpretation of what needs to be fixed. It clearly states that naming a dimension \"method\" causes an error when calling \".loc\", and suggests this happens because dimensions are passed to another method in unsanitized form. While it doesn't provide a specific error message, stack trace, or minimal reproducible example, the core problem is understandable: there's a naming collision between a dimension name and a parameter name in the xarray library. The Gold Patch confirms this interpretation - the fix changes from using **key (keyword arguments) to key (positional argument) in the sel() method call, which avoids the collision between a dimension named \"method\" and the \"method\" parameter of sel(). The test patch also clarifies the expected behavior by showing that dimensions named \"method\" should work properly with .loc indexing.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. Once you understand that the issue is about parameter name collision, the solution is relatively straightforward - changing from **key to key in the sel() call. The main time would be spent: (1) creating a minimal reproduction case with a dimension named \"method\", (2) tracing through the code to find where the collision occurs in the __getitem__ method of the loc indexer, and (3) understanding that sel() can take either keyword arguments or a dictionary argument. The actual code change is just one character (removing the **), making this a small but thoughtful fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined enough to solve, and the solution is testable. While the original issue description could be clearer with a reproduction example, an experienced engineer should be able to work with this level of specification.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-6599": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in but provides a sensible interpretation of what needs to be fixed. The core problem is clearly stated: `polyval` with timedelta64 coordinates produces wrong results in the latest version compared to version 2022.3.0. However, the issue lacks crucial details like a minimal reproducible example, specific input data, expected vs actual outputs, or error messages. While the general problem is understandable (a regression in polyval functionality with timedelta64 data), an engineer would need to create their own test cases and investigate the codebase to understand exactly what \"wrong results\" means. The provided patches clarify that the issue is in the `_ensure_numeric` function where timedelta64 data wasn't being properly converted to float, but this level of detail isn't available from just the issue description.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) The engineer needs time to understand xarray's polyval functionality and how it handles different data types, (2) They must create test cases with timedelta64 data to reproduce the issue since none are provided, (3) Investigation is required to locate the bug in the _ensure_numeric function within computation.py, (4) The engineer needs to understand the difference between datetime64 ('M' kind) and timedelta64 ('m' kind) dtype handling, and (5) They must figure out the correct conversion method (astype(float) for timedeltas vs datetime_to_numeric for datetimes). While the actual code change is small (just a few lines), the investigation and understanding phase requires substantial effort.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for evaluating coding ability as it tests several important skills: debugging regression issues, understanding data type handling in scientific computing libraries, creating appropriate test cases, and working with datetime/timedelta data types. The lack of a minimal reproducible example actually makes it more realistic as a coding challenge, since engineers often need to investigate and reproduce issues from incomplete bug reports.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-6721": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is somewhat vague and lacks crucial details. The title mentions \"Accessing chunks on zarr backed xarray seems to load entire array into memory\" but the issue text doesn't provide a clear problem statement, steps to reproduce, expected vs actual behavior, or code examples demonstrating the issue. While we can infer from the title that there's a performance problem where accessing chunk information unexpectedly loads data into memory, the specific circumstances, reproduction steps, and expected behavior are not clearly specified. An engineer would need to make assumptions about what \"accessing chunks\" means and when this unwanted loading occurs.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this appears to be a relatively simple fix - changing `v.data` to `v._data` in the `get_chunksizes` function in `xarray/core/common.py`. This is a one-line change that suggests the issue was accessing a property that triggered data loading instead of accessing the underlying data attribute directly. Once an engineer understands that the problem is in the chunksizes access pattern and identifies the problematic line in `get_chunksizes`, the fix is straightforward. However, finding the root cause might take some investigation through the codebase to understand how chunk access works and where the unwanted data loading occurs.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The main issue with this sample is that the problem description is too vague to be actionable without additional context. An engineer working only from the issue text would struggle to understand what specific operations trigger the unwanted behavior, what the expected behavior should be, and how to reproduce the issue. The lack of a minimal reproducible example or clear steps to demonstrate the problem makes this unsuitable for a benchmark where engineers need to work solely from the issue description. The solution requires domain knowledge about xarray's internal data handling that isn't clearly explained in the issue.",
            "q2_5_confidence": 4
        }
    },
    {
        "pylint-dev__pylint-6386": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. The reporter states that the short option `-v` for verbose expects an argument while the long option `--verbose` doesn't, and they want similar behavior between both options. The expected behavior is explicitly stated: \"Similar behaviour to the long option.\" The issue provides specific examples showing that `pylint mytest.py --verbose` works correctly, while presumably `-v` does not work the same way. The help message suggesting a `VERBOSE` value should be provided gives additional context about the current problematic behavior. This gives a clear understanding of what needs to be fixed: make the short `-v` option behave like the long `--verbose` option by not requiring an argument.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the patch, this is a relatively straightforward fix that involves: 1) Adding metavar support to the argument system to handle empty metavars (which prevents the help system from showing argument placeholders), 2) Adding the `-v` short option to the preprocessing options mapping, and 3) Modifying the preprocessing logic to handle both `--` and `-` prefixed arguments. While it touches multiple files, the changes are small and focused. The core issue is in the configuration system where the short option wasn't properly registered. An experienced engineer familiar with argument parsing and the codebase structure could identify and implement this solution within 15-60 minutes after understanding how pylint's configuration system works.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample because it's a clear, well-defined bug with a specific expected behavior. The solution requires understanding of argument parsing systems and configuration handling, which are common software engineering tasks. The issue has a clear test case (the short verbose option should work like the long one) and the fix is verifiable.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-6903": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It provides: (1) Specific context: running pylint in Kubernetes with --jobs=0, (2) Root cause: _query_cpu() function returns 0 due to integer casting of fractional CPU allocation, (3) Exact error condition: multiprocessing pool requires value > 0, (4) Suggested solution: append \"or 1\" to ensure minimum value of 1, (5) Environment details: Ubuntu 20.04, Kubernetes v1.18.6, Python 3.9.12, pylint>2.14.0. The problem is technical but straightforward - prevent a division/casting result from being 0 by ensuring a minimum value. An experienced engineer can clearly understand what needs to be fixed and how to implement it.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The problem is clearly identified (integer casting of fractional CPU yields 0), the location is specified (_query_cpu function), and the solution is straightforward (ensure minimum value of 1). The actual code change is minimal - just adding a check to prevent 0 values. Most of the time would be spent understanding the codebase structure, locating the _query_cpu function, and writing appropriate tests. The fix itself is trivial but requires some thought about edge cases and testing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. The problem is well-defined, the solution is clear, and the provided patches show exactly what needs to be implemented. The issue represents a realistic debugging scenario that tests understanding of system resource detection and error handling.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-8898": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clearly stated: the bad-names-rgxs option in pylint incorrectly splits regular expressions on commas, which breaks regex patterns that legitimately contain commas (like quantifiers such as `{1,3}`). The bug description explains that \"pylint splits on commas in this option, instead of taking a list of strings\" and that commas in regex patterns get \"mangled before being parsed.\" While the issue doesn't provide a specific example of a broken regex or the exact expected behavior, an experienced engineer can reasonably infer that the solution should parse comma-separated regex patterns while preserving commas that are part of valid regex syntax (particularly in quantifiers like `{1,3}`).",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the solution, it requires: 1) Understanding the current CSV parsing mechanism and identifying where it incorrectly splits on commas, 2) Implementing a more sophisticated parser that tracks brace context to avoid splitting on commas within quantifiers, 3) Creating a new function `_check_regexp_csv` with logic to handle open/close braces and comma detection, 4) Updating the transformer function and module exports. The solution involves writing a non-trivial parsing algorithm with state tracking (open_braces flag, deque manipulation) and touches multiple files. While not extremely complex, it requires careful thought about regex syntax and edge cases.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a solid issue for the benchmark. The problem is technical and specific enough to have a clear solution, but requires understanding both the pylint codebase and regex syntax. The solution demonstrates good software engineering practices with proper parsing logic and comprehensive test cases. The issue is self-contained and doesn't require external dependencies or complex domain knowledge beyond basic regex understanding.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-10051": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is well-specified and clear. It provides a specific problem statement: caplog.get_records() becomes decoupled from actual caplog records when caplog.clear() is called, resulting in frozen behavior. The description explains the root cause - that during setup, get_records() points to the same list as caplog.records, but clear() replaces rather than clears the latter, causing divergence. The environment details are provided, and the expected vs actual behavior is clearly described. An experienced engineer would understand exactly what needs to be fixed: ensure that caplog.get_records() and caplog.records stay synchronized after calling caplog.clear().",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue is clearly identified and localized to the logging module. Looking at the patch, the solution involves: 1) Adding a new clear() method to the handler that calls records.clear() instead of creating a new list, and 2) Updating caplog.clear() to call handler.clear() instead of handler.reset(). This is a small, focused change that requires understanding the difference between list.clear() and list assignment, but doesn't require extensive research or major architectural changes. The fix is only a few lines of code in a single file.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined bug with a clear reproduction case and straightforward solution. The test provided demonstrates the expected behavior clearly, making it easy to verify that a solution works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10081": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: when running pytest with --pdb, tearDown() methods are being executed for unittest.TestCase classes that are marked with @unittest.skip at the class level, which should not happen. The issue references #7215 which was apparently about the same problem but for function-level skips, providing helpful context. However, some details need to be inferred - such as understanding that this is specifically about pytest's behavior with unittest integration, and that the expected behavior is that tearDown should not be called for skipped tests (similar to setUp). An experienced engineer familiar with pytest and unittest would be able to understand the requirements from this description, though they would need to investigate the codebase to understand the current implementation and how to fix it.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: 1) It requires understanding pytest's unittest integration architecture and how the --pdb flag affects test execution, 2) The engineer needs to locate the relevant code in src/_pytest/unittest.py and understand the existing skip detection logic, 3) The solution involves modifying the condition that determines when to postpone tearDown, requiring understanding of the parent-child relationship between test functions and test classes, 4) Writing appropriate tests requires understanding pytest's testing patterns and creating test cases that verify both function-level and class-level skipping behavior. While the actual code change is small (3-4 lines), the investigation and understanding required make this a moderate complexity issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for the benchmark. The problem is specific and testable, the solution can be verified objectively through the provided test cases, and it represents a real-world debugging scenario that tests understanding of test framework internals. The reference to issue #7215 provides helpful context without making the current issue dependent on external information.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-5262": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some technical knowledge to fully understand. It clearly identifies the problem: _pytest.capture.EncodedFile incorrectly advertises a mode containing 'b' (indicating binary mode) when its write() method actually expects text/strings, not bytes. The issue mentions that youtube-dl uses the mode to determine whether to write bytes or strings, and this mismatch causes exceptions. However, some details require inference - like understanding that EncodedFile is a wrapper around streams during pytest's output capture, and that the solution should modify the mode property rather than the underlying behavior. An experienced engineer familiar with pytest's capture mechanism could reasonably determine that the mode property needs to be fixed to not include 'b'.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that would take 15 minutes to 1 hour. The solution involves adding a simple property method to the EncodedFile class that strips 'b' from the underlying buffer's mode. The core logic is just a one-liner: `return self.buffer.mode.replace(\"b\", \"\")`. The main time would be spent understanding the capture mechanism, locating the EncodedFile class in the codebase, and writing a simple test. No complex algorithms or extensive refactoring is required - just a targeted property override.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clean, targeted solution. The problem is specific and the fix is minimal, making it suitable for a coding benchmark. The test is also straightforward and directly validates the expected behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-5631": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that require interpretation. The core problem is clear: pytest 3.6.0 introduced a ValueError when collecting tests that use @patch with a NumPy array as the \"new\" parameter. The issue identifies the specific commit that introduced the bug and explains the root cause - the check \"p.new in sentinels\" returns an array of booleans instead of a single boolean when p.new is a NumPy array. However, the issue lacks some specifics like the exact error message, a minimal reproducible example, or the precise file/function where the error occurs. An experienced engineer would need to investigate the referenced commit and understand how the mock patching system works, but the general direction is clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The engineer needs to: (1) understand pytest's mock patching system and how it detects mock arguments, (2) investigate the specific commit mentioned to understand what changed, (3) understand why \"p.new in sentinels\" fails with NumPy arrays (they can't be compared for equality in boolean contexts), and (4) devise a solution using identity comparison instead of equality. Looking at the gold patch, the solution involves rewriting the num_mock_patch_args function to use \"is\" comparisons instead of \"in\" checks. This requires understanding both the mock module internals and NumPy's behavior, plus careful testing to ensure the fix doesn't break existing functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The test patch provides a good validation that the fix works correctly by creating a NumPy-like class that raises ValueError on equality comparison, which directly tests the problematic scenario. The issue is technical but well-bounded, and the solution can be validated objectively through the provided tests.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-5787": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description adequately describes the problem - exception serialization loses chained exceptions when using xdist. While it doesn't provide detailed reproduction steps or specific examples of the broken output vs expected output, an experienced engineer familiar with pytest's exception handling would understand that this refers to Python's chained exception mechanism (raise ... from ... or implicit chaining). The core problem is clear: exception chains are not being preserved during serialization/deserialization in distributed test execution. However, the issue lacks specific code examples or detailed reproduction steps that would make it completely unambiguous.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because: (1) It requires understanding pytest's internal report serialization mechanism and how xdist communicates between processes, (2) The solution involves substantial refactoring of the serialization/deserialization logic in _pytest/reports.py, moving from inline methods to separate functions and adding comprehensive support for ExceptionChainRepr, (3) It requires understanding Python's exception chaining mechanism and how to properly serialize/deserialize the chain attribute, (4) The patch shows ~140 lines of new code and significant restructuring of existing code, (5) Comprehensive test coverage is needed for the new functionality. While not the most complex issue, it requires deep understanding of pytest internals and careful implementation to handle all edge cases correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking purposes. The problem is technically well-defined, the solution requires solid engineering skills and understanding of serialization patterns, and the provided patches demonstrate a complete and robust solution. An experienced engineer would be able to work from this issue description to implement a working solution, even though some details would need to be figured out through code exploration.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-5809": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes what needs to be done. It identifies a specific problem: the --pastebin feature uses lexer=python3 which causes HTTP 400 errors for certain content when submitting to bpaste.net. The issue provides the exact file and line numbers where the problem occurs (src/_pytest/pastebin.py#L68-L73), explains the root cause (pytest output is not Python code but arbitrary text), and suggests the specific solution (change lexer from \"python3\" to \"text\"). The issue even references a related problem (#5764) for additional context. An experienced engineer would have all the information needed to implement the fix.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix that would take less than 15 minutes. The issue clearly identifies the exact location of the problem (line 68-73 in pastebin.py), explains exactly what needs to be changed (lexer parameter from \"python3\" to \"text\"), and provides clear reasoning. The actual code change is minimal - just modifying a single parameter value in a dictionary. Looking at the gold patch confirms this: it's a simple 4-line change that removes the conditional logic for lexer selection and hardcodes it to \"text\". An experienced engineer could locate the file, understand the issue, make the change, and update the corresponding test in well under 15 minutes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. This is a straightforward bug fix with clear requirements, minimal code changes, and obvious test updates. The issue is well-documented and the solution is unambiguous. This would be an excellent sample for evaluating basic debugging and code modification skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5840": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks critical details. It only states \"5.1.2 ImportError while loading conftest (windows import folder casing issues)\" and \"5.1.1 works fine. after upgrade to 5.1.2, the path was converted to lower case.\" This provides minimal context about what specific ImportError occurs, which conftest file is problematic, what the expected vs actual behavior should be, or how to reproduce the issue. An engineer would need to make significant assumptions about the root cause (likely related to case-insensitive file systems on Windows) and the desired solution approach. While the gold patch shows this involves path resolution and case handling in conftest loading, this information is not available from the issue description alone.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The solution involves understanding pytest's conftest loading mechanism, identifying the case-sensitivity problem with py.path vs pathlib.Path, and replacing the unique_path function with proper Path.resolve() calls. The change spans multiple files (config/__init__.py and pathlib.py), removes a utility function entirely, and requires understanding the subtle differences between path normalization approaches on case-insensitive file systems. An engineer would need time to trace through the conftest loading code, understand why unique_path was causing issues, and implement the pathlib-based solution. The fix isn't trivial but also doesn't require extensive research or massive code changes.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "Yes, there are significant issues that make this unsuitable for coding evaluation. The issue description is so minimal that it's essentially impossible to solve without external context or the ability to reproduce the problem. An engineer would need to guess at the specific ImportError, understand pytest's internal conftest loading mechanism, and deduce that this relates to case-sensitivity issues on Windows file systems. The solution requires intimate knowledge of pytest internals and the differences between py.path and pathlib.Path behavior. Most importantly, without a clear reproduction case or error message, an engineer couldn't verify their solution works. This makes it unsuitable for a benchmark where only the issue description is provided.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6197": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: pytest 5.2.3 is trying to import any `__init__.py` file under the current directory, which causes issues when some packages cannot be imported on certain platforms (e.g., Windows-only packages on Linux). The core problem is well-defined - pytest is being too eager in its collection phase and attempting to import modules that shouldn't be imported. However, the description lacks specific details about exactly what behavior should be expected or what the correct collection logic should be. An engineer would need to investigate the codebase to understand the proper collection semantics, but the high-level issue (unwanted imports during collection) is clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix based on the complexity shown in the patches. The solution involves understanding pytest's collection mechanism, specifically the PyobjMixin class and Module class behavior. The fix requires: 1) Modifying the obj property to avoid premature object mounting, 2) Removing the __init__.py special case handling in Module.__init__, 3) Removing a premature _mount_obj_if_needed() call in collect(). An engineer would need to understand pytest's collection architecture, the relationship between file system traversal and Python object importing, and how markers work. The changes span multiple methods and require careful consideration of when objects should be imported versus just discovered, making this a moderate complexity issue requiring both investigation and thoughtful implementation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability. While the description could be more detailed, it presents a real-world problem that requires understanding of Python import mechanics and pytest internals. The provided tests clearly validate the expected behavior, and the solution requires genuine problem-solving skills rather than just following specifications.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6202": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. The reporter provides a clear description of the problem: the string \".[\" is being incorrectly replaced with \"[\" in test report headlines. They provide specific examples showing the incorrect behavior (\"test_boo[..[]\" becomes \"test_boo[.[]\"). Most importantly, they trace through the exact source code locations causing the issue, pointing to the specific line in src/_pytest/python.py at line 291 where `return s.replace(\".[\", \"[\")` is problematic. They even provide the exact solution: replace this line with `return s` and mention they've already tested the fix. The issue includes concrete file paths, line numbers, and a clear before/after expectation, making it completely actionable for an engineer.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix that takes less than 15 minutes. The issue reporter has already done all the detective work - they identified the exact problematic line of code (line 291 in src/_pytest/python.py), explained why it's incorrect, and provided the exact solution (remove the `.replace(\".[\", \"[\")` call). The fix involves changing a single line of code from `return s.replace(\".[\", \"[\")` to `return s`. The reporter even mentions they tested the fix and it passes all tests. An experienced engineer would need only a few minutes to locate the file, find the line, make the change, and verify it works.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. The issue is clearly described, the solution is straightforward, and it represents a realistic debugging scenario where someone needs to trace through code to find the root cause of a string replacement issue. The provided test patch properly validates the fix by checking that parametrized test names with \".[\" are preserved correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7205": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is well-specified and clear about what needs to be fixed. It identifies a specific problem: when pytest prints cached bytes parameters in setup output using --setup-show, it calls str() on bytes instances which causes BytesWarning. The issue also suggests a clear solution direction - using a safe representation (like saferepr) instead of str(). The problem description includes the specific context (--setup-show flag), the exact symptom (BytesWarning), and a reasonable solution approach. An experienced engineer would understand they need to find where pytest handles parameter display in setup output and replace the str() call with something like saferepr() to safely represent bytes objects.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a straightforward fix that involves: 1) Adding an import for saferepr from _pytest._io.saferepr, 2) Changing one line in _show_fixture_action function from tw.write(\"[{}]\".format(fixturedef.cached_param)) to tw.write(\"[{}]\".format(saferepr(fixturedef.cached_param, maxsize=42))). The change is localized to setuponly.py and requires minimal code modification. An experienced engineer familiar with pytest codebase could identify the issue location by following the --setup-show functionality and make this change within 15-60 minutes. The main time would be spent understanding the codebase structure and locating the right function, but the actual fix is a simple one-line change plus import.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear problem statement, obvious solution direction, and straightforward implementation. The test changes show that the fix works correctly by running pytest with -bb flag (which makes BytesWarning fatal) and --setup-show, confirming the warning is eliminated. This is an excellent benchmark sample as it tests the ability to understand pytest internals, locate the relevant code, and apply a targeted fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7236": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in but provides a sensible interpretation of what needs to be fixed. The core problem is clearly stated: \"unittest.TestCase.tearDown executed on skipped tests when running --pdb\" and the reporter indicates this is a regression from pytest 5.4.1 to 5.4.2. However, the description lacks some specifics: (1) No concrete code example showing the problematic behavior, (2) No detailed steps to reproduce the issue, (3) The expected vs actual behavior could be more explicitly stated. Despite these gaps, an experienced engineer can reasonably interpret that skipped tests should not have their tearDown methods executed when using --pdb, and this behavior worked correctly in 5.4.1 but broke in 5.4.2. The gold patch confirms this interpretation - it adds a check for _is_skipped(self.obj) before setting up the explicit tearDown behavior.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue falls into the 1-4 hours category for several reasons: (1) Understanding the problem requires knowledge of pytest's unittest integration, the --pdb option behavior, and how pytest handles skipped tests. (2) The engineer needs to trace through the codebase to understand where tearDown execution is controlled when --pdb is used. (3) The solution involves creating a new helper function (_is_skipped) and applying it in multiple places in the unittest.py file. (4) The fix requires understanding the relationship between pytest's skipping mechanism and unittest's skipping mechanism. (5) Writing appropriate tests to verify the fix works for both @unittest.skip and @pytest.mark.skip decorators requires understanding both frameworks. While not extremely complex, this requires substantial investigation into pytest's internals and careful implementation to avoid breaking existing functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues that would prevent this from being used in the benchmark. The issue is focused on a specific technical problem with a clear behavioral expectation. The test patch provides good verification that the solution works correctly. The issue represents a realistic debugging scenario that tests knowledge of testing frameworks, which is valuable for assessing coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7324": {
            "q1_1_is_well_specified": 3,
            "q1_2_explanation": "This issue is extremely poorly specified. The title \"Pytest crashes the interpreter on debug build for 3.8+\" suggests a crash problem, but provides no details about what causes the crash, how to reproduce it, what error messages appear, or what the expected behavior should be. The only additional information is a reference to an external bug report (bpo-40870) that we cannot access. Without being able to reproduce the issue or understand the root cause, it would be nearly impossible for a software engineer to develop a meaningful solution. The issue lacks essential information like steps to reproduce, error messages, stack traces, or any concrete description of the problematic behavior.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the provided patch, the solution involves adding a prefix to identifiers in mark expression parsing to handle cases where Python keywords (True, False, None) are used as identifiers. The changes are relatively small - adding a prefix constant, modifying how identifiers are processed in the AST, and updating the matcher to handle the prefix. However, understanding that this specific approach would solve an interpreter crash would require significant investigation into how pytest's mark expression parsing interacts with Python's AST compilation, especially on debug builds. The actual code changes are modest but arriving at this solution would require substantial debugging and understanding of the interaction between pytest's expression parsing and Python's interpreter internals.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "This sample is fundamentally unsuitable for a coding benchmark because the issue description provides no actionable information. An engineer would have no way to understand what needs to be fixed, let alone develop a solution, based solely on the issue text. The connection between \"interpreter crashes\" and the actual solution (adding prefixes to handle Python keywords in mark expressions) is not discoverable from the issue description. This would not test coding ability but rather psychic debugging skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7490": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified but has some gaps. It clearly describes what worked before (pytest 5.x: dynamically added xfail markers prevented failures from being reported) and what's broken now (pytest 6.x: dynamically added xfail markers no longer work). The issue provides the specific API being used (request.node.add_marker(mark)) and identifies this as a regression between versions. However, it lacks a concrete code example demonstrating the problem, which would make it clearer. An engineer would need to infer the exact test case structure and understand how xfail markers should behave when added dynamically versus statically.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. The issue involves understanding pytest's internal marker evaluation system and test execution flow. Looking at the patch, it required changes to the skipping.py module, specifically modifying the pytest_runtest_setup and pytest_runtest_call hooks to re-evaluate xfail marks after test execution (since markers can be added dynamically during the test). The engineer would need to: 1) Understand how pytest's hook system works, 2) Trace through the marker evaluation logic, 3) Identify where dynamic markers aren't being picked up, 4) Implement the re-evaluation logic at the right point in the test lifecycle. The solution involves understanding the execution order and ensuring xfail marks are checked after they could have been dynamically added.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate regression that affects real-world usage patterns. The test cases provided in the patch show clear expected behaviors that would validate any solution. While the initial description could be more detailed, the core issue is understandable and the expected behavior is clear from the context of how xfail markers should work.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7521": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: capfd.readouterr() is converting \\r to \\n in pytest 6.0.0rc1, which the reporter believes is a regression. However, the issue lacks several important details: (1) No minimal reproducible example showing the actual vs expected behavior, (2) No specific test case demonstrating the problem, (3) No comparison with previous pytest versions to confirm it's actually a regression, (4) The reference to \"borgbackup 1.1.13\" failure doesn't provide concrete details about what exactly failed. While an experienced engineer could reasonably interpret that the solution should preserve original newline characters (\\r, \\n, \\r\\n) without conversion, they would need to infer the exact requirements and create test cases to verify the fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it's a simple one-line change adding newline=\"\" parameter to the TextIOWrapper constructor in capture.py. The issue is in how Python's TextIOWrapper handles newlines by default - it performs universal newline translation, converting \\r and \\r\\n to \\n. The fix is straightforward once you understand this behavior. An experienced engineer would need some time to: (1) Locate the relevant code in the capture module, (2) Understand how capfd works and where output is processed, (3) Recognize that TextIOWrapper's newline translation is the cause, (4) Add the newline=\"\" parameter to disable this translation, (5) Write tests to verify the fix. The core change is trivial, but requires understanding the codebase and Python's IO handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. While the original issue description could be more detailed with a concrete example, the problem is clear enough that an experienced engineer can work with it. The provided test patch shows exactly what behavior should be preserved (different newline types: \\n, \\r\\n, \\r), which helps validate that this is a reasonable interpretation of the issue. The fix is targeted and doesn't appear to have significant risk of unintended consequences.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-10297": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It states that RidgeClassifierCV is missing support for the store_cv_values parameter, which is documented in the class but not actually implemented. The issue provides specific evidence by quoting the documentation that mentions cv_values_ attribute being available \"if store_cv_values=True\", but this parameter doesn't exist in the RidgeClassifierCV constructor. The problem is concrete: add the missing store_cv_values boolean parameter to RidgeClassifierCV to match the documented functionality. An experienced engineer would understand that they need to: 1) Add store_cv_values parameter to the __init__ method, 2) Pass it to the parent class constructor, and 3) Update the documentation to be consistent with the implementation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward parameter addition that should take 15 minutes to 1 hour. The solution involves: 1) Adding store_cv_values=False to the RidgeClassifierCV.__init__ method signature, 2) Passing this parameter to the parent class constructor, and 3) Updating the docstring to document the parameter properly. The patch shows this is exactly what was done - minimal code changes focusing on parameter plumbing and documentation updates. An experienced engineer familiar with the codebase would quickly identify that RidgeClassifierCV inherits from _BaseRidgeCV (which already supports store_cv_values) and just needs to expose this parameter in its interface.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean, well-defined feature request with a straightforward implementation. The issue description clearly identifies the problem (missing parameter), provides evidence (documentation quote), and the solution is obvious from examining the class hierarchy. The test patch also demonstrates the expected behavior clearly.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10908": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: CountVectorizer's get_feature_names() method raises NotFittedError even when a vocabulary parameter is provided at initialization, but it shouldn't because the vectorizer can transform data without fitting when vocabulary is provided. The issue describes the expected behavior (get_feature_names should work with pre-provided vocabulary) and mentions the relevant method (_validate_vocabulary) that handles this case in transform(). However, the issue doesn't explicitly state what the method should return or provide a concrete example of the desired behavior. An experienced engineer would need to infer that get_feature_names should return the feature names from the provided vocabulary, similar to how it works after fitting.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves adding a simple check in the get_feature_names() method to call _validate_vocabulary() when vocabulary_ attribute doesn't exist, similar to what's done in the transform method. The fix is only 3 lines of code and follows an existing pattern in the codebase. An experienced engineer would need some time to understand the CountVectorizer class structure, locate the relevant methods, and understand how the vocabulary validation works, but the actual implementation is straightforward once the pattern is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is well-contained within the CountVectorizer class, the expected behavior is reasonable and follows existing patterns in the codebase. The test patch shows comprehensive testing of the new functionality, including verification of the fixed_vocabulary_ flag behavior. The solution maintains backward compatibility and doesn't introduce any breaking changes.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-12585": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. The user clearly describes the problem: the `clone` function fails when parameters are estimator types (classes) rather than instances. They provide specific context about their use case (sklearn-xarray wrapper project), explain exactly where the problem occurs, and even suggest the precise fix needed in base.py line 51. The description includes the specific condition that needs to be modified and the exact code change required. An experienced engineer would have no difficulty understanding what needs to be fixed and how to implement it.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The user has already identified the exact location of the problem (base.py line 51) and provided the specific code change needed. The fix is literally a one-line modification adding \"or isinstance(estimator, type)\" to an existing conditional. An engineer would need minimal time to locate the file, understand the existing logic, and implement the suggested change. The most time-consuming part would be writing a proper test case, but even that is straightforward given the clear problem description.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal benchmark sample - the problem is clearly defined, the solution is specific and implementable, and the scope is well-contained. The user even provides the exact line number and code change needed, making it very accessible for evaluation purposes.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13135": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is reasonably well-specified but has some gaps. It clearly identifies the problem: KBinsDiscretizer with strategy='kmeans' fails due to unsorted bin_edges that cause np.digitize to fail. The description mentions this happens when \"centers and consequently bin_edges being unsorted\" and provides context about when this occurs. However, it lacks a concrete reproducible example - while it mentions \"this simple example\" and references an edge case with n_bins close to the number of data points, no actual code example is provided to demonstrate the failure. An experienced engineer would understand the core issue (unsorted centers from k-means leading to failure) and could reasonably infer that the solution involves sorting the centers, but would need to investigate the codebase to find the exact location and implement a proper fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Once an engineer understands the problem (unsorted k-means centers causing np.digitize to fail), the solution is straightforward: sort the centers after k-means clustering. The actual code change is minimal (adding centers.sort()), and the location is easy to find in the KBinsDiscretizer fit method. The main time would be spent understanding the codebase structure, locating the relevant code in sklearn/preprocessing/_discretization.py, understanding how k-means is used in the discretization context, and writing a simple test to verify the fix. This doesn't require deep algorithmic knowledge or extensive code changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of a specific algorithmic issue (k-means can produce unsorted results) and requires a simple but non-obvious fix. The solution requires some domain knowledge about discretization and the behavior of k-means clustering, making it a reasonable coding challenge.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-13142": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It clearly states that when n_init>1 in GaussianMixture, fit_predict(X) and predict(X) produce different results, which is inconsistent behavior. The issue mentions that the existing unit test doesn't catch this because it doesn't set n_init. From the context, it's clear that these two methods should produce the same results for consistency. However, the issue doesn't explicitly explain what the expected behavior should be or provide a concrete example showing the discrepancy. An experienced engineer familiar with scikit-learn would understand that fit_predict should be equivalent to calling fit followed by predict, but this isn't explicitly stated in the issue description.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively simple fix that involves moving a few lines of code. The solution moves the final e-step execution from before setting the best parameters to after, ensuring that the labels returned are consistent with the final fitted model state. The fix is only about 6 lines being moved within the same function (fit_predict in base.py). An experienced engineer would need to understand the GaussianMixture algorithm and why the e-step timing matters, but once they understand the issue, the code change itself is straightforward. The accompanying tests are also simple additions that verify the fix works.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. This is a well-contained bug fix with clear test cases that verify the solution. The issue affects a specific edge case (n_init>1) in a machine learning library, which is appropriate for a coding benchmark. The solution requires understanding of the algorithm's internal workings but doesn't involve complex architectural changes.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-13328": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It states that HuberRegressor raises a TypeError when fitting with boolean predictors, while LinearRegression handles boolean arrays correctly by converting them to float internally. The expected behavior is clearly defined: HuberRegressor should behave like LinearRegression and accept boolean inputs by converting them to float. The issue provides a specific error case (TypeError with boolean X) and references the correct behavior from LinearRegression as a comparison point. An experienced engineer would understand they need to modify HuberRegressor to handle boolean inputs properly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that requires 15 minutes to 1 hour. Looking at the gold patch, the solution involves adding a dtype parameter to the check_X_y function call in the HuberRegressor.fit method. The engineer would need to: 1) Locate the HuberRegressor.fit method in sklearn/linear_model/huber.py, 2) Identify that the issue occurs in the check_X_y call which doesn't specify dtype conversion, 3) Add dtype=[np.float64, np.float32] parameter to allow automatic type conversion from boolean to float. This is a small, targeted change that requires understanding of scikit-learn's input validation patterns but doesn't require extensive research or complex logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-suited sample for the benchmark. The issue is clear, the solution is targeted and verifiable, and the test patch includes a specific test case (test_huber_bool) that validates the fix by ensuring HuberRegressor can handle boolean input without crashing. The fix follows established patterns in scikit-learn and doesn't introduce any unusual complexities.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13779": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is moderately well-specified. While it clearly states that \"voting estimator will fail at fit if weights are passed and an estimator is None\", there are some gaps that need to be filled in. The description mentions that \"we don't check for an estimator to be `None` in `sample_weight` support\" but doesn't specify exactly where this check should be added or what the expected behavior should be when an estimator is None. However, from the context and the gold patch, it's reasonable to infer that the solution should skip None estimators during the sample_weight validation loop rather than trying to check if they support sample weights. An experienced engineer familiar with scikit-learn's voting estimator patterns could reasonably arrive at this solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is straightforward once understood: add a None check before validating sample_weight support. The solution requires only 2 lines of code (if step is None: continue) in the fit method of voting estimators. An experienced engineer would need some time to locate the relevant code in sklearn/ensemble/voting.py, understand the existing flow where sample_weight validation occurs, and implement the simple check. The fix is localized and doesn't require deep architectural changes or extensive research.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This issue is suitable for the benchmark. The problem is well-defined enough that an experienced engineer can understand what needs to be fixed, and the solution is straightforward but requires some familiarity with the codebase structure. The test provided confirms the expected behavior clearly.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14053": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is relatively brief but provides the essential information needed. It clearly states that export_text returns an IndexError when there is a single feature, and includes version information. While it doesn't provide a minimal reproducible example in the issue text itself, the problem statement is clear enough that an experienced engineer could understand what needs to be fixed. The test patch shows exactly the scenario that was failing - a decision tree with a single feature. An engineer would need to investigate the export_text function to understand why single features cause IndexErrors, but the core problem is well-defined.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is localized to a specific function (export_text) and the fix is relatively straightforward - adding a conditional check to handle undefined tree features. An experienced engineer would need to: 1) Reproduce the error with a single-feature decision tree, 2) Debug the export_text function to understand why it fails, 3) Identify that the issue is with indexing into feature_names when tree_.feature contains TREE_UNDEFINED values, and 4) Add the conditional check as shown in the patch. The fix itself is only 2 lines of code and follows a simple pattern of checking for undefined values before indexing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within a single function, has a clear error condition (IndexError), and the solution is testable. The issue would be suitable for a benchmark as it requires understanding the codebase structure, debugging skills, and knowledge of how decision trees work, but doesn't require extensive domain expertise or major architectural changes.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14087": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is vague and lacks crucial details. While it mentions that an IndexError is thrown when using LogisticRegressionCV with refit=False, it provides no specifics about: (1) the actual error message or stack trace, (2) a minimal code example to reproduce the issue, (3) what specific conditions trigger the error, or (4) what the expected behavior should be. The description only states \"The following error is thrown\" but doesn't show the actual error. Without seeing the gold patch, an engineer would struggle to understand what exactly needs to be fixed - whether it's a bounds checking issue, an array indexing problem, or something else entirely. The issue requires significant detective work to identify the root cause.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the gold patch, this requires 1-4 hours of work. The engineer needs to: (1) reproduce the IndexError by creating test cases with LogisticRegressionCV and refit=False, (2) debug through the code to identify that the issue occurs when accessing l1_ratios_ array indices for non-elasticnet penalties, (3) understand the logic flow in the fit method around lines 2170-2180, (4) recognize that l1_ratios_ should only be populated for elasticnet penalty, and (5) implement the conditional logic to handle different penalty types. The fix involves understanding cross-validation scoring, penalty types, and attribute initialization patterns in scikit-learn. While the actual code change is small (adding an if-else block), the debugging and understanding phase would take substantial time.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "This sample has a critical flaw for benchmark use: the issue description lacks a reproducible example. An engineer would need to guess or experiment to create test cases that trigger the IndexError. The description doesn't specify which combinations of parameters cause the problem, making it nearly impossible to verify a solution without the original test cases. Additionally, the fix requires deep knowledge of scikit-learn's internal structure and the relationship between different penalty types and l1_ratios, which may not be apparent from the issue text alone. The vague description would likely lead to multiple different attempted solutions that may or may not address the actual problem.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14629": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. The reporter clearly identifies the problem: `cross_val_predict(method='predict_proba')` fails with `MultiOutputClassifier` due to an AttributeError. They pinpoint the exact source of the issue - the code tries to access `estimator.classes_` directly, but for `MultiOutputClassifier`, the classes should be accessed via `mo_clf.estimators_[i].classes_`. They provide the specific file and line numbers where the problem occurs (sklearn/model_selection/_validation.py#L857-L866). The issue includes version information and demonstrates understanding of what needs to be fixed. An experienced engineer would have enough information to understand that they need to modify how `classes_` is accessed for `MultiOutputClassifier` objects.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minutes to 1 hour fix. The issue is clearly diagnosed and the solution is straightforward: MultiOutputClassifier needs to properly expose the classes_ attribute by aggregating it from its constituent estimators. Looking at the gold patch, the solution involves adding a fit method override that sets self.classes_ = [estimator.classes_ for estimator in self.estimators_] after calling the parent fit method. This is a small, focused change that doesn't require extensive knowledge of the codebase architecture. An experienced engineer familiar with scikit-learn would recognize this as a common pattern for multi-output estimators and implement it quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is clean and follows scikit-learn conventions, and the test coverage demonstrates the fix works correctly. The issue represents a legitimate bug with a clear, testable solution that would be appropriate for evaluating coding ability.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14710": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when using HistGradientBoostingClassifier with string targets and early stopping enabled, there's a data type mismatch in the scorer where y_true contains integers but y_pred contains original string classes. The issue provides the exact file location (sklearn/ensemble/_hist_gradient_boosting/gradient_boosting.py), the specific method (_check_early_stopping_scorer), and even includes a detailed proposed solution with code diff. The problem statement is technical but precise, explaining that y_true needs to be encoded back to original classes before computing scores. An experienced engineer would have enough information to understand both the technical issue and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides a very clear problem description and even includes a proposed solution with specific code changes. The fix involves adding just a few lines of code to convert integer-encoded targets back to original string classes before scoring. The solution uses hasattr(self, 'classes_') to check if it's a classifier and then applies self.classes_[y.astype(int)] to decode the targets. While an engineer would need to understand the scikit-learn codebase structure and the early stopping mechanism, the actual implementation is straightforward once the problem is understood. The provided diff makes the solution approach very clear.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution approach is clear, and the test case provided in the test patch adequately covers the scenario. This would be a good benchmark sample as it tests understanding of data type handling in machine learning pipelines and the ability to work with existing sklearn code patterns.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14894": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides the key information needed: ZeroDivisionError occurs in _sparse_fit when support_vectors_ is empty with sparse data. While the description is brief, it clearly identifies the error type, the specific function (_sparse_fit), the context (SVM with sparse data), and the condition that triggers it (empty support_vectors_). The gold patch shows this is indeed about handling division by zero when n_SV (number of support vectors) is 0, particularly in the line \"dual_coef_indices.size / n_class\". An experienced engineer could reasonably identify that the issue is in the dual coefficient matrix construction and implement a similar fix by adding a check for empty support vectors.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward bug fix that requires 15 minutes to 1 hour. The issue is clearly a division by zero error that occurs when support_vectors_ is empty. The solution involves adding a simple conditional check before the problematic division operation. The fix is localized to one function (_sparse_fit) and involves adding just a few lines of code - an if/else block to handle the empty case. An experienced engineer familiar with the codebase could quickly identify the problematic division operation and implement the appropriate guard clause.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-suited sample for the benchmark as it represents a common type of bug (edge case handling) that requires understanding the code logic and implementing appropriate defensive programming. The test case provided clearly validates the fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-25747": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue is vague and lacks critical information needed to understand and reproduce the problem. While the user mentions \"FeatureUnion not working when aggregating data and pandas transform output selected\" and states they \"got an error\", they provide no error message, no code example, and no minimal reproducible case. The title mentions FeatureUnion but the actual bug is in the _wrap_in_pandas_container utility function. Without seeing the patches, it would be very difficult to understand what the actual problem is or how to fix it. An experienced engineer would need to do significant detective work to identify the root cause.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the patches, this is a relatively straightforward fix - removing 2 lines of code that incorrectly override the index of existing DataFrames in _wrap_in_pandas_container. However, the difficulty comes from the poor issue description. An engineer would need to spend significant time reproducing the issue, understanding the FeatureUnion workflow, and tracing through the code to identify that the problem is in the _wrap_in_pandas_container function. Once the root cause is identified, the fix itself is simple, but getting there would take 1-4 hours of investigation.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "This issue has significant problems for use in a coding benchmark. The issue description is so vague that without access to the original PR discussion or the ability to reproduce the problem, it would be nearly impossible for an engineer to arrive at the correct solution. The issue mentions FeatureUnion in the title but the actual bug is in a utility function. There's no error message, no code example, and no clear description of the expected vs actual behavior. This would not provide a fair assessment of coding ability since the problem is more about detective work than coding skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-25931": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clearly described: IsolationForest generates a spurious warning about missing feature names when fitted with a DataFrame and using non-default contamination values. The issue identifies the specific code path (line 337 in _iforest.py) and explains the root cause - the fit() method internally calls predict() on training data to determine offset_ parameters, which triggers the warning. However, the exact solution approach isn't explicitly stated - an engineer would need to figure out how to avoid the feature name validation issue during this internal prediction call. The problem is specific enough that an experienced engineer could reasonably determine that the solution involves either bypassing input validation or preserving feature names during the internal scoring operation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic code location and the root cause. Looking at the solution patch, it involves creating a private _score_samples method that bypasses input validation (which strips feature names) and using that instead of the public score_samples method during fitting. This is a relatively straightforward refactoring that doesn't require deep algorithmic changes or extensive research. An experienced engineer familiar with scikit-learn patterns would likely recognize this as a common issue where internal calls need to bypass validation that's meant for external API usage. The solution is elegant and follows established patterns in the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. It represents a realistic bug that could occur in production, has a clear reproduction case, and the solution demonstrates good software engineering practices (separating public API with validation from internal methods). The test case is also straightforward and directly validates that the warning doesn't occur when fitting with a DataFrame and non-default contamination. This type of issue tests an engineer's ability to understand API design patterns and internal vs external method usage.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-25973": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The reporter states that SequentialFeatureSelector should accept \"an iterable of splits\" for the cv parameter according to documentation, but when they try to pass splits from a cross validator, it fails. While they don't provide the exact error message or a complete code example showing the failure, the core problem is clear: the cv parameter should accept pre-generated splits but currently doesn't. An experienced engineer familiar with scikit-learn's cross-validation patterns would understand this refers to accepting generator objects or iterables of train/test index pairs, which is a common pattern across sklearn estimators. The solution direction is evident from the test patch which shows using LeaveOneGroupOut().split() as the cv parameter.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves importing check_cv from model_selection and calling it to normalize the cv parameter before use, which is a standard pattern in scikit-learn. The code changes are minimal: add an import, call check_cv() once in the fit method, and pass the result to _get_best_new_feature_score. The fix follows established sklearn conventions and requires only understanding how check_cv works to convert various cv inputs (integers, generators, iterables) into a standardized format. An experienced engineer familiar with sklearn's codebase would recognize this as a common pattern used throughout the library.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a straightforward bug fix that follows established scikit-learn patterns. The issue demonstrates a clear problem (cv parameter not accepting iterables of splits as documented), has a clean solution (using check_cv), and includes a proper test case to verify the fix works.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-10323": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem with the literalinclude directive in Sphinx when using prepend/append options - that leading whitespace is being removed incorrectly, disrupting code indentation. While the issue description is somewhat brief and doesn't provide a concrete example showing the incorrect vs. expected output, the problem is clear enough from the context. The gold patch reveals this is about the order of filters in sphinx/directives/code.py, where dedent_filter needs to run before prepend_filter and append_filter to preserve proper indentation. An experienced engineer familiar with Sphinx's literalinclude functionality would likely understand that this is about maintaining consistent indentation when adding content to included code blocks.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The solution involves reordering filters in sphinx/directives/code.py - moving dedent_filter before prepend_filter and append_filter in the filters list. Once an engineer understands the issue (that dedent should happen before prepend/append to preserve indentation), the fix is straightforward: changing the order of 3 lines in a list. The main time would be spent understanding how the filter pipeline works in the LiteralIncludeReader class and recognizing that filter order matters for correct indentation handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for the benchmark. While the description could be more detailed with concrete examples, it describes a real bug with a clear fix. The test patch shows the expected behavior and would properly validate solutions. The fix requires understanding of the codebase architecture but is not overly complex.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-10673": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear about what needs to be solved. It describes a concrete problem where Sphinx issues warnings about nonexisting documents when users try to add links to the toctree for generated indices (genindex, modindex, search). The issue provides specific examples from Stack Overflow showing this is a common user problem. The desired solution is clearly stated: the provided toctree directive should work without raising errors. The issue gives a concrete example of what should be possible, making it clear that the solution needs to allow these three specific generated document names (genindex, modindex, search) to be referenced in toctree without warnings.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires moderate complexity to solve (1-4 hours). Looking at the patch, the solution involves understanding Sphinx's internal architecture around toctree processing, document discovery, and generated documents. The engineer needs to: (1) Understand how Sphinx tracks documents and generated indices, (2) Identify that generated documents are stored in env.domains['std'].initial_data['labels'], (3) Modify toctree parsing logic in multiple files to recognize these generated documents, (4) Handle both the validation phase (to prevent warnings) and the rendering phase (to properly link to generated indices). The patch touches 3 different files with logic changes across multiple functions, requiring understanding of the codebase structure and the flow between document parsing, toctree resolution, and figure numbering systems.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. It has clear requirements, a specific problem to solve, and the test patch validates that the solution works correctly by ensuring toctree entries for genindex, modindex, and search are properly handled without warnings. The solution requires genuine engineering problem-solving rather than just following documentation.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7440": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks crucial details. While the title suggests \"glossary duplicate term with a different case,\" the actual problem description only provides external links and references to a Travis CI job failure. There's no clear explanation of what the bug actually is, what behavior is expected vs. observed, or how to reproduce the issue. An engineer would need to guess that this relates to case-sensitivity in glossary term handling based solely on the title and the provided patch, but the specific requirements are not explicitly stated in the issue text itself.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a relatively simple fix involving removing `lowercase=True` from XRefRole and changing `termtext.lower()` to `termtext` in the glossary term handling. The changes are minimal (2 lines in sphinx/domains/std.py) and involve straightforward parameter modifications. Once an engineer understands that the issue is about preserving case sensitivity in glossary terms, the solution is not complex and would likely take 15 minutes to 1 hour to implement after understanding the codebase structure.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The major issue is that the problem description relies heavily on external links and references that won't be available in the benchmark setup. The issue text doesn't contain a self-contained description of the problem, expected behavior, or reproduction steps. Engineers would essentially need to reverse-engineer the requirements from the title alone, making this unsuitable for a coding ability benchmark where clear problem specification is essential.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7462": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides a sensible interpretation. While it clearly states the error (IndexError: pop from empty list) and provides good environmental context (OS, Python version, Sphinx version, extensions), it lacks crucial details like the specific empty tuple notation that triggers the error or a minimal reproduction case. However, an experienced engineer could reasonably deduce that the problem involves Sphinx's handling of empty tuple type annotations based on the error message and context. The mention of \"notation for an empty tuple from a mypy discussion\" provides a strong hint about the root cause.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Once an engineer reproduces the error with an empty tuple annotation like Tuple[()], the IndexError stack trace would quickly point to the problematic code in the tuple unparsing logic. The issue is straightforward: the code assumes non-empty tuples and does result.pop() without checking if the list is empty. The fix requires adding a simple conditional check and providing appropriate handling for the empty case. While it requires understanding Sphinx's AST unparsing logic, the actual code change is small and the logic is clear.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Sphinx's type annotation parsing system, has a clear error manifestation, and the solution is testable. The fix demonstrates good software engineering practices by handling edge cases properly.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7985": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is mostly clear but lacks some specifics. The title and problem statement clearly indicate that linkcheck should be extended to check local/internal links. Looking at the provided patches helps clarify the intent: the solution involves checking if local files exist using path.exists(), treating non-existent local files as \"broken\", and respecting ignore patterns. However, the original issue description doesn't specify exactly what should happen when local links are found (should they be validated for existence? how should broken local links be reported?). An experienced engineer could reasonably infer from context and the title that local links should be validated for file existence, similar to how external links are validated for accessibility.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours of work. The engineer needs to: (1) understand how the existing linkcheck system works, including the URI classification logic, (2) modify the check() function to distinguish between different types of non-HTTP URIs (URI schemes vs local paths), (3) implement file existence checking for local paths using path.exists() and path.join(), (4) integrate with the existing ignore pattern system, and (5) update tests. The solution touches the core linkcheck logic and requires understanding the existing codebase structure, URI parsing, and file system operations. While not extremely complex, it requires careful thought about edge cases and integration with existing systems.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem domain (link checking) is well-understood, the codebase appears reasonably well-structured based on the patches, and the solution is testable with clear success criteria. The patches show a clean implementation that integrates well with existing patterns.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-8269": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It states that the `linkcheck` command reports \"Anchor not found\" when `linkcheck_anchors` is `True`, even when the real issue is that the server returned an HTTP error status code (like 404 or 500). The environment details are provided, and the expected behavior is clear: HTTP errors should be reported as HTTP errors, not as anchor-not-found errors. The issue description provides enough context about the linkcheck functionality and the specific scenario where the bug occurs.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a straightforward fix that takes 15 minutes to 1 hour. The solution involves adding a single line `response.raise_for_status()` in the `sphinx/builders/linkcheck.py` file to make HTTP requests raise exceptions for error status codes before attempting to check anchors. This requires understanding the flow of the linkcheck code and knowing that `raise_for_status()` is the standard way to handle HTTP errors in the requests library. An experienced engineer familiar with Python's requests library and basic debugging would identify this solution relatively quickly once they locate the relevant code section.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark issue. The problem is clearly defined, the solution is elegant and follows standard Python practices, and the test patch demonstrates proper testing methodology by setting up a local HTTP server that returns 500 errors to verify the fix works correctly. The issue tests both understanding of HTTP error handling and familiarity with the requests library.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-8551": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some background knowledge to fully understand. The description states that `:type:` and `:rtype:` info fields create implicit xrefs that use a different lookup mechanism than explicit xref roles - they search \"in every (sub)module instead of in the current module and then parent modules.\" The expected behavior is clear: \"No warnings, and the two mentioned types should resolve to `mod.submod.A`.\" However, the issue lacks a concrete code example showing the problematic behavior, which would help clarify exactly what input causes the false warnings. An experienced Sphinx developer familiar with cross-reference resolution would likely understand what needs to be fixed, but someone less familiar with Sphinx's internals might need to do additional investigation to understand the lookup mechanisms being referenced.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Sphinx's complex cross-reference resolution system and how info fields like :type: and :rtype: differ from explicit xref roles in their lookup behavior. (2) The solution involves modifying the xref creation process to pass proper context information (py:module and py:class) so that implicit xrefs use the same scoped lookup as explicit ones. (3) Looking at the patch, it touches two files (sphinx/domains/python.py and sphinx/util/docfields.py) and requires understanding how the environment context propagates through the documentation processing pipeline. (4) While the actual code changes are relatively small (adding a few lines), identifying exactly where and how to fix the context passing requires significant domain knowledge about Sphinx's architecture.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined within the Sphinx domain, the solution is testable, and the expected behavior is clear. The test patch shows comprehensive verification of the fix, checking that both :param str name: and :type age: int properly resolve with the correct module/class context. This is a legitimate bug that affects documentation quality and would be suitable for a coding benchmark.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-9711": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly describes the problem: version checking using string comparison instead of semantic version comparison, which causes incorrect behavior when comparing versions like \"0.6\" vs \"0.10.0\". The issue provides a specific example (sphinx-gallery 0.10.0 should be accepted if 0.6 is minimum), identifies the root cause (string comparison treats '0.6' > '0.10'), and states the expected behavior clearly. The problem is in the `needs_extensions` check functionality, and an experienced engineer would understand that this requires implementing proper semantic version parsing instead of string comparison.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-understood (string vs semantic version comparison), the location is identifiable (needs_extensions function), and the solution approach is straightforward (use a proper version comparison library like packaging.version). The actual implementation requires importing the packaging.version module, wrapping the comparison in try/catch for invalid versions, and falling back to string comparison when version parsing fails. This is a common pattern in Python codebases and doesn't require extensive research or architectural changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of version comparison semantics, proper error handling (try/catch for invalid versions), and knowledge of Python packaging standards. The issue is self-contained, has clear requirements, and the solution can be verified through the provided tests.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13372": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides some blanks to fill in, but there is a sensible interpretation. While the issue mentions \"UnboundLocalError in evalf\" and suggests a solution involving adding \"else: raise NotImplementedError\" clauses, it doesn't provide the specific error message, stack trace, or minimal reproducing example. However, the context about changing Mul arg order and the suggested fix location (elif clauses defining reprec and imprec) gives enough direction. An experienced engineer could reasonably infer that the problem occurs when variables reprec and imprec are referenced before being defined in certain code paths, and the solution is to add proper error handling for unhandled cases.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue description points directly to the problem area (elif clauses for reprec and imprec) and suggests the exact solution (adding else: raise NotImplementedError). The fix itself is trivial - just adding two else clauses with NotImplementedError. The main time would be spent understanding the evalf function context and verifying the fix works, but the actual code change is minimal and straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-localized to a specific function, the solution is clearly suggested, and the fix is minimal. The test case also provides a way to verify the solution works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-13480": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks important details. It only states that \".subs on coth(log(tan(x))) errors for certain integral values\" and lists some numbers (2, 3, 5, 6, 8, 9, 11, 12, 13, 15, 18). However, it doesn't explain: (1) What kind of error occurs, (2) What .subs() operation is being performed, (3) What the expected behavior should be, (4) How to reproduce the issue with actual code. An engineer would need to guess that this involves symbolic substitution in the coth function, likely related to hyperbolic cotangent simplification, but the specific problem and desired solution are unclear from the description alone.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is actually a simple one-character typo fix in sympy/functions/elementary/hyperbolic.py where \"cotm\" should be \"cothm\" on line 590. Once the issue is understood (that there's a NameError due to the typo), the fix is trivial - just correcting the variable name. The real work would be in understanding what the vague issue description is referring to and locating the bug, but the actual code change is minimal and straightforward.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "Yes, there are significant issues with using this sample: (1) The issue description is so vague that it would be very difficult for someone to reproduce the problem without trial and error or deep knowledge of the codebase, (2) The connection between the described symptoms and the actual bug (a typo) is not obvious from the issue text, (3) This is essentially a debugging exercise rather than a feature implementation, which may not effectively test coding ability as intended for the benchmark, (4) The fix is a trivial typo correction that doesn't demonstrate meaningful coding skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-13877": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks critical details needed for a meaningful solution. The title mentions \"Matrix determinant raises Invalid NaN comparison with particular symbolic entries\" but the description only provides a brief question about the Bareiss algorithm's validity for integer matrices. There's no specific error message shown, no code example demonstrating the problem, no stack trace, and no clear description of what \"particular symbolic entries\" cause the issue. The question about the Bareiss algorithm suggests the reporter suspects it's being used inappropriately for non-integer matrices, but this doesn't clearly specify what the expected behavior should be or how to reproduce the problem. An engineer would need to guess at the root cause and expected solution based on minimal information.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the patches, this issue required understanding the Bareiss algorithm implementation in matrix determinant calculation and identifying that the zero-checking function was causing NaN comparisons with symbolic expressions. The solution involved replacing a simple pivot-finding function with a more robust one that uses expand_mul for zero testing of symbolic expressions. This required knowledge of SymPy's symbolic computation internals, understanding when NaN comparisons occur with symbolic math, and modifying the _eval_det_bareiss method to use _find_reasonable_pivot with appropriate zero-checking. The changes span understanding matrix algorithms, symbolic computation edge cases, and SymPy's internal architecture - this would likely take 1-4 hours for an experienced engineer to research, understand, and implement properly.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The main issue is the extreme disconnect between the vague problem description and the sophisticated solution required. An engineer working only from the issue description would have no clear way to reproduce the problem, understand what \"Invalid NaN comparison\" means in practice, or know what the expected behavior should be. The issue doesn't provide any code examples, error messages, or specific scenarios that trigger the problem. This makes it nearly impossible to develop and test a solution based solely on the issue text, which is a fundamental requirement for the benchmark setup.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-16792": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that require interpretation. The core problem is clearly stated: when using cython backend for autowrap, array arguments that don't appear in the wrapped expression are incorrectly generated as scalars instead of pointers in the C function signature. The issue provides a clear symptom (wrong C signature generation) and mentions the root cause is in codegen. However, it lacks concrete examples showing the problematic behavior, specific function names or file locations where the bug occurs, and doesn't show what the expected vs actual generated code looks like. An experienced engineer could reasonably infer what needs to be fixed based on the description and the context that this affects interfacing with external libraries requiring fixed signatures.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours of work. The engineer needs to: 1) Understand the sympy codegen system and how autowrap works with cython backend, 2) Identify where array argument metadata is handled in the codegen pipeline, 3) Understand why unused array arguments lose their dimension information, 4) Modify the logic to preserve array type information even when arrays don't appear in expressions. The patch shows this involves understanding the relationship between symbols, array_symbols, and metadata handling in the routine generation. The solution touches core codegen logic and requires careful consideration of when to apply array metadata. While not trivial, it's a focused change to specific logic rather than a complete rewrite.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is technical but well-bounded, the solution is testable, and the patch demonstrates a clear fix that addresses the core issue described. The test case validates the fix by ensuring unused array arguments are properly generated as pointers in C code.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17139": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is somewhat brief but provides enough context to understand the problem. The title \"simplify(cos(x)**I): Invalid comparison of complex I (fu.py)\" clearly indicates that there's an issue with simplifying trigonometric expressions with complex exponents, specifically mentioning an \"Invalid comparison\" error in the fu.py file. While the description doesn't provide the full error traceback or detailed steps to reproduce, the problem is identifiable: the simplify function is failing when given cos(x)**I (where I is the imaginary unit). The gold patch shows this is indeed about handling complex exponents in the _f function within fu.py, adding a check for rv.exp.is_real. The test cases confirm the expected behavior - expressions like cos(x)**I should remain unchanged rather than cause an error. An experienced engineer could reasonably infer that the issue involves improper handling of complex exponents in trigonometric simplification routines.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The solution involves adding just 2 lines of code to check if the exponent is real before proceeding with the simplification logic. The patch shows a simple guard clause: \"if not rv.exp.is_real: return rv\". Once an engineer identifies that the issue is in the fu.py file's _f function and understands that complex exponents should be handled differently, the fix is straightforward. The main time would be spent understanding the existing code structure and identifying where the comparison issue occurs, but the actual code change is minimal and conceptually simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-suited for a coding benchmark as it requires understanding of mathematical libraries, error handling, and domain-specific logic for symbolic mathematics. The solution demonstrates good software engineering practices by adding proper input validation.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17318": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in, but there's a sensible interpretation. While it mentions \"sqrtdenest raises IndexError\" and should \"return unchanged if expression cannot be denested,\" it doesn't specify exactly when/why the IndexError occurs or what expressions cause it. However, from the context and function name, an experienced engineer could reasonably infer this relates to square root denesting operations failing on certain mathematical expressions. The test patch shows it involves complex numbers, which provides additional context about the problematic cases.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves adding a single condition `sq.is_positive` to an existing check in the `_sqrt_match` function. An experienced engineer would need to: (1) understand that the IndexError occurs when processing expressions with negative rational squares, (2) locate the relevant function in sqrtdenest.py, (3) identify that the issue is in the condition checking `(x**2).is_Rational`, and (4) add the additional `is_positive` check. The fix is conceptually straightforward - ensuring that when checking if squared terms are rational, we also verify they're positive to avoid issues with complex expressions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within the mathematical domain of square root denesting, the solution is clean and focused, and the test cases clearly demonstrate both the problematic input and expected behavior. An engineer with basic understanding of mathematical expressions and complex numbers should be able to solve this.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17630": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: when a BlockMatrix containing ZeroMatrix blocks is multiplied once, it works correctly, but multiplying it twice throws an exception. The user identifies that the issue seems to be caused by zeros in the intermediate result being Zero instead of ZeroMatrix. While the description doesn't provide a specific code example to reproduce the issue, it gives enough context about the problem domain (BlockMatrix, ZeroMatrix, block multiplication) and the expected vs actual behavior. The test patch confirms this understanding by showing the exact scenario: creating a BlockMatrix with ZeroMatrix blocks and performing multiple consecutive multiplications. An experienced engineer familiar with SymPy would be able to understand what needs to be fixed - ensuring that block multiplication preserves the correct matrix types throughout the operation chain.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: 1) It involves understanding SymPy's matrix expression system, particularly how BlockMatrix and ZeroMatrix interact during multiplication operations. 2) The engineer needs to trace through the block multiplication logic to identify where Zero is being substituted for ZeroMatrix. 3) The solution requires modifying the _postprocessor function in matexpr.py to handle MatAdd cases specifically, which requires understanding the internal expression processing pipeline. 4) While the fix itself is only 2 lines of code, identifying the root cause and the correct place to apply the fix requires substantial investigation into the codebase and understanding of how matrix expressions are processed and simplified in SymPy.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues identified. The problem is well-contained within SymPy's matrix expression system, has a clear test case that demonstrates both the problem and expected solution, and the fix addresses a legitimate bug in the library's handling of matrix operations.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17655": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear about what needs to be fixed. It states there is an \"unexpected exception when multiplying geometry.Point and number\" and that \"both multiplication orders give the same result\" should be the expected behavior. Looking at the provided patches, this clearly refers to implementing the `__rmul__` method in the Point class to support reverse multiplication (e.g., `5 * point` in addition to `point * 5`). The test patches show exactly what should work: `assert 5 * p4 == Point(5, 5)` for both Point and Point3D classes. The problem is straightforward - Python's multiplication operator should be commutative for Point objects and numbers, but currently only `point * number` works, not `number * point`.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The solution requires understanding Python's special methods for arithmetic operations, specifically that `__rmul__` is needed to handle reverse multiplication when the left operand doesn't support multiplication with the right operand. The fix itself is trivial - just add a `__rmul__` method that calls the existing `__mul__` method. An experienced engineer familiar with Python's data model would recognize this pattern immediately. The main time would be spent understanding the codebase structure and running tests to verify the fix works correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent example for a coding benchmark. The issue is clearly defined, the solution is straightforward but requires specific Python knowledge about special methods, and the test cases clearly demonstrate the expected behavior. It tests both understanding of the problem and knowledge of Python's operator overloading mechanisms.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-18211": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified but requires some interpretation. The core problem is clearly stated: solveset raises NotImplementedError when it should return a ConditionSet instead. The issue provides a specific example showing what the expected output should look like (a ConditionSet with the equation n*cos(n) - 3*sin(n) = 0). However, the issue description is somewhat brief and doesn't provide complete context about where this occurs in the codebase or the exact conditions that trigger this behavior. An experienced engineer would need to understand the sympy codebase structure, particularly how solveset, as_set(), and ConditionSet work together. The reference to PR #17771 suggests this was discovered during other work, but the core requirement is clear: replace NotImplementedError with appropriate ConditionSet return values.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution, this is a relatively straightforward fix that takes 15 minutes to 1 hour. The patch shows that the solution involves modifying the _eval_as_set method in sympy/core/relational.py to catch NotImplementedError from solve_univariate_inequality and return a ConditionSet instead. The core logic is simple: wrap the existing call in a try-catch block and provide a fallback. The main time would be spent understanding the sympy codebase structure, locating the right method, and understanding how ConditionSet should be constructed. The actual code change is minimal (about 8 lines added) and follows a clear pattern. An experienced engineer familiar with exception handling would find this straightforward once they locate the relevant code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. The issue is technically sound, the solution is clean and well-documented in the patch, and the test cases clearly verify the expected behavior. The problem domain (symbolic mathematics) is specialized but the programming concepts (exception handling, return value modification) are standard. This would make a good benchmark sample as it tests both code comprehension and basic software engineering practices.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20428": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem where clear_denoms() on a constant polynomial that equals zero creates a Poly object that prints as zero but behaves inconsistently (is_zero=False). The root cause is clearly identified: the internal DMP representation retains a leading zero coefficient when it should have an empty coefficient list for zero polynomials in the EX domain. While the issue doesn't explicitly state \"fix this inconsistency,\" the problem and expected behavior are clear enough for an experienced engineer to understand what needs to be corrected. The connection to issue #17990's ZeroDivisionError provides additional context about the severity of the problem.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the actual solution, this required changing the __bool__ method in expressiondomain.py from checking `f.ex != 0` to `not f.ex.is_zero`. While the fix itself is a one-line change, arriving at this solution would require: 1) Understanding the SymPy polynomial system and how the EX domain works, 2) Investigating the clear_denoms() method and its interaction with DMP representations, 3) Identifying that the issue stems from how boolean evaluation is handled in the ExpressionDomain class, 4) Understanding the difference between `!= 0` and `.is_zero` in SymPy's expression system. This level of investigation and domain-specific knowledge puts it in the 1-4 hour range, especially given the need to understand the intricate relationship between domains, polynomial representations, and expression evaluation in SymPy.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-defined within the SymPy ecosystem, the solution is verifiable through the provided test case, and it represents a legitimate bug fix that tests both understanding of the codebase and debugging skills. The issue demonstrates a subtle but important distinction in how mathematical objects should behave consistently across different operations.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20438": {
            "q1_1_is_well_specified": 3,
            "q1_2_explanation": "The issue description is extremely minimal and vague. It only states \"is_subset gives wrong results\" with a reference to a GitHub comment and pull request, but provides no concrete examples, error messages, expected vs actual behavior, or any specific details about what the problem is. Without being able to follow the external link or read the discussion, it's nearly impossible to understand what needs to be fixed. An engineer would have no clear starting point to identify which subset operations are failing, what the incorrect results are, or what the correct behavior should be.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the gold patch, this issue involves multiple complex changes across different files in SymPy's set handling system. The solution includes: (1) Adding type checks in relational.py to prevent operations on non-Expr objects, (2) Removing a dispatch handler in comparison.py and modifying the general Set equality handler, and (3) Adding a new ProductSet-FiniteSet subset handler in issubset.py. The test patch shows this addresses issues with ProductSet subset operations and equality comparisons. Understanding SymPy's dispatch system, set theory implementations, and the interaction between different set types would require substantial investigation and careful consideration of edge cases.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "This sample has a fundamental flaw for benchmarking purposes: the issue description is so vague that without access to the referenced external discussion, an engineer cannot possibly understand what needs to be fixed. The actual solution is quite complex and involves understanding SymPy's internal architecture, but the issue text provides no guidance about where to start or what the expected behavior should be. This makes it unsuitable for evaluating coding ability since the problem statement itself is inadequate.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-20590": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clear: Symbol instances in SymPy version 1.7 have a __dict__ attribute when they shouldn't, based on the use of __slots__. The issue author suspects this is due to a parent class accidentally stopping to define __slots__. However, there are some gaps: (1) The issue doesn't specify exactly which Symbol class is affected or its location in the codebase, (2) It doesn't provide concrete reproduction steps or code examples, (3) The connection between __slots__ and __dict__ behavior may not be immediately obvious to all developers. Despite these gaps, an experienced engineer familiar with Python's __slots__ mechanism and the SymPy codebase could reasonably interpret that the solution involves ensuring proper __slots__ definitions in the inheritance hierarchy to prevent __dict__ creation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves adding a simple `__slots__ = ()` declaration to the Printable class in _print_helpers.py. While it requires understanding Python's __slots__ mechanism and how it affects inheritance, the actual code change is minimal (adding 6 lines). The main time would be spent: (1) Understanding the issue and locating the problematic class in the inheritance hierarchy, (2) Understanding how __slots__ works in multiple inheritance scenarios, (3) Writing a simple test to verify the fix. The solution doesn't require substantial code changes or deep architectural modifications.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of Python's object model (__slots__), debugging skills to trace inheritance hierarchies, and the ability to write appropriate tests. The issue is technical but well-contained, and the solution demonstrates both problem-solving ability and knowledge of Python internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-21379": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is somewhat vague and lacks crucial details needed for a complete solution. While it mentions that a PolynomialError occurs when using subs() with specific expressions involving hyperbolic functions and piecewise arguments, it doesn't provide a concrete minimal reproducible example. The description lists various conditions that affect the error (only happens with cosh/tanh, goes away without division, only with real assumptions), but without seeing the actual code that triggers the error, it would be difficult for an engineer to understand exactly what needs to be fixed. The issue mentions \"casting from int to float of all int atoms\" and \"TensorFlow lambdify\" but doesn't show the problematic code. An experienced engineer would need to do significant detective work to reproduce the issue and understand the root cause.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the gold patch, the solution involves adding a try-catch block around the gcd() function call in sympy/core/mod.py to handle PolynomialError exceptions. This is a relatively straightforward fix once the problem is understood - it's about 8 lines of code changes including imports. However, the difficulty lies in: (1) reproducing the issue from the vague description, (2) understanding that the problem occurs in the Mod class evaluation when gcd() is called, (3) identifying that catching PolynomialError and defaulting to G = S.One is the appropriate solution. The debugging and investigation phase would likely take 1-3 hours, while the actual code change is simple. The test cases show the specific expressions that trigger the issue, which would help in understanding the problem scope.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The main issue is that the GitHub issue description lacks a minimal reproducible example. Without seeing the exact code that causes the PolynomialError, an engineer would struggle to reproduce and debug the problem. The issue mentions various conditions but doesn't provide concrete code snippets. This makes it unsuitable for a coding benchmark where engineers need to work solely from the issue description. The test patch provides the missing examples, but in the benchmark setup, engineers wouldn't have access to that information. Additionally, the issue mentions context about TensorFlow integration that may not be relevant to the core problem, adding confusion.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-22714": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem: calling sp.S on a Point2D expression within an evaluate(False) context incorrectly raises an \"Imaginary coordinates are not permitted.\" error. The issue provides a specific error message, the context in which it occurs (evaluate(False)), and contrasts it with working scenarios (without evaluate(False) or when evaluate=False is passed directly to sp.S). The problem statement is concrete and reproducible, making it clear what behavior needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves changing a single condition in the Point class constructor from checking `im(a)` (which can be unevaluated) to checking `im(a).is_zero is False` (which properly handles unevaluated expressions). The fix is a one-line change that requires understanding how SymPy's evaluation system works with imaginary parts, but doesn't require extensive research or major code restructuring. An experienced engineer familiar with SymPy would need to locate the Point validation logic and understand why the imaginary part check fails in unevaluated contexts, then apply the appropriate fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is straightforward, and the test case clearly validates the fix. This is a good benchmark sample as it tests understanding of SymPy's evaluation system and proper handling of unevaluated expressions.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23824": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly explains: (1) The specific function affected (kahane_simplify() in physics.hep), (2) The exact problem - leading uncontracted gamma matrices are being reversed in order, (3) Provides concrete mathematical examples showing the expected vs actual behavior (\u03b3^\u03bc \u03b3_\u03bc \u03b3^\u03c1 \u03b3^\u03c3 and \u03b3^\u03c1 \u03b3^\u03c3 \u03b3^\u03bc \u03b3_\u03bc should both simplify to 4 \u03b3^\u03c1 \u03b3^\u03c3, but the order is flipped in the second case), (4) Even identifies the root cause - matrices are removed at the beginning and inserted backwards at the end. The issue author has clearly done the debugging work and provides enough detail for someone to understand and fix the problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue description identifies the exact problem location and cause - the insertion loop at the end of kahane_simplify() is backward. Looking at the gold patch, it's a simple 2-line change that replaces a backward insertion loop with a proper concatenation. The bug is well-understood from the description, the location is specified, and the fix is straightforward once you locate the problematic loop. An experienced engineer would need minimal time to understand the gamma matrix context and implement the fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, good test coverage in the patch, and the mathematical context is sufficiently explained. The issue would work well in a coding benchmark as it tests ability to understand mathematical code, identify the correct location for a fix, and implement a simple but precise solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23950": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in but provides a sensible interpretation of what needs to be fixed. It clearly states that \"Contains.as_set returns Contains\" which is wrong because \"Contains is not a set (it's a boolean)\". The issue explains that this causes failures elsewhere because Contains doesn't have as_relational since it isn't a set. While the description could be more detailed about the expected behavior, the core problem is clear: the as_set() method should return the actual set being contained in, not the Contains object itself. The gold patch confirms this interpretation by showing that as_set() should return self.args[1] (the set argument) rather than raising NotImplementedError or returning self.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The solution is quite straightforward once you understand the problem - the as_set() method in the Contains class simply needs to return the set argument (self.args[1]) instead of raising NotImplementedError. The change is literally one line of code modification. An experienced engineer would need some time to understand the Contains class structure and the args indexing, but the actual implementation is trivial. The main time would be spent understanding the codebase structure and verifying that args[1] is indeed the set being tested for containment.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample because: (1) the problem is clearly defined with a specific incorrect behavior, (2) the solution requires understanding the class structure but is implementable with minimal code change, (3) the test cases clearly validate the expected behavior by checking that as_set() returns the correct set objects, and (4) it tests understanding of object-oriented design principles where method behavior should match the method name semantics.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-24066": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue title mentions that \"SI._collect_factor_and_dimension() cannot properly detect that exponent is dimensionless\" but the issue description itself is extremely sparse - it only contains the title with no additional context, examples, or explanation of the problem. While the method name and general nature of the problem (dimensionless detection) gives some direction, there's no specific example of failing behavior, no indication of what the expected vs actual behavior should be, and no context about when this occurs. An engineer would need to do significant investigation to understand the specific failure mode and what constitutes a correct solution. The patches provide more context than the issue description itself.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This would require 1-4 hours because: (1) The engineer needs to understand SymPy's units system and dimensional analysis concepts, (2) investigate the _collect_factor_and_dimension method to understand its current behavior, (3) figure out the specific case where exponent dimensionlessness detection fails, (4) understand when and why exponents should be dimensionless (particularly for functions like exp()), and (5) implement the logic to properly detect and handle dimensionless exponents. The solution involves modifying the Function handling branch to check for dimensionlessness and convert accordingly, which requires understanding the codebase's dimension system API.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The main issue is that the GitHub issue description is essentially just a title with no body text, examples, or context. This makes it extremely difficult to understand what specific problem needs to be solved without looking at the patches or doing extensive code investigation. For a benchmark setup where only the issue text is provided, this would be nearly impossible to solve correctly. An engineer would have to guess at the specific failure mode and might implement a different solution than what's actually needed.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-24443": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides some important details but leaves room for interpretation. It clearly identifies that there's a problem in the `_check_homomorphism` function, specifically in the internal `_image()` function when handling PermutationGroups. The issue states that when `r[i]` is an \"inverted generator,\" the `in gens` test fails. However, the description is somewhat vague about what exactly constitutes an \"inverted generator\" and how this failure manifests. The mention that \"the whole thing can be greatly simplified\" suggests a refactoring opportunity but doesn't specify what the simplified approach should be. An experienced engineer could reasonably infer that the problem involves handling generator inverses in PermutationGroups and that the current logic for mapping generators to their images is flawed, but would need to examine the code and understand the mathematical context to devise a proper solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue falls into the 1-4 hours category for several reasons. First, it requires understanding the mathematical concepts behind group homomorphisms, PermutationGroups, and how generators and their inverses work in this context. The engineer needs to familiarize themselves with the existing code structure in the homomorphisms.py file and understand how the _image() function currently processes generators. The solution involves a substantial rewrite of the _image() function, replacing the complex while-loop logic with a cleaner approach using array_form and symbol mapping. The fix also requires understanding the relationship between generators, their symbols, and their representations across different group types (PermutationGroup, FpGroup, etc.). While not extremely esoteric, it requires both mathematical understanding and careful code analysis to implement correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is well-contained within a specific function, has a clear test case that demonstrates the fix works, and the mathematical domain knowledge required (group theory) is reasonable for a software engineering benchmark focused on mathematical/scientific computing libraries.",
            "q2_5_confidence": 4
        }
    }
]