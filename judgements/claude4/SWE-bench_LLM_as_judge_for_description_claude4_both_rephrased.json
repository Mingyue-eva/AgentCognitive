[
    {
        "astropy__astropy-13033": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some blanks to fill in. The core problem is clear: when a required column (other than 'time') is removed from a TimeSeries object, the error message incorrectly complains about the 'time' column instead of identifying the actual missing required column. The issue provides clear steps to reproduce and explains the expected vs actual behavior. However, there are some gaps: the issue doesn't specify exactly how required columns are \"manually registered\" (though the test patch shows this is done via _required_columns attribute), and it doesn't provide the exact current error message text. An experienced engineer would need to explore the codebase to understand the TimeSeries implementation and required columns mechanism, but the overall goal is clear - improve the error message to correctly identify which required columns are missing.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is localized to improving an error message in the _check_required_columns method. The solution involves: (1) Understanding the current error message generation logic, (2) Modifying the error message to properly format and display which required columns are expected vs found, and (3) Adding a helper function to format column names appropriately. The code patch shows this is a small, focused change - adding a helper function and modifying the error message formatting. Most of the time would be spent understanding the existing code structure and testing the fix, rather than complex problem-solving.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample is suitable for evaluating coding ability as it tests understanding of error handling, message formatting, and working with existing class methods. The issue is realistic and the solution demonstrates good software engineering practices.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-13977": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is relatively well-specified with some blanks to fill in. The author provides a clear description of the problem: `Quantity.__array_ufunc__()` should return `NotImplemented` instead of raising `ValueError` when inputs are incompatible, particularly for duck typing scenarios. They provide a detailed minimal working example explaining the expected behavior versus actual behavior, and reference the numpy docs about returning `NotImplemented`. However, there are some implementation details that would need to be figured out by the engineer, such as exactly which conditions should trigger returning `NotImplemented` vs raising exceptions, and how to detect incompatible inputs that should trigger this behavior. The core requirement is clear but the specific implementation strategy requires some interpretation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task. The engineer needs to: 1) Understand the numpy `__array_ufunc__` protocol and when `NotImplemented` should be returned, 2) Analyze the existing `Quantity.__array_ufunc__` implementation to understand where ValueError is currently raised, 3) Implement logic to detect incompatible inputs that should return `NotImplemented` instead of raising exceptions, 4) Add proper exception handling with try/catch blocks, 5) Write comprehensive tests for the new behavior. Looking at the patch, this involves wrapping the existing logic in a try/catch block and adding logic to determine when to return `NotImplemented` vs re-raise the exception. The solution requires understanding of duck typing, the numpy protocol, and astropy's internal unit handling logic. It's more than a trivial change but not extremely complex.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The issue is well-documented with clear expectations, has a reasonable scope, and the solution can be properly tested. The provided patches show that this is a legitimate issue with a working solution that improves compatibility with duck typing.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14096": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with a clear reproduction case. The description explains that when subclassing SkyCoord and creating a property that accesses a non-existent attribute, the error message incorrectly reports that the property itself doesn't exist rather than the underlying attribute. The reproduction steps are clear: subclass SkyCoord, add a property that tries to access a non-existent attribute, instantiate with specific coordinates, and access the property. However, there are some minor gaps - the exact error message format isn't specified, and it requires some understanding of Python's attribute access mechanism to fully grasp what the \"correct\" behavior should be.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is in the __getattr__ method of SkyCoord class, which currently raises a generic AttributeError when an attribute isn't found. The solution shown in the patch is elegant and simple: instead of manually raising AttributeError, call self.__getattribute__(attr) which will provide the correct exception with proper attribute name. This requires understanding Python's attribute access methods (__getattr__ vs __getattribute__) but the actual code change is minimal - just 2 lines changed. An experienced engineer familiar with Python's object model could identify and fix this relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly reproducible, the solution is straightforward, and the test case properly verifies the fix. The issue demonstrates good software engineering practices with a clear regression test that checks the specific error message content. This is a good example for evaluating understanding of Python's attribute access mechanism and object-oriented programming concepts.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14182": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the desired functionality (supporting header_rows parameter in RestructuredText output), provides a concrete reproduction scenario with specific examples (table with \"wave\" and \"response\" columns, values of 350 nm/950 nm and 0.7/1.2), explains the current error behavior (TypeError because header_rows parameter not accepted), and shows the expected output format. The issue also explains the use case context (autogenerated documentation). An experienced engineer would have enough information to understand exactly what needs to be implemented.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours of work. The engineer needs to: (1) understand the existing FixedWidth and RST classes and how header_rows works in other formats, (2) modify the RST class constructor to accept header_rows parameter and pass it to the parent class, (3) update the write() method to handle variable header row counts in the line indexing logic, (4) modify the read() method to adjust start_line based on header rows, and (5) remove the hardcoded start_line from SimpleRSTData. While not trivial, this involves understanding the existing architecture and making logical extensions rather than fundamental redesign.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained feature enhancement with clear requirements and testable outcomes. The issue provides sufficient context and examples, and the solution involves logical extensions to existing functionality rather than complex algorithmic changes. It would make a good benchmark sample for assessing an engineer's ability to understand existing code patterns and extend them consistently.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14309": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified with clear problem description, reproduction steps, and expected behavior. It describes a specific IndexError when the FITS connector tries to access args[0] from an empty tuple in the identify_format function. The issue provides sufficient context including the likely cause (referenced astropy commit), steps to reproduce (calling registry with .ecsv file), and the expected outcome. An experienced engineer would understand they need to fix the is_fits function to handle the case where no positional arguments are provided, preventing the IndexError when checking for HDU objects.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is a straightforward bug in the is_fits function in astropy/io/fits/connect.py. Looking at the patch, the solution involves changing the logic flow to return early when filepath has a FITS extension, preventing the code from reaching the problematic args[0] access. The fix is only 4 lines of code changes - removing an if statement and returning the boolean expression directly. An experienced engineer could identify the problem location, understand the logic flow issue, and implement the fix relatively quickly after familiarizing themselves with the specific function.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is well-defined with clear reproduction steps, the solution is focused and testable, and it represents a realistic debugging scenario that tests ability to understand code flow and handle edge cases. The accompanying test ensures the fix can be validated automatically.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14365": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the ascii.qdp Table format assumes QDP commands must be upper case (e.g., \"READ SERR 1 2\"), but QDP itself is case-insensitive and should accept lowercase commands (e.g., \"read serr 1 2\"). The issue provides a concrete example of the expected behavior, showing that a file with \"read serr 1 2\" should load into a Table with errors rather than crashing. The reproduction steps are detailed and specific: create a test.qdp file with lowercase commands, import Table, and use the ASCII QDP reader. The current behavior (warning then exception) vs expected behavior (successful loading) is clearly articulated. An experienced engineer would have enough information to understand exactly what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it involves adding re.IGNORECASE flag to a regex compilation and changing one string comparison to use .upper(). The core issue is that the QDP parser uses case-sensitive regex matching and string comparisons when it should be case-insensitive. An experienced engineer would need to: (1) locate the QDP parsing code in astropy/io/ascii/qdp.py, (2) identify where case-sensitive matching occurs (the regex compilation and \"NO\" value checking), and (3) make these operations case-insensitive. The solution is straightforward once the problematic code is found, requiring minimal changes to existing logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-suited sample for the benchmark. The issue is clearly defined with concrete examples, the solution involves understanding case-sensitivity in parsing logic, and the fix requires targeted changes to specific functions. The test coverage shows both the fix and regression testing with lowercase/uppercase variants. An engineer can verify their solution works by testing the provided example case.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14508": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: `io.fits.Card` uses unnecessarily long string representations of floats (e.g., \"0.009124999999999999\" instead of \"0.009125\"), which causes comments to be truncated due to length limits. The issue provides a clear reproduction scenario with specific steps: 1) Read a FITS file with a card containing value 0.009125 and a comment, 2) Try to recreate the same card programmatically, 3) Observe that the float gets expanded to a longer representation causing comment truncation. The expected behavior is also clear: being able to create any valid FITS Card via `io.fits.Card` without unnecessary precision causing truncation. The issue includes specific values, error messages, and version information.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, the fix involves modifying the `_format_float` function in `astropy/io/fits/card.py`. The original implementation used `f\"{value:.16G}\"` which could create unnecessarily long representations. The fix simplifies this to use `str(value)` which provides a more concise representation, then applies the existing 20-character limit logic. The change is relatively small (replacing about 15 lines with 4 lines) and focused on a single function. An experienced engineer familiar with the codebase would need some time to understand the FITS card format requirements and the existing truncation logic, but the actual code change is straightforward once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the reproduction steps are specific, and the solution involves a focused change to a single function. The test cases provided in the patch validate that the fix works correctly for the reported scenario. This is a good benchmark sample as it tests understanding of string formatting, floating point representation, and working within character limits - all common programming challenges.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14995": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The specific problem - mask propagation fails in v5.3 when one operand doesn't have a mask, specifically with `handle_mask=np.bitwise_or`, (2) The expected behavior - the existing mask should be copied to the output (as worked in v5.2), (3) Detailed reproduction steps with specific operations (multiply masked reference by scalar, multiply masked by unmasked reference), (4) The exact error condition - bitwise OR cannot be applied between integer and None, and (5) Version information showing the regression from v5.2 to v5.3. The issue provides enough context about NDDataRef, mask propagation, and arithmetic operations for an experienced engineer to understand and solve the problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a simple one-line fix in the `_arithmetic_mask` method in `astropy/nddata/mixins/ndarithmetic.py`. The bug is on line 523 where `elif operand is None:` should be `elif operand.mask is None:`. The issue occurs because the code checks if the operand object is None rather than if the operand's mask is None, causing the wrong branch to execute when an operand exists but has no mask. This is a straightforward logical error that would take 15-60 minutes to identify and fix: time to understand the mask propagation logic, locate the bug in the conditional check, and make the correction. The accompanying test case is also relatively simple to write once the fix is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is clean and focused, and the test coverage adequately verifies the fix. This is a good example of a regression bug with clear reproduction steps and expected behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7336": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: the `units.quantity_input` decorator fails when applied to constructors that have a type hint return value of `None`. The issue includes a detailed summary explaining that the decorator tries to call `.to()` on `None` (which doesn't have this method), a reproducer section that describes exactly how to trigger the bug (define a class with `__init__` decorated with `units.quantity_input`, requiring a voltage quantity argument, and annotate the constructor to return `None`), the expected vs actual behavior (object should be created successfully but instead the decorator fails), a workaround (omitting the return type hint), and even suggests a possible fix (explicitly check for None return values). The technical context is provided including Python 3.6.3, Astropy 2.0.2, and NumPy 1.13.3 versions.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. Looking at the provided patch, the solution is a simple one-line change in `astropy/units/decorators.py` where the condition `if wrapped_signature.return_annotation is not inspect.Signature.empty:` is changed to `if wrapped_signature.return_annotation not in (inspect.Signature.empty, None):`. The issue is clearly identified - the decorator doesn't handle the case where the return annotation is explicitly `None`. An experienced engineer would need to: 1) Understand that constructors return None and this is a valid type hint, 2) Locate the decorator code that processes return annotations, 3) Add None to the check that determines whether to apply unit conversion to the return value. The fix requires minimal code change and the logic is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. It has a clear problem statement, reproducible steps, and the solution is focused and testable. The test additions show exactly what behavior should be verified (that a constructor with None return type annotation works correctly). An engineer would be able to understand and solve this without any external dependencies or complex domain knowledge beyond basic understanding of Python decorators and type hints.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7606": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) How to reproduce the problem - create a Unit instance using an unrecognized unit identifier with silent parsing enabled, then check equality with None; (2) The current problematic behavior - a TypeError is raised because the code tries to interpret None as a unit; (3) The expected behavior - the comparison should return False instead of raising an error. The issue provides specific context about UnrecognizedUnit and the equality comparison mechanism, making it clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the provided patches, the solution involves modifying the __eq__ methods in two classes (Unit and UnrecognizedUnit) to properly handle None comparisons by returning NotImplemented instead of trying to convert None to a Unit. The fix is relatively straightforward - it involves adding try-catch blocks around Unit conversion attempts and returning NotImplemented when conversion fails. The core logic change is small and localized to equality methods, requiring minimal code changes but some understanding of Python's equality comparison protocol.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is clearly described, the solution is focused and testable, and the patches show a clean implementation that follows Python best practices for equality comparison (returning NotImplemented when comparison with incompatible types fails). The test cases also adequately cover the reported issue.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7671": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The root cause - a bug in LooseVersion when comparing version strings with development labels like \"1.14dev\" against full release versions like \"1.14.3\", (2) The specific error - TypeError when comparing integer and string components, (3) The context - this was introduced by PR #7647 and affects the minversion utility function, (4) Concrete examples showing when it fails (\"1.14.3\" vs \"1.14dev\") and when it works (\"1.14\" vs \"1.14dev\"), and (5) Evidence that pkg_resources.parse_version (which was removed) previously handled this correctly. The problem is technical but precisely defined with clear reproduction cases.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-understood (LooseVersion comparison bug), the location is clear (minversion function in astropy/utils/introspection.py), and the solution approach is straightforward - extract only the numeric version parts before comparison. Looking at the gold patch, it's a focused 8-line addition using a regex to extract the dotted numeric portion of version strings before passing to LooseVersion. While some research might be needed for the exact regex pattern (though PEP440 is referenced), the core logic is simple and the change is localized to one function.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution scope is clear, and both the issue description and test cases provide good guidance for validation. This would make a good benchmark problem as it tests understanding of version comparison edge cases and regex usage for parsing.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-10554": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a specific reproduction scenario involving Django querysets with union operations and ordering. While it doesn't include exact code to reproduce the problem, it clearly describes the sequence of operations: retrieving dimension IDs, building a union queryset with ordering on one part, clearing ordering, getting primary keys, then attempting to display the original queryset again. The expected behavior (showing four dimensions) and the actual error (ORDER BY position not in select list) are clearly stated. The description mentions this is related to derived querysets and union operations with ordering, which gives sufficient context. An experienced Django developer could reasonably understand this refers to Django ORM queryset operations and construct a reproduction case from the description.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the provided patch, this issue requires understanding Django's SQL compiler internals, specifically how ORDER BY clauses are handled in union queries with values_list operations. The solution involves modifying the get_order_by method in the SQL compiler to handle cases where ORDER BY terms don't match columns in the result set by adding those columns to the select clause. This requires deep knowledge of Django's ORM internals, SQL generation, and how querysets are compiled. The fix spans multiple files (compiler.py and query.py) and requires adding a new method (add_select_col). While not massive in scope, it requires substantial understanding of Django's query compilation process and careful consideration of edge cases. This is not a trivial fix but also not extremely complex - it's a focused solution to a specific SQL generation issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. The description provides enough information for an experienced developer to understand the problem, even if they need to write their own reproduction code. The solution requires meaningful technical depth without being overly esoteric. The test cases in the patch provide clear verification criteria for whether a solution works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11087": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some blanks to fill in. The core problem is clearly described: Django's .delete() operation fails with a UnicodeDecodeError when upgrading from Python 2.7 to Python 3.6, and the error occurs because Django fetches unnecessary fields (like text_log_error.line) during cascade deletion that contain invalid unicode data. The issue identifies two specific problems: (1) mysqlclient-python behavior differences between Python versions, and (2) Django fetching fields not needed for deletion. The issue specifically mentions that fixing issue (2) would improve .delete() performance by only fetching required fields. However, some details are missing - the exact implementation approach isn't specified, and an engineer would need to understand Django's ORM deletion mechanics and determine which fields are \"required\" vs \"unnecessary\" for deletion operations. The provided SQL queries and error traces give good context, but the solution approach requires domain knowledge of Django's deletion system.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because: (1) It requires understanding Django's complex deletion system, particularly the cascade deletion logic in django/db/models/deletion.py; (2) The engineer needs to identify which fields are truly \"required\" for deletion (primary keys, foreign key references) vs unnecessary fields that cause unicode issues; (3) The solution involves modifying the collect() method to use .only() to defer non-referenced fields, which requires understanding Django's query optimization; (4) The implementation needs to handle edge cases like when deletion signals are connected (which require all fields) and select_related interactions; (5) The code changes span multiple concepts: signal handling, field referencing, and query optimization. While not extremely complex, it requires substantial Django ORM knowledge and careful consideration of various scenarios.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-motivated with a real-world scenario (Python 2 to 3 migration), the issue description provides good technical context including SQL queries and error traces, and the expected outcome (performance improvement + fixing unicode errors) is clear. The test case appropriately validates both the optimization (only required fields selected) and the signal handling edge case. This is a good benchmark sample that tests Django ORM knowledge, optimization skills, and edge case handling.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11265": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps that need filling. The description clearly explains the problem: using exclude() on a queryset with annotated FilteredRelation fails with a FieldError, saying it cannot resolve the annotation name. The issue provides a concrete example from Django's test suite involving an Author model with a book_alice annotation filtered by book title. It also identifies the likely problematic function (split_exclude) and suggests the root cause (new query created without extra data from original). However, some details are missing: the exact error message, the specific test case being referenced, and the complete code snippet that reproduces the issue. An experienced engineer could reasonably infer what needs to be fixed based on the description and the context of Django ORM behavior.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the gold patch, the fix involves understanding Django's complex query system, specifically how split_exclude() works and how filtered relations are handled. The engineer needs to: 1) Understand Django's ORM internals, particularly Query objects and how annotations/filtered relations are stored 2) Debug why the split_exclude function creates a new Query without preserving _filtered_relations 3) Identify that filtered relations need special handling in the trim_start method to avoid removing necessary joins. The actual code changes are relatively small (adding one line and modifying a condition), but requires deep understanding of Django's query construction and join optimization logic. This type of ORM internals issue typically requires substantial investigation and testing to ensure the fix doesn't break other functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is well-contained within Django's ORM query construction logic, the test case clearly demonstrates the expected behavior, and the solution is focused on a specific technical issue rather than requiring broad architectural changes or subjective design decisions.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11299": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: Django generates incorrect SQL for CheckConstraints with OR operators on SQLite and Oracle, where table names are inconsistently qualified in different parts of the constraint. The issue describes the symptoms (migration failure during table rename), provides a reproduction scenario (model with integer field and boolean flag), and explains the expected behavior (no table qualification in constraint definitions). However, it lacks concrete code examples showing the problematic model definition and the exact malformed SQL being generated. An experienced engineer would need to infer the specific model structure and constraint syntax from the description, but the overall requirement is clear enough to work toward a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix based on several factors: (1) The issue involves Django's query generation system, specifically how CheckConstraints are processed with complex Q objects containing OR operations, requiring understanding of Django's ORM internals. (2) The fix involves modifying the query building logic in django/db/models/sql/query.py, specifically ensuring the simple_col parameter is properly propagated through recursive _add_q calls. (3) While the actual code change is small (adding one parameter), identifying the root cause requires tracing through Django's complex query building system and understanding how different database backends handle table qualifications. (4) The engineer needs to understand the interaction between migrations, schema operations, and constraint generation across multiple database backends.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is database-specific (SQLite and Oracle) but the solution appears to be in the core query building logic. The test cases provide good coverage for the fix, testing both the constraint addition operation and the underlying query building with complex Q objects containing OR operations.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11532": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It clearly describes the problem: when the computer hostname contains non-ASCII characters (like \"\u6b63\u5b97\") and email encoding is set to a non-unicode format (like iso-8859-1), Django fails to properly encode the Message-ID header, resulting in a UnicodeEncodeError. The issue provides specific reproduction steps, identifies the root cause (hostname in unicode cannot be represented in ISO-8859-1), points to relevant code locations (django/core/mail/message.py#L260), and suggests a clear solution direction (convert domain name to punycode). However, some implementation details need to be figured out, such as exactly where to implement the punycode conversion and how to handle it consistently across the codebase.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) The problem requires understanding Django's email system architecture and how headers are encoded, (2) The solution involves creating a new punycode utility function and updating multiple files (message.py, utils.py, validators.py, html.py, encoding.py), (3) It requires understanding internationalized domain names (IDN) and punycode conversion, (4) The fix needs to be applied consistently across different parts of the codebase where domain encoding occurs, (5) Writing appropriate tests to verify the fix works correctly. While the core concept is straightforward (convert unicode domains to punycode), implementing it properly across Django's mail system requires moderate engineering effort.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, reproducible, and the solution approach is well-understood. The provided test case gives a clear success criteria. This is a legitimate bug that affects real-world usage when systems have unicode hostnames.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11551": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is exceptionally well-specified. It provides:\n\n1. A clear problem statement: admin.E108 error is incorrectly raised on fields accessible only via instance (specifically PositionField from django-positions library)\n2. Detailed reproduction steps: create a model with PositionField, register it with ModelAdmin including the field in list_display\n3. Expected vs actual behavior: should start without error (as in Django 2.0.7) but fails in Django 2.2.1\n4. Root cause analysis: points to specific commit (47016adbf54b54143d4cf052eeb29fc72d27e6b1) and explains how the hasattr(model, item) check breaks functionality\n5. Technical explanation of why PositionField fails: its __get__ method throws an exception when called on the class rather than instance\n6. Comprehensive truth table showing all logical test cases and their behavior across versions\n7. Complete proposed solution with corrected _check_list_display_item function\n\nThe issue targets a specific function (django.admin.contrib.checks._check_list_display_item) and provides both the problematic logic flow and a concrete fix. An engineer would have all the information needed to understand the problem, verify the root cause, and implement the solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because:\n\n1. The issue description provides the exact function that needs modification (_check_list_display_item in django/contrib/admin/checks.py)\n2. A complete corrected implementation is provided in the issue description\n3. The root cause is clearly explained with the problematic hasattr(model, item) check\n4. The fix involves restructuring the conditional logic to use try/except blocks instead of hasattr checks\n5. While the logic flow needs careful consideration to handle all edge cases correctly, the scope is limited to a single function\n6. The changes are localized and don't require understanding complex interdependencies across the codebase\n\nAn experienced engineer would need some time to understand the validation logic and verify the proposed solution handles all cases correctly, but the implementation itself is straightforward given the detailed analysis provided.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample as it tests understanding of Django's admin validation system, descriptor protocols (__get__ method behavior), exception handling patterns, and logical flow restructuring. The issue is self-contained within Django's codebase and doesn't require external dependencies beyond understanding how custom field descriptors work.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11734": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear technical problem: OuterRef in exclude() or ~Q() uses the wrong model. The description explains the specific scenario - using Exists subqueries with OuterRef in exclude operations or negated Q expressions causes a ValueError about outer query references. The description mentions that a test is added in tests/queries/test_qs_combinators that demonstrates the issue in two stages. However, some details need to be inferred: the exact models (Number, Item, tag) and their relationships aren't fully specified in the description, and the specific error message details could be clearer. Despite these gaps, an experienced engineer familiar with Django ORM could reasonably understand this is about incorrect handling of OuterRef expressions in exclude operations and work toward a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue because it requires understanding Django's ORM internals, specifically how OuterRef expressions are handled in subqueries and exclude operations. The solution involves changes to multiple files (fields/__init__.py, related_lookups.py, and sql/query.py) and requires understanding the interaction between OuterRef expressions, query compilation, and the exclude mechanism. The engineer would need to trace through Django's query building process, understand how OuterRef expressions should be preserved during exclude operations, and implement the correct handling logic. The patch shows it's not a trivial fix but also not extremely complex - it involves targeted changes to specific methods rather than massive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined within Django ORM context, the test case provides clear verification of the fix, and the solution addresses the core issue of OuterRef handling in exclude operations. The issue is suitable for evaluating coding ability as it requires understanding of Django internals and careful implementation.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11740": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The user clearly describes the problem: when changing a UUID field to a foreign key field in Django, the migration autodetector fails to create proper dependencies between apps, causing migration failures. The user provides a concrete scenario with two apps (testapp1 and testapp2), describes the initial state (UUID field), the desired state (FK field), and the observed behavior (missing dependencies in migration). However, some details are missing - the user mentions an attached archive and test suite that we cannot see, and references an external forum post. The core technical issue is clear enough that an experienced Django developer could understand what needs to be fixed: the migration autodetector should detect when a field change introduces a foreign key relationship and add appropriate inter-app dependencies.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the gold patch, the solution requires understanding Django's migration autodetector system, specifically the `generate_altered_fields` method in `django/db/migrations/autodetector.py`. The fix involves adding dependency detection for foreign keys when fields are altered (not just when they're created). The engineer needs to: 1) Understand the existing dependency detection logic for foreign keys, 2) Identify why it's missing for field alterations, 3) Locate the `_get_dependencies_for_foreign_key` method and understand its purpose, 4) Integrate it into the `generate_altered_fields` flow. The code change is relatively small (adding a few lines) but requires navigating Django's complex migration system and understanding how dependencies work across apps. The test also shows this requires understanding the migration testing framework. This goes beyond a simple fix but isn't extremely esoteric.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate Django framework bug with a clear technical solution. The issue description provides enough context for an experienced developer to understand and reproduce the problem, even without the referenced external materials.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11815": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when using an Enum object as a default value for a CharField, Django's migration system serializes the enum's value instead of its name, causing issues with translated values. The issue provides a clear reproduction scenario involving Django's lazy translation functions, explains the root cause (migrations embed translated text instead of enum member names), and suggests the correct solution (refer to enum members by their constant names using Status['GOOD'] syntax). The description includes specific technical details about when the error occurs (when applying migrations in different locales) and what type of error is raised (ValueError about translated terms not being valid choices).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the provided solution, it's a focused change to the EnumSerializer class in django/db/migrations/serializer.py. The fix involves changing how enum values are serialized - instead of serializing the enum's value, it now serializes a reference to the enum member by name using bracket notation (e.g., 'EnumClass[\"MEMBER_NAME\"]'). This requires understanding the Django migration serialization system and enum behavior, but the actual code change is small and straightforward - just replacing a few lines in the serialize() method. The accompanying test changes show the expected behavior clearly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with a clear solution that demonstrates good understanding of Django's migration system, Python enums, and internationalization concerns. The issue description provides sufficient context, the solution is focused and elegant, and the test coverage appropriately validates the fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11964": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that when using TextChoices/IntegerChoices enums in Django model fields, freshly created instances return enum members (like \"MyChoice.FIRST_CHOICE\") instead of the raw values (like \"first\") when accessed. The issue provides a concrete example with MyChoice enum and MyObject model, explains the reproduction steps, describes the expected behavior (plain string values), and identifies the runtime error (enum member string representation instead of raw value). The problem is particularly clear about the inconsistency: freshly created instances behave differently from database-retrieved instances, and the __str__() method returns the enum name rather than the value property, causing issues with external API communication.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the provided solution, it's a very straightforward implementation - just overriding the __str__ method in the Choices class to return str(self.value) instead of the default enum string representation. The fix is only 6 lines of code including comments. An experienced engineer would quickly understand that the issue is with how enum members are converted to strings, and the solution involves customizing the __str__ method. The django/db/models/enums.py file location is logical, and the fix doesn't require complex logic or extensive research into the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the test case provided validates the expected behavior across different enum types. This is a good benchmark sample as it tests understanding of Python enums, Django model behavior, and method overriding concepts.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12155": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) the problem - docutils fails when rendering docstrings that don't have an empty first line, (2) the exact location of the bug in the trim_docstring function, (3) the specific line causing the issue: \"indent = min(len(line) - len(line.lstrip()) for line in lines if line.lstrip())\", (4) the root cause - the indentation of the first line is 0 which breaks the calculation, and (5) a proposed solution to skip the first line. The issue provides enough technical detail for an engineer to understand both the symptom (docutils error about no content permitted) and the underlying cause in the code.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic code and suggests a specific solution. Looking at the patch, the actual solution was even more elegant - replacing the custom trim_docstring function with Python's built-in inspect.cleandoc function, which already handles this edge case correctly. An experienced engineer would need some time to: (1) understand the docstring processing flow, (2) recognize that inspect.cleandoc is the standard solution for this problem, (3) verify that cleandoc handles the edge case properly, and (4) update the imports and function calls. The changes are localized to a few files and involve replacing function calls rather than complex logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution space is clear, and the patch demonstrates a clean resolution using standard library functions. This would be a good benchmark issue as it tests understanding of Python docstring conventions, knowledge of standard library utilities, and ability to refactor code appropriately.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12262": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clearly described: custom template tags with keyword-only arguments that have defaults are raising TemplateSyntaxError with misleading error messages. The issue mentions that when a keyword-only argument with a default is used, it triggers an \"unexpected keyword argument\" error, and when the same argument is supplied twice, it also gives the wrong error message instead of reporting duplicate arguments. However, the issue could be clearer about the exact template syntax being used and what the expected behavior should be. The mention of \"inclusion tags\" having the same behavior provides additional context. While there are some details to infer, an experienced Django developer could reasonably understand what needs to be fixed based on the description and the provided patches.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves a single line change in django/template/library.py, specifically changing the condition in the parse_bits function from checking `param not in params` to `param not in params and param not in kwonly`. The fix is quite simple once you understand that keyword-only parameters need to be treated differently from regular parameters in the validation logic. The main time would be spent understanding the template parsing code and how keyword-only arguments are handled, but the actual code change is minimal and straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a clear bug in Django's template system with a well-defined fix. The test cases show the expected behavior clearly, and the solution doesn't require any complex architectural changes. This would be a good benchmark sample as it tests understanding of Python's keyword-only argument semantics and Django's template parsing logic.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12325": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem scenario: when defining a Django model with inheritance (Document base class, Picking subclass) that has two OneToOne relationships to the parent, Django incorrectly raises an ImproperlyConfigured error demanding that both fields be marked as parent_link=True, when only one should be. The issue also mentions that field order affects this behavior, which shouldn't happen. While the description could be more precise about the exact model definitions, there's enough information to understand the core problem: Django's model inheritance logic is incorrectly processing multiple OneToOne fields to the same parent model. The solution direction is clear - fix the logic that determines which OneToOne fields should be considered parent links.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's model inheritance system, specifically how parent_link detection works in the metaclass and model options preparation. The engineer needs to: 1) Understand Django's MTI (Multi-Table Inheritance) implementation, 2) Locate the code that processes OneToOne fields during model creation, 3) Identify why the current logic incorrectly flags all OneToOne fields to parent models as needing parent_link=True, 4) Modify the logic to only consider fields explicitly marked with parent_link=True or the automatically created parent pointer. Looking at the patches, this involves changes to django/db/models/base.py and django/db/models/options.py, requiring understanding of Django's internal model metaclass and field processing logic. The fix itself is relatively small (adding a condition and removing error-raising code), but understanding the problem domain and testing the solution properly takes significant time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant additional issues. The problem is well-contained within Django's model inheritance system, and the test patches show comprehensive test coverage for the fix. An experienced engineer familiar with Django's internals should be able to solve this with the information provided.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12663": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified but has some gaps that require reasonable interpretation. The description clearly explains: (1) there's a regression introduced by a specific commit, (2) it involves SimpleLazyObject usage with nested subquery annotations, (3) provides concrete models (A, B, C with specific relationships), and (4) describes the expected behavior vs actual failure. However, the issue doesn't provide the exact test case code that reproduces the problem - it only describes the steps conceptually (\"you first build a subquery\", \"you create a Django user through SimpleLazyObject\", etc.). An experienced engineer would need to construct the actual failing code based on the description, but the intent and expected outcome are clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django's ORM internals, specifically how queryset filtering and value conversion works, (2) The engineer needs to trace through the code path where SimpleLazyObject values are being incorrectly processed during field conversion, (3) Looking at the actual fix in query.py, it's a small but non-trivial change that requires understanding the difference between 'field' and 'target' attributes on select objects, and when to use each, (4) The engineer would need to write comprehensive tests to ensure the fix works correctly, (5) The issue involves complex interactions between subqueries, annotations, and lazy objects, requiring careful analysis of the Django ORM codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample is suitable for evaluating coding ability as it requires understanding of Django ORM internals, debugging skills, and the ability to write appropriate test cases. The fix, while small, demonstrates understanding of object attribute handling and field resolution in complex ORM scenarios.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12708": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with a clear reproduction scenario and runtime error description. The user explains that when a model has both unique_together and index_together on the same fields, and you try to remove only the index_together, Django crashes during migration because it can't distinguish between the two constraints. The problem is clear: Django's migration system fails when trying to delete an index that shares fields with a unique constraint. However, some implementation details are left to be figured out by the engineer, such as exactly how to modify the constraint identification logic in the Django codebase.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django's migration system and database schema operations, specifically how _delete_composed_index works; (2) The engineer needs to identify that the issue is in the constraint identification logic where Django can't distinguish between unique constraints and index constraints on the same fields; (3) The solution involves modifying the constraint parameters passed to _delete_composed_index to be more specific (adding 'unique': False); (4) While the actual code change is small (just a few lines), it requires understanding the intricate relationship between Django's ORM, migrations, and database introspection systems. The engineer also needs to understand how database constraints work and how Django represents them internally.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear problem statement and a targeted solution. The test case provided shows exactly what behavior is expected, and the fix demonstrates a good understanding of Django's constraint management system. The issue is realistic and represents a genuine bug that developers would encounter in practice when working with Django models and migrations.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12754": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is well-specified with some blanks to fill in. The description clearly explains the problem: when creating a model subclass and moving a field to it in the same migration, Django's migrate command fails with a FieldError even though makemigrations works. The issue provides a concrete reproduction scenario (base model \"Readable\" with \"title\" field, refactoring to move \"title\" to new \"Book\" subclass), describes the expected result (migration should apply cleanly), and explains the runtime error (FieldError about field clash). The issue also mentions that reversing operation order fixes it and suggests the auto-detector should handle this. While some implementation details need to be figured out, the core problem and desired outcome are clear enough for an experienced engineer to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because it involves understanding Django's migration system internals, specifically the autodetector logic that determines operation ordering and dependencies. The engineer needs to: 1) Understand how Django's migration autodetector works and generates dependencies between operations, 2) Identify why the current dependency logic doesn't handle the case of moving fields from base to subclass, 3) Implement logic to detect when a field is being removed from a base model and added to a subclass, and 4) Add appropriate dependencies to ensure proper operation ordering. The solution requires modifying the autodetector.py file to add dependency logic for removed base fields, which involves understanding Django's model state management and migration dependency system. While not extremely complex, it requires substantial domain knowledge and careful implementation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues that would prevent this from being a good benchmark sample. The issue is well-documented, has a clear reproduction case, and the solution is focused and testable. The provided test case effectively validates that the fix works by ensuring the correct operation ordering (RemoveField before CreateModel). This is a realistic Django development scenario that tests understanding of migration internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12858": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some details to fill in. The core problem is clear: Django's system check (models.E015) incorrectly raises an error when ordering uses lookups (like `__isnull`) that are not transforms. The issue provides a concrete example with the `Stock` model using `supply__product__parent__isnull` ordering that works in practice but fails validation. The reporter mentions this used to work before issue #29408, providing historical context. However, some specifics need interpretation: the exact location of the validation code, the difference between lookups and transforms in Django's context, and how the validation logic should be modified. The provided patches clarify that the fix involves updating the `_check_ordering` method in `django/db/models/base.py` to also check for lookups via `fld.get_lookup(part)`, not just transforms.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution is quite straightforward once you understand the problem: the validation logic in `_check_ordering` only checks for transforms but should also check for lookups. The fix is a simple one-liner addition to the conditional check, adding `and fld.get_lookup(part) is None` to the existing condition. An experienced engineer would need some time to locate the `_check_ordering` method in the Django codebase, understand the difference between transforms and lookups, and verify that `get_lookup` is the appropriate method to call, but the actual code change is minimal and the logic is clear from the error description.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue demonstrates a clear bug in Django's validation logic with a concrete example. The solution requires understanding Django's ORM internals (transforms vs lookups) but is straightforward to implement once identified. The test case also properly validates the fix by ensuring that ordering with lookups like `__isnull` no longer triggers validation errors.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13012": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. The core problem is clearly described: when wrapping a constant expression in an ExpressionWrapper, Django incorrectly includes that constant in the GROUP BY clause, causing PostgreSQL to reject the query. The issue provides a clear reproduction scenario involving ExpressionWrapper with integer output field, annotations, grouping, and aggregation. It also mentions the key contrast that unwrapped constants work correctly (are omitted from GROUP BY). However, the issue lacks specific code examples showing the exact reproduction steps, specific function names, or the precise SQL being generated. An experienced engineer would need to experiment to understand the exact Django ORM patterns that trigger this bug, but the high-level problem and expected behavior are clear enough to work towards a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the patch, the solution is quite simple - adding a single method `get_group_by_cols()` to the ExpressionWrapper class that delegates to the wrapped expression. The fix is only 3 lines of actual code. An experienced engineer familiar with Django's ORM would need some time to: 1) Understand Django's expression system and how GROUP BY clauses are generated, 2) Locate the ExpressionWrapper class in django/db/models/expressions.py, 3) Recognize that the issue is that ExpressionWrapper doesn't override the get_group_by_cols method, so it falls back to default behavior that includes the wrapper itself rather than delegating to the wrapped expression, 4) Implement the simple delegation pattern. The conceptual understanding might take 30-45 minutes, but the actual code change is trivial once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. The problem is well-contained within Django's expression system, the solution is clean and focused, and the test cases are straightforward. This would make a good benchmark problem as it tests understanding of object-oriented design patterns (specifically delegation/forwarding) within the context of a real framework. The issue requires both debugging skills to trace the GROUP BY generation logic and design skills to implement the proper delegation pattern.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13028": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that require interpretation. The core problem is clear: Django raises a NotSupportedError when filtering by a model field that has a `filterable=False` attribute, even when that field is just a regular model field (not an expression). The issue provides concrete model definitions showing ProductMetaDataType with a `filterable` field and ProductMetaData that references it. The reproduction steps are described (filtering ProductMetaData by value and metadata_type), and the error behavior is explained. However, some details are unclear: the exact query syntax that triggers the error isn't provided, and the connection between the model field named \"filterable\" and Django's internal filterable attribute checking mechanism requires domain knowledge to understand. The mention that changing \"label to filterable_test\" fixed it provides a crucial clue that the issue is related to the field name conflicting with Django's internal attribute checking.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) Understanding the problem requires knowledge of Django's ORM internals, specifically how the check_filterable method works and when it's called during query construction. (2) The developer needs to trace through Django's query building process to understand why a model field named \"filterable\" is being confused with Django's internal filterable attribute on expressions. (3) The solution involves modifying the check_filterable method in django/db/models/sql/query.py to add a hasattr check for 'resolve_expression' before checking the filterable attribute, distinguishing between Django expressions (which have this method) and regular model instances (which don't). (4) This requires understanding Django's expression system and the difference between expressions and model instances. (5) Writing appropriate tests also requires understanding Django's test framework and creating test cases that reproduce the specific scenario.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate bug in Django's ORM where field name conflicts with internal attribute checking cause incorrect behavior. The provided solution and tests are appropriate and the issue is suitable for a coding benchmark.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13112": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly describes the problem: Django's makemigrations crashes when dealing with ForeignKey references in apps with mixed-case names. The issue provides: (1) Clear reproduction steps - add a mixed-case app 'DJ_RegLogin' to INSTALLED_APPS, create models with ForeignKey relationships, run makemigrations; (2) Specific error description - Django reports that the field was declared with a lazy reference to lowercase 'dj_reglogin.category' but the app 'dj_reglogin' isn't installed; (3) Complete code examples including model.py showing the Category and Content models with the ForeignKey relationship, settings.py showing the mixed-case app name in INSTALLED_APPS, and apps.py with the AppConfig; (4) Context that this worked in Django 3.0 but breaks in 3.1b1. The root cause is clear: Django is converting the app name to lowercase when creating lazy references, but the actual installed app name is mixed-case, creating a mismatch.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution patch, this is a 15 min - 1 hour fix. The issue is in django/db/models/fields/related.py where the code was incorrectly converting the entire model reference to lowercase. The fix requires understanding that when a model reference contains a dot (app.model format), only the model name should be lowercased, not the app name. The solution is a small, focused change: splitting on the dot, keeping the app_label as-is, and only lowercasing the model_name. This requires some thought to understand Django's model reference system and case handling, but the actual code change is minimal and localized to one function.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the test case demonstrates the fix works correctly. This is a good sample for evaluating coding ability as it tests understanding of string manipulation, conditional logic, and Django's model reference system.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It describes a specific scenario: creating an Experiment model with DateTime fields (start and end), then running a query that annotates records by subtracting start from end and adding a zero-length duration. The issue states this fails with a field error about mixed types requiring an output_field. However, the description lacks concrete code examples showing the exact query that fails, the specific error message, or the expected behavior. An experienced Django developer familiar with ORM expressions would understand this relates to temporal arithmetic in database queries and the need for proper type handling in expressions.py, but some details need to be inferred from context and Django ORM knowledge.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the solution patch, it involves moving complex type detection logic from the as_sql() method to the resolve_expression() method in the CombinedExpression class in django/db/models/expressions.py. The fix requires understanding Django's ORM expression system, particularly how temporal subtraction works, when to use DurationExpression vs TemporalSubtraction, and the proper lifecycle of expression resolution. The engineer needs to understand why the type checking was happening too late in as_sql() and needs to be moved earlier to resolve_expression(). While the code changes aren't massive (~50 lines), they require deep understanding of Django's expression compilation pipeline and database backend feature detection.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The test changes clearly show the expected behavior - temporal subtraction should work without requiring ExpressionWrapper. The issue is well-contained within Django's ORM expression system and has comprehensive test coverage showing various scenarios (date subtraction, time subtraction, datetime subtraction, subqueries, etc.). An experienced developer with Django ORM knowledge should be able to tackle this successfully.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13297": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The description clearly explains the problem: TemplateView.get_context_data()'s kwargs returns SimpleLazyObjects in Django 3.1 instead of plain strings like in Django 3.0, causing database lookup failures. The issue provides a concrete reproduction scenario - subclassing TemplateView, overriding get_context_data(), reading offer_slug from kwargs, and using it for database lookups. It explains the symptom (SQLite rejecting the parameter due to unsupported type) and mentions a workaround (converting to string first). However, some details need to be inferred: the exact location in the codebase where this occurs, the specific mechanism causing SimpleLazyObjects to be created, and the precise technical approach for the fix. An experienced engineer would need to investigate the TemplateView implementation and URL parameter handling to understand the root cause, but the problem description provides sufficient context to identify what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve. An engineer would need to: 1) Investigate the TemplateView codebase to understand how kwargs are processed in get_context_data(), 2) Identify where SimpleLazyObjects are being created (likely in URL parameter handling), 3) Understand the difference between Django 3.0 and 3.1 behavior, 4) Research the appropriate way to preserve the original type while maintaining the lazy evaluation for deprecation warnings, and 5) Implement the fix using django.utils.functional.lazy instead of SimpleLazyObject. The solution involves understanding Django's internal mechanisms, lazy evaluation patterns, and requires modifying the _wrap_url_kwargs_with_deprecation_warning function. While not extremely complex, it requires substantial investigation into Django's internals and careful consideration of the lazy evaluation requirements.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample appears suitable for evaluating coding ability as it requires understanding of Django internals, lazy evaluation concepts, and careful debugging skills. The test case appropriately verifies that kwargs can be used for database filtering, which directly tests the fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13406": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. It clearly describes the problem: when a Django queryset with values()/values_list() is recreated from a pickled query, it crashes or returns incorrect results (model instances instead of dictionaries). The issue provides a concrete model example (Toy with name, material, price fields), describes the expected behavior (list of dictionaries), and explains the actual broken behavior (model instances with AttributeError when accessing attributes). However, the issue lacks a complete runnable code example - while it mentions \"reproduction steps\" and provides the model definition, it doesn't show the exact code that demonstrates the problem. An experienced engineer would need to infer the specific queryset operations and pickling/unpickling code based on the description, but the core problem and expected outcome are clear enough to work with.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The solution requires understanding Django's queryset internals and the pickling mechanism, but the actual code change is minimal (just 2 lines as shown in the patch). The core issue is that when a query object is pickled and unpacked, the queryset loses track of its iterable class information. An experienced Django developer familiar with the codebase would need to: 1) Understand how Django's values()/values_list() methods work with _iterable_class, 2) Identify that the query setter needs to restore this information, 3) Implement the simple check for values_select and set the appropriate iterable class. The fix itself is straightforward once the problem is understood - it's essentially restoring lost state information during query assignment.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is specific to Django ORM internals, the solution is focused and testable, and the test cases provided in the patch are comprehensive. The issue represents a legitimate bug with clear expected behavior that can be objectively verified through the test suite.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13449": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It provides: (1) Clear reproduction steps with specific Django version (3.0.7), database (SQLite), and model field types (DateField, FloatField, DecimalField), (2) Expected vs actual behavior clearly described, (3) The exact error message (\"near 'OVER': syntax error\"), (4) The problematic generated SQL query with explicit explanation of what's wrong (CAST placement), (5) Root cause analysis explaining that Django casts only the LAG expression instead of the entire window call, (6) Specific field types where the issue occurs (DecimalField) vs where it works (FloatField), and (7) A working workaround using output_field=FloatField(). The description includes enough technical detail about SQL generation, window functions, and Django ORM behavior for an experienced engineer to understand exactly what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) Understanding the problem requires knowledge of Django ORM internals, SQL window functions, SQLite-specific behavior, and how Django handles field type casting, (2) The solution involves creating a specialized SQLite method in the Window class and understanding the SQLiteNumericMixin pattern, (3) The fix requires modifying the expressions.py core file and understanding how Django's database backend system works with different field types, (4) Testing requires setting up window function queries with DecimalFields and understanding the test framework. While the issue description is clear about what's wrong, implementing the fix requires deep Django internals knowledge and careful handling of field type conversions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is testable, and the issue represents a legitimate bug in Django's ORM that affects real-world usage. The provided patches show both the implementation fix and comprehensive test coverage, making this suitable for evaluating coding ability.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13568": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes what needs to be fixed. It explains that Django's system check auth.E003 incorrectly flags USERNAME_FIELD fields as non-unique when they use UniqueConstraint instead of unique=True. The problem is clearly stated: the system check should be extended to recognize USERNAME_FIELD uniqueness when enforced via Model._meta.constraints, not just the field's unique attribute. The issue provides clear context about why someone would use UniqueConstraint over unique=True (to avoid extra implicit *_like indexes on PostgreSQL), and specifies exactly what the system check should do differently. The gold patch confirms this interpretation by showing the check needs to be modified in django/contrib/auth/checks.py to also examine cls._meta.total_unique_constraints.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution requires understanding Django's system check framework and model constraints, but the actual code change is quite small and focused. Looking at the gold patch, it only adds a few lines to an existing conditional check in django/contrib/auth/checks.py. The engineer would need to: (1) locate the auth.E003 system check code, (2) understand how UniqueConstraint works in Django models, (3) modify the existing condition to also check total_unique_constraints. While it requires some thought about Django's ORM and constraint system, the scope is narrow and the implementation is straightforward once you understand the problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained Django framework enhancement that tests a specific system check behavior. The test cases provided are comprehensive and would properly validate any solution attempt. The issue has clear requirements and the solution scope is well-defined.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13807": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) Clear reproduction steps - create a Django model called \"Order\", create a fixture, run loaddata, (2) Expected vs actual behavior - fixture should load successfully but instead fails with OperationalError, (3) Root cause analysis - missing quotes around table names in SQL PRAGMA statements in django/db/backends/sqlite3/base.py line 327, (4) Specific file, function, and line numbers where the problem occurs, (5) Code context showing the problematic SQL string formatting, (6) Confirmed Django versions affected. An experienced engineer has everything needed to understand the problem and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the root cause (unquoted table names in SQL PRAGMA statements) and the exact location (line 327 in sqlite3/base.py). The solution involves adding proper SQL identifier quoting using self.ops.quote_name() around table names in a few places. While it requires understanding Django's database operations and SQL identifier quoting, the change is straightforward once the problem is understood. The patch shows it's mainly adding quote_name() calls to existing string formatting operations.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the problem is clearly described, has a specific technical root cause, and the solution requires understanding both SQL identifier quoting rules and Django's database abstraction layer. The test case appropriately validates the fix by creating a model with SQL keyword conflicts.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13810": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified, though it requires some technical knowledge to fully understand. The reporter describes a specific problem with Django's ASGI middleware handling where middleware chains get \"poisoned\" when MiddlewareNotUsed exceptions occur. They provide concrete details: the issue occurs in django/core/handlers/base.py around line 58, involves the adapt_method_mode() function, and results in HttpResponse objects being treated as coroutines. The reporter also provides a reproduction case and identifies the root cause - that the handler variable gets overwritten but not properly restored when MiddlewareNotUsed is raised. While the description is somewhat verbose and technical, an experienced Django developer should be able to understand the core issue: the middleware loading logic incorrectly handles the handler variable when MiddlewareNotUsed exceptions occur, leading to async/sync mismatch problems.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The fix itself is quite small (just a few lines changing variable assignment logic), but arriving at the correct solution requires: 1) Understanding Django's middleware system and async/sync adaptation mechanisms, 2) Analyzing the control flow in the load_middleware method to see where the handler variable gets incorrectly overwritten, 3) Understanding why MiddlewareNotUsed exceptions cause this issue, and 4) Designing a solution that preserves the original handler when middleware is skipped. The patch shows the solution involves introducing an adapted_handler variable and only updating the main handler variable when middleware is successfully instantiated. While conceptually straightforward once understood, this requires careful analysis of the existing code flow and Django's async handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. It tests understanding of Django internals, async/sync programming concepts, exception handling, and requires careful code analysis skills. The solution is verifiable through the provided test case, and the fix is localized to a specific area of the codebase. An experienced engineer should be able to solve this given the detailed problem description and their understanding of Django's middleware system.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14017": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly describes the problem: the & and | operators are not commutative when combining Q objects with Exists expressions. The issue provides a specific reproduction case showing that Exists(...) & Q(...) works but Q(...) & Exists(...) raises a TypeError. The expected behavior is clearly stated - these operations should be commutative. The author also provides a reasonable hypothesis about the root cause (missing __rand__ definition). An experienced engineer would understand that they need to fix the operator precedence/commutativity issue between Q and Exists objects.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix. The solution involves modifying a single line in the _combine method of django/db/models/query_utils.py to change the type checking from \"isinstance(other, Q)\" to also accept objects that have a conditional attribute. This requires understanding Django's query system and the relationship between Q and Exists objects, but the actual code change is minimal. An experienced engineer familiar with the codebase could identify and implement this fix within 15 minutes to 1 hour after understanding the problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of operator overloading, object-oriented design principles (commutativity), and Django's ORM internals. The issue is well-contained and has a clear, testable solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14140": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some gaps. It clearly describes the inconsistent behavior in Q object deconstruction and provides a specific runtime error scenario with Exists expressions. The reproduction steps are detailed enough to understand the problem. However, it doesn't provide exact code examples for reproduction, and some technical details about the expected behavior could be clearer. The mention of a patch and backward compatibility considerations shows good understanding of the solution space, but an engineer would need to fill in some implementation details.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours because: 1) Understanding Django's Q object internals and deconstruct method requires domain knowledge, 2) The fix involves modifying core query logic in django/db/models/query_utils.py, 3) Need to understand the implications for backward compatibility, 4) Must handle edge cases like Exists expressions and different connector types, 5) The solution touches multiple aspects (args vs kwargs handling, special case removal) and requires careful testing to ensure no regressions in Django's ORM functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for a coding benchmark. It tests understanding of Django ORM internals, object deconstruction patterns, and handling of edge cases with boolean expressions. The provided tests cover the main scenarios well, and the fix is self-contained within the Q class. An experienced engineer should be able to solve this given the clear description of the inconsistent behavior and runtime error.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14238": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear information about what needs to be fixed. It describes a specific problem: when Django's DEFAULT_AUTO_FIELD is set to a custom subclass of BigAutoField or SmallAutoField, Django incorrectly rejects it during the subclass check in AutoFieldMeta.__subclasscheck__. The issue clearly states the expected behavior (Django should recognize custom fields that inherit from BigAutoField/SmallAutoField as valid), the actual behavior (Django throws an error), and even hints at the solution location (AutoFieldMeta.__subclasscheck__ method and the _subclasses property). The description provides enough context for an experienced engineer to understand the Django settings system, model field inheritance, and the specific technical problem without ambiguity.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic method (AutoFieldMeta.__subclasscheck__) and suggests the solution approach. Looking at the gold patch, the fix is indeed a single line change: replacing 'subclass in self._subclasses' with 'issubclass(subclass, self._subclasses)'. This changes from checking exact membership to checking inheritance hierarchy. An experienced engineer would need some time to understand Django's AutoField metaclass system and verify the fix doesn't break existing functionality, but the core change is straightforward once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. The issue description is clear, the problem is well-defined, and the solution is testable. The provided test cases adequately verify that custom subclasses of BigAutoField and SmallAutoField are properly recognized by the subclass check, which directly addresses the reported issue. This would be a good benchmark sample for testing an engineer's ability to understand Python inheritance, metaclasses, and Django's model field system.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14351": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with clear reproduction steps and extensive debugging information. The reporter provides: (1) A specific error scenario with Django ORM queries involving Q objects and __in vs __id__in lookups, (2) Working code examples showing the difference between broken and working approaches, (3) Generated SQL demonstrating the exact problem (subquery returning multiple columns instead of one), (4) Detailed debugging output showing the difference in select_fields between the two approaches, and (5) A clear description of the root cause (Q object aliases behaving differently when OR'd). While some Django-specific knowledge is assumed, an experienced engineer familiar with Django ORM internals could understand the problem and work toward a solution. The issue could benefit from a more minimal reproduction case, but the provided information is sufficient.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a moderately complex Django ORM internals issue that would take 1-4 hours to solve. The engineer needs to: (1) Understand Django's query compilation and SQL generation process, (2) Investigate how Q objects with __in lookups are processed differently from __id__in lookups, (3) Trace through the django/db/models/sql/query.py code to understand how get_default_columns works, (4) Identify that the issue is in the In lookup class in django/db/models/lookups.py where get_group_by_cols method is missing, and (5) Implement the fix to ensure that when a queryset is used as RHS in an IN lookup, it only selects the primary key field. The provided patch shows this is a 10-line addition to the In class, but discovering this solution requires deep understanding of Django's ORM internals and careful analysis of the query compilation process.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate Django ORM bug with a clear fix. The issue demonstrates important concepts about SQL subquery constraints and Django's query compilation. The test provides good verification that the fix works correctly without breaking existing functionality.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14493": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides clear reproduction steps (create a custom storage class inheriting from ManifestStaticFilesStorage, override max_post_process_passes to 0, run collectstatic), describes the exact error (UnboundLocalError for 'substitutions'), explains the root cause (variable only initialized inside a loop that doesn't run when max_post_process_passes is 0), and even links to the specific problematic code in the Django repository. The issue gives enough context about the motivation and points to the exact file and line numbers where the problem occurs.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a very simple fix that should take less than 15 minutes. The issue clearly identifies the problem: a variable 'substitutions' is used without being initialized when max_post_process_passes is 0. The solution is straightforward - initialize the variable before the loop. Looking at the provided patch, it's literally a one-line addition of 'substitutions = False' before the for loop. An experienced engineer would quickly identify this as a simple variable initialization issue and implement the fix almost immediately.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is clearly described, the problem is well-defined, the solution is straightforward but still requires understanding the code flow, and the test ensures the fix works correctly. It's a good example of a real-world bug that can be solved with minimal code changes but requires proper understanding of variable scope and initialization.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14580": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The reporter clearly describes the problem: Django generates migration files that are missing the \"from django.db import models\" import statement, causing a NameError when the migration references models.Model. The reproduction steps are outlined (create custom text field subclass, abstract base model, mixin, and concrete model using custom field as primary key), and the expected vs actual behavior is stated. However, the issue lacks concrete code examples that would demonstrate the exact scenario, making it somewhat harder to reproduce without additional investigation. The reporter also mentions suspecting django.db.migrations.writer module but isn't certain, which provides helpful direction but isn't definitive.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is quite simple: adding the import statement ['from django.db import models'] to the special_cases tuple for models.Model in the TypeSerializer class in django/db/migrations/serializer.py. The fix is a one-line change that adds the missing import. An experienced engineer familiar with Django's migration system would likely identify that the issue is in the serialization logic fairly quickly, and the actual code change is minimal. The main time would be spent understanding how Django's migration writer works and locating the right file/function, but this is straightforward given the hint about django.db.migrations.writer.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This sample appears suitable for the benchmark. The issue is clearly a bug with a definitive correct solution, the description provides sufficient context to understand what needs to be fixed, and the solution can be verified through automated testing. The test patch shows a clear test case that validates the fix works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14672": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the technical problem: in Django 3.2, the identity property was added to ForeignObjectRel for comparison purposes, and a hash is derived from this identity tuple. The issue is that `through_fields` can be a list, but there's a missing `make_hashable` call on `self.through_fields` in `ManyToManyRel`. The problem manifests when Django runs model checks and tries to hash the relation identity, causing a TypeError because lists are unhashable. The issue provides a clear technical explanation of what's happening, describes the specific scenario where it occurs (proxy models), gives a minimal reproduction case, explains the expected vs actual behavior, and even provides the exact solution needed: \"Add missing make_hashable call on self.through_fields in ManyToManyRel.\" The description references specific Django internals and the problem is clearly scoped to a single missing function call.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue description clearly identifies the exact problem and solution: adding a `make_hashable` call around `self.through_fields` in the `ManyToManyRel.identity` property. Looking at the gold patch confirms this - it's literally a one-line change in the `identity` method of `ManyToManyRel` class, changing `self.through_fields,` to `make_hashable(self.through_fields),`. An experienced engineer would need some time to: 1) Locate the `ManyToManyRel` class in the codebase (likely in django/db/models/fields/reverse_related.py based on the patch), 2) Find the `identity` property method, 3) Understand how `make_hashable` is used elsewhere in the codebase, and 4) Apply the fix. The fix itself is trivial once the location is found, but finding the right file and understanding the context would take some time for someone new to the Django codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear technical explanation and straightforward solution. The issue description provides sufficient context about Django's internal architecture and the specific problem. The reproduction case is clear, and the solution is precisely specified. This would make for a good benchmark problem as it tests understanding of Python hashing, Django's ORM internals, and the ability to make a targeted fix to a specific technical issue.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14787": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the core problem: method_decorator() passes a functools.partial object to decorators, which lacks standard function attributes like __name__ and __module__, causing AttributeError when decorators try to access these attributes. The reproduction steps are clear - create a logging decorator that accesses func.__name__, apply it to a method using method_decorator, and the error occurs. However, the issue description lacks specific implementation details about what exactly method_decorator() should do to fix this (i.e., preserve wrapper assignments using functools.wraps). An experienced engineer would need to understand Django's decorator utilities and figure out that the solution involves using functools.wraps to copy attributes from the original method to the partial object.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The problem is well-understood once you grasp that functools.partial objects don't have function attributes, and the solution is straightforward - wrap the partial object with functools.wraps to copy the original method's attributes. Looking at the gold patch, it's literally a one-line change: wrapping the partial call with wraps(method). An experienced engineer familiar with Python decorators and functools would quickly identify this as a standard pattern for preserving function metadata. The main time would be spent understanding Django's method_decorator implementation and writing/running tests to verify the fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark issue. It requires understanding of Python's functools module, decorator patterns, and how method_decorator works in Django. The solution is elegant and demonstrates knowledge of best practices for preserving function metadata in decorators. The test case provided in the patch clearly validates that the fix works by checking that __name__ and __module__ attributes are properly preserved.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15104": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. The author clearly describes the problem: a KeyError with 'to' occurs when Django's migration autodetector encounters a custom ForeignKey field that hardcodes its 'to' argument and removes it from deconstructed kwargs. The issue provides a detailed reproduction case, explains the root cause (the autodetector tries to delete a 'to' entry that isn't present), and even suggests the exact fix: changing `del deconstruction[2]['to']` to `deconstruction[2].pop('to', None)` in the autodetector code. The problem location is clearly identified, and the solution approach is straightforward and well-justified.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a simple fix that would take 15 minutes to 1 hour. The issue description clearly identifies the exact line of code causing the problem and provides the specific solution. The fix is a one-line change from `del deconstruction[2]['to']` to `deconstruction[2].pop('to', None)` in django/db/migrations/autodetector.py. An experienced engineer would need minimal time to locate the file, understand the context, and implement the change. The logic is straightforward: use pop() with a default value instead of del to avoid KeyError when the key doesn't exist.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample because: 1) The issue description is clear and complete, 2) The fix is simple but requires understanding the Django migration system, 3) The test case provided validates the specific scenario, and 4) The solution demonstrates defensive programming practices. The sample tests both code comprehension and the ability to implement a robust fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps. The reporter provides a clear description of the problem: Django's Query.change_aliases raises an AssertionError when performing union operations (OR) on QuerySets in a specific order. They explain the technical cause - sequential aliases (T4, T5) create conflicts during alias renaming where change_map keys intersect with values. The issue includes specific Django models (Foo, Bar, Baz, Qux) with their relationships, reproduction steps, and even suggests a potential solution approach. However, there are some missing details: no actual code snippet showing the exact reproduction case, no stack trace of the AssertionError, and the model definitions are described in prose rather than code. Despite these gaps, an experienced engineer could reasonably understand the core problem (non-commutative QuerySet OR operations causing alias conflicts) and work toward a solution involving alias management in Query.combine and related methods.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the reporter provides good insight into the root cause (alias conflicts in Query.combine), implementing the fix requires deep understanding of Django's ORM internals, specifically the query compilation and alias management system. The solution involves modifying the Query.combine method to handle alias conflicts by bumping the prefix of the right-hand side query, updating the Query.bump_prefix method to accept an exclude parameter, and ensuring the base table alias is preserved. The fix spans multiple methods across django/db/models/sql/query.py and requires careful consideration of edge cases to avoid breaking existing functionality. The complexity comes from understanding Django's query alias system, the interaction between different QuerySet operations, and ensuring the fix doesn't introduce regressions in other query operations.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is a legitimate bug in Django's ORM that affects QuerySet union operations, and the solution requires genuine software engineering skills including debugging complex systems, understanding ORM internals, and implementing fixes that maintain backward compatibility.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15277": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: CharField.__init__ always creates a MaxLengthValidator even when max_length is None, which causes performance issues and runtime errors. The issue provides detailed context including: (1) the specific code path that triggers the problem (Value._resolve_output_field() creating CharField), (2) the exact location of the problematic code (CharField.__init__ unconditionally adding MaxLengthValidator), (3) a clear reproduction case, (4) performance benchmarks showing the impact, (5) the exact proposed solution (add a null check before creating the validator), and (6) precedent from BinaryField.__init__ that follows the same pattern. The issue even mentions that local testing shows all existing tests pass with the proposed change.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward 15 min - 1 hour fix. The issue clearly identifies the exact problem location (CharField.__init__) and provides the exact solution (wrap the validator addition in an if statement checking max_length is not None). The gold patch confirms this is indeed a simple 2-line change. An experienced engineer would need minimal time to understand the Django field validation system, locate the CharField class, and implement the fix. The most time-consuming part would be understanding the codebase structure and running tests to verify the change doesn't break anything.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the issue is clearly specified, has a well-defined solution, involves a meaningful coding task (understanding validator patterns and conditional logic), and the test ensures the fix works across multiple data types. The performance optimization aspect makes it particularly valuable for demonstrating understanding of Django's field system.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15280": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some gaps to fill in. It provides concrete Django models (User and Profile), describes the exact queryset construction using prefetch_related with nested Prefetch objects, and explains the expected vs actual behavior. The issue includes the specific queries being executed and mentions that user.profile.user.get_deferred_fields() returns {'kind'}, indicating the problem is with deferred field handling. However, the issue doesn't provide the exact queryset code that reproduces the problem - an engineer would need to construct the queryset based on the description. The core problem is clear: when using nested prefetches back to parent objects, deferred fields are incorrectly inherited, causing unnecessary database queries.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue because it involves Django ORM internals, specifically the complex interaction between prefetch_related, deferred fields, and object caching. An engineer would need to: 1) Understand Django's prefetch mechanism and how it manages object instances, 2) Trace through the code to find where deferred fields are being incorrectly inherited from outer querysets to inner prefetched objects, 3) Identify the specific location in related_descriptors.py where the fix is needed. The actual fix is relatively small (adding a condition to check if the field is already cached), but finding the root cause requires deep understanding of Django's ORM internals and careful debugging of the prefetch relationship handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a solid benchmark issue. It requires understanding complex Django ORM behavior with clear success criteria (no additional database queries when accessing nested prefetched attributes). The test case provided validates the fix properly by checking both query count and cached field access.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15315": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Field.__hash__() changes value when a field is assigned to a model class, which breaks the field's ability to act as a stable dictionary key. The issue provides a concrete reproduction scenario: create a field object, use it as a dictionary key, assign it to a model class, then attempt to look up the original field object in the dictionary (which fails because the hash changed). The issue also references the specific PR (#31750) that introduced the bug and explains the root cause. The description includes enough technical detail for an experienced Django developer to understand exactly what's wrong and what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that should take 15 minutes to 1 hour. The issue clearly identifies that the problem was introduced in PR #31750, which modified the __hash__ method. Looking at the gold patch, the solution is simple: revert the __hash__ method to only use creation_counter instead of including model metadata (app_label and model_name) that changes when the field is assigned to a model. The fix involves changing just a few lines in one method. An experienced engineer familiar with Django's codebase could quickly locate the Field class in django/db/models/fields/__init__.py, understand why including mutable model information in the hash breaks immutability, and implement the fix. The test is also straightforward - just verify that a field's hash doesn't change after being assigned to a model.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined bug with a clear reproduction case and straightforward solution. The issue provides good context about why the bug exists and what the expected behavior should be. The fix is localized to a single method and doesn't require extensive knowledge of Django internals.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15375": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. The problem is clearly described: using aggregate() with a 'default' parameter after annotate() crashes with a syntax error near the FROM clause, generating an empty SELECT list. The issue provides a specific reproduction case using Book.objects.annotate(idx=F(\"id\")).aggregate() with Sum and default parameter, and shows that the long form using Coalesce works as expected. However, the issue doesn't provide the exact failing code snippet or the complete error message, requiring some interpretation. An experienced engineer would understand this is about Django ORM query generation where the default parameter handling in aggregates is broken when combined with annotations, and the solution likely involves fixing how the default parameter is processed in the aggregate query compilation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. Looking at the gold patch, the solution requires understanding Django's ORM internals, specifically how aggregates work with the 'default' parameter and the resolve_expression method. The engineer needs to: 1) Understand that when an aggregate with default is wrapped in Coalesce, the is_summary attribute is lost, 2) Identify that this attribute is crucial for proper query generation, 3) Locate the right place in django/db/models/aggregates.py to make the fix, and 4) Understand that preserving the is_summary attribute from the original aggregate to the Coalesce wrapper solves the issue. While the actual code change is small (2 lines), it requires significant understanding of Django's query compilation process and the role of the is_summary attribute in aggregate handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is reproducible, the issue description provides enough context about the expected vs actual behavior, and the test cases are straightforward to understand. This is a good benchmark sample as it tests understanding of ORM internals and debugging skills in a complex codebase.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15380": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It clearly describes the problem: Django's migration autodetector crashes when renaming both a model and one of its fields simultaneously. The issue provides a reproduction scenario (rename MyModel to MyModel2 and rename one of its fields, then run makemigrations), describes the expected behavior vs actual behavior (should generate migration file but instead crashes with key lookup error), and even identifies this as a regression with a specific commit hash. However, there are some details that need interpretation - the exact nature of the \"internal lookup failure\" and \"key lookup error\" would require examining the codebase and stack trace to understand the root cause. An experienced engineer could reasonably infer that this is likely an issue with the autodetector's internal state management when tracking both model and field renames simultaneously.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is a simple one-line change in django/db/migrations/autodetector.py, changing `old_model_name` to `model_name` when looking up the new model state. The bug is in the generate_renamed_fields method where it incorrectly uses the old model name to look up the new model state, but after a model rename, the new state would be indexed by the new model name. While it requires understanding Django's migration system and the autodetector logic, the actual fix is straightforward once the problem is identified. An experienced engineer familiar with Django would likely spot this logical error relatively quickly by tracing through the code and understanding how the model state dictionaries are keyed.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of Django's migration system, debugging skills to trace through the autodetector logic, and the ability to identify a subtle but critical bug in state management. The issue is specific enough to be actionable, has a clear reproduction case, and the solution can be validated through the provided test case.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15503": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes what needs to be fixed. It provides: (1) A clear problem statement - JSONField has_key lookups fail with numeric keys on SQLite/MySQL/Oracle but work on PostgreSQL, (2) Specific reproduction steps - create records with string key \"foo\" and numeric key \"1111\", then query for both, (3) Expected vs actual behavior - both queries should return 1 record but numeric key query returns 0 on SQLite, (4) Environment details including Django 4.0.3, Python 3.9.6, and SQLite versions. The issue gives enough context for an engineer to understand this is a database-specific bug in Django's JSONField implementation where numeric keys are not being handled consistently across different database backends.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Understanding Django's JSONField implementation across multiple database backends, (2) Investigating why PostgreSQL works but SQLite/MySQL/Oracle don't for numeric keys, (3) Analyzing the JSON path compilation logic and how different databases handle numeric vs string keys in JSON paths, (4) Implementing a solution that handles the final key in JSON paths differently to ensure numeric keys are properly quoted/escaped, (5) Creating a new HasKeyOrArrayIndex class and modifying multiple lookup classes. The patch shows this touches core Django ORM functionality and requires understanding of database-specific JSON handling, making it a substantial but not overly complex fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-documented database compatibility bug with a focused scope. The solution involves Django's ORM internals but doesn't require external dependencies or highly specialized domain knowledge beyond understanding JSON field implementations in different databases.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15525": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear problem statement - loaddata fails on non-default database when natural keys use foreign keys, (2) Complete reproduction steps with specific commands and parameters (--database other --format json), (3) The exact error behavior - DoesNotExist exception when Book.natural_key() tries to access author that doesn't exist yet in the secondary database, (4) Complete model code showing the AuthorManager, Author, BookManager, and Book classes with their natural_key implementations, (5) Sample JSON fixture data that triggers the issue, and (6) A link to a test project demonstrating the bug. The issue clearly explains that it works in the default database but fails in secondary databases, providing enough context for an engineer to understand both the problem and expected behavior.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) The engineer needs to understand Django's serialization/deserialization system, specifically how natural keys work with foreign key dependencies across multiple databases, (2) The issue requires diving into Django's core serializer code (django/core/serializers/base.py) to understand how build_instance works, (3) The solution involves understanding Django's model state management and database routing - specifically that when creating a temporary object to get its natural key, the object's _state.db needs to be set to the target database, (4) While the actual code change is small (3 lines), identifying the root cause requires understanding the interaction between natural keys, foreign key lookups, and database routing, (5) The engineer needs to write appropriate tests covering the multi-database scenario. This goes beyond a simple bug fix and requires architectural understanding of Django's ORM.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-documented, the solution is targeted and clean, and the issue represents a legitimate Django framework bug that requires solid understanding of ORM internals. The test coverage is appropriate and the fix is minimal but requires deep framework knowledge to identify.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15695": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: RenameIndex() crashes when re-applying the operation after running backward and forward migrations on unnamed indexes. The issue provides a specific reproduction case with code examples showing exactly when the crash occurs. The runtime error is explained (PostgreSQL reports that an index with the target name already exists), and the context about unique_together indexes with auto-generated names is provided. An experienced engineer would have enough information to understand both the problem and the expected behavior.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. The problem is clearly identified: the RenameIndex operation doesn't check if the old and new names are the same before attempting to rename. The solution is simple - add a guard clause that returns early if old_index.name == self.new_name. The fix requires minimal code change (3 lines) in a single location in django/db/migrations/operations/models.py. An experienced engineer would quickly understand the issue from the description and test case, locate the relevant code in the database_forwards method, and implement the straightforward check.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is straightforward, and the expected behavior is clear. The issue provides sufficient context about Django migrations and database operations that an experienced engineer should be able to solve this without additional complications.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15732": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is well-specified and clear about what needs to be fixed. It describes a specific scenario where a Django model has a primary key field \"id\" that is also listed in a unique_together constraint, and when attempting to remove that unique_together constraint via migration, the operation fails. The issue provides clear reproduction steps, explains the runtime error (migration framework finds two unique indexes on the \"id\" column and fails because it expects only one), shows the actual database indexes present, and specifies the database being used (PostgreSQL). An experienced engineer would understand that they need to modify Django's migration framework to handle the case where multiple unique constraints exist on the same field, specifically distinguishing between primary key constraints and unique_together constraints when dropping the latter.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is clearly described, solving it requires understanding Django's migration system, database schema operations, and constraint handling. The solution involves modifying the `_delete_composed_index` method in `django/db/backends/base/schema.py` to properly differentiate between different types of unique constraints when multiple exist on the same field. The engineer needs to: 1) Understand how Django's migration framework handles unique constraints, 2) Identify why the current logic fails when multiple unique indexes exist, 3) Implement logic to specifically target the unique_together constraint while preserving primary key constraints, 4) Add proper constraint name matching to ensure the correct constraint is removed. The actual code changes are moderate in scope but require careful thought about database constraint semantics and Django's internal constraint naming conventions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements and testable outcomes. The issue provides sufficient context and the solution can be verified through the migration system's behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15814": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear description of the problem - QuerySet.only() crashes after select_related() on proxy models, (2) Complete reproducible code including the exact model definitions (CustomModel, ProxyCustomModel, AnotherModel), (3) The specific error scenario - calling select_related(\"custom\").only(\"custom__name\") on AnotherModel instances, (4) Expected vs actual behavior - should return objects with only the related name field loaded but instead raises an error about missing primary key \"id\", (5) The exact location of the bug at django/db/models/sql/query.py line 745, and (6) Even provides the specific fix needed - replacing \"opts = cur_model._meta\" with \"opts = cur_model._meta.concrete_model._meta\". The issue gives enough technical detail and context for an experienced Django developer to understand the problem and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because: (1) The issue description literally provides the exact file and line number where the problem occurs (django/db/models/sql/query.py line 745), (2) It even suggests the exact fix needed - adding \".concrete_model\" to get the concrete model's metadata instead of the proxy model's metadata, (3) The problem is conceptually straightforward - proxy models need to use their concrete model's metadata for field resolution, (4) The fix is a one-line change that requires understanding Django's proxy model architecture but doesn't involve complex logic or multiple files. The main time would be spent understanding why this fix works (proxy vs concrete model metadata) and ensuring it doesn't break other functionality, but the actual implementation is trivial.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is actually an excellent benchmark sample because: (1) It tests understanding of Django's ORM internals, specifically proxy models and QuerySet operations, (2) The problem is realistic and would occur in real-world Django applications, (3) The test provided in the patch validates the fix properly by creating the exact scenario described in the issue, (4) The solution requires both understanding the problem domain (Django ORM) and making precise code changes. The issue is well-contained and has a clear, testable solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15930": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear reproduction scenario: building a Django query with a Case expression that tests whether a primary key does not appear in an empty list (~Q(pk__in=[])). The problem is well-defined - the generated SQL creates malformed WHEN clauses that result in \"CASE WHEN THEN true ELSE false END\" syntax errors. However, there are some minor gaps: the exact Django models/setup isn't specified, and the description could be clearer about the technical details of how ~Q(pk__in=[]) gets compiled to empty SQL. Despite these minor ambiguities, an experienced Django developer would understand this is about fixing the SQL compilation of negated Q objects with empty lists in Case expressions.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a focused 5-line fix in django/db/models/expressions.py that handles the edge case where condition_sql compiles to an empty string by replacing it with a True predicate. The fix is straightforward once you understand the problem: when ~Q(pk__in=[]) compiles to empty SQL, it needs to be replaced with a condition that always evaluates to True. An experienced Django developer familiar with the ORM internals could identify this issue and implement the fix within 15 minutes to 1 hour, as it requires understanding how Q objects compile but the actual code change is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a solid benchmark sample. The issue is well-contained within Django's ORM expression compilation, has a clear reproduction case, and the solution requires understanding of both Q object compilation and Case expression handling. The test case also clearly validates the fix by ensuring that ~Q(pk__in=[]) in a Case expression works correctly and marks all records as \"selected\" since no primary keys are in the empty list.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15957": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the problem: Django refuses to run queries when trying to prefetch sliced querysets (e.g., limiting to 3 posts per category), resulting in an assertion failure. The use case is well-explained - displaying categories with a few example objects from each. However, the issue lacks specific technical details like which assertion fails, the exact error message, or code examples demonstrating the problem. The description mentions it's \"not documented in Django Docs\" but doesn't provide reproduction steps. An experienced engineer could reasonably interpret this as needing to implement support for sliced querysets in Django's prefetch functionality, but some implementation details would need to be inferred.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a 4+ hour fix based on the complexity evident in the solution. The patch shows this required: (1) Deep understanding of Django's ORM internals, specifically prefetch_related mechanisms, query compilation, and database engines; (2) Creating a sophisticated new function `_filter_prefetch_queryset` that handles sliced querysets using Window functions, RowNumber, and partition logic; (3) Modifying multiple descriptor classes and their get_prefetch_queryset methods; (4) Understanding the intricate relationship between query limits, ordering, and database-level operations. The solution involves advanced SQL concepts like window functions and requires careful handling of database-specific behaviors. An engineer would need substantial time to research Django's prefetch implementation, understand why slicing fails, design a window function approach, and implement it correctly across multiple related descriptor classes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is a legitimate Django ORM limitation that affects real-world use cases. The solution is technically sound and the test cases demonstrate proper validation of the fix across different relationship types (M2M forward/reverse, ForeignKey reverse). This represents a good benchmark for evaluating advanced Django ORM knowledge and problem-solving skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15973": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified with some minor gaps to fill in. The problem is clearly described: when defining a many-to-many field with a \"through\" model in a separate Django app, migrations fail with an AttributeError because Django treats the through model reference as a string instead of a model class. The reproduction steps are detailed, explaining the three-app structure (fonte, variavel, fonte_variavel) and the specific model relationships. The error condition is clear - migration fails with \"'str' object has no attribute '_meta'\". The workaround is also mentioned (moving the through model to the same app works). While some specific model field details could be clearer, there's enough information to understand the core problem: Django's migration system has trouble resolving cross-app through model references during schema generation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This appears to be a 15 minutes to 1 hour fix based on the provided patch. The solution is a very targeted change in django/db/migrations/autodetector.py, changing one line from passing `remote_field_model` to `field.remote_field.through` in the resolve_relation call. The bug is in the _get_dependencies_for_foreign_key method where the wrong variable was being used to resolve the through model relationship. Once the engineer understands Django's migration autodetector code and locates this specific function, the fix itself is straightforward - using the correct field reference to properly resolve cross-app through models. The patch shows this is a simple one-line change, though it requires understanding the migration dependency resolution system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-documented, has a clear reproduction case, and the test case comprehensively covers the cross-app through model scenario. The issue represents a legitimate bug in Django's migration system that would be valuable for testing debugging and Django framework knowledge.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16032": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear reproduction scenario with specific steps: (1) get Book records with pages > 400, (2) add computed field with alias, (3) find Publisher records with books in that set, (4) collect only publisher names. It specifies the expected result (four publisher names: Apress, Sams, Prentice Hall, Morgan Kaufmann) and describes the actual error (database operational error due to nested query returning ten columns when one expected). However, it lacks some technical details like the exact Django ORM methods to use, the specific database error message, or which Django version is affected. An experienced developer familiar with Django ORM would likely understand this relates to QuerySet field selection issues when combining annotate() and alias() with __in lookups, but some interpretation is required to map the description to specific ORM operations.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django ORM internals, specifically how QuerySet field selection works with annotate(), alias(), and __in lookups. (2) The bug involves the interaction between multiple ORM features and requires tracing through query compilation logic. (3) From the patch, the fix involves modifying the `has_select_fields` property implementation and field selection logic in `related_lookups.py` and `query.py`, requiring understanding of when/how to clear select clauses. (4) The solution changes core ORM behavior that could affect other functionality, requiring careful analysis. (5) While the actual code changes are relatively small (removing one line, changing another, moving a property to an attribute), identifying the root cause and ensuring the fix doesn't break other cases requires substantial investigation into Django's query generation system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate Django ORM bug with a clear reproduction case and well-defined expected behavior. The test cases provide good validation that the fix works correctly. While some Django ORM knowledge is required, this represents a realistic debugging scenario for a Django developer.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16136": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is reasonably well-specified but has some blanks to fill in. It clearly describes the problem: when a Django View subclass has only an async POST method, GET requests cause a server crash with a type error instead of returning a proper 405 Method Not Allowed response. The reproduction steps are detailed (Django 4.1.1, Python 3.10.6, specific URL setup). However, some details are missing: the exact error message/stack trace isn't provided, and there's no example code showing the problematic View class implementation. An experienced engineer would need to infer that the issue stems from Django's async/sync handling in the http_method_not_allowed method, but the core problem and expected behavior (returning 405 instead of 500) are clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This would take 1-4 hours to solve. The issue requires understanding Django's async view handling mechanism and the interaction between sync and async methods. An engineer would need to: (1) reproduce the issue by creating a test case with an async-only view, (2) trace through Django's request handling to find where the type error occurs, (3) understand that http_method_not_allowed returns a sync response but needs to be awaitable when called from an async context, and (4) implement a solution that conditionally wraps the response in an async function when view_is_async is True. The solution involves understanding Django's internal async/sync detection and implementing a conditional async wrapper, which requires moderate Django framework knowledge and async/await understanding.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate Django framework bug with a clear expected behavior. The solution is testable and the provided test case adequately verifies both sync and async scenarios. The issue is scoped well within Django's codebase and doesn't require external dependencies or complex architectural changes.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16429": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The exact problem - timesince() raises TypeError when USE_TZ=True and the datetime interval is >1 month, (2) The root cause - mixing timezone-naive and timezone-aware datetimes, (3) Specific reproduction steps - turn on timezone support, set USE_TZ=True, get current time as timezone-aware datetime, go back 31 days, call timesince(), (4) Expected vs actual behavior - should see \"1 month\" but instead gets TypeError, (5) The exact location of the bug with a GitHub link to line 93-100 in django/utils/timesince.py, and (6) A clear proposed solution - add tzinfo from input datetime to the pivot call. The issue author even points to the specific problematic code where the pivot point doesn't carry over the original datetime's tzinfo. This level of detail makes it clear what needs to be fixed and how to fix it.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue clearly identifies the exact location of the bug (lines 93-100 in timesince.py) and provides the solution (add tzinfo=d.tzinfo to the pivot datetime creation). Looking at the actual patch, it's literally a one-line addition of \"tzinfo=d.tzinfo,\" to the datetime constructor. The problem is well-understood (timezone-aware vs timezone-naive datetime mixing), the location is pinpointed, and the fix is straightforward. An experienced engineer familiar with Django would need minimal time to understand the timezone handling issue and implement the fix. The most time would be spent understanding the existing timesince logic and writing appropriate tests.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is clearly described with specific reproduction steps, the problem is well-defined, and the solution is targeted and minimal. The test patch shows proper test coverage for the timezone-aware case. An engineer would be able to solve this based solely on the issue description, and the tests would properly validate their solution. The issue demonstrates good software engineering practices around timezone handling in Django.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16454": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Django's CommandParser class has custom error formatting arguments that are not being passed to subparsers created via add_subparsers().add_parser(). This results in stack traces instead of user-friendly error messages when subparser arguments are missing. The issue provides a concrete example scenario (a \"create\" subcommand requiring a positional name argument) and explains the exact behavior difference between main parser errors (proper usage messages) and subparser errors (Python tracebacks). The solution direction is also clearly indicated: \"ensure that the subparser action returned by add_subparsers() copies the relevant arguments through to constructed subparsers.\" The problem location is identified (CommandParser class and its relationship to argparse.ArgumentParser), making this a well-defined technical issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15min-1hour fix. The solution involves overriding the add_subparsers() method in CommandParser to ensure that subparsers inherit the same error formatting behavior as the parent parser. Looking at the gold patch, the fix is quite elegant and small: it adds an add_subparsers method that uses functools.partial to pass the called_from_command_line parameter to subparsers when they are CommandParser instances. The code change is only about 10 lines and requires understanding argparse inheritance patterns and Django's CommandParser class, which an experienced engineer could grasp relatively quickly. The main time would be spent understanding the argparse subparser mechanism and testing the solution, but the core implementation is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, testable behavior, and a focused scope within Django's management command system. The issue provides sufficient context and examples to understand both the problem and the expected solution behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16485": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the floatformat filter crashes on \"0.00\" when requesting zero decimal places, when it should return \"0\". The description provides specific reproduction steps (applying floatformat to \"0.00\" or Decimal(\"0.00\") with 0 decimal places), the expected behavior (should return \"0\"), and the actual error behavior (ValueError about invalid precision). The function name (floatformat) and specific input types (string \"0.00\" and Decimal object) are clearly identified. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly localized to the floatformat function, and the problem is straightforward - a boundary condition error where the code doesn't handle precision=0 correctly. Looking at the patch, it's literally a one-character change (< to <=) in a conditional check. The main time would be spent understanding the floatformat function logic and writing appropriate tests, but the actual fix is trivial once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - it has a clear, reproducible bug description, a simple but non-trivial fix that requires understanding the code logic, and the solution can be easily verified with the provided test cases. The issue demonstrates good debugging skills (identifying edge case) and precision in fixing boundary conditions.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16569": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the add_fields() method in Django formsets fails when index is None under specific conditions (can_delete=True and can_delete_extra=False). The issue provides the exact error scenario (TypeError when comparing None with integer), identifies the problematic line of code (line 493 in django.forms.formsets), and even suggests the exact fix needed. The reproduction steps are clear and detailed, explaining how to create a formset factory with the specific settings that trigger the bug. The proposed solution is precise and shows exactly what code change is needed to handle the None case properly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. The issue description pinpoints the exact problem location and provides the solution. An experienced engineer would need to: 1) Understand the formset logic and why index can be None, 2) Locate the problematic line in django/forms/formsets.py, 3) Implement the suggested null check, and 4) Write or update tests. The actual code change is minimal (adding \"index is not None and\" to the conditional), but requires understanding the Django formset logic and testing the fix properly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. It's an excellent example for a coding benchmark as it tests the ability to understand Django's formset internals, handle edge cases with None values, and implement a precise fix. The issue is realistic, well-documented, and has a clear solution that can be objectively verified through tests.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16667": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a complete, minimal reproducible example with specific code (ReproForm class, repro_view function, and URL configuration). The problem is clearly described: when extremely large year values are submitted to SelectDateWidget, it causes an OverflowError that crashes the server instead of being handled gracefully. The issue explains the root cause - Python cannot convert very large integers to C long when creating datetime.date objects, and this exception bubbles up unhandled. The expected behavior is implied: the form should handle this gracefully and return validation errors instead of crashing. The technical details are precise, mentioning sys.maxsize and the specific line where the crash occurs: datetime.date(int(y), int(m), int(d)).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified and the solution is straightforward: add an exception handler to catch OverflowError in the SelectDateWidget.value_from_datadict method. Looking at the provided patch, it's just adding a simple try-catch block with 2 lines of code. The main time would be spent understanding the Django forms codebase structure to locate the right method (value_from_datadict in widgets.py), but the actual fix is minimal. An experienced engineer familiar with Django would likely complete this in under 30 minutes, while someone less familiar with Django might take up to an hour to navigate the codebase and understand the form validation flow.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, a specific reproducible case, and an obvious solution approach. The issue is realistic and represents the kind of edge case handling that developers commonly encounter. The test coverage in the patch is also appropriate, testing both the widget behavior and form validation.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16950": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The reporter provides clear reproduction steps: create a Thing with UUIDField, save it (gets UUID), then add a SubThing inline and save again (UUID becomes null, causing integrity error). The models are clearly defined with UUIDModel having a UUIDField with uuid.uuid4 default, Thing inheriting from it, and SubThing having a ForeignKey to Thing's 'id' field. The admin configuration is provided showing StackedInline setup. However, some technical details need interpretation - the reporter mentions \"UUID field is being set to null\" and \"integrity error on the id column\" but doesn't specify exactly which part of Django's form processing is causing this. A skilled engineer would understand this relates to Django's inline formset handling of default values for foreign key relationships, particularly when the target field has a default value.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix that requires understanding Django's inline formset internals. Looking at the gold patch, the solution involves modifying django/forms/models.py in the add_fields method with a conditional check around setattr(self.instance, to_field.attname, None). The engineer needs to: (1) understand how Django handles inline formsets and foreign key field initialization, (2) identify that the issue occurs when a field with has_default() gets its value nullified inappropriately, (3) determine the correct conditions for when to preserve vs. null the default value (checking if it's the parent model's pk and whether form data is provided), and (4) write appropriate test cases. This requires deep knowledge of Django's ORM, formsets, and admin interface interactions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The bug report provides sufficient context including models, admin configuration, and clear reproduction steps. The issue is a legitimate Django framework bug affecting UUID fields in inline formsets. While some Django internals knowledge is required, an experienced engineer with time to familiarize themselves with the codebase should be able to solve this.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-20859": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: adding a legend to a SubFigure doesn't work and fails with a type error. The bug report includes a clear reproduction case (create figure, extract subfigure, plot line with label, call legend method on subfigure), describes the actual outcome (type error saying legend needs Axes or Figure as parent), and states the expected outcome (legend should work on subfigures). Most importantly, it even provides the exact solution: changing line 437 in matplotlib/legend.py to check against FigureBase instead of Figure. The issue includes specific file and line references, making it completely unambiguous what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue explicitly tells you exactly what needs to be changed: modify line 437 in legend.py to check against FigureBase instead of Figure. Looking at the gold patch confirms this - it's a minimal change involving: (1) changing the import from Figure to FigureBase, (2) changing isinstance(parent, Figure) to isinstance(parent, FigureBase), and (3) updating the error message. The change is conceptually simple - SubFigure inherits from FigureBase but not Figure, so the type check needs to be more general. An experienced engineer would need some time to understand the matplotlib codebase structure and the inheritance hierarchy, but the specific fix is straightforward once that's understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, the solution is well-scoped and testable, and it represents a realistic bug that requires understanding of object-oriented inheritance patterns. The test patch also provides a clear way to verify the solution works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-22719": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: a deprecation warning is inappropriately triggered when plotting empty data on axes with categorical units. The reporter provides a clear reproduction scenario (create axis with categorical units, then plot empty sequences) and describes the actual vs expected behavior. However, the issue description lacks the specific code snippet to reproduce the problem - it only describes the steps conceptually. An experienced engineer could reasonably infer what needs to be done (suppress the warning for empty data) based on the description, the expected outcome, and understanding that empty data should not trigger conversion warnings. The mention of the traceback and API change notes provides additional context about the underlying cause.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minutes to 1 hour fix. Looking at the gold patch, the solution is quite simple: adding `values.size and` and `data.size and` checks before issuing deprecation warnings in two locations in lib/matplotlib/category.py. The fix requires understanding that empty arrays should not trigger the deprecation warning, then identifying where these warnings are generated and adding appropriate guards. An experienced engineer familiar with matplotlib's codebase could locate the relevant warning messages, trace them to their source, and implement the size checks relatively quickly. The solution doesn't require complex algorithmic changes or extensive refactoring - just adding conditional checks to prevent warnings on empty data.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is a legitimate bug with a clear, focused solution. The test case verifies that no deprecation warning is emitted when plotting empty data with categorical units, which appropriately validates the fix. This is a good benchmark sample as it tests understanding of matplotlib's unit conversion system and the ability to identify and fix inappropriate warning conditions.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-23299": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with clear reproduction steps and expected behavior, but there are some gaps that require interpretation. The bug report clearly describes that `get_backend()` clears figures from `Gcf.figs` when the first figure was created under `rc_context`. The reproduction steps are detailed and the expected outcome is clear. However, the issue description doesn't explicitly explain WHY this happens or point to the root cause in the code. An engineer would need to investigate the relationship between `rc_context`, `get_backend()`, and figure management to understand that the problem lies in how `rc_context` restores rcParams including the backend, which triggers figure cleanup. While the \"what\" is clear, the \"how to fix it\" requires some detective work in the codebase.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding the interaction between multiple matplotlib components. An engineer would need to: 1) Reproduce the bug and understand the figure management system (Gcf), 2) Investigate how rc_context works and why it affects figure cleanup, 3) Trace through the get_backend() function to see what triggers the clearing, 4) Understand that resetting the backend parameter causes figure cleanup, and 5) Design a solution that prevents backend reset in rc_context. The actual code change is small (just a few lines), but the investigation and understanding of the matplotlib internals takes significant time. The fix involves modifying rc_context to exclude the backend parameter from restoration, which requires understanding the subtle interaction between rcParams and figure management.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The bug report is well-written with clear reproduction steps, and the solution is verifiable through the provided test case. This is a good candidate for a coding benchmark as it tests understanding of complex interactions between different parts of a codebase and requires both investigative skills and careful implementation.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-23476": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when a figure is unpickled on M1 Mac, its DPI is doubled each time. The issue provides specific reproduction steps (create a figure, pickle/unpickle in a loop, observe DPI values), shows the actual outcome (DPI doubling: 200.0 -> 400.0 -> 800.0, etc.), and provides the expected outcome (DPI should remain constant at 200.0). The environment details are comprehensive (M1 Mac, Matplotlib 3.5.2, Python 3.9.12, MacOSX backend). The reproduction code description is clear enough that an engineer could write the test case. The bug is platform-specific but the description makes this clear.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is quite simple: modify the __getstate__ method in figure.py to preserve the original DPI value instead of the potentially modified one. The fix involves understanding that on M1 Macs, the canvas can modify the DPI due to device pixel ratio changes, but when pickling we want to store the original DPI. The actual code change is just 3 lines. An experienced engineer would need some time to understand the matplotlib figure architecture and the relationship between DPI, device pixel ratio, and pickling, but once they locate the __getstate__ method and understand the issue, the fix is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, platform-specific but clearly documented, and the solution approach is testable. The issue includes sufficient detail for reproduction and the expected behavior is clear. An engineer would be able to create appropriate test cases and verify the fix works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24026": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The problem is clearly described: stackplot() is incorrectly modifying the axes color cycler when given color cycle alias references (like \"C0\", \"C1\", \"C2\"), causing a ValueError because cycle aliases can't be used in property cyclers. The expected behavior is also clear - stackplot should use the specified colors without modifying the axes' color cycle state. However, some implementation details need to be inferred, such as exactly how to handle the color cycling internally without affecting the axes cycler. The usecase scenario provides good context about the workflow (line plots, patches, then stackplot) that should maintain color consistency.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is well-defined, the solution requires: (1) Understanding matplotlib's color cycling system and how axes.set_prop_cycle() works, (2) Understanding how stackplot currently handles colors and why it's modifying the axes cycler, (3) Designing a solution that resolves colors internally without affecting the axes state, (4) The actual implementation involves changing the color handling logic in stackplot.py, replacing axes.set_prop_cycle() with itertools.cycle() and modifying how colors are retrieved throughout the function. This requires understanding the codebase structure and testing the changes carefully to ensure the fix works for both explicit colors and default cycling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-isolated to the stackplot function, has a clear test case, and the solution doesn't appear to require major architectural changes. The issue description provides sufficient context about the expected behavior and the error condition.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-24149": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes that `ax.bar` raises a StopIteration exception in matplotlib 3.6.1 when passed all-NaN data, while it should return a BarCollection with NaN values like it did in 3.6.0. The issue provides specific reproduction code showing that `ax.bar([np.nan], [0])` raises an exception while `ax.bar([0], [np.nan])` works, indicating the problem is specifically with NaN x-positions. The expected behavior is clearly stated: it should return a BarCollection with one Rectangle having NaN for x and height, matching the 3.6.0 behavior. The issue even identifies the likely cause in the release notes about fixing barplot being empty when first element is NaN.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-localized to the bar chart implementation, and the reproduction case clearly shows where the StopIteration exception occurs. Looking at the provided patch, the solution involves adding exception handling for StopIteration in two places in the `_convert_dx` function within `lib/matplotlib/axes/_axes.py`. The fix is straightforward: catch StopIteration exceptions and fall back to using `cbook.safe_first_element()` instead of `cbook._safe_first_finite()` when no finite elements are found. This requires understanding the existing code flow and adding appropriate exception handlers, but doesn't involve complex logic or extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear reproduction steps and expected behavior. The issue provides good context about the regression from 3.6.0 to 3.6.1, and the fix is localized to specific exception handling. The test case also properly validates the fix by ensuring all-NaN bar charts work as expected.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24970": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that require interpretation. The bug summary clearly states the problem: NumPy 1.24 deprecation warnings are being triggered. The reproduction steps are described conceptually (import matplotlib.pyplot, import numpy, create empty uint8 array, pass to get_cmap) but lack the exact code snippet. The expected outcome is clear (no warnings). However, an engineer would need to write the actual reproduction code and then investigate the matplotlib codebase to understand where these warnings originate and how to fix them. The issue provides enough context about what's happening (out-of-bound integer conversion warnings) but leaves the implementation details to be figured out.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve. First, the engineer needs to reproduce the issue by writing the described code sequence. Then they must trace through matplotlib's color handling code to identify where the NumPy deprecation warnings are generated. Looking at the gold patch, the solution involves understanding how numpy.errstate works and moving the context manager to wrap only the necessary astype conversion. This requires understanding both NumPy's warning system and matplotlib's colormap implementation. The fix involves modifying the error state handling in the __call__ method of what appears to be a colormap class, which requires careful analysis of the existing logic to ensure the change doesn't break other functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate bug fix that requires understanding NumPy deprecation warnings and matplotlib's internals. The solution is focused and doesn't require extensive architectural changes. The test case is straightforward and verifies the fix works across different data types.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25311": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified with clear reproduction steps, expected vs actual behavior, and specific error information. It provides sufficient context including the operating system, Python version, and Matplotlib version. The problem statement is unambiguous: figures with draggable legends cannot be pickled due to unpickleable canvas objects. An experienced engineer would understand that the solution needs to make draggable legends pickleable by avoiding storing references to unpickleable canvas objects.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: (1) It requires understanding matplotlib's architecture, particularly how draggable functionality works and the relationship between artists, canvases, and pickling. (2) The engineer needs to identify that the problem is in the DraggableBase class storing a direct canvas reference. (3) The solution requires understanding Python's property mechanism and how it can solve pickling issues by avoiding stored references to unpickleable objects. (4) Testing requires understanding pickling mechanics and ensuring the fix doesn't break draggable functionality. While the actual code change is small (a few lines), the investigation and understanding required pushes this into the 1-4 hour range.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear problem statement, reproducible steps, and a focused solution that doesn't require external dependencies or complex setup. The issue is suitable for evaluating coding ability as it tests understanding of Python object serialization, matplotlib architecture, and property vs attribute usage.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25332": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The problem statement is clear - pickling fails after calling align_labels() - and the reproduction steps are provided in prose form. However, there are some blanks to fill: (1) The code reproduction is described in natural language rather than actual executable code, requiring the engineer to translate this into specific matplotlib API calls, (2) The specific error message or exception type is described vaguely as \"a type error...indicating that an internal reference type...cannot be serialized\", (3) No concrete code example is provided. Despite these gaps, an experienced engineer familiar with pickling and matplotlib could reasonably interpret what's needed and work toward a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This falls into the 1-4 hour category because: (1) The engineer needs to reproduce the issue first by translating the prose description into working matplotlib code, (2) Understanding the root cause requires debugging why pickling fails specifically after align_labels(), which involves tracing through matplotlib's internal label alignment mechanism, (3) The solution requires knowledge of Python's pickle protocol and implementing custom __getstate__/__setstate__ methods, (4) The fix involves understanding weak references and their conversion to/from strong references, (5) While the actual code change is relatively small (~15 lines), the investigation and understanding phase is substantial. The issue is non-trivial but not extremely complex - it's a focused problem with a targeted solution in a single class.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is reproducible, the expected outcome is clear (successful pickling), and the solution can be verified by the test. The issue represents a legitimate bug that affects real-world usage when users want to serialize matplotlib figures after label alignment. The test coverage added is appropriate and directly validates the fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25479": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The user provides a clear example demonstrating the problem: when a colormap is registered with one name but has a different internal name, using the registered name fails because matplotlib looks up the internal name instead. The expected behavior is clearly stated - the user expects to be able to reference the colormap by its registered name. However, the issue doesn't explicitly specify the exact technical approach needed (like whether the internal name should be updated upon registration), leaving some implementation details to be figured out by the developer.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-contained within the colormap registration system. Looking at the gold patch, the solution involves: 1) Adding a few lines to update the colormap's internal name when registering with a different name, and 2) Modifying the equality comparison to not rely on name matching. Both changes are small and localized. An experienced engineer familiar with the codebase could identify the registration logic, understand the name mismatch issue, and implement the fix relatively quickly. The changes are straightforward once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a solid issue for benchmarking. The problem is clearly demonstrated with reproducible code, the expected behavior is reasonable, and the solution requires understanding matplotlib's colormap system without being overly complex. The test cases also properly verify that the fix works correctly, ensuring registered colormaps can be referenced by their registered names and that equality comparisons work appropriately.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-26291": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides enough information for a meaningful solution attempt. The user describes:\n\n1. **Clear context**: They're following the first example from the matplotlib gallery for inset_locator_demo\n2. **Reproducible steps**: Set up figure with two subplots, add inset box to left subplot with relative width/height\n3. **Specific error**: AttributeError due to missing renderer object (internal component is None)\n4. **Expected behavior**: Should create an empty box in top right of first subplot\n5. **Environment details**: Specific versions of matplotlib (3.7.2), Python (3.8.17), OS, backend\n\nHowever, the issue lacks the exact code that reproduces the problem and the complete error traceback. While this creates some ambiguity about the precise reproduction steps, an experienced engineer familiar with matplotlib's inset_axes functionality could reasonably infer what's happening and work towards a solution. The core problem (renderer being None) is clearly stated, and the expected outcome is well-defined.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix based on several factors:\n\n1. **Simple solution**: The actual fix is just 2 lines of code that check if renderer is None and get a renderer from the figure if needed\n2. **Localized problem**: The issue is contained within a single function (__call__ method in inset_locator.py)\n3. **Clear error pattern**: AttributeError due to None renderer is a common pattern that experienced developers recognize\n4. **Minimal code change**: No complex logic, architectural changes, or multiple file modifications required\n\nAn experienced engineer who spends a few minutes understanding the inset_locator module would quickly identify that the __call__ method needs to handle the case where renderer is None. The solution follows a standard defensive programming pattern of checking for None and providing a fallback.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a solid sample for evaluating coding ability. The issue represents a real-world bug that requires understanding of matplotlib's rendering system and defensive programming practices. The solution is straightforward but not trivial, making it appropriate for testing an engineer's debugging and problem-solving skills. The test case also validates the fix properly by reproducing the scenario that triggers the bug (savefig with bbox_inches=\"tight\").",
            "q2_5_confidence": 4
        }
    },
    {
        "psf__requests-1724": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The problem is clearly described: when using Unicode method names (like u'POST') instead of byte strings in Python 2.7.2, a UnicodeDecodeError occurs when the requests library tries to send headers and body. The issue provides a specific scenario (opening /usr/bin/diff in binary mode, calling requests.request with Unicode method), explains the failure point (merging Unicode header value with raw multipart body encountering non-ASCII byte 0xCF), and even identifies the likely root cause in sessions.py:313 where `req.method = method.upper()` doesn't handle Unicode properly. However, there are some details that need to be inferred, such as the exact implementation approach for the fix and understanding how the builtin_str function should be used (which is visible in the patch but not explicitly described in the issue).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location (sessions.py:313) and the root cause (Unicode method names not being properly converted to byte strings). The solution is straightforward: convert the method parameter to a proper string type before using it. Looking at the patch, it's a simple 1-line addition using builtin_str(method) to ensure the method is converted to the appropriate string type for Python 2.7. The fix requires understanding the Python 2/3 compatibility implications and knowing about the builtin_str utility function, but once that's understood, the implementation is trivial.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The issue is specific to Python 2.7.2 behavior with Unicode strings, which is a legitimate historical problem that tests both understanding of Python 2/3 differences and the requests library internals. The test case is appropriate and checks the exact scenario described in the issue.",
            "q2_5_confidence": 4
        }
    },
    {
        "psf__requests-5414": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: attempting to GET `http://.example.com` raises a UnicodeError instead of the expected InvalidURL exception. The issue provides specific details including: (1) the exact URL that triggers the problem, (2) the current behavior (UnicodeError from IDNA encoder due to empty domain label), (3) the expected behavior (InvalidURL with message \"URL has an invalid label\"), (4) reference to existing similar handling in the codebase, (5) clear reproduction steps, and (6) complete system information. The problem is unambiguous - URLs with leading dots in hostnames should be caught and raise InvalidURL rather than allowing the underlying IDNA encoder to fail with UnicodeError.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves a simple one-line change to extend an existing condition in the `prepare_url` method. The code already handles URLs starting with '*' by raising InvalidURL, and the fix just extends this check to also catch URLs starting with '.'. Looking at the patch, it changes `host.startswith(u'*')` to `host.startswith((u'*', u'.'))` in requests/models.py line 406. The engineer needs to: (1) locate the existing URL validation logic, (2) understand the pattern of existing checks, (3) add the dot check to the existing condition. The accompanying test addition is also straightforward. No complex logic, algorithm changes, or deep architectural understanding is required.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified with concrete reproduction steps, the solution is focused and testable, and the difficulty level is appropriate for evaluating coding ability without being trivial or overly complex. The fix demonstrates understanding of error handling patterns and URL validation logic.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-3151": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear MCVE (Minimal Complete Verifiable Example) that describes exactly how to reproduce the problem: create two xarray Dataset objects with identical non-monotonic 'y' coordinates (\"a\", \"c\", \"b\") but different 'x' coordinates, then call xr.combine_by_coords on them. The expected behavior is clearly stated (should merge successfully), and the actual problem is well-described (ValueError about non-monotonic global indexes). The issue also explains the contradiction with documentation that says coordinate dimensions which don't vary between datasets should be ignored. An experienced engineer would have sufficient information to understand both the problem and what constitutes a correct solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is quite straightforward: change the loop from iterating over all dimensions (`concatenated.dims`) to only iterating over the dimensions being concatenated (`concat_dims`). The core issue is that the monotonicity check was being applied to all dimensions, including \"bystander\" dimensions that aren't involved in the concatenation. The fix involves changing just 3-4 lines of code in a single function. An experienced engineer familiar with the codebase would need some time to understand the combine_by_coords function and locate where the monotonicity check occurs, but once identified, the fix is relatively simple and logical.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is testable, and the provided test case clearly demonstrates both the issue and validates the fix. The issue represents a legitimate bug where the function was being overly restrictive in its monotonicity checks.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-3677": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the dataset's merge() method fails when passed a DataArray, while the top-level xr.merge() function works correctly with the same inputs. The issue provides a clear reproduction scenario (create dataset with variable \"a\" set to zero, create DataArray \"b\" with value one), explains the expected behavior (both should result in a dataset containing both variables), and identifies the root cause (the merge implementation expects objects with an items() method, but DataArray doesn't provide this interface). The error type (AttributeError) is also mentioned. An experienced engineer would have all the information needed to understand and solve this problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a simple one-line fix that converts a DataArray to a Dataset before processing. The solution is straightforward: check if the input is a DataArray and convert it using .to_dataset() if so. This requires minimal understanding of the xarray API and the existing merge implementation. An experienced engineer familiar with the codebase could identify this solution within 15-60 minutes by examining the merge method, understanding that it expects dataset-like objects, checking the DataArray API for conversion methods, and implementing the simple type check and conversion.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described, the solution is focused and testable, and it represents a realistic API usability problem. The test case properly verifies that the dataset merge method produces the same result as the top-level merge function, ensuring behavioral consistency across the API.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-4094": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some blanks to fill in. It clearly describes the problem (to_unstacked_dataset broken for single-dim variables), provides a conceptual MCVE (though not actual runnable code), states the expected behavior (working roundtrip), and identifies the root cause (merge conflict error). However, an engineer would need to create the actual test code from the description and figure out the specific technical details of why the merge conflict occurs. The problem description is clear enough that an experienced engineer could understand what needs to be fixed and work toward a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The actual solution is just a one-line change adding `drop=True` to a method call. However, it would require some investigation to: 1) Reproduce the issue by writing the test code described in the MCVE, 2) Understand why the merge conflict occurs (likely due to coordinate conflicts when variables share the same coordinate), 3) Identify that the `drop=True` parameter in the `sel()` method is the solution. The fix itself is trivial once identified, but requires some debugging and understanding of the xarray library's coordinate handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This appears to be a well-contained regression issue with a clear problem statement and straightforward solution. The test provided is appropriate and would verify the fix works. The timing seems reasonable for a benchmark as it requires understanding the codebase but isn't overly complex.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-4695": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear reproduction case: creating a 2x2 labeled array where naming a dimension \"method\" causes an error when using .loc indexing, while other dimension names work fine. The issue explains that the problem occurs because \"method\" is being mistaken for a fill-method argument in the selection logic. The reporter even provides specific steps to reproduce and mentions they've tested with xarray 0.12. The expected behavior is clear - the .loc operation should work regardless of dimension naming. The root cause is identified as improper sanitization of dimension names. This gives an engineer everything needed to understand and solve the problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively simple fix that took a small but important change. The issue was in the DataArray.__getitem__ method where key arguments were being passed as **key (keyword arguments) to the sel() method, causing dimension names like \"method\" to be interpreted as sel() parameters. The fix changed `self.data_array.sel(**key)` to `self.data_array.sel(key)`, passing the dictionary directly instead of unpacking it as keyword arguments. This is a 1-line change that requires understanding the issue (parameter collision) and knowing the proper way to call sel() with a dictionary. An experienced engineer would likely identify this quickly once they trace through the .loc implementation and understand that \"method\" is a valid parameter to sel().",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear reproduction case, identifiable root cause, and straightforward fix. The test case provided in the patch validates the fix appropriately by testing dimension name collision with sel parameters. The issue would make for a good benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-6599": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes that polyval with timedelta64 coordinates produces wrong results in the development version compared to version 2022.3.0. The user provides specific details about the expected behavior (6\u00d73 array with values around 4.447\u00d710\u2076) versus actual behavior (3\u00d76 array with values around 10\u00b3\u2070), indicating both dimension swapping and incorrect computation. However, the issue lacks the actual MVCE code despite mentioning it exists, which makes it harder to reproduce the problem. The description of the setup (datetime stamps, elapsed time coordinate, polynomial coefficients) is detailed enough to understand the context. While some implementation details need to be figured out, the core problem is clear: timedelta64 coordinates are not being handled correctly in polyval function.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that takes 15 minutes to 1 hour. The problem is in the _ensure_numeric function in computation.py where timedelta64 dtypes (kind \"m\") were being handled the same way as datetime64 dtypes (kind \"M\"). The fix simply separates these cases: datetime64 uses datetime_to_numeric conversion while timedelta64 uses astype(float). This is a small, localized change that requires understanding the difference between datetime and timedelta handling, but doesn't involve complex logic or multiple file changes. An experienced engineer familiar with numpy dtypes and the codebase structure could identify and implement this fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. While the missing MVCE code is a minor inconvenience, the problem description provides enough context to understand and solve the issue. The patch demonstrates this is a legitimate bug with a clear, testable solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-6721": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: accessing the `chunks` property on a zarr-backed xarray dataset is causing the entire array to be loaded into memory instead of just inspecting metadata. The expected behavior is also stated - it should simply inspect the `encoding` attribute on underlying DataArrays as suggested by @rabernat. However, there are some missing details: no minimal reproducible example is provided, and the specific steps to reproduce the issue are not detailed. The log output mentions \"low-level getitem calls\" but doesn't provide actual logs. Despite these gaps, an experienced engineer could reasonably understand that the chunks property implementation needs to be fixed to avoid triggering data loading operations.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a very small fix - changing `v.data` to `v._data` in a single line in the `get_chunksizes` function. The issue is that accessing `.data` triggers loading of the actual data, while `._data` accesses the underlying data structure without loading. This suggests the problem was relatively straightforward to identify and fix once you understand the xarray codebase structure. An experienced engineer familiar with the codebase could likely identify this issue within 15 minutes to 1 hour by examining how the chunks property works and understanding the difference between lazy and eager data access patterns in xarray.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-understood in the context of lazy loading vs eager loading, which is a common pattern in data processing libraries. The fix is clean and targeted, and the test case provided is appropriate for verifying the fix works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "pylint-dev__pylint-6386": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the short verbose option `-v` expects an argument when it should behave like the long option `--verbose` (which doesn't expect an argument). The issue provides specific examples of what works (`pylint mytest.py --verbose`) and what doesn't work (using `-v`). The expected behavior is explicitly stated: \"Similar behaviour to the long option.\" The bug description is concrete and actionable - there's a clear discrepancy between how `-v` and `--verbose` behave, and the solution path is obvious (make `-v` work the same as `--verbose`). An experienced engineer would understand exactly what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves: 1) Adding `-v` to the preprocessing options mapping in utils.py (1 line change), 2) Adding metavar support to handle empty metavar for boolean flags (several small additions), and 3) Updating the verbose option configuration to include an empty metavar. The core issue is that `-v` wasn't properly mapped to the same handler as `--verbose`. This requires understanding the configuration system and option parsing, but once you locate the relevant files (which would take some time in a large codebase), the actual changes are straightforward. The patch shows it's mostly adding missing configurations rather than complex logic changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements and a straightforward solution. The issue has sufficient detail for someone to understand and fix it, and the fix doesn't require deep architectural knowledge or complex algorithmic thinking.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-6903": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: pylint crashes when running with --jobs=0 in a Kubernetes Pod because the _query_cpu() function returns 0, which causes multiprocessing to fail since it needs a value > 0. The issue provides specific details including the exact file paths and line numbers where the problem occurs (pylint/lint/run.py lines 34, 55, 60), the specific values from the cgroup files that cause the issue (cpu.cfs_quota_us = -1, cpu.cfs_period_us = 100000, cpu.shares = 2), the calculation that leads to the problem (2/1024 = 0 when cast to int), and even suggests a specific solution (append \"or 1\" to prevent the value from being 0). The environment details are provided (Ubuntu 20.04, Kubernetes v1.18.6, Python 3.9.12, pylint>2.14.0) and the expected behavior is clear.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. The problem is clearly identified in the issue description with specific line numbers and the root cause (division resulting in 0 when cast to int). The solution is simple: add a check to ensure the CPU count is never 0, setting it to 1 as a minimum. Looking at the gold patch, this is exactly what was implemented - a simple conditional check that sets avail_cpu = 1 when it equals 0. The fix requires minimal code changes (just a few lines), no complex logic, and the location of the fix is explicitly identified in the issue description.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is well-documented with clear steps to reproduce, specific technical details, and a logical solution. The problem domain (CPU detection in containerized environments) is realistic and the fix demonstrates good defensive programming practices. The test coverage in the patch is also comprehensive, mocking the specific cgroup file conditions that trigger the bug.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-8898": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly describes the problem: pylint's bad-names-rgxs option splits on commas without considering that commas can be legitimate parts of regex patterns (like in quantifiers {1,3}). The reproduction steps are clear - configure the option with a regex containing a comma and run pylint on any Python file. The expected behavior is explicitly stated: any valid regex should be expressible. The error is described as pylint crashing with specific error messages about missing parentheses. An experienced engineer would understand exactly what needs to be fixed: the CSV parsing logic for regex options needs to be smarter about when to split on commas.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task. The engineer needs to: 1) Understand how pylint's configuration system works and locate the CSV transformation code, 2) Analyze the specific case where regex patterns with commas are being incorrectly split, 3) Design a new parsing function that can distinguish between separator commas and regex quantifier commas by tracking brace context, 4) Implement the _check_regexp_csv function with proper state tracking for open/close braces, 5) Update the imports and function calls in multiple files. While the core logic isn't extremely complex, it requires understanding regex syntax, implementing stateful parsing logic, and navigating the pylint codebase to make changes in the right places.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution approach is clear, and the scope is reasonable for a coding benchmark. The issue demonstrates good software engineering skills including parsing logic, understanding regex syntax, and working with configuration systems.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10051": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: `caplog.get_records()` becomes decoupled from `caplog.records` when `caplog.clear()` is called, causing `get_records()` to become frozen and not reflect new records or clearing. The issue provides a detailed reproductive example with 5 specific steps, clearly states the expected vs observed behavior, and explains the root cause (that `caplog.records` gets replaced rather than cleared in `caplog.clear()`, causing the two objects to diverge). An experienced engineer would have a clear understanding of what needs to be fixed: ensure that `caplog.get_records()` and `caplog.records` stay synchronized after `caplog.clear()` is called.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution, this is a relatively straightforward fix that involves adding a new `clear()` method to the handler class that calls `records.clear()` instead of replacing the list, and modifying the caplog's `clear()` method to use this new handler method instead of `reset()`. The change is localized to the logging module and requires minimal code changes (adding one method and changing one method call). An experienced engineer familiar with the codebase could identify and implement this solution within 15 minutes to 1 hour, as it requires understanding the relationship between the handler and caplog objects but doesn't involve complex logic or extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is testable, and the provided test case adequately verifies the fix. The issue represents a realistic debugging scenario that tests understanding of object references and method behavior, making it suitable for a coding benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10081": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when running `pytest --pdb`, the `tearDown()` method of `unittest.TestCase` classes decorated with `unittest.skip` at the class level is being executed when it shouldn't be. The issue provides a minimal reproducible example (test_repro_skip_class.py), shows the expected vs actual behavior, includes full version information, and references a related issue (#7215) that was for function-level skips. The problem statement is concrete: tearDown should not run for skipped test classes when using --pdb, just like it doesn't run for skipped test functions. The gold patch confirms this understanding - it modifies the condition in src/_pytest/unittest.py to check if either the test method OR the parent class is skipped before deciding whether to postpone tearDown.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-isolated to the unittest handling code in pytest. The gold patch shows it's a small, focused change: adding a check for whether the parent class (UnitTestCase) is also skipped, not just the individual test method. An experienced engineer familiar with pytest's codebase would need to: 1) Understand how pytest handles unittest integration, 2) Locate the existing skip logic in src/_pytest/unittest.py, 3) Recognize that the current code only checks _is_skipped(self.obj) for the test method but not the parent class, 4) Add the additional check for _is_skipped(self.parent.obj). The logic is straightforward and the change is minimal (just a few lines), but requires understanding the pytest codebase structure and unittest integration patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, good test coverage in the patches, and a focused solution. The issue provides excellent documentation with minimal reproduction steps, making it suitable for a coding benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5262": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: `_pytest.capture.EncodedFile` incorrectly advertises a binary mode (rb+) but its write() method only accepts text, causing a TypeError when youtube-dl tries to write bytes based on the advertised mode. The issue includes a detailed explanation of the root cause, specific steps to reproduce, expected vs actual behavior, environment details, and mentions the specific class `_pytest.capture.EncodedFile` that needs to be fixed. The solution requirement is clear: the mode property should not include 'b' (binary) so that applications like youtube-dl don't attempt to write bytes to what is actually a text-only stream.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-understood: the `EncodedFile` class needs a `mode` property that strips the 'b' from the underlying buffer's mode. Looking at the gold patch, the solution is quite simple - adding a 4-line property method that returns `self.buffer.mode.replace(\"b\", \"\")`. The engineer would need some time to locate the `EncodedFile` class in the codebase and understand how it works, but the actual fix is straightforward once the problem is understood. The test addition is also simple - just verifying that sys.stdout.mode doesn't contain 'b' when using capfd.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear problem statement and solution. The issue provides excellent reproduction steps and the fix is localized to a single class. The test verifies the expected behavior effectively.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5631": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The problem: pytest 3.6.0 fails during test collection when using @patch decorator with numpy arrays as replacement values, while pytest 3.1.3 works fine. (2) The root cause: membership test \"p.new in sentinels\" returns an array of booleans instead of a boolean when p.new is a numpy array, causing ValueError when converting to truth value. (3) The specific commit that introduced the regression. (4) A concrete example of the problematic usage: @patch(target='XXXXXX', new=np.array([-5.5, 3.0])). The issue provides enough technical detail for an experienced engineer to understand both the symptom and the underlying cause in the num_mock_patch_args function.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic code location and the root cause. The solution involves changing the membership test \"p.new in sentinels\" to use identity comparison \"p.new is sentinel\" instead, which avoids the numpy array equality comparison issue. Looking at the provided patch, it's a straightforward change that replaces the list comprehension with explicit identity checks against mock sentinels. An experienced engineer familiar with the codebase could quickly locate the num_mock_patch_args function in src/_pytest/compat.py and implement the fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-isolated bug with a clear reproduction case and solution. The issue demonstrates good technical understanding and provides sufficient context for debugging. The test case also properly validates the fix by creating a numpy-like object that raises ValueError on equality comparison, ensuring the identity-based approach works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5787": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is reasonably well-specified but has some gaps to fill in. The core problem is clear: exception serialization in pytest-xdist truncates chained exceptions, showing only the final exception instead of the full chain. The issue provides a specific reproduction case with two test functions that raise sequences of ValueError exceptions - one with explicit chaining using 'from' and one without. It clearly explains that regular pytest shows the full exception chain while pytest-xdist with parallel workers only shows the last exception. However, there are some details that need to be inferred: 1) The solution needs to be in the serialization/deserialization logic for reports, 2) The specific classes and methods that need modification aren't explicitly mentioned, 3) The exact format for preserving chain information in serialized form isn't specified. An experienced engineer would need to explore the codebase to understand how pytest handles report serialization and how chained exceptions are represented internally, but the overall goal and expected behavior is clear from the description.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the solution patch, it requires: 1) Understanding pytest's report serialization system in _pytest/reports.py, 2) Adding support for ExceptionChainRepr objects in both serialization (_report_to_json) and deserialization (_report_kwargs_from_json) functions, 3) Implementing recursive serialization/deserialization of chained exception data structures, 4) Modifying multiple files (reports.py and test files), 5) Writing comprehensive tests for the new functionality. The solution involves around 200+ lines of new code across serialization and deserialization logic, plus test updates. While the concept is straightforward (preserve exception chains during serialization), the implementation requires understanding Python's exception chaining mechanism, pytest's internal representation of exceptions (ExceptionChainRepr, ReprTraceback, etc.), and how the serialization system works. An experienced engineer would need time to familiarize themselves with these internal structures and ensure the recursive serialization handles all edge cases correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues that would prevent this from being used in the benchmark. The issue is well-suited for evaluating coding ability as it tests understanding of serialization, exception handling, recursive data structures, and working with existing codebases. The reproduction case is clear and testable.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-5809": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem: the --pastebin feature uses lexer=\"python3\" when submitting pytest output to bpaste.net, but this causes HTTP 400 Bad Request errors for certain content. The issue explains that the root cause is that pytest output is not Python code but arbitrary text, so using lexer=\"python3\" is inappropriate. The solution is clearly stated: change the lexer from \"python3\" to \"text\". The issue even provides the specific file location (src/_pytest/pastebin.py) and line numbers (68-73) where the change needs to be made. A data.txt file is mentioned as an example that reproduces the problem, and the issue references #5764 as related. The technical details are sufficient for an experienced engineer to understand exactly what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15-60 minutes. The issue clearly identifies the exact location of the problem in src/_pytest/pastebin.py and the solution is simply changing the lexer parameter from \"python3\" to \"text\". Looking at the actual patch, it's a very minimal change - just removing the conditional logic that chooses between \"python3\" and \"python\" lexers and replacing it with a hardcoded \"text\" lexer. The engineer would need to: 1) Locate the create_new_paste function, 2) Find the params dictionary, 3) Change the lexer value from the conditional to \"text\", and 4) Update any related tests. The most time would be spent understanding the codebase structure and running tests to ensure the change works correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the patch shows it's a minimal change. The issue provides good technical context and the fix is self-contained within the pastebin functionality. This would be a good sample for evaluating basic debugging skills and ability to make targeted fixes based on clear requirements.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5840": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the problem: pytest 5.1.2 lowercases folder paths on Windows, causing conftest.py loading failures when the actual directory has different casing (like \"PIsys\"). The reproduction steps are provided (upgrade from 5.1.1 to 5.1.2, run pytest against \"PIsys\" directory with smoke marker). However, some details could be clearer - the exact error message isn't provided, and the connection between \"lowered path\" and \"module named python\" error is not fully explained. An experienced engineer could reasonably infer this is a case-sensitivity issue in file path handling that needs to preserve original casing for proper module resolution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix based on the solution complexity. The engineer needs to: 1) Understand pytest's conftest loading mechanism and path handling, 2) Identify that the issue is in the unique_path function using normcase() which lowercases paths on Windows, 3) Research the difference between py.path.local and pathlib.Path behavior regarding case preservation, 4) Modify multiple functions in config/__init__.py to use Path().resolve() instead of unique_path(), 5) Remove the unique_path utility function entirely, and 6) Update related test files. The solution touches multiple files and requires understanding of cross-platform path handling nuances, particularly Windows case-insensitive but case-preserving filesystems.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is platform-specific to case-insensitive filesystems (primarily Windows), but this is a legitimate and important issue for pytest compatibility. The solution demonstrates good software engineering practices by replacing a custom utility function with standard library functionality that handles the edge case properly. The test additions appropriately verify the fix works for both the reported case and similar scenarios.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6197": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a regression between pytest versions 5.2.2 and 5.2.3, provides concrete reproduction steps with specific setup (tox configuration, directory structure with foobar/__init__.py containing a failing assertion), and clearly explains the expected behavior (pytest should only collect test files) versus the observed behavior (pytest tries to import random __init__.py files during collection, causing failures). The issue gives enough context to understand that this is a collection mechanism problem where pytest is being too aggressive in importing __init__.py files that aren't meant to be test modules.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This appears to be in the 1-4 hour range based on the patch complexity. The solution involves understanding pytest's collection mechanism and modifying the PyobjMixin class and Module collector. The patch shows changes to multiple methods including removing eager collection (_mount_obj_if_needed calls), restructuring the obj property, and removing special __init__.py handling in Module.__init__. While the actual code changes aren't massive, understanding the collection flow in pytest and identifying where the regression was introduced would require significant investigation of the codebase. The fix touches core collection logic which requires careful understanding of pytest internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined regression with clear reproduction steps and a targeted fix. The issue would be suitable for a benchmark as it tests understanding of collection mechanisms in a testing framework, which is a realistic software engineering challenge. The solution requires both debugging skills to identify the root cause and careful implementation to fix the regression without breaking other functionality.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6202": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when pytest runs parametrized tests with parameter values containing \"..[\", the string \".[\" gets replaced with \"[\" in the test name display. The issue provides a clear reproduction case, explains the expected behavior (showing the full \"..[\" sequence), and even includes a detailed investigation tracing the problem to specific code in src/_pytest/python.py line 290 where `return s.replace(\".[\", \"[\")` is the culprit. The issue author has done the debugging work and identified the exact line causing the problem, along with a proposed solution to simply return the unmodified string.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix that takes less than 15 minutes. The issue description provides the exact location of the problematic code (src/_pytest/python.py, line 290 in the getmodpath method) and the exact change needed: removing the `.replace(\".[\", \"[\")` call. The fix involves changing a single line of code from `return s.replace(\".[\", \"[\")` to `return \".\".join(parts)`. No research is needed since the problem analysis is already complete, and the solution is straightforward - just remove the string replacement that's causing the issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample because: 1) The problem is clearly defined with a specific reproduction case, 2) The solution is well-bounded and involves a simple code change, 3) The test case provided in the patch validates the fix effectively by checking that parametrized test names preserve the original parameter values including \"..[\", and 4) The issue demonstrates good debugging skills by tracing through the codebase to identify the root cause.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7205": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The exact reproduction steps: run pytest under Python 3.8.2 with -bb flag and --setup-show option on a test parameterized with bytes values, (2) The expected behavior: pytest should display the byte parameter in a safe, readable form without raising warnings, (3) The actual problem: pytest tries to convert bytes parameter to string for display, which raises BytesWarning that gets treated as error due to -bb flag, and (4) A suggested solution direction: using saferepr instead of implicit str() conversion. The issue provides concrete context about the setuponly.py module and the specific scenario where the problem occurs.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified: in setuponly.py, the code uses direct string formatting of fixturedef.cached_param which causes BytesWarning for bytes parameters. The solution is straightforward - replace the implicit str() conversion with saferepr() for safe representation. Looking at the gold patch confirms this: it's a simple 2-line change importing saferepr and using it with maxsize parameter. The fix requires understanding the issue, locating the problematic line in _show_fixture_action function, and making a targeted change. No complex logic or multiple file modifications needed.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described with specific reproduction steps, the context is well-defined (pytest's --setup-show functionality), and the solution requires understanding both the problem (BytesWarning) and the appropriate fix (saferepr vs str). An experienced engineer could easily understand what needs to be done and implement the correct solution. The test changes also make sense - they verify that the output format changes from unquoted to quoted parameter display, which is exactly what saferepr would produce.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7236": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some minor gaps to fill in. The problem is clearly described: when running pytest with --pdb flag, skipped unittest.TestCase tests still execute their tearDown methods, which shouldn't happen. The issue provides a specific scenario (skipped test with tearDown that references undefined variables), mentions the expected behavior (tearDown should not run for skipped tests), and notes this is a regression between pytest versions 5.4.1 and 5.4.2. However, the issue doesn't provide the actual test code mentioned, requiring the engineer to create a minimal reproduction case. The core requirement is clear: prevent tearDown execution for skipped tests when --pdb is used.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. The engineer needs to: 1) Understand the pytest codebase structure, particularly the unittest integration in src/_pytest/unittest.py, 2) Reproduce the bug by creating a test case with skipped tests and tearDown methods, 3) Trace through the code to understand why --pdb causes tearDown to run for skipped tests, 4) Identify that the issue is in the runtest() method where it checks for usepdb but doesn't also check if the test is skipped, 5) Implement the fix by adding a skip check condition, and 6) Write comprehensive tests. The solution involves understanding pytest's internal workings, unittest integration, and the pdb feature interaction, requiring moderate debugging skills and codebase familiarity.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, reproducible, and the solution scope is reasonable. The test patch shows good coverage testing both @unittest.skip and @pytest.mark.skip decorators, and the fix is localized to the unittest integration module. This is a suitable benchmark problem that tests debugging skills, understanding of test framework internals, and ability to write regression tests.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7324": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem where pytest crashes Python debug builds when dealing with AST expressions containing False literals. The description clearly states the symptoms (interpreter crash, internal assertion failure, core dump) and provides context about the underlying cause. However, there are some gaps: it doesn't specify exactly which pytest functionality is affected, what specific code paths trigger this, or provide a minimal reproduction case. While an experienced engineer could reasonably infer that this relates to pytest's mark expression parsing (given the patch context), the issue text alone doesn't make this completely obvious. The connection between \"AST expression from literal False\" and pytest's specific use case would require some investigation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) The engineer needs to understand the root cause - that Python identifiers like \"True\", \"False\", \"None\" are illegal in AST contexts but legal as pytest mark identifiers, (2) They must identify that the solution involves adding a prefix to avoid Python keyword conflicts, (3) The fix requires changes across multiple functions in the expression parser and the MatcherAdapter class, (4) Understanding the interaction between pytest's mark expression parsing and Python's AST compilation requires domain knowledge, (5) While the actual code changes are relatively small (~10 lines), the conceptual understanding and debugging to arrive at this solution would take significant time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within pytest's codebase, the solution is elegant and focused, and the test additions are straightforward. The issue represents a good balance of requiring debugging skills, understanding of AST/compilation concepts, and careful implementation without being overly complex.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7490": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a regression where dynamically adding xfail markers using `request.node.add_marker(mark)` worked in pytest 5.x but fails in pytest 6.0.0rc0. The description provides specific version information, exact reproduction steps (create a test file with a function that gets the request fixture, adds an xfail marker dynamically, and performs a failing assertion), expected behavior (test should be treated as xfailed like in 5.x), and actual behavior (test is reported as a regular failure). The technical context is sufficient for an experienced engineer to understand that this involves the pytest skipping/xfail machinery and dynamic marker addition during test execution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours of work. Looking at the gold patch, the solution involves understanding pytest's internal marker evaluation system and modifying the skipping.py module. The fix requires: 1) Understanding how xfail markers are evaluated at different stages of test execution, 2) Recognizing that dynamically added markers aren't being re-evaluated after the test run, 3) Modifying the pytest_runtest_call hook to re-evaluate xfail marks after the test runs to capture dynamically added markers, 4) Restructuring the conditional logic in multiple functions. The patch touches multiple hook functions and requires understanding pytest's plugin architecture and test execution lifecycle. While not extremely complex, it requires significant domain knowledge and careful analysis of when markers are evaluated.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is testable, and the patch shows a focused fix to the core issue. The test cases provided in the test patch validate both the basic functionality (dynamic xfail with failing test) and edge cases (strict xfail with passing test). This is a suitable benchmark problem for evaluating coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7521": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides clear reproduction steps, explains the expected vs actual behavior, gives version context (pytest 5 vs 6), and includes a minimal demonstration. The problem is clearly stated: capfd.readouterr() should preserve carriage returns (\\r) but is converting them to newlines (\\n) in pytest 6. An engineer would have all the information needed to understand the regression and work toward a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is in the 15 min - 1 hour range. The issue is clearly identified as a regression in how pytest handles newlines in captured output. Looking at the solution, it's a simple one-line fix adding newline=\"\" parameter to TextIOWrapper. An experienced engineer familiar with Python's text I/O handling would likely recognize this as a newline translation issue relatively quickly. The main time would be spent understanding the capture mechanism and identifying where the conversion happens, but the actual fix is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - it's well-specified, has a clear regression scenario, and tests a specific technical understanding of Python I/O and pytest internals. The solution is focused and verifiable.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10297": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes that RidgeClassifierCV is missing the `store_cv_values` parameter that is documented but not implemented. The issue provides: (1) Clear steps to reproduce with specific parameter values, (2) Expected vs actual results showing a TypeError for unrecognized parameter, (3) Documentation evidence showing the parameter should exist (cv_values_ attribute documentation mentions store_cv_values=True), (4) Exact error message received. The problem is unambiguous - the constructor needs to accept the store_cv_values parameter and pass it to the parent class, just like RidgeCV does.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves: (1) Adding store_cv_values parameter to RidgeClassifierCV.__init__() with default False, (2) Passing it to the parent class constructor, (3) Updating documentation to match RidgeCV format. The fix is straightforward since RidgeCV already implements this functionality - it's just missing from RidgeClassifierCV. An engineer would need to examine the RidgeCV implementation, understand the inheritance hierarchy, and make the simple parameter addition. Most time would be spent understanding the codebase structure rather than complex problem-solving.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, has a straightforward solution, and the test cases adequately verify the fix works for both 1D and 2D target cases. This is a good benchmark sample as it tests understanding of class inheritance, parameter passing, and API consistency.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10908": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: CountVectorizer raises NotFittedError when calling get_feature_names() even when a vocabulary parameter is provided during initialization. The issue explains the expected behavior (get_feature_names should work immediately when vocabulary is provided), gives concrete examples of the problematic behavior, identifies the root cause (_validate_vocabulary method sets vocabulary_ attribute), and proposes a clear solution. The description includes specific method names (get_feature_names, transform, _validate_vocabulary) and explains the internal state (vocabulary_ attribute). An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The solution is straightforward: add a check in get_feature_names() to call _validate_vocabulary() if the vocabulary_ attribute doesn't exist, similar to what transform() already does. The patch shows this is exactly what was done - just 3 lines of code added. The logic is simple: if not hasattr(self, 'vocabulary_'), call self._validate_vocabulary(). An experienced engineer familiar with the codebase could identify this pattern by looking at the transform method and apply the same logic to get_feature_names. The test additions are also minimal and follow existing patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. This is a well-contained bug fix with clear requirements, straightforward implementation, and good test coverage. The issue description provides sufficient context and the solution follows existing patterns in the codebase. This would make an excellent benchmark sample for evaluating coding ability.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-12585": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The problem: `clone` fails when parameters are estimator types (classes) rather than instances, (2) A concrete reproduction case: creating a StandardScaler with its with_mean parameter set to the StandardScaler class itself, then calling clone, (3) Expected vs actual results: should work without error but instead throws a TypeError about missing self argument when get_params is called on the class, (4) The specific location and proposed fix: change line 51 in base.py to add `or isinstance(estimator, type)` check. The issue author even provides the exact code change needed and explains the root cause (calling get_params on a class without an instance). An experienced engineer would have all the information needed to implement and test this fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that would take 15-60 minutes. The issue clearly identifies the problem location (base.py line 51), provides the exact fix needed (adding `or isinstance(estimator, type)`), and explains the root cause. An engineer would need to: (1) Understand the clone function and why it's failing on class types, (2) Verify the proposed fix makes sense, (3) Add the one-line change, (4) Write a simple test case. The solution is a minimal, well-defined change that doesn't require deep architectural understanding or extensive research.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. It's an excellent benchmark candidate because: (1) The problem is clearly defined with specific reproduction steps, (2) The solution is focused and testable, (3) It represents a real-world edge case that requires understanding of Python's type system and object model, (4) The test case is straightforward to write and verify. This would effectively test an engineer's ability to understand the codebase, diagnose the root cause, and implement a targeted fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13135": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem (KBinsDiscretizer with kmeans strategy fails due to unsorted bin_edges), provides exact reproduction steps with specific data values [0, 0.5, 2, 3, 9, 10] and configuration (5 bins, kmeans strategy, ordinal encoding), explains the root cause (centers from kmeans are unsorted, causing np.digitize to fail), and states the expected behavior (should work without exceptions). The issue gives enough technical detail about the underlying cause to guide an engineer toward the correct solution approach.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue clearly identifies the root cause (unsorted centers from kmeans), the location in the code where this occurs (in the KBinsDiscretizer fit method), and the specific algorithm component that fails (np.digitize with unsorted bin_edges). The solution is straightforward - sort the centers before computing bin_edges. Looking at the patch, it's literally a 1-line fix adding centers.sort(). An experienced engineer familiar with the codebase would quickly locate the kmeans strategy code in the fit method and implement this simple fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, has a focused scope, tests a specific algorithmic edge case, and has a clean solution that can be verified through the provided test case. The failure mode is deterministic and reproducible with the given dataset.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13142": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when n_init>1 in GaussianMixture, fit_predict(X) and predict(X) return different results, which should not happen. The issue provides specific steps to reproduce the problem (generate 1000 samples with 5 features from standard normal distribution, create GaussianMixture with 5 components, compare results with and without n_init=5). The expected behavior is clearly stated - the two methods should return identical cluster labels. The actual results are quantified (88.6% of labels don't match). The issue even mentions that the existing unit test doesn't catch this because it doesn't set n_init, providing insight into why this bug exists. An experienced engineer would have a clear understanding of what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that involves moving a few lines of code within the fit_predict method in sklearn/mixture/base.py. The solution moves the final e-step (which ensures consistency between fit_predict and predict) to occur after setting the best parameters rather than before. This is not a complex algorithmic change but rather a timing/ordering issue. The fix involves moving 5 lines of code and adding appropriate comments. An experienced engineer familiar with the codebase could understand the issue, trace through the logic, and implement this fix within 15 minutes to 1 hour. The corresponding test additions are also straightforward - just adding a test that verifies fit_predict equals predict when n_init>1.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-defined, has a clear solution, and the fix is testable. This would be an appropriate sample for evaluating coding ability as it requires understanding the codebase logic and identifying the correct ordering of operations.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13328": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear problem statement - HuberRegressor crashes with TypeError when using boolean arrays as predictors, (2) Detailed reproduction steps that walk through creating synthetic data, converting to boolean features, and demonstrating the failure, (3) Clear expected behavior - the regressor should handle boolean arrays like other estimators (e.g., LinearRegression) by implicitly converting them to floats, (4) Specific actual results - the failure occurs due to attempting unary minus operator on boolean arrays in the loss-and-gradient routine, with NumPy suggesting bitwise NOT or logical_not instead, (5) Complete version information for reproducibility. The issue clearly identifies both the symptom (TypeError) and the root cause (unary minus on boolean arrays), making it straightforward for an engineer to understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is quite simple: adding `dtype=[np.float64, np.float32]` parameter to the `check_X_y` call in the `fit` method. This forces type conversion of the input data to float types, preventing the boolean array from reaching the loss function where the unary minus operation fails. The fix requires understanding sklearn's validation utilities and where type conversion should occur, but once you locate the `check_X_y` call in the `fit` method, the solution is straightforward. An experienced engineer familiar with sklearn patterns would likely identify this approach quickly, as similar fixes exist throughout the codebase for handling different input types.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability as it tests: understanding of type handling in scientific computing libraries, familiarity with sklearn's validation utilities, and ability to trace error messages back to root causes. The test case is also simple and appropriate - it verifies that boolean arrays don't crash the regressor. This is a realistic issue that could occur in practice when users work with categorical or binary features.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13779": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides a clear problem description with a concrete reproduction scenario. The issue explains that a voting estimator fails when sample weights are passed and one of the estimators is set to None. It provides specific steps to reproduce: train a voting classifier with logistic regression and random forest on iris dataset with sample weights, then set the logistic regression estimator to None and try fitting again. The expected behavior is clearly stated - the voting classifier should detect the missing estimator and handle its absence gracefully. The error type is also specified (AttributeError when trying to call fit on NoneType). The problem is in the sample_weight support logic in the voting estimator's fit method, which doesn't check if an estimator is None before trying to validate sample weight support.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is straightforward - adding a simple None check in the fit method before validating sample weight support. The fix involves adding just 2 lines of code (if step is None: continue) in the sklearn/ensemble/voting.py file. The problem is well-defined, the location is clear from the error description, and the solution is a simple conditional check. An experienced engineer familiar with the codebase would quickly identify that the issue is in the sample weight validation loop and add the necessary None check.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is clearly described, the expected behavior is well-defined, and the solution is straightforward. The test case also provides good coverage by testing both VotingClassifier and VotingRegressor scenarios, ensuring the fix works for both cases. This is an appropriate sample for evaluating coding ability as it tests understanding of error handling and defensive programming practices.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14053": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is well-specified and provides clear information about the problem. It describes exactly what happens: `export_text` function throws an `IndexError` when working with a decision tree trained on a single feature. The steps to reproduce are clearly outlined: load iris dataset, extract only sepal length (single feature), create and fit a DecisionTreeClassifier, then call export_text with feature name \"sepal_length\". The expected behavior (plain-text representation) vs actual behavior (IndexError) is clearly described. The error indicates an out-of-bounds list access, which gives a strong hint about the nature of the bug.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the issue is in the export_text function in sklearn/tree/export.py where feature_names are being accessed by index. The problem occurs because when there's only one feature, some tree nodes may have TREE_UNDEFINED as their feature index, causing an IndexError when trying to access feature_names[i]. The fix is simple - add a conditional check to handle TREE_UNDEFINED indices. This requires understanding the tree structure and the export_text function logic, but once identified, it's a straightforward 2-line fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, reproducible, and the solution is clean and targeted. The test case in the patch validates that the fix works correctly for single-feature trees with feature names provided.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14087": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the problem: an IndexError occurs when using LogisticRegressionCV with refit=False. The steps to reproduce are detailed enough (1000 samples, 3 features, normal distribution, binary targets, 5 folds, 'saga'/'liblinear' solver, tolerance 1e-2, refit=False). The expected vs actual results are clear - it should complete without errors but instead throws an IndexError during coefficient path averaging. However, the issue description doesn't specify which exact solvers trigger the problem, what penalty types are affected, or provide the exact error traceback. An experienced engineer would need to reproduce the issue and debug to understand the root cause, but the core problem and general solution direction are clear enough to work with.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the gold patch, the solution involves 3 small, focused changes: (1) Fix a variable reference bug (self.multi_class -> multi_class), (2) Add a conditional check around l1_ratio computation for elasticnet penalty, and (3) Set l1_ratio to None for non-elasticnet penalties. These are relatively straightforward fixes once you reproduce the error and trace through the code. An experienced engineer familiar with sklearn's logistic regression implementation could identify and fix these issues within an hour. The changes are localized to one function and don't require major architectural changes or extensive research.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is reproducible, the solution is testable, and the changes are focused and appropriate for the scope described in the issue. The test patch shows good coverage of the fix across different penalty types and multi-class settings.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14629": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem: `cross_val_predict(method='predict_proba')` fails with `MultiOutputClassifier` due to an AttributeError because `MultiOutputClassifier` doesn't have a `classes_` attribute. The issue description pinpoints the exact location in the code where the problem occurs (_validation.py line 857-866) and explains that for `MultiOutputClassifier`, the classes should be accessed via `mo_clf.estimators_[i].classes_` instead. The steps to reproduce are clearly outlined: create a synthetic multilabel dataset, use `MultiOutputClassifier` with `LinearDiscriminantAnalysis`, and call `cross_val_predict` with `method='predict_proba'`. The expected vs actual results are clearly stated, and the root cause is identified. An experienced engineer would have enough information to understand both the problem and the general approach to fix it.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem and its location. Looking at the gold patch, the solution involves adding a `fit` method to `MultiOutputClassifier` that sets `self.classes_` as a list of classes from individual estimators. This requires understanding the existing codebase structure and the relationship between `MultiOutputClassifier` and its base estimators, but the actual code change is relatively small and straightforward. The main work involves: (1) understanding how `MultiOutputEstimator` works, (2) adding the `fit` method override, and (3) ensuring the `classes_` attribute is properly set. The fix is about 20 lines of code plus documentation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear problem statement, identifiable root cause, and straightforward solution. The issue provides good context including version information and reproducing steps. The test patch also shows that the solution can be properly validated through unit tests.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14710": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: during early stopping in HistGradientBoostingClassifier, y_true is provided as integer codes while y_pred remains as original string classes, causing a type mismatch error. The issue provides concrete reproduction steps (create 100 samples with 10 features, 50 'x' and 50 'y' labels, use HistGradientBoostingClassifier with early stopping), expected vs actual results, and even includes a detailed potential solution with specific code changes to the _check_early_stopping_scorer method in gradient_boosting.py. The core problem is that the scorer receives differently typed values (integers vs strings) and needs to convert the integer-encoded labels back to original classes using self.classes_[y.astype(int)] before scoring.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides a clear suggested solution that closely matches the actual patch. The fix involves adding 4 lines of code to convert integer-encoded labels back to original classes before scoring, using `self.classes_[y.astype(int)]` when `is_classifier(self)` is true. An experienced engineer familiar with scikit-learn would understand the problem quickly - it's a straightforward type conversion issue in the early stopping logic. The solution requires minimal code changes and the logic is clear: convert encoded labels back to original format before passing to scorer.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for the benchmark. The description is clear, the problem is specific and reproducible, and the solution is focused and testable. The test patch confirms the fix works with string targets and early stopping. An engineer could reasonably arrive at the same solution given the problem description.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14894": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear description of the problem: ZeroDivisionError occurs in _sparse_fit for SVM when support_vectors_ is empty. The issue includes detailed steps to reproduce the bug with specific parameter values (C \u2248 316.23, epsilon = 0.1, gamma = 1.0, linear kernel), clear expected vs actual results, and version information. The root cause is clearly identified - a division by zero error when there are no support vectors in sparse data handling. The expected behavior is also clearly stated: no error should be thrown and dual_coef_ should be set to sp.csr_matrix([]). An experienced engineer would have all the information needed to understand and fix this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is a straightforward edge case handling problem in the _sparse_fit method of sklearn's SVM implementation. Looking at the gold patch, the solution is simple: add a conditional check for when n_SV (number of support vectors) is 0, and handle that case by setting dual_coef_ to an empty sparse matrix instead of performing the division that causes the error. The fix involves adding just a few lines of code with an if-else block. An experienced engineer familiar with the codebase could identify the problematic division operation, understand why it fails when n_SV=0, and implement the straightforward fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is clearly described with reproducible steps, the solution is non-trivial enough to test coding skills (requires understanding of edge cases and sparse matrix handling), but not so complex as to be overwhelming. The test case provided in the patch is also comprehensive and directly tests the specific bug scenario. This type of debugging and edge case handling is very representative of real-world software engineering tasks.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25747": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clear: when using pandas transform output with FeatureUnion and a custom transformer that aggregates data (changing the number of rows), an error occurs because the framework tries to reattach the original index to the aggregated result with mismatched lengths. The bug description provides a clear scenario: hourly timestamps (96 records) being aggregated to daily sums (4 records), causing a length mismatch when pandas tries to preserve the original index. However, some details need to be inferred, such as the exact implementation of the custom transformer and the specific error message/stacktrace. The expected vs actual results are clearly stated, making the success criteria evident.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution patch, this is a relatively straightforward fix that takes 15 minutes to 1 hour. The issue is in the `_wrap_in_pandas_container` function in `sklearn/utils/_set_output.py`, where the code was incorrectly trying to override the index of DataFrames that already had their own index set by transformers. The fix simply involves removing 2 lines of code that force index assignment when the input is already a DataFrame. The logic is: if a transformer returns a DataFrame with its own index (like aggregated data), don't override it with the original input's index. This requires understanding the pandas output mechanism in scikit-learn, but the actual code change is minimal and the reasoning is straightforward once you understand the problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within scikit-learn's pandas output handling mechanism, and the solution is clean and targeted. The test case provided shows the fix works correctly by ensuring that when a transformer returns a DataFrame with its own index, that index is preserved rather than being overridden.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-25931": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly describes the bug: when fitting an IsolationForest with a pandas DataFrame and a non-default contamination parameter, an unexpected warning about invalid feature names appears. The issue provides: (1) Clear steps to reproduce with specific data values, (2) Expected vs actual results, (3) Root cause analysis explaining that the warning occurs because the estimator internally calls predict on training data when contamination != \"auto\", and (4) Complete version information. The description gives enough context for an engineer to understand both the problem and why it happens.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution, this is a 15min-1hour fix. The core issue is that score_samples() performs input validation which removes feature names, but when contamination != \"auto\", the fit() method calls score_samples() internally. The solution involves: (1) Creating a private _score_samples() method that skips input validation, (2) Having the public score_samples() method call _score_samples() after validation, and (3) Modifying fit() to call _score_samples() instead of score_samples(). This requires understanding the codebase structure and the validation flow, but the actual code changes are relatively small and localized to one file.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - the problem is clearly defined, has a specific reproducible case, and the solution requires understanding both the immediate symptoms and the underlying cause. The fix demonstrates good software engineering practices by creating a clean separation between public and private methods while preserving the existing API.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25973": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some blanks to fill in. The core problem is clear: SequentialFeatureSelector should accept an iterable of splits for the cv parameter (as documented), but currently fails when passed splits from a cross-validator like LeaveOneGroupOut. The steps to reproduce are detailed and specific, including the exact sequence of operations (creating synthetic data, group labels, using LeaveOneGroupOut, instantiating KNeighborsClassifier and SequentialFeatureSelector, then calling fit). The expected behavior (should work without errors) and actual behavior (IndexError about list index out of range) are clearly stated. However, some implementation details are left to be figured out, such as how exactly the cv parameter should be processed and where in the code the fix should be applied. An experienced engineer would need to investigate the SequentialFeatureSelector implementation to understand why it's not properly handling iterables of splits.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. Looking at the gold patch, the solution is quite straightforward: import check_cv and is_classifier, then call check_cv to properly validate and convert the cv parameter before using it. The fix involves adding just 2 imports, 1 line to process the cv parameter, and updating 2 function calls to pass the processed cv parameter. The core issue is that SequentialFeatureSelector was directly passing self.cv to cross_val_score without proper validation, while scikit-learn's check_cv function handles the conversion of iterables to proper cross-validation objects. An experienced engineer familiar with scikit-learn patterns would recognize this as a common issue and know to use check_cv for proper cv parameter handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues detected. This is a good benchmark sample because: (1) The problem is clearly reproducible with specific steps, (2) The issue involves understanding scikit-learn's cross-validation patterns and conventions, (3) The solution requires knowledge of existing utility functions (check_cv), (4) The fix is focused and doesn't require extensive codebase changes, and (5) The test case provided in the patch validates the fix appropriately. The issue demonstrates a real-world scenario where documentation promises certain functionality that isn't properly implemented.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-10323": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the problem: when using Sphinx's literalinclude directive with the `prepend` option, the prepended text loses its leading whitespace, causing misalignment in the rendered documentation. The user provides a specific example showing the expected XML output with proper indentation. However, there are some missing details: (1) The exact file structure and content being included isn't fully specified - we know it's XML with a `<plugin>` element but don't see the original file, (2) The specific literalinclude directive syntax being used isn't shown, and (3) The relationship between the `dedent` option and the core problem could be clearer. Despite these gaps, an experienced developer familiar with Sphinx could reasonably interpret what needs to be fixed: the prepend/append filters should preserve indentation when combined with other processing filters.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that involves reordering the filter operations in the LiteralIncludeReader class. The solution moves the dedent_filter to execute before prepend_filter and append_filter, rather than after them. This ensures that dedenting happens on the original content before prepending/appending, so the added text retains its intended formatting. The change is minimal (just reordering 3 lines in a list) and the logic is clear once you understand the filter pipeline. An experienced engineer would need some time to understand the Sphinx codebase structure and how these filters work together, but the actual implementation is simple. The test shows exactly what the expected behavior should be.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is well-bounded within a specific Sphinx feature, the test case clearly demonstrates the expected behavior, and the fix is clean and focused. This would be a good benchmark problem for testing understanding of filter pipelines and text processing order.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-10673": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The problem is clearly described: users want to include generated index pages (genindex, modindex, search) in their toctree without getting warnings that these documents don't exist. The desired behavior is also clear - the provided toctree directive should work without errors. However, there are some implementation details left to the engineer to figure out, such as how Sphinx internally handles these generated documents and where exactly in the codebase the changes need to be made. The issue provides good context with Stack Overflow references showing this is a common user need.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the gold patch, the solution involves modifying multiple files (sphinx/directives/other.py, sphinx/environment/adapters/toctree.py, sphinx/environment/collectors/toctree.py) and requires understanding Sphinx's internal architecture for handling documents and generated content. The engineer needs to understand how Sphinx tracks documents, how the toctree directive processes entries, and how generated documents like indices are handled differently from regular documents. The solution involves accessing the 'std' domain's initial_data['labels'] to get generated document names and updating the logic in several places to recognize these as valid references. While not extremely complex, it requires substantial domain knowledge of Sphinx's internals and careful coordination across multiple components.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with clear requirements and the solution can be objectively tested by checking if the toctree directive works without warnings when including genindex, modindex, and search. The test patch confirms this by creating a test case that verifies the toctree entries are properly recognized.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7440": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: Sphinx is reporting duplicate glossary entries for \"mysql\" due to case sensitivity, causing build failures. The issue mentions that \"MySQL != mysql term right?\" suggesting the user understands these should be treated as different terms. However, the issue description lacks some specifics: it doesn't clearly state what the expected behavior should be (should terms be case-sensitive or case-insensitive?), and it doesn't provide the exact error message or point to the specific glossary entries causing the conflict. The link to the glossary file and Travis CI logs provides some context, but the core requirement - that case-sensitive terms should be treated as distinct - can be reasonably inferred from the context.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the patches, this is a relatively straightforward fix that involves removing automatic lowercasing in two places in sphinx/domains/std.py. The issue is in the StandardDomain class where terms are being automatically converted to lowercase, causing \"MySQL\" and \"mysql\" to be treated as duplicates. The fix requires: (1) removing the .lower() call in the make_glossary_term function, and (2) removing the lowercase=True parameter from the XRefRole for terms. An experienced engineer familiar with the codebase could identify this issue and implement the fix within 15 minutes to 1 hour, as it requires understanding the glossary processing logic and making targeted changes to preserve case sensitivity.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue that tests a specific functionality change in Sphinx's glossary handling. The test changes clearly verify that case-sensitive terms are preserved correctly, making it suitable for a coding benchmark. The issue requires understanding of how Sphinx processes glossary terms and knowledge of where case conversion happens in the codebase.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7462": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but requires some interpretation. The bug description clearly states that when a function is annotated to return an empty tuple using empty Tuple type syntax, Sphinx's Python domain code raises an IndexError when trying to pop from an empty list. The reproduction steps are clear: create a function with empty tuple return type annotation, configure Sphinx autodoc, and run the build process. However, there are some blanks to fill in - the issue doesn't explicitly show what the problematic annotation syntax looks like (e.g., `Tuple[()]`) or provide a minimal code example. An experienced engineer would need to infer from the context and the expected behavior (\"valid type annotations\") that the goal is to handle empty tuple annotations without crashing. The mention of a specific project link provides additional context but isn't essential for understanding the core problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves adding a simple conditional check in two files to handle empty tuples. In sphinx/domains/python.py, the fix adds an `if node.elts:` check before trying to pop from the result list, and provides alternative handling for empty tuples. In sphinx/pycode/ast.py, it adds a similar check to return \"()\" for empty tuples instead of joining an empty list. The logic is straightforward - check if the tuple has elements before processing them, and provide appropriate fallback behavior for empty tuples. An experienced engineer familiar with AST parsing would quickly identify that the issue occurs when processing tuple annotations, and the fix is a simple defensive programming pattern.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear error condition (IndexError: pop from empty list) and a specific reproduction scenario. The solution is straightforward and doesn't require extensive domain knowledge beyond understanding AST nodes and basic Python syntax. The test cases in the patch confirm the expected behavior clearly.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7985": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks to fill in, but there is a sensible interpretation of what is required. The core problem is clearly stated: \"linkcheck currently doesn't check local (internal) links, but this would be useful.\" The reproduction procedure provides helpful context by describing how to create a test case with both external and internal links, and explains that currently only external links are checked. However, some details are unclear: the \"Error logs / results\" section mentions that external link check fails and \"causing the linkcheck builder to report an error and terminate with a non-zero exit status\" but this seems to describe the external link behavior rather than clearly explaining what should happen with internal links. The \"Expected results\" section simply says \"Also a check for the local link\" which is quite brief. Despite these gaps, an experienced engineer can reasonably infer that the goal is to extend the linkcheck functionality to validate local/internal file references in addition to external URLs, checking if the referenced local files actually exist.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because it requires: 1) Understanding the existing linkcheck.py architecture and how it currently handles different URI types, 2) Modifying the check() function logic to distinguish between external URLs, unsupported schemes, and local file paths, 3) Implementing file existence checking using path operations, 4) Integrating with existing ignore patterns functionality, 5) Adding proper status reporting for local files (working/broken/ignored). The patch shows this involves meaningful logic changes to the URI processing flow, adding regex pattern matching, file system checks, and updating the conditional logic structure. While not extremely complex, it requires understanding multiple components (URI parsing, file system operations, ignore patterns) and careful integration with existing code paths.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is technically sound and the solution approach is clear from the context. The test files provide good validation coverage for the new functionality, checking both valid and invalid local file references. The issue fits well within the scope of extending existing linkcheck functionality.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-8269": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug description: when linkcheck_anchors is enabled, Sphinx reports \"Anchor not found\" even when the server returns HTTP error codes (like 404 or 500), when it should instead report the actual HTTP error. The reproduction steps are detailed and specific: create a Sphinx project, add a link to a non-existent file (https://google.com/test.txt#test), run linkcheck, and observe the incorrect error message. The expected behavior is clearly stated with exact before/after message formats. The environment details are provided. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution shown in the gold patch is extremely simple: adding a single line `response.raise_for_status()` in the linkcheck.py file. The issue is straightforward - when checking anchors, the code needs to validate the HTTP response status before trying to parse the anchor. An experienced engineer familiar with Python's requests library would quickly recognize that raise_for_status() is the standard way to handle HTTP errors. The main time would be spent understanding the linkcheck codebase flow and locating the right spot to add the check, but the actual fix is trivial.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear, simple solution. The issue description provides sufficient context about Sphinx's linkcheck functionality, and the fix doesn't require deep domain knowledge beyond basic HTTP concepts. The test patch shows comprehensive testing including setting up a mock HTTP server, which demonstrates the issue can be properly validated.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-8551": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but requires some domain knowledge to fully understand. It clearly describes the problem (false ambiguous class lookup warnings for :type: and :rtype: fields), provides a detailed reproduction scenario with specific steps, and explains the expected vs observed behavior. However, someone unfamiliar with Sphinx's cross-reference resolution system might need to do some investigation to understand the technical details of how module context should be handled. The issue provides enough information about what should happen (unqualified references should resolve to the current module first) and what's going wrong (warnings about ambiguous references), making it possible for an experienced developer to work toward a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: (1) It requires understanding Sphinx's internal cross-reference resolution system and how module context is handled, (2) The developer needs to trace through how :type: and :rtype: fields create cross-references vs explicit xref roles, (3) The solution involves modifying two files (sphinx/domains/python.py and sphinx/util/docfields.py) to ensure proper context passing, (4) While the actual code changes are small (adding a few lines), identifying where and how to pass the module/class context requires significant investigation into the codebase architecture, (5) Writing appropriate tests requires understanding the internal node structure and testing framework.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is technical but well-defined, the reproduction steps are clear, and the expected behavior is specified. The patches show this is a legitimate issue with a focused solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-9711": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes that the `needs_extensions` function in Sphinx performs string-based version comparison instead of semantic version comparison, leading to incorrect rejection of newer versions (e.g., treating '0.6' as greater than '0.10'). The issue provides a concrete reproduction case using sphinx-gallery versions 0.9 vs 0.10.0, explains the expected behavior (0.10.0 should be accepted when 0.6 is minimum), and identifies the root cause (string comparison treats '0.6' > '0.10'). The problem is located in the version comparison logic of the `needs_extensions` functionality, making it clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-understood (string vs semantic version comparison) and the solution is straightforward - replace string comparison with proper semantic version comparison using a library like `packaging.version`. Looking at the gold patch confirms this: it imports `Version` from `packaging.version` and modifies the comparison logic in `verify_needs_extensions` function to use proper version objects instead of string comparison, with fallback to string comparison for invalid versions. This requires understanding the existing code structure and implementing a relatively simple fix with proper error handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described, has a well-defined problem and solution, and tests both the engineer's ability to understand version comparison semantics and implement proper error handling. The reproduction steps are clear and the expected behavior is unambiguous.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13372": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is well-specified with some minor gaps to fill in. It clearly describes the problem: an UnboundLocalError occurs in evalf when Max(0, y) precedes x in multiplication with evaluation turned off. The issue provides specific context about the error being related to uninitialized precision variables (reprec and imprec) and suggests a concrete fix. However, an engineer would need to locate the exact file and function where this occurs, understand the evalf implementation details, and reproduce the issue to fully understand the problem. The gold patch shows this is in sympy/core/evalf.py around line 1301, adding else clauses that raise NotImplementedError for both real and imaginary parts when they're not numbers.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem (UnboundLocalError due to uninitialized variables), suggests the solution approach (add else clauses), and the gold patch shows it's a simple 4-line addition adding else: raise NotImplementedError blocks. An experienced engineer would need to: 1) Locate the evalf functionality in the codebase, 2) Find the specific elif clauses mentioned, 3) Reproduce the issue to understand the flow, 4) Add the missing else clauses. The actual code change is minimal and straightforward once the location is found.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is straightforward, and the test case validates the fix properly. This is a good benchmark sample as it tests understanding of error handling in numerical evaluation code and requires some code exploration skills to locate the right function.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-13480": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) Clear reproduction steps - import SymPy, declare x as symbolic variable, form coth(log(tan(x))), substitute with integer values like 2; (2) Expected behavior - should get a numerical value after substitution; (3) Actual behavior - NameError due to undefined \"cotm\" variable; (4) Specific examples of failing inputs (2, 3, 5, 6, 8, 9, 11, 12, 13, 15, 18). The error description points directly to the root cause (undefined \"cotm\" name), making this a clear bug report with sufficient information for an experienced engineer to identify and fix the issue.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial typo fix taking less than 15 minutes. The gold patch shows it's a simple one-character change: \"cotm\" should be \"cothm\" on line 590 of hyperbolic.py. The variable \"cothm\" is defined on line 589 but referenced incorrectly as \"cotm\" on line 590. An experienced engineer would quickly reproduce the NameError, locate the undefined variable reference, and spot the obvious typo. The fix requires changing exactly one character in one line of code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal benchmark sample - the issue is clearly specified, easily reproducible, and tests a fundamental debugging skill (identifying and fixing typos in variable names). The solution is deterministic with no ambiguity about correctness.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13877": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with clear reproduction steps and expected behavior. The problem describes: (1) How to construct the matrix (n\u00d7n matrix with entry at row j, column i being i + a*j where a is symbolic), (2) Specific observed behavior for different values of n (determinants for n=1,2,3,4,5), (3) Clear expected behavior (should return meaningful symbolic expressions instead of NaN), and (4) The specific error about \"Invalid NaN comparison\" during factorization. However, there are some gaps: the issue doesn't specify which library/module is being used for matrix operations, and the final comment about \"Bareiss algorithm only valid for integer matrices\" suggests the reporter may not fully understand the technical details. An experienced engineer would need to infer that this is likely about SymPy's matrix determinant calculation based on the symbolic nature of the problem.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the solution patch, this is a 1-4 hour fix because: (1) It requires understanding the Bareiss algorithm implementation in the matrix determinant code, (2) The root cause is in the pivot selection logic where the default zero-checking function is too slow/problematic for symbolic expressions, (3) The solution involves introducing a new zero-checking function (_is_zero_after_expand_mul) that uses expand_mul for polynomial expressions, (4) The engineer needs to understand the trade-offs between accuracy and performance in pivot selection, and (5) While the actual code change is relatively small (~20 lines), it requires deep understanding of numerical linear algebra algorithms and symbolic computation challenges. The issue involves debugging a complex algorithmic problem in mathematical computation, which requires substantial domain knowledge.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is mathematically well-defined with clear test cases, and the solution approach is reasonable. The issue demonstrates a legitimate bug in symbolic matrix computation that would be valuable for evaluating debugging and mathematical programming skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-16792": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It provides a clear problem statement: autowrap with cython backend fails when array arguments don't appear in the wrapped expression. The issue includes: 1) A detailed reproduction case showing exactly how to trigger the bug, 2) The specific observed error (type error about size-1 arrays), 3) The root cause analysis showing the incorrect C function signature (double x instead of double *x), 4) A working counterexample that demonstrates when the issue doesn't occur, 5) Clear context about why this matters (interfacing with external libraries requiring predefined signatures). The problem is precisely defined and an experienced engineer would have all the information needed to implement a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: 1) The issue requires understanding the codegen module and how it handles array symbols and function signatures, 2) Looking at the patch, it involves modifying the routine method in codegen.py to properly handle array symbols that don't appear in expressions but are in the argument sequence, 3) The fix requires adding logic to detect IndexedBase/MatrixSymbol types and apply proper metadata with dimensions, 4) It's not a trivial one-line change but requires understanding the flow of how arguments are processed and ensuring array types get proper metadata even when unused in expressions, 5) The engineer would need time to understand the existing codebase structure, trace through the argument processing logic, and implement the conditional logic for handling unused array arguments.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with clear reproduction steps, the expected behavior is obvious, and the patch shows this is a legitimate bug with a reasonable solution. The test case also provides good validation that the fix works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17139": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is well-specified and clear about what needs to be fixed. It provides a specific reproduction case: simplify(cos(x)**I) where x is a symbolic variable and I is the imaginary unit. The problem is clearly defined - the simplification routine fails with a TypeError when trying to compare the complex exponent I to zero. The issue states that \"comparing it to zero is not allowed\" for complex values, which gives a clear direction for the solution. The expected behavior is also clear: the expression should be simplified without throwing an error. The reproduction steps are straightforward and the error condition is well-described.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a relatively simple fix that takes 15-60 minutes. The solution involves adding a simple check in the fu.py file to return early if the exponent is not real (if not rv.exp.is_real: return rv) before attempting the comparison rv.exp < 0. This prevents the TypeError from occurring when dealing with complex exponents. The fix is only 2 lines of code and requires understanding the immediate context of the function where the comparison happens. An experienced engineer would need to: 1) Reproduce the error, 2) Locate the problematic comparison in the codebase (fu.py), 3) Add the appropriate guard clause to handle complex exponents. The logic is straightforward once the problem location is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly reproducible, the problem is well-defined, and the solution requires understanding SymPy's symbolic computation concepts but doesn't require extensive domain expertise beyond basic complex number handling. The fix is surgical and focused, making it suitable for evaluating coding ability in debugging and fixing type-related issues in mathematical software.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17318": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear reproduction case with a specific mathematical expression: (3 - sqrt(2)*sqrt(4 + 3*I) + 3*I)/2. It clearly states the expected behavior (function should either simplify or return unchanged), the actual buggy behavior (IndexError during surd-splitting), and shows the desired output format. The problem is unambiguous - fix the IndexError so the function gracefully handles expressions it cannot denest.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The patch shows it's a targeted change in the _sqrt_match function in sqrtdenest.py, adding a check for positive rationals (sq.is_positive) alongside the existing rational check. The issue provides a clear reproduction case, and the fix involves understanding why the IndexError occurs (accessing tuple elements that don't exist) and adding appropriate validation. An experienced engineer familiar with the codebase could trace through the error, identify the problematic assumption in the code, and implement the fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the reproduction case is clear, and the expected solution behavior is unambiguous. The mathematical context is accessible to anyone working on a symbolic math library. The fix requires understanding the sqrtdenest algorithm but not deep mathematical research.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17630": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. The description clearly explains: (1) the problem occurs when multiplying a BlockMatrix containing ZeroMatrix blocks, (2) multiplying twice works correctly but three times fails, (3) the failure happens because zero blocks become plain numeric zero values instead of ZeroMatrix objects, (4) the error occurs when trying to access the 'cols' attribute on a bare zero. The issue includes a specific code snippet showing the type difference. However, there are some details missing: no minimal reproducible example is provided to demonstrate the exact construction of the BlockMatrix, and the specific error message/traceback isn't shown. The core problem is clear enough for an experienced engineer to understand and work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the actual fix, it's a small code change (adding 2 lines to handle MatAdd case in a postprocessor function), but understanding why this fix works requires significant investigation. An engineer would need to: (1) reproduce the issue by constructing the appropriate BlockMatrix, (2) trace through the block multiplication logic to understand why ZeroMatrix objects are being converted to plain Zero objects, (3) understand the relationship between MatAdd operations and how they're processed, (4) identify that the postprocessor needs special handling for MatAdd to preserve ZeroMatrix types. The fix itself is simple, but the debugging and understanding of SymPy's internal matrix expression system would take considerable time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-contained within SymPy's matrix expression system, has a clear reproducible case, and the solution is appropriately scoped. The test cases confirm the fix works for the described scenario.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17655": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: multiplying a SymPy geometry.Point by a number works when written as \"point * number\" but fails when written as \"number * point\" due to a missing __rmul__ method. The issue provides a concrete example with points at (0,0) and (1,1), explains the expected behavior (both operations should give the same result), and describes the specific error (GeometryError during point addition). The problem is mathematically clear - multiplication should be commutative for scalars and points. An experienced engineer can immediately understand that this requires implementing the __rmul__ magic method to handle right-multiplication, which is a standard Python pattern for making binary operations commutative.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward 15 min - 1 hour fix. The problem is a classic Python magic method implementation issue. An experienced engineer would quickly recognize that the missing __rmul__ method is needed to handle right-multiplication (2.0 * point). The solution is simple: implement __rmul__ to delegate to __mul__, which is a standard pattern. Looking at the gold patch confirms this - it's just 4 lines of code adding the __rmul__ method. The engineer would need some time to locate the Point class in the codebase, understand the existing __mul__ implementation, and write appropriate tests, but the core fix is trivial once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clear, the solution is well-scoped, and it tests knowledge of Python magic methods and operator overloading. The test changes are also straightforward and directly verify the fix. The problem represents a common real-world issue in mathematical libraries where commutativity needs to be explicitly implemented.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-18211": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: solveset raises NotImplementedError for the equation n\u00b7cos(n) \u2212 3\u00b7sin(n) = 0 instead of returning a ConditionSet. The issue provides a concrete example with the specific equation, shows exactly what the expected output should look like using a ConditionSet constructor, and explains that this should happen when the equation cannot be solved analytically. The behavior change is unambiguous - catch NotImplementedError and return a ConditionSet instead of propagating the exception.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward 15 min - 1 hour fix. The solution involves adding a simple try-catch block around an existing function call to handle NotImplementedError by returning a ConditionSet instead. The patch shows it's only about 8 lines of new code in a single method (_eval_as_set). An experienced engineer would need to: 1) locate the relevant method in sympy/core/relational.py, 2) understand that solve_univariate_inequality can raise NotImplementedError, 3) add the try-catch logic to return ConditionSet instead, and 4) import the needed ConditionSet class. The logic is simple and the change is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues with this sample. The issue is well-defined with clear expected behavior, the solution is straightforward, and the test cases verify the correct functionality. This would make a good benchmark problem as it tests the ability to understand error handling patterns in mathematical libraries and implement appropriate fallback behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-20428": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It describes a specific bug where clear_denoms() returns a polynomial that prints like zero but behaves incorrectly internally due to an unstripped leading zero in the DMP representation. The issue provides clear examples showing the incorrect behavior (DMP([EX(0)], EX, None)) versus the expected correct behavior (DMP([], EX, None)). However, the description is quite technical and assumes familiarity with SymPy's polynomial internals, DMP structures, and the clear_denoms() function. An experienced engineer would need to understand the domain-specific context but could reasonably infer that the solution involves properly stripping leading zeros from polynomial representations.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix for several reasons: (1) The issue involves understanding SymPy's complex polynomial representation system and the interaction between different domains (EX domain specifically), (2) The root cause is in the ExpressionDomain's __bool__ method using simple != comparison instead of the proper .is_zero check, which requires understanding how symbolic expressions should be evaluated for zero-ness, (3) The engineer needs to trace through the clear_denoms() flow to understand when and why unstripped zeros remain, and (4) While the actual fix is a one-line change (f.ex != 0 to not f.ex.is_zero), identifying this specific location requires debugging through the polynomial manipulation code. The technical complexity of SymPy's domain system and the need to understand symbolic expression evaluation makes this more than a trivial fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability. It tests understanding of symbolic computation systems, debugging skills for tracing through complex interactions, and knowledge of proper zero-checking methods for symbolic expressions. The provided test case gives a clear success criterion, and the fix, while requiring domain expertise, is well-defined once the root cause is identified.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20438": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides specific reproducible steps and clear expected vs. actual behavior. It describes three related problems: (1) is_subset returns None instead of boolean for ProductSet vs FiniteSet comparisons, (2) simplify() crashes with AttributeError on Complement objects lacking equals method, and (3) unexpected representation when rewriting ProductSet as FiniteSet. The description gives concrete examples with specific sets containing numbers 1 and 2, and their Cartesian product. However, some technical details about the internal implementation and expected behavior of certain edge cases could be clearer, requiring the engineer to make reasonable assumptions about the intended behavior.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires understanding multiple interconnected parts of the SymPy sets system: ProductSet, FiniteSet, subset operations, equality comparisons, and simplification logic. The solution touches 3 different files (relational.py, comparison.py, issubset.py) and requires implementing type checking logic, fixing dispatch handlers, and adding new subset comparison logic. An engineer would need 1-4 hours to: understand the sets hierarchy, trace through the failing operations, identify why is_subset returns None, understand the AttributeError in simplify(), and implement proper subset checking for ProductSet vs FiniteSet. The changes are not trivial but also not extremely complex - mainly adding type guards and implementing missing dispatch methods.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-contained within the SymPy sets module and has clear test cases that verify the fix. The problem is reproducible and the expected behavior is mathematically well-defined. The solution demonstrates good software engineering practices with appropriate type checking and dispatch method implementation.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20590": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It clearly describes a behavioral change between versions 1.6.2 and 1.7 where Symbol instances went from having no __dict__ attribute (raising AttributeError) to having an empty __dict__. The user correctly identifies this as likely related to __slots__ and suspects a parent class accidentally stopped defining __slots__. However, there are some gaps: (1) It doesn't specify which exact Symbol class is being referenced, though context suggests sympy.Symbol, (2) The desired behavior isn't explicitly stated - should symbols have __dict__ or not?, (3) No concrete reproduction steps are provided. Despite these gaps, an experienced engineer familiar with Python's __slots__ mechanism and the codebase could reasonably infer that the goal is to restore the 1.6.2 behavior where Symbol instances don't have __dict__ by ensuring proper __slots__ usage in the inheritance hierarchy.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves adding `__slots__ = ()` to the Printable class in _print_helpers.py, which is a small, targeted change. The problem is well-understood in Python - when a class in an inheritance hierarchy doesn't define __slots__, instances get a __dict__. The fix requires: (1) Understanding the __slots__ mechanism, (2) Identifying which class in the hierarchy is missing __slots__ (likely through code inspection or testing), (3) Adding the single line `__slots__ = ()`. The provided patch confirms this - it's literally adding 6 lines (mostly comments) with the key line being `__slots__ = ()`. This is conceptually straightforward for someone familiar with Python's object model.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is technically sound, the solution is clean and minimal, and the test adequately verifies the fix by checking that Symbol instances don't have __dict__ and can't have attributes assigned. This is a good example of a focused bug fix that tests core Python language feature understanding.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-21379": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear reproduction case with specific steps, expected vs. actual behavior, and detailed observations about when the error occurs. The user provides a minimal working example showing that subs({1: 1.0}) fails with PolynomialError when x and y are declared as real symbols, but works when they have no assumptions. They also provide helpful debugging information about what conditions trigger the error (specific functions like sinh/cosh/tanh, presence of division by z, etc.). While there are some minor gaps (like the exact expression structure could be clearer), an experienced engineer would have enough information to reproduce the issue and understand what needs to be fixed - namely, preventing PolynomialError from being raised during substitution operations on expressions with piecewise functions and real symbol assumptions.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) It involves understanding the interaction between multiple complex subsystems (substitution, polynomial operations, piecewise functions, and symbol assumptions), (2) The developer needs to trace through the SymPy codebase to understand why gcd() calls are failing with PolynomialError in specific contexts, (3) The solution requires identifying the right place to catch the exception and handle it gracefully, which involves understanding the mod.py evaluation logic, (4) While the actual code change is small (adding a try-except block), finding the right location and understanding the implications requires substantial investigation. The patch shows this is in sympy/core/mod.py around the gcd computation, but discovering this location and understanding why this specific exception handling is appropriate takes significant debugging effort.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained bug report with a clear reproduction case and specific error condition. The issue is focused on a concrete technical problem (unexpected PolynomialError during substitution) rather than a feature request or design question. The test case provided in the patch confirms the issue can be reliably reproduced and verified. This makes it suitable for a coding benchmark as success can be objectively measured by whether the provided test passes after the fix is implemented.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-22714": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear reproduction case showing that `sp.S('Point2D(Integer(1),Integer(2))')` works normally but fails when called within a `with sp.evaluate(False)` context, raising \"Imaginary coordinates are not permitted\" error despite using real integers. The issue includes specific code examples that work and the exact error condition. The expected behavior is clear: Point2D should be created successfully with real integer coordinates regardless of the evaluation context. The problem is localized to the interaction between evaluate(False) and Point2D coordinate validation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The issue is in the Point2D coordinate validation logic in sympy/geometry/point.py. Looking at the gold patch, the fix is a one-line change from `if any(a.is_number and im(a) for a in coords):` to `if any(a.is_number and im(a).is_zero is False for a in coords):`. The problem occurs because with evaluate(False), `im(a)` returns an unevaluated expression rather than 0, causing the validation to incorrectly flag real numbers as imaginary. An experienced engineer would need to: 1) reproduce the issue, 2) trace through the Point2D creation code to find the validation logic, 3) understand how evaluate(False) affects symbolic expressions, and 4) modify the check to properly handle unevaluated expressions. This requires some familiarity with SymPy's evaluation system but is a straightforward logical fix once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is focused and minimal, and the test case clearly validates the fix. This is a good benchmark sample that tests understanding of symbolic computation and evaluation contexts in SymPy.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23824": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is exceptionally well-specified. It clearly identifies the specific function `kahane_simplify()` in the physics.hep module, provides concrete examples demonstrating the bug (\u03b3^\u03c1 \u03b3^\u03c3 \u03b3^\u03bc \u03b3_\u03bc should simplify to 4 \u03b3^\u03c1 \u03b3^\u03c3 but instead produces 4 \u03b3^\u03c3 \u03b3^\u03c1), explains the expected vs actual behavior, and even identifies the root cause of the bug (the insertion loop being backward when reinserting leading matrices). The author provides mathematical notation and specific test cases that make the problem unambiguous. An experienced engineer would have all the information needed to understand what needs to be fixed and how to verify the solution works correctly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue description explicitly identifies the root cause (backward insertion loop) and the location of the problem (kahane_simplify function). Looking at the actual fix, it's a simple one-line change that replaces a backwards loop with a list comprehension that maintains the correct order. The bug is in a specific algorithmic step rather than requiring deep understanding of gamma matrix mathematics. An engineer would need to locate the function, understand the insertion logic, and reverse the order - a relatively simple task once the problem is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described, the fix is non-trivial enough to test coding ability while being achievable, and the test cases provided demonstrate both the broken and expected behavior. The mathematical nature adds domain complexity but the actual programming fix is straightforward. The sample effectively tests an engineer's ability to understand algorithmic ordering issues and implement clean solutions.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23950": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that Contains.as_set() incorrectly returns the Contains object itself instead of the actual set object. The issue provides a concrete example with Contains(x, Reals) and explains the downstream consequences - that this causes failures because Contains objects don't have an as_relational method since they aren't sets. The description also provides context about how this manifests in piecewise expressions. The expected behavior is clear: as_set() should return the actual set object (the second argument of Contains), not the Contains object itself. The gold patch confirms this interpretation by showing that as_set() should return self.args[1] (the set argument) rather than raising NotImplementedError.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that requires minimal time. The problem is clearly identified - the as_set() method in the Contains class currently raises NotImplementedError but should return the set object (second argument). Looking at the gold patch, the solution is a simple one-line change from \"raise NotImplementedError()\" to \"return self.args[1]\". An experienced engineer would need to: 1) Locate the Contains class in sympy/sets/contains.py, 2) Find the as_set() method, 3) Replace the NotImplementedError with returning the second argument. The logic is straightforward since Contains(x, S) represents \"x in S\", so as_set() should return S. This requires understanding the Contains class structure but no complex algorithmic thinking.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement and straightforward solution. The test patch also provides good validation that the fix works correctly for different types of sets (FiniteSet, Integers, Reals). The issue would make for a good benchmark sample since it tests understanding of object-oriented design and method implementation.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-24066": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with clear reproduction steps and error description. The user describes a specific problem: SI._collect_factor_and_dimension() incorrectly identifies an exponent as having non-dimensionless units when it should be dimensionless. The reproduction steps are detailed: (1) create a ratio of second/(ohm*farad), (2) add a number to the exponential of that ratio, (3) observe that the routine throws an exception claiming the exponent has dimensions when it should be dimensionless. However, there are some gaps - the exact error message isn't provided, and the specific function names/module locations aren't mentioned. An experienced developer familiar with the SymPy physics units module could reasonably interpret this and work toward a solution, though some exploration of the codebase would be needed to locate the exact problematic code.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The problem requires understanding the SymPy physics units system, specifically how dimension analysis works for mathematical functions like exponentials. Looking at the patch, the solution involves modifying the _collect_factor_and_dimension method to properly handle dimensionless quantities in function arguments by checking if dimensions are dimensionless and converting them to Dimension(1). This requires: (1) understanding the existing dimension system logic, (2) recognizing that mathematical functions like exp() require dimensionless arguments, (3) implementing the check using is_dimensionless(), and (4) writing appropriate tests. The actual code change is relatively small but requires domain knowledge of physics units and mathematical function requirements.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is domain-specific to physics units in SymPy, but this is appropriate for a coding benchmark as it tests understanding of mathematical concepts, debugging skills, and ability to work with existing APIs. The reproduction steps are clear enough that a developer could recreate the issue and verify their solution works.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-24443": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear reproduction case: create a dihedral group of order six and attempt to define an identity homomorphism by mapping each generator to itself. The expected behavior (should succeed and yield identity homomorphism) vs actual behavior (ValueError about images not defining a homomorphism) is clearly stated. The issue also points to the specific problematic code location in the `_image()` function within homomorphisms.py and identifies the root cause - when `r[i]` is an inverted generator, the `in gens` test fails. While there are some implementation details to figure out, the core problem and expected solution are well-defined enough for an experienced engineer to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the patch, this requires substantial understanding of the homomorphism checking logic and group theory concepts. The solution involves a complete rewrite of the `_check_homomorphism` function, simplifying the complex iteration logic in `_image()` to use a cleaner approach with symbol-to-generator mapping. The engineer needs to: 1) Understand the mathematical concepts of homomorphisms and group presentations, 2) Debug why the existing logic fails with inverted generators, 3) Design a cleaner implementation that avoids the problematic `in gens` checks, and 4) Ensure the new logic correctly handles all cases. This is more than a simple bug fix - it's a thoughtful redesign of a core algorithm that requires both domain knowledge and careful implementation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within a specific function, has a clear test case for validation, and the solution demonstrates good software engineering practices by simplifying complex logic. The mathematical domain knowledge required (group theory, homomorphisms) is appropriate for a coding benchmark as it tests both algorithmic thinking and ability to work with mathematical abstractions.",
            "q2_5_confidence": 4
        }
    }
]