[
    {
        "astropy__astropy-13033": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when a required column is removed from a TimeSeries object, the error message is misleading and confusing. The issue provides: (1) A clear description of expected vs actual behavior, (2) A complete reproducible example showing exactly how to trigger the bug, (3) Specific details about what's wrong with the current error message (\"it suggests a problem with the time column rather than indicating that flux is missing\"), and (4) What the improved behavior should be (\"An exception that informs the users required columns are missing\"). The code example demonstrates the exact scenario where removing the required 'flux' column produces a confusing error about the 'time' column. An experienced engineer can clearly understand that they need to improve the error message in the _check_required_columns method to be more informative about missing required columns.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is located in the error message formatting within the _check_required_columns method. Looking at the provided patch, the solution involves: (1) Adding a helper function to format column names properly as strings or lists, (2) Modifying the error message format to show the full list of expected vs actual columns rather than just the first column. The core logic doesn't change - only the error message presentation is improved. This requires understanding the existing validation logic and making a small localized change to improve the user experience. An experienced engineer familiar with the codebase could identify the issue location, understand the problem, and implement the fix within an hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix that improves error message clarity. The issue has clear reproduction steps, the expected vs actual behavior is well-defined, and the solution scope is focused on improving user experience through better error messaging. The provided test case also validates that the fix works correctly for multiple required columns scenarios.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-13977": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The author clearly explains the problem: when using a duck type of astropy.units.Quantity, operations like `(1 * u.m) + DuckArray(1 * u.mm)` fail because `Quantity.__array_ufunc__()` raises a ValueError instead of returning NotImplemented. They provide a complete minimal working example of their duck type implementation and demonstrate the specific failure case. The desired solution is explicitly stated: `Quantity.__array_ufunc__()` should return NotImplemented when inputs are incompatible, allowing reflected operators to be called. However, some implementation details need to be inferred, such as exactly when to return NotImplemented vs raise exceptions, and how to distinguish between truly incompatible inputs and other error conditions. The issue references the numpy documentation about returning NotImplemented for unimplemented operations, providing good context for the expected behavior.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue falls into the 1-4 hour category because it requires: 1) Understanding the numpy array function protocol and when NotImplemented should be returned vs exceptions raised, 2) Analyzing the existing `__array_ufunc__` implementation in astropy's Quantity class to understand the current error handling logic, 3) Implementing proper exception handling that distinguishes between cases where NotImplemented should be returned (incompatible duck types) vs cases where exceptions should still be raised (actual errors), 4) The solution involves wrapping the existing logic in a try-catch block and adding logic to detect when other types have their own `__array_ufunc__` implementations. While not extremely complex, it requires careful thought about the different error conditions and when to defer to other types vs raise exceptions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-structured issue for benchmarking coding ability. It provides clear context, a minimal reproduction case, explains the expected behavior according to numpy conventions, and the solution involves understanding protocol design patterns that are common in scientific Python libraries. The tests provided in the patch are comprehensive and would effectively verify whether a solution correctly implements the NotImplemented behavior while maintaining existing functionality.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14096": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproducible example showing the exact problem: when a subclassed SkyCoord has a property that tries to access a non-existent attribute, the error message incorrectly identifies the property name instead of the actual missing attribute name. The expected behavior is clearly implied - the error should mention 'random_attr' (the actual missing attribute) rather than 'prop' (the property trying to access it). The example code is complete and demonstrates the issue perfectly, making it straightforward for a developer to understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it's a very small change in the `__getattr__` method of the SkyCoord class. The fix involves replacing the manual AttributeError raising with a call to `__getattribute__`, which provides the correct exception behavior. The core issue is in the exception handling logic, and once identified, the fix is straightforward. An experienced developer would need some time to locate the `__getattr__` method in the codebase and understand why it's raising the wrong attribute name, but the actual code change is minimal - just replacing 2-3 lines with 1 line.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is well-defined, has a clear expected outcome, and the solution requires understanding of Python's attribute access mechanism (__getattr__ vs __getattribute__). The test case provided is comprehensive and properly validates that the fix works as expected. The problem demonstrates understanding of Python internals while being focused enough to solve in a reasonable timeframe.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14182": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, concrete example showing exactly what functionality is desired - adding support for the `header_rows` parameter to the RST writer, just like the existing `ascii.fixed_width` format. The user demonstrates the current working behavior with fixed_width format and shows the exact error when trying the same with RST format. The expected behavior is clearly implied: the RST format should accept and properly handle the `header_rows` parameter to output additional header information like units and names in separate rows.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because it requires: (1) Understanding the existing FixedWidth base class implementation and how header_rows works there, (2) Modifying the RST class constructor to accept and pass through the header_rows parameter, (3) Updating the write() method to properly handle the variable number of header rows when adding the border lines, (4) Adding read() method to correctly parse tables with header rows, and (5) Writing comprehensive tests. The solution touches multiple methods and requires understanding the inheritance hierarchy and RST format specifics.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is achievable, and the test case provides good validation. This is a suitable benchmark task for evaluating coding ability as it requires understanding existing code patterns, inheritance, and format-specific implementation details.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14309": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the FITS identifier in astropy's `identify_format` function is attempting to access `args[0]` even when the `args` tuple is empty, causing an IndexError. The issue provides: (1) Clear context about when this started failing (after a specific commit), (2) A precise explanation of the root cause from maintainer discussion, (3) An exact reproduction case with code that demonstrates the error, (4) Reference to the specific commit that introduced the regression. The example code `identify_format(\"write\", Table, \"bububu.ecsv\", None, [], {})` clearly shows the problematic call that triggers the IndexError. An experienced engineer can understand exactly what needs to be fixed: the `is_fits` function needs to handle the case where `args` is empty before trying to access `args[0]`.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because: (1) The problem is clearly identified and localized to the `is_fits` function in the FITS format checker, (2) The root cause is straightforward - accessing `args[0]` without checking if `args` is non-empty, (3) The solution requires minimal code change - just adding a guard condition or restructuring the logic to handle empty args, (4) Looking at the gold patch, it's a simple 3-line change that removes the problematic fallthrough case, (5) The fix doesn't require deep understanding of complex algorithms or extensive codebase knowledge, just basic defensive programming. An experienced engineer familiar with the codebase could identify the issue location, understand the problem, and implement the fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. This is a good benchmark sample because: (1) The issue is clearly reproducible with provided code, (2) The problem statement is unambiguous, (3) The fix requires understanding the code logic but is not overly complex, (4) It tests defensive programming skills and understanding of function parameter handling, (5) The solution can be verified with the provided test case. This represents a realistic debugging scenario that software engineers commonly encounter.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14365": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the ascii.qdp parser in astropy only accepts uppercase QDP commands, but QDP files are case-insensitive and often written with lowercase commands. The issue provides a concrete example of the problem (a QDP file with \"read serr 1 2\" instead of \"READ SERR 1 2\"), exact reproduction steps including file creation and Python code to trigger the error, the expected behavior (the file should read successfully into a Table with errors), and even mentions the specific ValueError that occurs. The problem statement is unambiguous - make the QDP parser case-insensitive to match QDP specification behavior.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves two simple changes: (1) adding re.IGNORECASE flag to the regex compilation in _line_type_re, and (2) changing the \"NO\" string comparison to use v.upper() == \"NO\" for case-insensitive matching. These are minimal, focused changes that don't require substantial code rewriting. An experienced engineer familiar with the codebase would need to locate the QDP parsing logic in astropy/io/ascii/qdp.py, understand how the regex matching works for QDP commands, and apply case-insensitive matching. The changes are straightforward once the relevant code is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is focused and testable, and the test patch shows appropriate test coverage including parameterized tests for both uppercase and lowercase scenarios. The issue would make a good benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14508": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: `io.fits.Card` uses unnecessarily long string representations of floats (e.g., \"0.009124999999999999\" instead of \"0.009125\"), which forces comments to be truncated. The issue provides a concrete example showing how a valid FITS card can be read successfully but cannot be recreated with the same string representation due to this float formatting issue. The reporter even identifies the root cause in the `_format_float()` function at line 1300-1302 and suggests a potential solution approach (using `str(value)` first before attempting custom formatting). The expected behavior is clear: being able to create any valid FITS Card via `io.fits.Card`.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue pinpoints the exact function (`_format_float()`) causing the problem and suggests the solution approach. The actual fix involves replacing the complex formatting logic with a simpler approach that uses Python's default string representation first, only applying custom formatting when necessary. The gold patch shows this is essentially rewriting one function (~15 lines changed) with straightforward logic. An experienced engineer familiar with the codebase could understand the problem quickly since it's clearly explained with examples, and implement the solution without extensive research.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined with concrete examples, the solution approach is well-motivated, and the fix is localized to a single function. The test case also appropriately validates the fix by ensuring float values are formatted correctly without truncating comments. This is a good benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14995": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides clear context about a regression between v5.2 and v5.3 in astropy's NDDataRef mask propagation functionality. The problem is precisely described: when one operand doesn't have a mask and handle_mask=np.bitwise_or is used, a TypeError occurs because bitwise_or cannot operate on an integer and None. The issue includes complete reproduction code showing exactly when the error occurs (mask * no mask scenarios) and when it works correctly (mask * mask scenarios). The expected behavior is clearly stated: when one operand lacks a mask, the existing mask should be copied to the output, as it worked in v5.2. The version information and complete error context make this an exemplary well-specified bug report.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is quite simple: changing `elif operand is None:` to `elif operand.mask is None:` in the _arithmetic_mask method. The bug is a logical error where the code was checking if the operand object is None instead of checking if the operand's mask is None. An experienced engineer familiar with the codebase would need some time to: 1) understand the mask propagation logic in ndarithmetic.py, 2) trace through the _arithmetic_mask method to identify the incorrect condition, and 3) verify the fix handles all the cases shown in the reproduction code. The fix itself is a one-line change, but understanding the logic and ensuring correctness requires some thought and testing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. This is an excellent benchmark sample with a clear regression bug, comprehensive reproduction steps, and a precise fix. The issue tests both understanding of the codebase logic and ability to identify a subtle logical error in conditional statements.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7336": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) A clear summary explaining that the `units.quantity_input` decorator fails when constructors have type hints with `-> None` return annotation, (2) A complete, minimal reproducer showing exactly how to trigger the bug with a simple class constructor, (3) The exact error message: \"'NoneType' object has no attribute 'to'\", (4) A clear workaround (omitting the return type hint), and (5) A suggested fix direction (explicitly check for None returns). The issue gives an engineer everything needed to understand the problem: the decorator is trying to call `.to()` on a None return value from `__init__` methods, which should be handled as a special case since constructors always return None.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-defined and the solution is straightforward: modify the decorator logic to handle None return annotations. Looking at the actual patch, it's a one-line change in `astropy/units/decorators.py` - simply changing the condition from checking if return annotation is not empty to also excluding None. The fix is `if wrapped_signature.return_annotation not in (inspect.Signature.empty, None):` instead of just checking for empty. This requires understanding the decorator code and Python's `inspect` module, but the logic is simple once you locate the right place in the code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear reproducer and straightforward fix. The issue demonstrates good software engineering practices with a minimal example, clear error description, and suggested solution direction. The test additions show the fix works correctly for the `-> None` annotation case.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7606": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly demonstrates the problem with a concrete code example showing that `x == None` raises a TypeError when x is a Unit created with 'asdf' and parse_strict='silent'. The expected behavior is explicitly stated: \"Should be False\". The issue involves the equality comparison operator (__eq__) for UnrecognizedUnit objects when compared to None. Looking at the gold patch, the solution involves returning NotImplemented instead of False in exception handling, which allows Python's comparison protocol to work correctly. The problem and desired outcome are unambiguous.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is localized to the equality comparison methods in the Unit and UnrecognizedUnit classes. The solution requires understanding Python's comparison protocol and the proper use of NotImplemented. An experienced engineer would need to: 1) Locate the __eq__ methods in astropy/units/core.py, 2) Understand that returning False in exception handlers prevents proper comparison protocol fallback, 3) Change the return value from False to NotImplemented in the exception handlers. The actual code changes are minimal (just changing return values and adding a try-except block), but requires some thought about Python's comparison semantics.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-bounded, the solution is straightforward once you understand Python's comparison protocol, and the test case clearly verifies the fix. This is a good benchmark sample as it tests understanding of Python's special methods and proper exception handling in comparison operations.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7671": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly outlines the problem. It provides concrete examples demonstrating the failure case where minversion('numpy', '1.14dev') raises a TypeError due to LooseVersion trying to compare an integer with a string. The issue identifies the root cause as a bug in LooseVersion when handling version strings with development indicators like 'dev'. It shows specific examples of what fails and what works, including the comparison between LooseVersion and pkg_resources.parse_version behavior. The issue mentions PR #7647 as the triggering change and provides enough context about the expected behavior - that minversion should handle version strings with development indicators without raising TypeErrors.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly defined with specific examples of the failure case. The root cause is identified (LooseVersion bug with dev versions), and the solution approach is relatively straightforward - modify the minversion function to handle version strings with development indicators before passing them to LooseVersion. Looking at the gold patch, the solution involves adding a regex to extract just the numeric version components, which is a focused change to a single function. An experienced engineer would need some time to understand the version comparison logic, research the appropriate regex pattern (though PEP440 is mentioned as guidance), and implement the fix, but it's not a complex algorithmic problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution space is clear, and the test demonstrates the expected behavior. The gold patch shows a clean, focused solution that addresses exactly the issue described.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-10554": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with a clear reproduction case showing the problem. The user provides concrete code examples demonstrating that when a Django queryset with union() and order_by() operations is later modified with order_by().values_list(), it raises a ProgrammingError about \"ORDER BY position 4 is not in select list\". The core problem is clear: the generated SQL includes an ORDER BY clause referencing a column position that doesn't exist in the SELECT clause. However, there are some gaps - the exact models (Dimension) and the 'order' field aren't fully defined, and the user mentions \"the exact models are not relevant I think\" which suggests some ambiguity about the scope. An experienced engineer would likely be able to work with this description to reproduce and fix the issue, though they'd need to make some reasonable assumptions about the model structure.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a moderately complex issue that would likely take 1-4 hours to solve. The problem involves Django's ORM query compilation, specifically how union querysets handle ordering when combined with values_list operations. Looking at the solution patch, it requires understanding Django's SQL compiler internals (django/db/models/sql/compiler.py) and query handling (django/db/models/sql/query.py). The fix involves modifying the get_order_by method to handle cases where ORDER BY columns aren't in the SELECT clause by automatically adding them, plus adding a new add_select_col method to the Query class. This requires deep understanding of Django's ORM internals, SQL generation, and the interaction between different queryset operations. An experienced engineer would need time to trace through the code, understand why the current behavior fails, and implement a solution that doesn't break existing functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues that would prevent using this sample. The problem is reproducible with the provided code example, and the solution is testable through the accompanying test cases. While the issue description could be slightly more detailed about model definitions, an experienced Django developer would be able to create appropriate test models to reproduce the issue. The fix is well-contained and addresses a specific bug in Django's ORM behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11087": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but requires some interpretation. The main problem is clearly described: Django's .delete() method is fetching unnecessary fields during cascade deletion, causing UnicodeDecodeError when those fields contain invalid UTF-8 data. The issue specifically identifies that text_log_error.line field is being fetched unnecessarily since it's not needed for the deletion process (not a primary key or foreign key). The user provides detailed STR (Steps to Reproduce), expected vs actual behavior, SQL queries showing the problem, and identifies two root causes. However, the exact implementation approach is left to the engineer to determine - the issue suggests optimizing .delete() to use \"only required fields\" but doesn't specify exactly which fields should be considered \"required\" or how to implement the optimization.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour complexity issue. It requires: 1) Understanding Django's ORM deletion mechanism and cascade behavior, 2) Analyzing the deletion.py file to understand how related objects are collected and processed, 3) Identifying what constitutes \"required fields\" for deletion (primary keys, foreign keys, fields referenced by relationships), 4) Implementing logic to defer non-essential fields using .only(), 5) Handling edge cases like signal listeners and select_related queries. The solution involves modifying core ORM functionality with careful consideration of performance and edge cases. The actual patch shows significant logic changes in the collect() method, adding a new helper method, and requiring understanding of Django's signal system and relationship traversal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-motivated by a real performance and compatibility issue, the solution is testable, and the scope is appropriately bounded to Django's deletion optimization. The issue provides good context and the test shows the expected behavior clearly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11265": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified with clear details about the problem. It provides a concrete example showing the exact code change needed to reproduce the bug (changing `.filter(book_alice__isnull=False)` to `.exclude(book_alice__isnull=False)`), the specific error message (FieldError about not being able to resolve \"book_alice\"), and even points to the likely problematic function `split_exclude(self, filter_expr, can_reuse, names_with_path)`. The issue description includes the exact file location (django/tests/filtered_relation/tests.py) and explains that the problem occurs because a new query is created without the extra data from the original query. This gives a developer enough information to understand both the symptom and the likely cause.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution, this is a 15 minute to 1 hour fix. The core fix is adding a single line `query._filtered_relations = self._filtered_relations` in the split_exclude method to copy filtered relations to the new query. There's also a small logical change to prevent trimming INNER JOINs from filtered relations. While it requires understanding Django's ORM internals and the specific issue with filtered relations not being copied during exclude operations, the actual code changes are minimal and focused. An experienced engineer familiar with Django's codebase could identify and implement this fix relatively quickly once they understand the problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-described, the solution is reasonably straightforward, and the test case clearly demonstrates both the bug and the expected behavior. This would be a good benchmark sample for testing understanding of ORM internals and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11299": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly describes the problem: CheckConstraint with OR operator generates incorrect SQL on SQLite and Oracle by including fully qualified field names (e.g. \"my_table\".\"my_field\") which causes migration failures. The issue provides a concrete example with a TestConstraint model showing the problematic behavior, includes the actual generated SQL that fails, explains why it fails (the constraint refers to \"new__app_testconstraint\".\"field_1\" after table rename), and shows exactly what the correct SQL should look like. The technical details are precise, including the root cause explanation that AND clause items use Col while OR clause uses SimpleCol. An experienced engineer would have everything needed to understand and fix this issue.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's query building internals, specifically how Q objects with OR/AND combinations are processed and how Col vs SimpleCol are used. The engineer needs to trace through the _add_q method in django/db/models/sql/query.py to understand why the simple_col parameter isn't being propagated properly in recursive calls. While the actual fix is small (adding one parameter to a recursive call), identifying the exact location and understanding the query building flow requires substantial investigation of Django's ORM internals. The issue involves complex SQL generation logic and requires understanding table aliasing during migrations.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-documented bug with clear reproduction steps, expected vs actual behavior, and a focused solution. The test cases appropriately verify the fix works for OR constraints and that SimpleCol is used correctly in query building.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11532": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear context about the problem. It describes a specific scenario where Django crashes when processing email messages with non-ASCII domain names (like \"\u6b63\u5b97\") when the email encoding is set to a non-unicode encoding like iso-8859-1. The issue includes: (1) A clear description of when the problem occurs - when hostname contains unicode characters and email encoding is iso-8859-1, (2) Specific steps to recreate the issue - set hostname to non-ASCII value and run mail tests, (3) A clear fix suggestion - convert domain name to punycode before using, (4) A concrete test case showing expected behavior with xn--p8s937b (punycode representation), (5) Technical details about why it fails - UnicodeEncodeError when trying to encode Message-ID header. The issue references specific files (django/core/mail/message.py line 260) and explains the encoding flow that causes the crash. An experienced engineer would understand they need to implement punycode conversion for domain names in the email system.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Understanding the Unicode/encoding issue and how punycode works for internationalized domain names, (2) Creating a reusable punycode utility function in django.utils.encoding module, (3) Identifying all places in the codebase where domain encoding occurs (multiple files: mail/message.py, mail/utils.py, validators.py, html.py), (4) Replacing existing idna encoding patterns with the new punycode utility, (5) Writing appropriate tests. While the core concept is straightforward, the implementation touches multiple modules and requires careful consideration of where domain encoding happens throughout Django. The patch shows changes across 5 files, indicating this is more than a simple local fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution approach is clear, and the scope is reasonable for a coding evaluation. The issue provides good technical depth without being overly complex.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11551": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. The author clearly explains: (1) The specific problem - admin.E108 error is incorrectly raised for PositionField from django-positions library, (2) The root cause - a specific commit (47016adbf54b54143d4cf052eeb29fc72d27e6b1) that changed the logic in _check_list_display_item function, (3) Why the change broke existing functionality - the hasattr(model, item) check now prevents trying model._meta.get_field(item) when it should still be attempted, (4) A complete proposed solution with the exact code changes needed in the _check_list_display_item function in django/contrib/admin/checks.py, and (5) A detailed logical analysis of different test cases. The issue includes concrete example code that reproduces the problem and explains the technical details thoroughly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This issue would take 15 minutes to 1 hour to fix. The problem is well-diagnosed with the exact root cause identified, the specific function and file are mentioned (django/contrib/admin/checks.py, _check_list_display_item function), and a complete working solution is provided in the issue description. An experienced engineer would mainly need to: (1) Understand the existing logic flow, (2) Verify the proposed fix makes sense by examining the edge cases described, and (3) Implement the restructured try/catch logic. The actual code change is relatively small - restructuring the conditional logic in one function - but requires understanding the validation flow and exception handling patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal benchmark sample - it has a clear problem statement, detailed technical analysis, specific reproduction case, and tests for the fix. The issue demonstrates good software engineering practices with thorough explanation of the root cause and proposed solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11734": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a concrete test case that demonstrates the exact problem: OuterRef() works with filter() but fails with exclude() or ~Q(), raising a specific ValueError. The test code shows the three scenarios (filter working, exclude failing, filter with ~Q failing) with clear expected behavior. An experienced engineer can understand exactly what needs to be fixed - the inconsistent behavior of OuterRef() across these different query operations. The error message is also specified, making it clear what the current problematic behavior is.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. Looking at the provided patches, the solution involves understanding Django's ORM internals, specifically how OuterRef expressions are handled in different query contexts. The fix touches multiple files (fields/__init__.py, related_lookups.py, and sql/query.py) and requires understanding the differences between how filter() and exclude() process expressions. The engineer would need to trace through Django's query compilation process, understand why OuterRef works in filter but not exclude, and implement proper handling in the split_exclude method. While not trivial, it's a contained issue within the ORM's expression handling system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined with a reproduction case, the expected behavior is evident (OuterRef should work consistently across filter/exclude), and the solution can be tested against the provided test case. This is a good benchmark sample for evaluating an engineer's ability to understand ORM internals and fix expression handling inconsistencies.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11740": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps that need to be filled in. The user provides a concrete example showing two Django models (App1 and App2) where App1 initially has a UUIDField called \"another_app\" that gets changed to a ForeignKey pointing to App2. The core problem is clearly stated: when Django's migration autodetector generates a migration for this field change, it doesn't create the necessary dependencies on App2, which causes runtime errors when trying to resolve the foreign key relationship. The user mentions that a test fails with a ValueError about not being able to locate the App2 model, which provides additional context about the expected behavior. While some details could be clearer (like the exact migration structure or the specific error message), there's enough information for an experienced Django developer to understand that this is about Django's migration dependency detection system failing to recognize that changing a UUIDField to a ForeignKey should create a migration dependency on the target model's app.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because it involves understanding Django's complex migration autodetector system. The engineer needs to: 1) Understand how Django's migration autodetector works, specifically the generate_altered_fields method, 2) Identify that when a field changes from a non-relational field (UUIDField) to a relational field (ForeignKey), the system should detect and add dependencies for the target model, 3) Locate the existing _get_dependencies_for_foreign_key method and understand how to integrate it into the altered fields logic, 4) Modify the generate_altered_fields method to extract dependencies for foreign key changes and pass them to the migration operation. The solution involves editing the autodetector.py file in a specific, targeted way, but requires substantial understanding of Django's internals and migration system architecture. While not extremely complex, it's more than a simple bug fix and requires knowledge of Django's dependency resolution system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is technically sound, the solution is well-contained within Django's codebase, and the test case adequately verifies the fix. This is a good benchmark sample as it tests understanding of Django's migration system, ability to navigate complex codebases, and skill in making surgical changes to framework internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11815": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Django migrations are serializing enum objects using their values instead of their names, which causes failures when the enum values are translated strings. The issue provides a concrete example showing the problematic behavior with a Status enum that has translated values (_('Good') and _('Bad')). It explains exactly what happens: the migration generates Status('Good') instead of Status['GOOD'], and when translations change the enum values, old migrations fail with ValueError. The issue also provides the exact desired solution: use Status['GOOD'] syntax instead of Status('Good'). The problem statement is clear, the root cause is identified, and the expected solution is explicitly stated.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-defined and the solution involves modifying the EnumSerializer class in django/db/migrations/serializer.py. Looking at the gold patch, the fix is quite straightforward: instead of serializing the enum value with EnumClass(value), it needs to serialize using EnumClass['name'] syntax. The change is localized to the serialize() method of EnumSerializer class, replacing the logic that serializes the enum value with logic that serializes using the enum name. While it requires understanding how Django's migration serialization works, the actual code change is small (about 6 lines modified) and the logic is straightforward once you understand the problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined problem with a clear solution that tests core Django functionality around enum serialization in migrations. The issue is realistic and represents a practical problem developers would encounter when using translated enum values in Django models.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11964": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a complete, reproducible example with specific model definitions (MyChoice, MyObject), test cases that demonstrate the problem, and clear expected vs actual behavior. The issue clearly states that str(my_object.my_str_value) returns \"MyChoice.FIRST_CHOICE\" instead of the expected \"first\". The problem is that newly created objects have enum values that stringify differently than retrieved objects, which should both return the underlying value. An engineer would understand exactly what needs to be fixed: ensuring consistent string representation of enum choices regardless of how the object was obtained.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly defined and the solution is straightforward once you understand Python's enum behavior. The fix involves adding a simple __str__ method to the Choices class in django/db/models/enums.py that returns str(self.value). While it requires understanding Django's enum implementation and Python's method resolution order, the actual code change is minimal (just 6 lines added). The main time would be spent understanding the existing codebase structure and testing the fix, but the core solution is conceptually simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark candidate as it tests understanding of Python enums, Django model fields, string representation methods, and the ability to write a minimal, targeted fix. The issue is realistic, well-documented, and has a clear solution that can be verified through the provided tests.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12155": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: docutils fails to render docstrings when the first line is not empty due to an indentation calculation issue in the trim_docstring function. The issue provides a concrete example showing the problematic docstring format, explains the root cause (the indent calculation includes the first line which has zero indentation), identifies the specific problematic code line \"indent = min(len(line) - len(line.lstrip()) for line in lines if line.lstrip())\", and proposes a specific solution to skip the first line in the calculation. The description includes enough technical detail for an engineer to understand both the problem and the approach to fix it.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue description clearly identifies the problem and suggests the solution approach. Looking at the actual patch, the solution involves replacing the custom trim_docstring function with Python's built-in inspect.cleandoc function, which properly handles docstring trimming including cases where the first line is not empty. This requires understanding the existing code, recognizing that cleandoc is the appropriate replacement, and updating the imports and function calls accordingly. It's a straightforward change that requires some thought about the implications but doesn't involve complex logic or extensive code changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is clearly defined, the solution approach is evident from the issue description, and the patch shows a clean implementation. The test case also demonstrates the expected behavior, making this a good benchmark sample for evaluating coding ability.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12262": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is well-specified and clearly explains the problem. It provides concrete examples showing that Django's template system incorrectly handles keyword-only arguments with defaults in custom template tags. The issue demonstrates two specific problems: (1) when a keyword argument with a default value is provided, Django raises a TemplateSyntaxError saying it doesn't recognize the argument, and (2) when a keyword argument is supplied multiple times, Django gives the wrong error message. The examples show the exact template tag definitions (@register.simple_tag def hello(*, greeting='hello')) and template usage ({% hello greeting='hi' %}) that trigger the bugs. The issue also mentions that the same problem affects inclusion tags and notes that the problematic code dates back to Django 2.0. This level of detail with concrete code examples and expected vs actual behavior makes it very clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because: (1) The issue is well-defined with clear examples of the problem, (2) Looking at the provided patch, the fix is a simple one-line change in django/template/library.py - changing 'param not in params' to 'param not in params and param not in kwonly', (3) The logic is straightforward - the code needs to check if a parameter is in the kwonly (keyword-only) parameters list before rejecting it as unexpected, (4) While some time would be needed to understand Django's template parsing system and locate the relevant code in the codebase, the actual fix itself is minimal and doesn't require complex refactoring or extensive research. An experienced engineer familiar with Django would be able to trace through the template tag parsing logic, identify where keyword arguments are validated, and implement the fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly specified, the examples are concrete and reproducible, and the fix appears to be straightforward. The issue would work well for a coding benchmark as it tests understanding of Django's template system, debugging skills to locate the problematic code, and the ability to implement a targeted fix. The test cases provided in the patch also give good coverage of the expected behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12325": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some inference. It clearly describes a problem where Django raises an ImproperlyConfigured error about the 'origin' field needing parent_link=True, and shows that changing the field order resolves the issue. The core problem is evident: Django's model inheritance logic incorrectly identifies OneToOneField references as parent links when multiple OneToOneFields exist. However, some details need to be inferred - specifically that the issue is in Django's model metaclass logic that processes OneToOneFields during class creation, and that the solution involves making the parent link detection more precise by checking the parent_link attribute rather than just field type and order.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django's complex model metaclass system and inheritance mechanics, (2) The engineer needs to trace through the model creation process to find where parent links are incorrectly identified, (3) The solution involves modifying core Django ORM code in multiple files (base.py and options.py), (4) It requires understanding the relationship between OneToOneField detection, parent_link attributes, and model inheritance, (5) The fix needs careful consideration to avoid breaking existing functionality. While the actual code changes are relatively small (a few lines), the research and understanding required makes this a substantial task.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is technically sound, the solution is well-defined, and the test cases properly validate the fix. The issue demonstrates a clear bug in Django's model inheritance system that can be objectively verified.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12663": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It provides a clear reproduction case with specific models (A, B, C with their relationships), a concrete test case showing the problem, and describes the expected vs actual behavior. The issue states that SimpleLazyObject usage with nested subquery annotations worked before commit 35431298226165986ad07e91f9d3aca721ff38ec but now fails with a TypeError when Django tries to convert SimpleLazyObject to an integer. However, some details could be clearer: the exact error message/stack trace isn't provided, and the description of what the \"correct\" behavior should be could be more explicit. Still, an experienced engineer could reasonably infer that the fix should allow SimpleLazyObject to work in this context as it did before the regression.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours of work. The engineer needs to: 1) Understand Django's ORM query processing, particularly how subqueries and annotations work, 2) Investigate how SimpleLazyObject is handled in query filters, 3) Trace through the code to find where the int() conversion is failing, 4) Understand the regression introduced by the specific commit mentioned, and 5) Implement a fix that properly handles SimpleLazyObject in this context. The solution patch shows it's a targeted fix in django/db/models/sql/query.py involving the output_field property, but finding this requires understanding Django's query internals and debugging the specific failure case. This is more than a trivial change but doesn't require extensive code refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is specific enough to be solvable, has a clear reproduction case, and the test patch shows it can be properly validated. The issue involves Django ORM internals but is well-contained to a specific regression scenario.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12708": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clearly described: Django migrations crash when trying to delete an index_together when there's also a unique_together on the same fields, because Django finds two matching constraints and can't decide which to remove. The issue provides clear reproduction steps (create models with fields in both unique_together and index_together, then delete index_together). However, some details require inference - the engineer would need to understand Django's migration system, the relationship between unique constraints and indexes, and how the _delete_composed_index method works. The mention of \"Options.indexes feature in Django 1.11\" provides helpful context but isn't essential. While the issue identifies two separate problems, the primary focus is on the migration crash, which is actionable.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The solution requires: 1) Understanding Django's migration system and schema manipulation, 2) Locating the _delete_composed_index method in django/db/backends/base/schema.py, 3) Understanding that the issue occurs because the method finds both unique constraints and indexes when searching for constraints to delete, 4) Modifying the constraint filtering to be more specific (adding 'unique': False to distinguish between unique constraints and regular indexes), and 5) Writing comprehensive tests. The actual code change is small (adding one parameter), but requires understanding the intricate relationship between Django's ORM, migrations, and database schema management. An experienced engineer would need time to trace through the migration logic and understand why the ValueError occurs.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking. The problem is well-contained within Django's migration system, has a clear reproduction case, and the solution involves understanding database schema concepts that are fundamental to Django development. The test patch shows comprehensive verification of the fix, ensuring both that indexes can be removed properly and that unique constraints remain intact. This tests both Django ORM knowledge and migration system understanding, making it a good benchmark for intermediate-level Django developers.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12754": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear context. It describes a specific Django migration problem where moving a field from a base class to a subclass in the same migration step causes a FieldError during migration execution. The issue includes concrete examples showing the \"before\" and \"after\" model structures, explains that makemigrations works but migrate fails, identifies the root cause (Django reports field conflicts between Book's title field and the inherited title field), and suggests a solution (reversing the order of migration operations). The reference to analogous issue #21890 provides additional context. An experienced engineer would have sufficient information to understand that the auto-detector needs to be modified to generate migration operations in the correct order.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's migration auto-detection system, specifically how dependencies between migration operations are determined. The engineer needs to identify that the issue occurs in the autodetector.py file where CreateModel operations need to properly depend on RemoveField operations when there are field name conflicts with base classes. Looking at the gold patch, the solution involves adding logic to detect when a new model has fields with the same name as fields being removed from its base class, and creating appropriate dependencies. This requires understanding Django's model state representation, dependency tracking, and the migration generation process, but doesn't require extensive codebase changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. The problem is Django-specific but well-contained, has a clear test case, and the solution can be verified by running the provided test. The issue demonstrates good software engineering practices with proper problem description, reproduction steps, and expected behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12858": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some blanks to fill in. The core problem is clear: Django's system check (models.E015) is incorrectly flagging a valid ordering field 'supply__product__parent__isnull' as invalid, even though the ordering works fine in practice. The issue provides concrete evidence that the ordering works (showing actual query results) and identifies the specific error being raised. However, there are some details that need to be inferred: the exact model structure is described but not shown in code, and the connection to PR #29408 is mentioned but not detailed. An experienced engineer would need to understand Django's model checking system and investigate how the isnull lookup should be handled in the ordering validation logic. The solution direction is reasonably clear - the validation logic needs to be updated to properly recognize valid lookups like 'isnull' in addition to transforms.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is quite straightforward: adding a single condition to check for valid lookups (fld.get_lookup(part) is None) alongside the existing transform check in the _check_ordering method. The change is minimal (adding just 2 lines) and the logic is clear once you understand that Django needs to validate both transforms and lookups in ordering fields. An experienced engineer familiar with Django would need some time to locate the right validation function in django/db/models/base.py and understand the existing logic, but the actual fix is simple and surgical.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is well-contained, has a clear problem statement with reproducible evidence, and the solution is focused on a specific validation logic. The test case is also appropriate, verifying that the ordering validation correctly handles lookup fields like 'isnull'. The issue demonstrates a real-world Django development scenario where system checks need to properly validate model meta options.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13012": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a specific problem: when a constant expression is wrapped in an ExpressionWrapper object, Django incorrectly includes it in the GROUP BY clause, causing a PostgreSQL error. The issue provides concrete code examples showing both the problematic behavior (with ExpressionWrapper) and the correct behavior (without ExpressionWrapper). It shows the exact SQL queries generated and the specific error encountered. The expected behavior is clear - constant expressions should be omitted from the GROUP BY clause regardless of whether they're wrapped in ExpressionWrapper. The problem is isolated to the ExpressionWrapper class in Django's ORM expressions module.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that requires 15 minutes to 1 hour. The issue is well-isolated to the ExpressionWrapper class, and the solution is simple - adding a get_group_by_cols method that delegates to the wrapped expression's get_group_by_cols method. The gold patch shows this is just a 3-line addition. An experienced developer would need some time to understand Django's ORM expression system and how GROUP BY columns are determined, but the actual fix is minimal once the problem is understood. The test cases also clearly show what the expected behavior should be.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described with concrete examples, the problem is well-isolated, and the solution is elegant and minimal. The test cases provide clear validation criteria. An engineer working on this would have all the information needed to understand the problem and implement a correct solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13028": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some minor gaps. It clearly describes the problem: Django raises NotSupportedError when filtering with a model that has a 'filterable' field. The issue provides concrete model definitions, shows the exact error scenario, and mentions a workaround. However, there are some details missing - the exact error message isn't provided, and the description could be clearer about the distinction between model fields and expression attributes. An experienced developer could reasonably interpret that Django is incorrectly treating model instances as expressions when checking the 'filterable' attribute, which is what the bug actually is.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The issue involves understanding Django's query filtering mechanism and the check_filterable method. The solution is a targeted 3-line change that adds a condition to check if something has 'resolve_expression' before applying filterable restrictions. An experienced Django developer would need to: 1) Reproduce the issue, 2) Locate the check_filterable method in django/db/models/sql/query.py, 3) Understand that model instances shouldn't be subject to expression filtering rules, 4) Add the hasattr check for 'resolve_expression'. The concept is straightforward once you understand the distinction between model instances and query expressions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is reproducible, the solution is clean and targeted, and the test coverage is appropriate. This is a good benchmark sample as it tests understanding of Django's ORM internals and the ability to distinguish between different types of objects in the query system.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13112": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) Clear error description - makemigrations crashes with ValueError due to case mismatch between app name in INSTALLED_APPS ('DJ_RegLogin') and the lazy reference generated ('dj_reglogin'), (2) Complete code context including the problematic model.py with Category and Content models, the ForeignKey relationship, settings.py showing INSTALLED_APPS with mixed-case naming, and apps.py with the AppConfig, (3) Specific technical details about Django 3.1b1 behavior and the exact error condition. An experienced engineer would have enough information to understand that Django is incorrectly lowercasing the entire app reference instead of preserving the app label case while only lowercasing the model name.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-isolated to Django's handling of string model references in ForeignKey fields. The solution involves modifying the deconstruct method in django/db/models/fields/related.py to handle app labels with mixed case correctly - specifically checking if there's a dot separator and preserving the app label case while only lowercasing the model name. The logic change is straightforward: split on dot, preserve app_label case, lowercase only model_name. While it requires understanding Django's model reference system, the actual code change is small and localized.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution approach is evident from the issue description, and the test case validates the fix properly. This would make a good benchmark sample as it tests understanding of Django's model reference handling, string manipulation, and case sensitivity issues in framework code.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified. It provides a clear example of the problem - temporal subtraction fails without ExpressionWrapper and raises a FieldError when combining DateTimeField with DurationField. The title \"make temporal subtraction work without ExpressionWrapper\" clearly indicates what needs to be fixed. However, there are some gaps: it doesn't explain exactly how the fix should be implemented, what the expected behavior should be in edge cases, or provide comprehensive test scenarios. An experienced engineer would need to infer the solution approach and handle implementation details, but the core problem and desired outcome are clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. The patch shows significant refactoring of Django's expression system - moving complex logic from as_sql() to resolve_expression() method, handling multiple field types (DateTimeField, DurationField, etc.), and ensuring proper type checking and conversion. The engineer needs to understand Django's ORM internals, expression compilation process, and temporal field handling. The solution involves modifying core expression classes and requires careful consideration of different database backends and field combinations. While not >100 lines, it's architecturally significant and requires deep understanding of the framework.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is well-contained within Django's ORM expression system, the test cases are comprehensive and clearly validate the fix, and the issue represents a meaningful functionality improvement that would be valuable for evaluating coding ability in the context of ORM development.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13297": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear information about what the problem is. It includes: (1) A concrete code example that works in Django 3.0 but fails in 3.1, (2) The specific error behavior (SimpleLazyObject causing crashes when filtering), (3) A workaround that demonstrates the root cause (converting to string with str()), (4) The exact error context (Django's SQLite backend can't serialize SimpleLazyObject), and (5) The affected component (TemplateView.get_context_data()). The issue clearly shows that kwargs values are being wrapped in SimpleLazyObject inappropriately, causing database query failures. An experienced engineer can understand that the solution needs to ensure kwargs values maintain their original types for proper database interaction.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is clearly described, solving it requires: (1) Understanding Django's lazy evaluation system and SimpleLazyObject behavior, (2) Locating the specific code in TemplateView that wraps kwargs in SimpleLazyObject, (3) Understanding why this change was made (likely for deprecation warnings), (4) Finding the right replacement approach that maintains the deprecation warning functionality while preserving type compatibility, and (5) Testing across different scenarios. The actual code change is small (as seen in the patch), but requires careful analysis of Django's internals and the lazy evaluation system. The solution involves replacing SimpleLazyObject with the lazy() function while preserving the original value's type, which is not immediately obvious without deep Django knowledge.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly described with good reproduction steps, the solution space is well-defined, and the test patch confirms the fix works correctly. This is a good benchmark sample that tests understanding of Django's lazy evaluation system and ability to balance feature requirements (deprecation warnings) with compatibility concerns.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13406": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a complete minimal reproducible example with specific model definitions (Toy model with name, material, price fields), exact code that demonstrates the problem, clear expected vs actual behavior (should return dicts but returns broken model instances), and a precise error message. The issue clearly explains that when a queryset with values()/values_list() is pickled and recreated, it loses the information about what type of objects it should return, causing it to return model instances instead of dictionaries. An engineer would have all the information needed to understand and reproduce the problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves understanding Django's queryset internals, specifically how the _iterable_class attribute determines what type of objects are returned. The fix is just 2 lines of code that check if the query has values_select and set the appropriate iterable class. While it requires some understanding of Django's ORM internals, the problem is well-localized and the solution is straightforward once you understand that the issue is about preserving the iterable class information when recreating a queryset from a pickled query.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good example of a well-specified bug report with a clear, focused solution. The issue is about a specific Django ORM feature (pickling querysets) that could realistically come up in real-world usage. The solution requires understanding Django internals but is not overly esoteric.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13449": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A complete, runnable code example showing the exact model definition with specific field types (DateField, FloatField, DecimalField), (2) The exact query code that reproduces the problem, (3) The specific error message and generated SQL that demonstrates the issue, (4) A clear explanation of the root cause - that CAST() only covers the LAG() call instead of the entire window expression, (5) Confirmation that the issue is specific to DecimalField by showing that FloatField works correctly, and (6) A working workaround that hints at the solution direction. An experienced engineer would have all the information needed to understand the problem, reproduce it, and implement a fix.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django's ORM expression system and how different database backends handle type casting, (2) The engineer needs to locate the Window class in django/db/models/expressions.py and understand how SQLiteNumericMixin works, (3) The solution involves adding SQLite-specific logic to handle DecimalField casting at the window level rather than just the inner expression, (4) While the core change is relatively small (adding as_sqlite method), it requires understanding the interaction between window functions, type casting, and SQLite's syntax requirements, (5) The fix involves modifying the source expressions and their output fields, which requires careful handling to avoid side effects.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is database-specific (SQLite), well-documented, and the provided test case clearly validates the fix. The issue demonstrates good software engineering practices by providing reproduction steps, error analysis, and a workaround.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13568": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that Django's system check auth.E003 currently flags a username field as needing to be unique when it's designated as USERNAME_FIELD, even when uniqueness is enforced through a UniqueConstraint in the model's Meta.constraints. The issue provides a concrete example showing a User model with a username field that has a UniqueConstraint but not unique=True, and explains that the system check should be extended to recognize constraints as well as the unique field attribute. The desired behavior is clear: skip the auth.E003 check when USERNAME_FIELD has uniqueness enforced through total UniqueConstraints. The technical context about PostgreSQL index creation provides good motivation for why someone would use constraints instead of unique=True.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because it requires modifying the existing system check logic in django/contrib/auth/checks.py to additionally check for UniqueConstraints. The solution involves understanding Django's constraint system and modifying a conditional statement to also check cls._meta.total_unique_constraints. While it requires some familiarity with Django's meta system and constraint handling, the change itself is relatively small and focused - just extending an existing if condition to consider an additional case. The patch shows this is indeed a small, targeted change that adds a few lines of logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a straightforward enhancement to Django's system checks that improves the framework's ability to recognize different ways of enforcing uniqueness constraints. The issue is well-documented with clear examples and the solution is focused and testable.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13807": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) Clear steps to reproduce: create a Model called \"Order\", create fixtures, use loaddata; (2) Specific error details: SQLite rejects unquoted table name, OperationalError with syntax error near \"order\"; (3) Root cause analysis: identifies the exact file (django/db/backends/sqlite3/base.py), function (check_constraints), line number (327), and the specific problem (missing back ticks around %s in SQL statement); (4) Code context showing the problematic lines 327 and 333; (5) Version information confirming the issue exists in Django 3.1.0 and 3.1.2. The issue clearly explains that SQL keywords like \"order\" need to be quoted when used as table names in SQLite, and points to the exact location where this quoting is missing.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides the exact root cause and location of the problem. The solution involves adding proper quoting around table names using self.ops.quote_name() in the SQLite backend. Looking at the gold patch, it's a straightforward change that replaces bare table_name variables with self.ops.quote_name(table_name) in 3 specific locations within the same function. An experienced engineer familiar with Django's database abstraction layer would quickly recognize this as a standard SQL injection prevention/identifier quoting issue and implement the fix using Django's existing quote_name utility.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the root cause is identified with precision, and the solution approach is well-understood in Django's codebase. The test patch also shows exactly how to verify the fix works with SQL keyword table names.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13810": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The reporter clearly describes a concrete problem: when using ASGI with Django middleware that raises MiddlewareNotUsed, the middleware chain gets \"poisoned\" causing subsequent middleware to receive an incorrectly adapted handler. The issue includes a specific error (TypeError when Django attempts to await an HttpResponse), points to the exact problematic code location (django/core/handlers/base.py#L58), and provides a clear hypothesis about the root cause (handler variable being overwritten but not reset when MiddlewareNotUsed is raised). However, some details require inference - the exact mechanism of how the \"poisoning\" occurs and what the correct behavior should be isn't completely explicit, requiring an engineer to understand Django's middleware loading logic to fill in the gaps.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue falls into the 1-4 hour range because it requires understanding Django's complex middleware loading system, particularly the interaction between sync/async adaptation and exception handling. The engineer needs to: (1) understand how MiddlewareNotUsed exceptions work in the middleware chain, (2) comprehend the async/sync adaptation logic in adapt_method_mode(), (3) trace through the middleware loading loop to understand how the handler variable state gets corrupted, and (4) devise a solution that properly handles the handler state when exceptions occur. While the actual code change is small (only a few lines), the debugging and comprehension required is substantial. The fix involves careful state management in exception handling, which requires thorough understanding of the control flow.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's middleware system, the test case adequately covers the scenario, and the solution is verifiable. The issue represents a legitimate bug with clear reproduction steps and a targeted fix that doesn't have broad architectural implications.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14017": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It states that Q(...) & Exists(...) raises a TypeError while Exists(...) & Q(...) works correctly. The issue provides a minimal reproducible example showing the working case and mentions that the non-working case raises a TypeError. The problem is identified as a missing __rand__ method, and the requirement is clear: make the & (and |) operators commutative on Q-Exists pairs. An experienced engineer would understand they need to modify the Q class's _combine method to accept objects with a 'conditional' attribute (like Exists objects) in addition to Q instances.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified in the issue description - the _combine method in the Q class only accepts other Q instances, but needs to also accept objects with a 'conditional' attribute like Exists. The solution involves modifying a single line in the _combine method from checking 'isinstance(other, Q)' to also checking 'getattr(other, 'conditional', False) is True'. This requires minimal code understanding and is a straightforward conditional logic change. An experienced engineer would quickly locate the relevant method and implement the fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear problem statement, reproducible example, and straightforward solution. The test patch demonstrates good coverage of both & and | operations with empty Q objects. This sample would be suitable for evaluating coding ability as it tests understanding of operator overloading, duck typing concepts, and the ability to make targeted fixes to existing code.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14140": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the problem: a TypeError occurs when calling deconstruct() on a Q object wrapping an Exists expression, because the code assumes children are tuples but Exists is not subscriptable. The error scenario is specific and technical. However, there are some missing details that an engineer would need to figure out: the exact location of the problematic code in the codebase, the specific conditions that trigger this (what does \"wraps an Exists expression\" mean exactly), and what the expected behavior should be instead of the error. Looking at the provided patch, the solution involves modifying the deconstruct method in django/db/models/query_utils.py to treat all children uniformly as args rather than trying to convert single non-Q children to kwargs. An experienced Django developer could reasonably infer the requirements and locate the relevant code, but some investigation would be needed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue for several reasons: 1) Understanding the problem requires familiarity with Django's Q object system, the deconstruct() method, and Exists expressions - this takes time to research if not already familiar. 2) Locating the exact code causing the issue requires navigating the Django codebase to find the deconstruct method implementation. 3) The solution involves understanding the logic of how Q objects handle their children and deciding on the appropriate fix - whether to add special handling for Exists objects or simplify the overall approach. 4) Writing comprehensive tests to cover the edge case and ensure the fix works correctly. The actual code change is relatively small (removing conditional logic), but the investigation, understanding, and testing phases require substantial time investment.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is technical and specific enough to have a clear solution. The test cases provided show that the fix can be verified programmatically. The issue represents a legitimate bug that would be encountered in real Django usage, making it suitable for evaluating coding ability in the context of understanding and fixing framework-level issues.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14238": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A concrete reproduction case with specific code showing DEFAULT_AUTO_FIELD = \"example.core.models.MyBigAutoField\" and the MyBigAutoField class definition, (2) The exact error that occurs - a ValueError saying the primary key must subclass AutoField, (3) Clear identification of the root cause - the subclass check in AutoFieldMeta.__subclasscheck__ method, and (4) A specific suggestion for the fix - allowing subclasses in the _subclasses property. The issue description gives an experienced developer all the information needed to understand the problem, locate the relevant code (AutoFieldMeta.__subclasscheck__), and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic method (AutoFieldMeta.__subclasscheck__) and suggests the approach. Looking at the actual fix, it's a one-line change from \"subclass in self._subclasses\" to \"issubclass(subclass, self._subclasses)\" in django/db/models/fields/__init__.py. The core issue is that the current code uses \"in\" to check if a subclass is directly in the _subclasses tuple, rather than using \"issubclass()\" which would properly handle inheritance. An experienced developer would need some time to understand Django's metaclass structure and verify the fix works correctly, but the change itself is minimal and straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear reproduction case, specific error message, and targeted fix. The issue is suitable for benchmarking coding ability as it tests understanding of Python inheritance, metaclasses, and the difference between membership testing and subclass checking.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14351": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks but provides a sensible interpretation of what needs to be fixed. It clearly identifies a specific problem: there's a difference in how Q object aliases are set up when OR'd, where `agent__property_groups__id__in` uses only 1 column in get_default_columns, but `agent__property_groups__in` pulls in every field, causing database rejection due to subqueries returning multiple columns when only one is allowed. The error message is specific (ProgrammingError with details about which fields are being selected). However, the issue lacks some context about what Q objects are, the specific codebase structure, and doesn't provide a minimal reproducible example. An experienced Django developer could reasonably infer this relates to Django ORM query construction and the need to ensure consistent column selection in subqueries when using IN lookups with OR operations.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This appears to be a 1-4 hour fix based on several factors: (1) It requires understanding Django ORM internals, specifically how Q objects, lookups, and subquery column selection work; (2) The solution involves adding a new method `get_group_by_cols` to handle column selection consistency in IN lookups; (3) It requires knowledge of when to clear select clauses and add primary key fields; (4) The fix is relatively focused (adding one method with ~9 lines) but requires understanding the interaction between different ORM components; (5) An experienced engineer would need time to trace through the Django ORM code to understand why the inconsistency occurs and how to properly fix it without breaking existing functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is technical but well-defined, the solution is focused and testable, and it represents a legitimate bug fix that would be valuable for assessing Django ORM knowledge and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14493": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides clear reproduction steps, explains the exact error (UnboundLocalError for 'substitutions'), identifies the root cause (the variable is only initialized inside a loop that doesn't execute when max_post_process_passes=0), and even links to the specific problematic code location in Django's staticfiles/storage.py. The motivation for setting max_post_process_passes to 0 is also explained. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix requiring less than 15 minutes. The issue clearly identifies the problem: the 'substitutions' variable is referenced before assignment when max_post_process_passes=0. The solution is simply to initialize the variable before the loop that might not execute. Looking at the gold patch confirms this - it's a single line addition: 'substitutions = False' before the loop. An experienced engineer would immediately recognize this as a classic uninitialized variable bug and fix it in minutes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the problem is clearly specified, the solution is straightforward but requires understanding the code flow, and the test case appropriately verifies that collectstatic runs without crashing when max_post_process_passes=0. The issue represents a real-world Django bug that would be commonly encountered.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14580": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides a complete, minimal reproducible example with the exact models.py code that triggers the bug. The issue clearly shows the generated migration file that Django produces, and explicitly identifies the problem: the migration references 'models.Model' in the bases list but never imports 'models' from django.db. The error is precisely described (NameError: name 'models' is not defined), and the expected vs actual behavior is clearly stated. The issue even suggests the likely location of the bug (django.db.migrations.writer). An experienced engineer has all the information needed to understand, reproduce, and fix this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly a missing import in the migration serializer. Looking at the gold patch, the fix is a simple one-line change in django/db/migrations/serializer.py, adding the import requirement for models.Model in the special_cases list. The problem location is well-defined (migration writer/serializer), and the solution is straightforward - ensuring that when models.Model is serialized, the necessary import is included. An experienced engineer familiar with Django's migration system could identify and implement this fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is crystal clear, has a well-defined problem and solution, and tests a developer's ability to understand Django's migration serialization system and fix import dependencies. The fix requires understanding how Django's migration writer handles special cases for serialization and ensuring proper imports are included.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14672": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides clear technical context about Django 3.2's identity properties for ForeignObjectRel, explains the specific problem (through_fields being a list that needs to be made hashable), includes a complete minimal reproduction case with exact model definitions, describes the error condition (TypeError: unhashable type 'list'), and explicitly states the solution needed (add make_hashable call on self.through_fields in ManyToManyRel). An experienced Django developer would have all the information needed to implement the fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that involves adding a single function call (make_hashable) around self.through_fields in the identity property of ManyToManyRel. The issue clearly explains what needs to be done and where. An experienced developer familiar with Django would need minimal time to locate the relevant file (django/db/models/fields/reverse_related.py), understand the ManyToManyRel class, and make the one-line change. The main time would be spent understanding the Django codebase structure and locating the correct file, but the actual fix is trivial once located.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is clearly technical, well-documented, has a precise solution, and the test cases verify the fix works correctly. This would be a good benchmark sample for evaluating coding ability in Django-specific contexts.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14787": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It provides a concrete example showing that when using @method_decorator(logger) on a class method, the decorator receives a functools.partial object instead of a regular function, which lacks attributes like __name__ and __module__. The example code demonstrates the exact scenario where this fails - when the logger decorator tries to access func.__name__, it raises an AttributeError. The issue title \"method_decorator() should preserve wrapper assignments\" clearly indicates what needs to be fixed. The problem is that method_decorator should ensure that function attributes are properly preserved when wrapping methods, which is a standard expectation for decorators in Python.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is quite straightforward - it involves adding wraps(method) around the partial object creation in the method_decorator function. The fix is a one-line change that applies the @wraps decorator to preserve function attributes. An experienced engineer familiar with Python decorators would quickly understand that the issue is about preserving function metadata (like __name__, __module__) when creating partial objects, and the solution follows standard Python decorator patterns. The main time would be spent understanding the existing method_decorator implementation and testing the fix, but the core concept and solution are well-established patterns in Python.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. This is a well-suited benchmark problem as it tests understanding of Python decorators, functools.wraps, and method binding mechanisms. The issue is realistic, the solution is clean, and the test case properly validates the fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15104": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. The reporter provides: (1) Clear symptom description - a KeyError occurring during Django test suite execution, (2) Root cause analysis - the migration autodetector's `only_relation_agnostic_fields` method assumes all field deconstructed kwargs contain a 'to' key and uses `del deconstruction[2]['to']`, but custom ForeignKey fields may have already removed this key, (3) Complete reproduction case with a self-contained test showing the exact problem, (4) Proposed solution - change `del deconstruction[2]['to']` to `deconstruction[2].pop('to', None)` in the autodetector code, (5) Specific file and method references. The issue text contains everything needed to understand the problem and implement the fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides the exact problem location (django/db/migrations/autodetector.py in the only_relation_agnostic_fields method) and the precise solution (change del to pop with None default). The actual code change is a single line modification. The main time would be spent understanding the Django migration system context and writing appropriate tests. The reproduction case and proposed fix make the implementation straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue demonstrates good software engineering practices with clear problem description, root cause analysis, reproduction case, and proposed solution. It tests understanding of Python exception handling (KeyError vs graceful key removal), Django's migration system, and defensive programming practices. The fix is simple but requires understanding the context and implications.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified. It provides a clear reproduction case with specific model definitions and the exact code that triggers the AssertionError. The reporter identifies the specific location where the error occurs (Query.change_aliases in query.py, line 854) and provides a technical explanation of the root cause: sequential aliases (T4, T5) create overlapping sources and targets during QuerySet OR operations, violating an internal assertion. The reporter even suggests a potential solution approach (providing alias_map to Query.join and incrementing suffixes). However, there are some gaps - the exact assertion that's failing isn't shown, and while the technical explanation is detailed, an engineer would need to dive into the Django ORM codebase to fully understand the alias management system and implement the fix.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: (1) It requires understanding Django's complex ORM query system, particularly the alias management and query combination logic, (2) The solution involves modifying core query functionality in multiple methods (combine, bump_prefix, change_aliases), (3) The fix requires careful consideration of edge cases to avoid breaking existing functionality, (4) An engineer needs to understand the relationship between QuerySet operations and SQL alias generation, (5) The solution involves both algorithmic changes (preventing alias conflicts) and code refactoring (updating method signatures and parameters). While the reproduction case is clear, implementing a robust solution that handles all cases without introducing regressions requires substantial understanding of the Django ORM internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of complex ORM internals, debugging skills, and the ability to implement a solution that maintains backward compatibility. The issue has clear acceptance criteria (the provided test case should pass) and the solution quality can be objectively verified through the test suite.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15277": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem: CharField.__init__ unconditionally adds a MaxLengthValidator even when max_length is None, which causes issues when used with Value objects. The issue provides concrete evidence with timing benchmarks, error traces, and specific code examples. It identifies the exact location of the problem (CharField.__init__), explains why it's problematic (MaxLengthValidator can't handle None max_length), provides performance measurements, and even suggests the exact solution (adding a conditional check like BinaryField does). The issue includes before/after performance numbers and references existing precedent in the codebase.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that requires minimal time. The issue clearly identifies the problem location (CharField.__init__ in django/db/models/fields/__init__.py), provides the exact solution (wrap the validator addition in a conditional check), and even references existing precedent in BinaryField. The actual code change is just adding an if statement around one line. An experienced engineer would need only a few minutes to understand the issue, locate the file, and implement the fix. The most time-consuming part would be writing appropriate tests, but even that's straightforward given the clear problem description.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, the solution is straightforward to implement, and it represents a real-world optimization problem that tests understanding of Django's field system and validator mechanics. The fix is small but requires understanding the codebase structure and the relationship between fields and validators.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15280": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproducible example with specific Django models (User and Profile), a complete test case that demonstrates the expected vs actual behavior, and detailed explanation of what's going wrong. The issue description includes the exact queries being executed, shows what the deferred fields contain, and explains the root cause hypothesis. The author provides concrete models, a failing test with assertions, shows the unexpected database query being executed, and explains that the problem occurs when accessing user.profile.user.kind triggers an extra query despite the field being prefetched. This gives a clear target for what a successful solution should accomplish.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is well-described, it involves Django's complex ORM internals around prefetch_related, deferred fields, and object caching mechanisms. Understanding the issue requires deep knowledge of how Django handles nested prefetches and field deferring. The actual fix shown in the patch is relatively small (adding a 3-line conditional check), but arriving at this solution requires understanding the relationship between prefetched objects and cached fields, debugging through Django's ORM code to find where the caching logic goes wrong, and identifying that the issue is in the related_descriptors.py file where cached fields aren't being properly checked before overwriting relationships. An experienced engineer would need time to trace through the ORM's prefetch mechanics to understand why deferred fields from outer querysets are incorrectly inherited by inner prefetched objects.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-defined, the test case validates the expected behavior clearly, and the solution can be verified objectively. The issue involves Django ORM internals which are complex but well-documented, and the fix doesn't require external dependencies or system-specific configurations.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15315": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is generally well-specified with some blanks to fill in. The core problem is clearly stated: Field.__hash__() changes value when a field is assigned to a model class, which breaks dictionary lookups since the hash changes after the field is used as a dictionary key. The issue provides a concrete example of the problem (CharField used as dict key before binding to model) and mentions it was introduced in #31750. However, there are some gaps: the exact mechanism of how the hash changes isn't detailed in the description, and the reference to #31750 and #26472 provides context that wouldn't be available to someone working solely from this issue text. An experienced engineer would need to investigate the current __hash__ implementation and understand why it's problematic, but the overall goal (making __hash__ immutable) and the suggested approach (reverting the __hash__ change) provide clear direction.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the provided patch, the solution is quite simple - changing the __hash__ method from returning a hash of multiple fields (including model metadata) to just hashing the creation_counter. The core insight needed is understanding that hash values must remain constant for the lifetime of an object to work properly with dictionaries and sets. An experienced engineer would need to: 1) Locate the Field.__hash__ method in django/db/models/fields/__init__.py, 2) Understand why the current implementation is problematic (it includes model information that changes when field is bound), 3) Simplify it to only use immutable attributes like creation_counter. The test case also shows this is straightforward - just verify the hash doesn't change before and after model assignment. Most of the time would be spent understanding the codebase structure and the specific Django ORM concepts, but the actual code change is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement, identifiable solution approach, and straightforward implementation. The issue demonstrates good software engineering principles (hash immutability) and the test case properly validates the fix. An engineer unfamiliar with Django would need some time to understand the Field class and model binding concepts, but the core computer science principle (hash immutability) is universal and well-understood.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15375": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when using Django's aggregate() with a 'default' parameter after annotate(), it crashes with a SQL syntax error. The issue provides a complete reproduction case showing that Book.objects.annotate(idx=F(\"id\")).aggregate(Sum(\"id\")) works fine, but adding default=0 to Sum causes it to fail. It shows the exact error (OperationalError with syntax error near \"FROM\") and the malformed SQL query that starts with \"SELECT FROM\" without columns. The issue also demonstrates that a workaround using Coalesce works correctly. This gives enough information to understand the root cause: the aggregate with default generates incorrect SQL when combined with annotations, missing the column selection in the outer query.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15min-1hour difficulty level fix. The issue is well-isolated to Django's aggregate functionality with defaults. Looking at the gold patch, the solution is quite straightforward: when wrapping an aggregate with Coalesce (which happens when default is provided), the is_summary attribute needs to be preserved from the original aggregate to the Coalesce wrapper. The fix is only 2 additional lines of code. An experienced developer familiar with Django's ORM would need some time to understand how aggregates work with annotations and identify that the is_summary attribute is what determines proper SQL generation, but once identified, the fix itself is simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly described with a reproduction case, the problem is well-isolated to a specific feature interaction, and the solution requires understanding Django's ORM internals but isn't overly complex. The test cases also properly verify that the fix works for the specific scenario and doesn't break related functionality.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15380": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes the problem: a KeyError occurs when renaming both a model and field in a single migration step, specifically when makemigrations tries to look up the old model name in its internal state after confirming the rename. The issue provides the exact error scenario (renaming test_one.MyModel to MyModel2) and mentions it's a regression from a specific commit. However, there are some blanks to fill in - the exact nature of the KeyError, which specific part of the code is failing, and what the expected behavior should be are not fully detailed. An experienced engineer would need to investigate the codebase to understand the migration autodetector's internal workings, but the core problem is clear enough to work with.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that took only one line to change. The bug was in the generate_renamed_fields method where it was incorrectly using old_model_name instead of model_name when looking up the new model state. An experienced engineer familiar with Django's migration system could identify this issue within 15-60 minutes by: 1) Reproducing the KeyError, 2) Tracing through the autodetector code to find where the lookup fails, 3) Recognizing that the code should be looking up the new model name rather than the old one. The fix itself is trivial once the root cause is identified, requiring only changing one variable reference.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This issue is well-suited for a coding benchmark as it represents a realistic bug that could occur in production Django applications. The issue is specific enough to reproduce, has a clear test case that verifies the fix, and requires understanding Django's migration system but doesn't involve overly complex domain knowledge.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15503": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a specific bug where Django's JSONField has_key lookup fails with numeric keys on certain database backends (SQLite, MySQL, Oracle) while working correctly on PostgreSQL. The issue provides comprehensive details including: (1) exact Django and Python versions, (2) complete database configuration, (3) a minimal reproducible model definition, (4) a complete test case demonstrating the problem with expected vs actual behavior, (5) clear explanation of which backends work vs fail. The problem is that when querying for a JSON key that looks like a number (e.g., '1111'), the lookup returns 0 results instead of the expected 1 result on SQLite. An experienced engineer would have all the information needed to reproduce the issue and understand what constitutes a successful fix.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue because it requires: (1) Understanding Django's JSONField implementation across multiple database backends, (2) Investigating how JSON path compilation differs between PostgreSQL vs SQLite/MySQL/Oracle, (3) Identifying that numeric keys are being interpreted as array indices rather than object keys in the problematic backends, (4) Creating a solution that distinguishes between numeric keys and actual array access, (5) Implementing the HasKeyOrArrayIndex class and modifying the compile_json_path_final_key method across multiple files. The patch shows this touches the core JSON field logic and requires understanding the subtle differences in how different databases handle JSON paths. While not extremely complex, it requires solid understanding of Django's ORM internals and cross-database compatibility issues.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. The issue is well-documented, has a clear reproduction case, and the test patch provides comprehensive coverage of the fix including various scenarios (nested keys, array access, multiple key checks). This would make an excellent benchmark sample as it tests knowledge of Django internals, cross-database compatibility, and JSON handling specifics.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15525": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly describes the problem: when loading fixtures into a non-default database, Django fails while resolving natural keys for Book objects because the related Author records haven't been created yet in the secondary database. The issue provides complete model definitions (Author and Book classes with their natural_key methods and managers), sample data in JSON format, and explains the exact error scenario. The problem stems from the Book.natural_key() method trying to access book.author.natural_key() during deserialization, which triggers an ORM lookup on the \"other\" database where the Author doesn't exist yet. This creates a clear understanding of what needs to be fixed: ensuring that during natural key resolution, the database context is properly set so that related object lookups happen on the correct database.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is quite straightforward: in django/core/serializers/base.py, the fix involves setting the database state on the model instance before calling natural_key(). The change is minimal (adding just 2 lines: obj = Model(**data); obj._state.db = db) but requires understanding Django's ORM internals, specifically how database routing works and the _state.db attribute. An experienced engineer would need some time to identify that the issue is in the build_instance function and understand that setting obj._state.db = db ensures that related object lookups use the correct database. The fix is conceptually simple but requires familiarity with Django's internal model state management.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is testable, and the required change is reasonable in scope. The test patch shows that the fix can be verified by loading fixtures with natural keys and foreign key dependencies into a non-default database, which provides a clear success criterion.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15695": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that require interpretation. The core problem is clear: RenameIndex() crashes when reapplied after moving an unnamed index backward and forward. The issue provides a concrete test case in tests/migrations/test_operations.py showing the failure scenario, and mentions the specific error (ProgrammingError on PostgreSQL because index already exists). However, there are some details that need to be inferred: (1) What exactly \"moving backward and forward\" means in the context of migrations, (2) The relationship between unnamed indexes and unique_together constraints, and (3) What the expected behavior should be (likely that reapplying should be a no-op rather than crash). Looking at the provided solution patch, it becomes clear the fix is to check if the old and new index names are the same and return early, making it a no-op operation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-localized to the RenameIndex operation in Django's migration system. The solution, as shown in the patch, is quite simple: add a 3-line check in the database_forwards method to return early if the old and new names are the same. An experienced engineer would need to: (1) Understand Django's migration system and the RenameIndex operation, (2) Reproduce the issue using the provided test case, (3) Identify that the problem occurs when old_name == new_name, (4) Implement the simple guard clause. The code change is minimal and the logic is straightforward - just preventing unnecessary work when the names match.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's migration system, the reproduction case is clear, and the solution is straightforward. The issue provides enough context for an engineer to understand the problem domain and implement a fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15732": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: there's an erroneous unique_together constraint on a model's primary key field ('id') that cannot be dropped by Django migrations. The migration fails because it detects both the primary key constraint and the additional unique constraint, but only expects to find one constraint to remove. The issue mentions PostgreSQL as the database backend. While the description doesn't provide the exact model definition or migration code, an experienced Django developer would understand this is about the Django ORM's migration system and schema editor functionality. The problem statement gives enough context to understand that the solution needs to handle cases where multiple constraints exist on the same field(s) during unique_together removal operations.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: 1) It requires understanding Django's migration system, schema editor, and constraint handling mechanisms. 2) The engineer needs to debug why the migration fails when multiple constraints exist on the same fields. 3) Looking at the solution, it involves modifying the _delete_composed_index method in django/db/backends/base/schema.py to properly handle cases where multiple constraints exist on the same fields by filtering to the specific constraint that matches the unique_together naming convention. 4) The fix also involves refactoring the _create_unique_sql method and extracting a _unique_constraint_name helper method. 5) This requires understanding Django's internal constraint naming conventions and database backend abstraction. While not extremely complex, it requires deep knowledge of Django internals and careful consideration of edge cases.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate Django bug that requires understanding of ORM internals and migration system behavior. The issue description provides sufficient context for an experienced developer to understand the problem domain and work towards a solution, even though some details need to be inferred from understanding Django's architecture.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15814": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) Clear description of the problem: QuerySet.only() after select_related() crashes on proxy models, (2) Complete reproducible example with specific model definitions (CustomModel, ProxyCustomModel, AnotherModel), (3) Exact command that triggers the error, (4) Specific error details: ValueError stating that 'id' is not in the list, (5) Environment details (Windows 10, Python 3.10, Django 4.0.5), (6) Precise location of the problematic code (django/db/models/sql/query.py line 745), and (7) Even suggests a potential fix by replacing 'opts = cur_model._meta' with 'opts = cur_model._meta.concrete_model._meta'. An experienced engineer has everything needed to understand and solve this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides the exact location of the problem (line 745 in query.py) and even suggests the solution. The actual code change is minimal - just one line modification to use the concrete model's meta instead of the proxy model's meta. However, it requires some understanding of Django's proxy model system and ORM internals to verify the fix is correct and doesn't break other functionality. The engineer would need to understand why proxy models cause this issue with field selection and ensure the fix handles all edge cases properly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is clearly specified, has a focused solution, and tests Django ORM knowledge specifically around proxy models and query optimization. The test case provided in the patch is straightforward and directly validates the fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15930": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, reproducible code example that demonstrates the problem: a Django query using Case() with ~Q(pk__in=[]) that crashes with a syntax error. The issue explains exactly what happens (Django generates invalid SQL with no condition before THEN), what the expected behavior should be (annotate all rows with True since they all match), and provides context about why this pattern is relevant (~Q(pk__in=[]) as a sentinel value in application code). The error description is specific enough that an experienced engineer could understand the problem involves SQL generation for CASE expressions when dealing with empty Q objects that get negated.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is localized to Django's SQL compilation for CASE expressions. Looking at the gold patch, the solution is quite straightforward: detect when the condition SQL is empty (which happens with ~Q(pk__in=[])) and replace it with a condition that's always True. The fix involves adding just 5 lines of code with a simple conditional check in the as_sql method of what appears to be a When class in django/db/models/expressions.py. An experienced engineer familiar with Django's ORM would quickly identify this as a SQL generation issue and the fix requires understanding the compilation process but not extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - the problem is clearly defined, reproducible, and has a focused solution that tests understanding of Django's ORM internals without being overly complex.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15957": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some blanks to fill in. It clearly describes the problem: Prefetch() objects don't work with sliced querysets, provides a concrete code example showing the assertion error, and explains the use case (displaying categories with a few example objects each). However, there are some gaps: the exact assertion error mentioned isn't quoted, the specific behavior expected isn't fully detailed, and while the use case is clear, the exact API design for the solution could have multiple reasonable interpretations. An experienced engineer would likely understand that the goal is to make the provided code example work without errors while maintaining the slicing behavior, but some implementation details would need to be inferred.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a complex issue requiring 4+ hours of work. The solution involves: 1) Deep understanding of Django's ORM query system, prefetch mechanics, and how slicing interacts with filtering, 2) Creating a sophisticated `_filter_prefetch_queryset` function that handles sliced querysets using window functions (RowNumber, partition_by), 3) Understanding when and how to clear query limits and apply complex predicates, 4) Modifying multiple descriptor classes (ForwardManyToOneDescriptor and related), 5) The solution spans multiple files and requires knowledge of Django's internal query compilation, window functions, and database-level operations. The use of Window functions with RowNumber and partition_by to simulate slicing behavior while allowing filtering is quite advanced and not obvious.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate Django ORM enhancement that addresses a real limitation. The issue description provides sufficient context and a clear use case. The solution is technically sound and the tests comprehensively cover the functionality. This would be a good benchmark sample for evaluating advanced Django/ORM knowledge.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15973": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: when defining a ManyToManyField with a \"through\" model in a separate Django app, migrations fail with \"AttributeError: 'str' object has no attribute '_meta'\". The issue includes complete code examples showing the three models (FonteModel, VariavelModel, FonteVariavelModel) in separate apps, the generated migration file, and the exact error scenario. The user clearly explains that the problem occurs specifically when the intermediary model is in a different app, and that bundling it in the same app works as a workaround. The root cause is also well-explained: Django treats the through model label as a plain string rather than resolving it to the actual model class during migration, causing the error when trying to access _meta.auto_created. An experienced engineer would have sufficient information to understand the problem and work toward a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution, this is a relatively straightforward fix that changes one line in django/db/migrations/autodetector.py. The fix changes `remote_field_model` to `field.remote_field.through` in the resolve_relation call. This suggests the problem was in how the through model reference was being passed to the resolver function. An experienced Django developer familiar with the migration system could identify this issue within 15 minutes to 1 hour. The error message points directly to the problem area (_meta attribute access), and the issue description clearly explains the through model resolution problem. The fix requires understanding Django's migration autodetector and model resolution, but doesn't require extensive refactoring or deep architectural changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. This is a well-documented Django migration bug with a clear reproduction case, specific error message, and straightforward solution. The test case provided also properly validates the fix by testing the scenario of ManyToManyField with through model in separate apps. This would be a good benchmark sample for evaluating understanding of Django's migration system and model relationship handling.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16032": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear title explaining the problem: \"__in doesn't clear selected fields on the RHS when QuerySet.alias() is used after annotate()\", (2) A complete, runnable test case that reproduces the bug with specific test data and expected results, (3) The exact error being encountered: \"OperationalError indicating that the subquery returns ten columns when only one was expected\", (4) Clear context about Django ORM functionality with QuerySet.alias() and annotate() methods. An experienced engineer would have all the information needed to understand that the issue occurs when using both annotate() and alias() on a QuerySet that is then used in a subquery with __in lookup, causing incorrect column selection in the generated SQL.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django's ORM internals, specifically how QuerySet.alias(), annotate(), and __in lookups interact, (2) The engineer needs to trace through the query compilation process to understand why extra columns are being selected in subqueries, (3) Looking at the solution, it involves modifying the related_lookups.py and query.py files, requiring understanding of how has_select_fields property works and when to call set_values vs add_fields, (4) The fix involves changing the has_select_fields from a computed property to a boolean attribute and modifying the logic in get_prep_lookup method, which requires careful analysis of the query building process, (5) While not extremely complex, it requires substantial knowledge of Django's SQL compilation internals and careful testing to ensure the fix doesn't break other functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample is well-suited for the benchmark as it has a clear problem statement, reproducible test case, and tests a specific Django ORM edge case that requires understanding of internal query compilation logic. The provided test case would effectively validate whether a solution correctly resolves the issue.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16136": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is extremely well-specified. It clearly explains the problem: when a Django View subclass has only async methods (like async post), making a request to an unsupported HTTP method (like GET) causes Django to return HttpResponseNotAllowed, but then tries to await this non-awaitable object, resulting in a TypeError. The issue provides a complete, minimal reproduction case with specific code examples (the Demo view class and URL pattern), exact steps to reproduce (start dev server, access the URL), and the expected vs actual behavior (500 error instead of proper handling). The technical details are precise - it specifies Django 4.1.1, Python 3.10.6, and the exact error condition. An experienced engineer would have everything needed to understand and fix this async/sync mismatch issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The problem is clearly identified - Django's http_method_not_allowed method returns HttpResponseNotAllowed directly, but in async views this needs to be wrapped in a coroutine. The solution shown in the patch is straightforward: detect if the view is async (using self.view_is_async) and if so, wrap the response in an async function before returning it. This requires understanding Django's async view handling and Python coroutines, but the fix itself is small and localized to one method in django/views/generic/base.py. An experienced engineer familiar with async/await patterns would quickly identify this as a common async/sync boundary issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the reproduction case is clear and minimal, and the solution space is constrained. The test patch shows exactly what behavior should be verified. This is an excellent example for a coding benchmark as it tests understanding of async/await patterns, Django's view system, and HTTP method handling.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16429": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is exceptionally well-specified. It clearly describes: (1) The exact problem - timesince() raises TypeError with USE_TZ=True and >1 month intervals, (2) The root cause - attempting to subtract timezone-naive datetime from timezone-aware datetime, (3) The specific location in code where the bug occurs (django/utils/timesince.py#L93-L100), (4) A complete test case that reproduces the issue, (5) The exact solution needed - adding tzinfo=d.tzinfo to the datetime.datetime call. The issue author even provides a link to the specific line of code and mentions they're ready to send a PR, indicating deep understanding of the codebase and problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified and the solution is straightforward - adding a single parameter (tzinfo=d.tzinfo) to an existing datetime.datetime() call. The issue author has already pinpointed the exact location and provided the fix. An experienced engineer would need minimal time to understand the timezone handling issue, locate the problematic code at line 93-100 in timesince.py, and implement the one-line fix. The gold patch confirms this - it's literally adding one line with tzinfo=d.tzinfo parameter.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clear, the fix is well-defined, and it tests understanding of Python datetime/timezone handling - a common and important programming concept. The test case provided would effectively verify if a solution works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16454": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Django management command subparsers don't retain error formatting from the parent CommandParser class. The description includes a concrete example showing the difference between expected behavior (usage messages) and actual behavior (stack traces). It provides sufficient context about CommandParser being a subclass of ArgumentParser with extra arguments for error formatting, and explicitly states that these arguments are not copied to subparsers. The issue even suggests the general approach for the solution. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This would take 1-4 hours to solve. While the code change itself is relatively small (about 10 lines), it requires: (1) Understanding Django's management command architecture and how CommandParser extends ArgumentParser, (2) Investigating how argparse subparsers work and how they inherit from parent parsers, (3) Identifying the specific parameters that control error formatting (called_from_command_line), (4) Understanding the use of functools.partial to create a factory function, and (5) Writing appropriate tests. The solution requires good knowledge of both Django internals and argparse behavior, plus some research to understand the inheritance patterns involved.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues identified. The problem is well-defined, the solution is testable, and it addresses a legitimate usability concern in Django's management command system. The issue would work well for evaluating coding ability as it tests understanding of inheritance patterns, library internals, and proper error handling.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16485": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproduction case with exact code that demonstrates the problem: calling floatformat('0.00', 0) and floatformat(Decimal('0.00'), 0) both raise a ValueError about precision parameters. The error message is described (\"precision parameter must be between 1 and MAX_PREC\"), making it clear that the function incorrectly rejects precision=0. An experienced engineer would understand that the fix involves allowing precision=0 as a valid parameter in the floatformat function in django.template.defaultfilters.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that requires 15 minutes to 1 hour. The reproduction case clearly identifies the problem location (floatformat function), and the error message indicates it's a parameter validation issue. An engineer would need to: 1) Locate the floatformat function in django/template/defaultfilters.py, 2) Find the validation logic that checks precision bounds, 3) Modify the condition to allow precision=0. The actual fix is changing one comparison operator from < to <=. Most time would be spent familiarizing with the codebase structure rather than solving the logic problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues identified. The reproduction case is clear, the expected behavior is obvious (precision=0 should be allowed), and the test cases show the expected outputs. This is an ideal sample for evaluating coding ability as it tests understanding of parameter validation logic and requires minimal but precise code changes.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16569": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is exceptionally well-specified. It provides: (1) A clear description of the exact problem - FormSet.add_fields() method fails when index is None under specific conditions (can_delete=True, can_delete_extra=False), (2) The exact location of the bug in django/forms/formsets.py line 493, (3) A complete, self-contained reproduction script that demonstrates the issue, (4) The exact proposed fix with code showing how to handle the None case by adding \"index is not None\" check, and (5) Clear explanation of why the bug occurs (None being compared to integer with '<' operator). An experienced engineer has everything needed to understand and fix this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward 15-minute to 1-hour fix. The issue provides the exact location of the bug (line 493), explains the root cause (None comparison), and even suggests the precise fix needed. The solution involves adding a simple null check (\"index is not None\") to an existing conditional statement. No complex logic, architecture changes, or extensive research is required. An engineer would need minimal time to understand the Django formsets code context, verify the issue with the provided reproduction script, and implement the one-line fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an ideal benchmark sample. The issue is clearly defined, has a simple focused solution, includes a complete reproduction case, and the test patch appropriately verifies the fix by checking that the DELETE field is not present in empty_form.fields when can_delete_extra=False. The fix is localized and doesn't require deep Django framework knowledge beyond understanding basic formsets functionality.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16667": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) A complete, reproducible example with exact Django form code, view function, and URL configuration, (2) The exact URL that triggers the crash with specific parameter values, (3) Clear explanation of the root cause - SelectDateWidget.value_from_datadict converts user input to integers and passes them to datetime.date() without handling OverflowError, (4) The specific line of problematic code: date_value = datetime.date(int(y), int(m), int(d)), (5) A demonstration of the error with Python code showing how datetime.date(sys.maxsize+1, 3, 4) raises OverflowError. The issue clearly states what needs to be fixed: catch the OverflowError in SelectDateWidget.value_from_datadict to prevent server crashes from malicious or invalid input.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The problem is clearly identified in SelectDateWidget.value_from_datadict method where datetime.date() is called without exception handling. The solution requires adding a try-except block around the datetime.date() call to catch OverflowError and return an appropriate fallback value (like \"0-0-0\" as shown in the patch). An experienced engineer would need minimal time to: (1) Locate the SelectDateWidget class in django/forms/widgets.py, (2) Find the value_from_datadict method, (3) Add the try-except block around the datetime.date() call, (4) Choose an appropriate return value for the exception case. The fix is straightforward and localized to a single method.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is a real security-related bug (DoS via server crash), has a clear reproduction case, and tests both the understanding of Django's form widget system and basic exception handling. The solution is simple enough to implement quickly but requires understanding the codebase structure. The test cases provided in the patch are comprehensive and would effectively validate any solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16950": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear information about what's wrong and how to reproduce it. The reporter provides complete model definitions (UUIDModel, Thing, SubThing) and admin.py configuration that demonstrates the problem. The issue describes a specific scenario: when adding a SubThing alongside a Thing in Django admin, an IntegrityError occurs because the UUIDField 'id' on the Thing instance is left unset (null), violating a NOT NULL constraint. The reporter clearly states the expected behavior (it should work like when saving without subthings) and identifies this as a bug in Django. The models show the key details - Thing inherits from UUIDModel which has a UUIDField with default=uuid.uuid4, and SubThing has a ForeignKey to Thing using the 'id' field (to_field='id'). This provides enough context to understand that Django admin inlines are not properly handling UUID default values in this specific configuration.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because it involves understanding Django's formset and inline handling mechanisms, which are complex parts of the framework. Looking at the provided patch, the solution is in django/forms/models.py and involves modifying the add_fields method to handle cases where form data is provided and the to_field is not the parent model's primary key. The engineer would need to: (1) Understand how Django admin inlines work with formsets, (2) Debug why UUID defaults aren't being set properly when form data is present, (3) Identify that the issue is in the add_fields method where to_field.has_default() logic needs refinement, (4) Implement a solution that distinguishes between primary keys and alternate keys when handling defaults. The fix itself is relatively small (about 6 lines of logic), but understanding the problem requires deep knowledge of Django's form handling and the interaction between inlines and default values.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is clearly reproducible, the models and admin configuration are complete, and the expected vs actual behavior is well-defined. The test patch shows comprehensive test coverage for the fix, indicating this is a legitimate bug with a proper solution. The issue involves standard Django functionality that any experienced Django developer should be familiar with, making it suitable for a coding benchmark.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-20859": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) A clear bug summary stating that adding a legend to a SubFigure doesn't work, (2) Complete, minimal reproducible code that demonstrates the exact problem, (3) Clear description of the actual outcome (TypeError with specific error message), (4) Clear expected outcome (the legend should work), (5) The exact root cause and suggested fix - changing line 437 in legend.py to check against FigureBase instead of Figure, with a direct link to the specific code location, (6) Complete environment information. An experienced engineer has everything needed to understand and solve this issue without any ambiguity.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue author has already identified the exact problem and solution: change the isinstance check from Figure to FigureBase in legend.py line 437, and update the corresponding error message. The fix involves changing just 2-3 lines of code in a single file. An experienced engineer would need minimal time to understand the inheritance hierarchy (SubFigure inherits from FigureBase), locate the specific lines mentioned, make the changes, and verify the fix works. The provided gold patch confirms this is indeed a very small, targeted change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is crystal clear, the solution is well-defined, the code change is small but meaningful, and it tests understanding of object-oriented inheritance concepts in Python. The test case provided validates that the fix works correctly. There are no concerns about using this in a coding evaluation.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-22719": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug report with: (1) A minimal reproducible code example that demonstrates the problem, (2) A detailed description of the actual outcome including the specific deprecation warning being triggered, (3) Clear expected outcome - either continue producing artists with no data or provide more accurate error messaging, (4) Additional context about the API change and why the current behavior seems inconsistent. The reporter has done thorough investigation, including attempting the suggested workaround and explaining why it doesn't work. An experienced engineer would have sufficient information to understand that the issue is about a deprecation warning being incorrectly triggered when empty data is passed to axes with category units, and that the solution should prevent this warning for empty data cases.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it involves adding simple size checks (`values.size and is_numlike` and `data.size and convertible`) to prevent warnings from being triggered on empty data. The fix is localized to two places in lib/matplotlib/category.py and requires minimal code changes - just adding `values.size and` and `data.size and` conditions. While it requires understanding the codebase structure to locate the right files and understand the warning logic, the actual fix is straightforward once you identify where the warnings are being generated. The test case is also simple to write.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear reproduction case, specific expected behavior, and a focused solution. The issue demonstrates good software engineering practices in bug reporting and would make an excellent benchmark sample for evaluating coding ability, as it tests understanding of conditional logic, debugging skills, and the ability to write appropriate test cases.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-23299": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug summary stating that `matplotlib.get_backend()` removes all figures from `Gcf` if the first figure was created in an `rc_context`. The reproduction code is complete and runnable, showing exactly when the bug occurs and when it doesn't (with helpful comments about uncommenting lines). The expected vs actual outcome is clearly described - figures should remain in `Gcf.figs` after calling `get_backend()`, but they are being cleared. The consequences are also explained (e.g., `plt.close(fig2)` doesn't work). An experienced engineer would have all the information needed to understand the problem and work toward a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the bug reproduction is clear, solving it requires understanding the interaction between matplotlib's backend system, rc_context implementation, and figure management in Gcf. The engineer needs to trace through how `get_backend()` triggers backend reinitialization, how `rc_context` affects rcParams restoration, and why this causes figure clearing. The actual fix appears relatively small (modifying `rc_context` to exclude 'backend' from restoration), but discovering this solution requires substantial investigation into matplotlib's internal architecture and the root cause of the backend reset behavior.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified with working reproduction code, the problem is non-trivial and requires real debugging skills, and the solution involves understanding matplotlib's internal behavior rather than just surface-level changes. The test case is also straightforward and directly validates the fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-23476": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear bug summary stating that DPI doubles after unpickling on M1 Mac, (2) Complete reproducible code that demonstrates the problem, (3) Actual output showing the DPI doubling from 200.0 to 400.0, 800.0, etc., leading to OverflowError, (4) Expected outcome showing DPI should remain constant at 200.0, (5) Specific platform information (M1 Mac, macOS-12.4-arm64, matplotlib 3.5.2, MacOSX backend). The problem is crystal clear: the DPI value should remain constant after pickle/unpickle cycles but instead doubles each time, specifically on M1 Macs.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "The fix is relatively straightforward once you understand the issue. Looking at the provided solution, it's a simple 3-line addition to the `__getstate__` method in `figure.py` that preserves the original DPI value during pickling by resetting `_dpi` to `_original_dpi` if available. The core insight is that M1 Macs apply device pixel ratio scaling that affects the DPI, and this scaled DPI shouldn't be preserved during pickling. An experienced engineer familiar with matplotlib's codebase could identify that the issue is in the serialization/pickling logic and implement this fix within 15-60 minutes after understanding how matplotlib handles DPI scaling on high-DPI displays.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained bug fix that demonstrates good software engineering practices. The issue is platform-specific but clearly documented, the reproduction case is excellent, and the fix is clean and targeted. The test case provided also properly validates the fix by simulating the device pixel ratio scenario. This would be a good benchmark sample as it tests understanding of both the matplotlib codebase structure and platform-specific rendering behaviors.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24026": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear use case (keeping colors synchronized across different plot types), includes a concrete code example that reproduces the problem, explains exactly what goes wrong (ValueError when stackplot tries to reset axes' color cycle with 'CN' aliases), and describes the expected behavior (stackplot should accept 'CN' color aliases like other matplotlib functions). The problem statement is unambiguous and the desired outcome is clear.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-defined and the solution involves modifying how colors are handled in the stackplot function. Looking at the patch, it's a relatively small change that replaces the problematic `axes.set_prop_cycle(color=colors)` call with `itertools.cycle(colors)` and adjusts how colors are retrieved during plotting. An experienced engineer familiar with matplotlib would need to understand the color cycling mechanism and make targeted changes to a single file, which doesn't require extensive research or major architectural changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution scope is reasonable, and the test case validates the fix appropriately. This is a good candidate for evaluating coding ability as it requires understanding the matplotlib color cycling system and making a targeted fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24149": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) A clear bug summary explaining that ax.bar raises an exception with all-NaN data in matplotlib 3.6.1, (2) Precise code for reproduction showing the exact failing case: ax.bar([np.nan], [np.nan]), (3) Clear description of the actual outcome - a StopIteration exception when trying to find finite x coordinates, (4) Expected behavior based on 3.6.0 where it should return a BarCollection with one Rectangle having nan values, (5) Additional debugging information showing ax.bar([np.nan], [0]) raises but ax.bar([0], [np.nan]) works, indicating the issue is specifically with x position handling. The issue even references the likely cause in the release notes. An experienced engineer has all the information needed to understand the problem and implement a fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly identified as a StopIteration exception when looking for finite elements in all-NaN data. The patch shows the solution is straightforward: add try-except blocks around two existing cbook._safe_first_finite() calls to catch StopIteration and fall back to cbook.safe_first_element(). The fix involves adding just 8 lines of code (two try-except blocks) in a single function (_convert_dx) in one file. An experienced engineer familiar with the codebase would quickly locate the problematic function, understand that the StopIteration needs to be caught, and implement the fallback logic. The solution doesn't require complex algorithmic changes or deep architectural understanding.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal benchmark sample with a clear bug report, specific reproduction case, well-defined expected behavior, and a targeted fix. The test case also properly validates the solution by ensuring all-NaN bar plots work without raising exceptions.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24970": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It provides a minimal code reproduction case that demonstrates the deprecation warnings from NumPy 1.24. The issue states that matplotlib emits deprecation warnings when using `plt.get_cmap()(np.empty((0, ), dtype=np.uint8))`, and the expected outcome is no warnings. The bug summary, reproduction code, actual outcome, and expected outcome are all clearly defined. An experienced engineer can understand that they need to modify matplotlib's colormap handling code to avoid triggering NumPy's deprecation warnings related to out-of-range integer conversion to uint8 arrays.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that involves moving the `np.errstate(invalid=\"ignore\")` context manager to wrap the `astype(int)` conversion operation. The core issue is that NumPy 1.24 now emits deprecation warnings during dtype conversions, and the fix requires wrapping the problematic conversion with proper error state handling. The change is localized to a single function in colors.py and involves restructuring existing error handling rather than implementing new complex logic. An experienced engineer familiar with NumPy and matplotlib would likely identify and implement this solution within 15 minutes to 1 hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is clearly defined, has a specific reproduction case, and the solution involves understanding NumPy deprecation warnings and proper error state handling - all of which are reasonable expectations for an experienced software engineer working with scientific Python libraries.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25311": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear about what needs to be fixed. It provides a concrete reproduction case showing that pickling a matplotlib figure fails when the legend is set to draggable (leg.set_draggable(True)). The error message indicates that FigureCanvasQTAgg cannot be serialized. The expected outcome is clearly stated: \"Pickling successful\". The code example is complete and runnable, making it easy to reproduce the issue. An experienced engineer would understand that they need to fix the pickling mechanism to work with draggable legends by addressing the serialization issue with the canvas object.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is clearly defined, solving it requires: (1) Understanding matplotlib's internal architecture, specifically the offsetbox.py module and how draggable functionality works, (2) Debugging why the canvas object reference prevents pickling, (3) Understanding Python's pickle mechanism and what makes objects serializable/non-serializable, (4) Finding the right approach to fix the issue - converting the canvas attribute to a property that references the canvas dynamically rather than storing a direct reference, (5) Understanding the implications of this change for the draggable functionality. The solution shown in the patch is elegant but not obvious - it requires understanding that the issue is caused by storing a direct reference to the canvas object in the DraggableBase.__init__ method, and that converting it to a property solves the pickling issue while maintaining functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a high-quality issue for a coding benchmark. The reproduction case is clear and minimal, the problem is well-defined, and the solution requires both debugging skills and understanding of Python internals. The test case also properly validates the fix by checking that no canvas references exist in the pickle stream. This would be an excellent test of an engineer's ability to debug serialization issues and understand object lifecycle management in a complex codebase.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-25332": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a complete, minimal reproducible example showing exactly how to trigger the bug. The problem is clear: calling `align_labels()` on a matplotlib figure makes it unpickleable, raising a TypeError about weakref.ReferenceType objects not being pickleable. The expected outcome is straightforward - the figure should remain pickleable after calling `align_labels()`. The reproduction code is self-contained and includes all necessary imports, variable definitions, and the exact sequence of operations that cause the failure. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is clearly defined, solving it requires understanding matplotlib's internal architecture, specifically how the `align_labels()` function works and why it creates unpickleable weak references. The engineer needs to: (1) investigate the `align_labels()` implementation to understand what weak references it creates, (2) understand Python's pickle protocol and why weak references can't be pickled, (3) implement custom `__getstate__` and `__setstate__` methods to handle the pickle/unpickle process by converting between weak and strong references. The solution shown in the patch involves adding substantial logic to the Grouper class in cbook.py, requiring knowledge of weakref handling and pickle protocols. This goes beyond a simple bug fix and requires architectural understanding.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-structured bug report with clear reproduction steps, expected vs actual behavior, and the test patch appropriately verifies the fix by adding `align_ylabels()` to the existing pickle test suite. The issue represents a legitimate functionality gap where a core matplotlib feature (align_labels) breaks another core feature (pickling), making it suitable for a coding benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-25479": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, reproducible example showing exactly what goes wrong: when a colormap is created with one name and registered with a different name, plt.set_cmap() fails to find it even though cm.get_cmap() works. The issue includes specific code examples, error descriptions, and explains the root cause - that matplotlib has inconsistent name lookup behavior between different parts of the API. The expected behavior is clear: users should be able to register a colormap with any name and then use that registered name consistently across all matplotlib functions.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding matplotlib's colormap registration and lookup system across multiple modules (cm.py, colors.py), (2) The solution involves changes to both the registration logic and the equality comparison logic, (3) It requires understanding the interaction between different parts of the matplotlib API and how they handle colormap names differently, (4) The developer needs to identify that the issue stems from inconsistent name handling between the registration system and the pyplot interface, and (5) Writing appropriate tests to cover the edge case. While not extremely complex, it requires substantial investigation into the codebase architecture.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the expected behavior is reasonable, and the solution scope is well-contained within matplotlib's colormap handling system. The issue demonstrates good software engineering practices with clear reproduction steps and examples.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-26291": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear bug summary explaining the problem with creating inset axes, (2) Minimal reproducible code that demonstrates the issue, (3) The exact error being encountered (AttributeError with '_get_renderer'), (4) A detailed technical explanation of what's happening (figure renderer not initialized, OffsetBox calling get_window_extent, etc.), (5) Clear expected outcome (empty box in top right of subplot), and (6) Complete environment details. An experienced engineer has all the information needed to understand that this is a renderer initialization issue in the inset_locator module when the figure's renderer is None.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides a clear technical explanation pointing directly to the problem: the figure renderer is None when _get_renderer() is called. Looking at the gold patch, the solution is just a 2-line null check that gets the renderer from ax.figure._get_renderer() when renderer is None. The fix location is clearly indicated by the error traceback (inset_locator.py in the __call__ method), and the solution logic is straightforward - just handle the case where renderer is None. This requires minimal code change and basic defensive programming logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is well-defined, the solution is targeted and simple, and the test case appropriately validates the fix by reproducing the original failure scenario with bbox_inches=\"tight\" which triggers the renderer issue.",
            "q2_5_confidence": 5
        }
    },
    {
        "psf__requests-1724": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: using unicode method names (like u'POST' instead of 'POST') causes a UnicodeDecodeError in Python 2.7.2. The issue provides two concrete code examples - one that works and one that fails. It explains the root cause: the unicode string is \"infecting\" the header when it should be a byte string, and points to the specific line in sessions.py:313 where req.method = method.upper() occurs. The issue author even provides their hypothesis about what's happening (unicode/byte string mixing during HTTP message building). This gives an experienced engineer everything needed to understand the problem and devise a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified in the issue description, and the solution shown in the patch is quite simple: adding method = builtin_str(method) before the existing method.upper() call. An experienced engineer would need to: 1) Understand that unicode strings need to be converted to byte strings in Python 2.7, 2) Find the builtin_str utility in the codebase (already imported in compat module), 3) Add the single line conversion. The fix requires minimal code change and understanding of Python 2/3 compatibility, but doesn't require extensive research or complex logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is straightforward, and the test case properly validates the fix by ensuring unicode method names work correctly. This is a good benchmark sample as it tests understanding of Python 2/3 compatibility issues and string handling in HTTP libraries.",
            "q2_5_confidence": 5
        }
    },
    {
        "psf__requests-5414": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: (1) The problem: attempting to get \"http://.example.com\" raises a UnicodeError instead of the expected InvalidURL exception, (2) The expected behavior: should raise InvalidURL with message \"URL has an invalid label\" based on PR #774, (3) Exact reproduction steps with a simple code snippet, (4) Current behavior vs desired behavior is clearly contrasted, (5) References existing similar handling for URLs starting with \"*\" in the codebase at line 401 in requests/models.py. The issue provides all necessary context including system information and links to relevant code sections. An experienced developer would have clear understanding of what needs to be fixed - catching UnicodeError for URLs with invalid labels (like those starting with \".\") and converting it to InvalidURL exception, similar to existing handling for \"*\" prefixed URLs.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves a small, focused change to the existing URL validation logic in requests/models.py. Looking at the gold patch, it's literally a one-line change: modifying the condition `elif host.startswith(u'*'):` to `elif host.startswith((u'*', u'.')):` to also catch URLs starting with \".\". The logic and pattern already exists for handling \"*\" prefixed URLs, so extending it to handle \".\" prefixed URLs is straightforward. The developer needs to: (1) locate the existing URL validation code (clearly referenced in the issue), (2) understand the current pattern for handling invalid URL labels, (3) extend the condition to include \".\" prefix. The test changes are also minimal - just adding two test cases. No complex logic, no architectural changes, and minimal code understanding required beyond the specific validation function.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. This is an excellent benchmark sample - the issue is clearly described, has a focused solution scope, includes proper reproduction steps, and the fix is verifiable through the provided test cases. The solution directly addresses the stated problem and follows existing patterns in the codebase.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-3151": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: `combine_by_coords` fails when y-coordinate labels are the same across datasets but not in sorted order, causing a ValueError about non-monotonic indexes. The description includes a specific example with `yCoord = ['a', 'c', 'b']` that triggers the issue. However, there are some gaps that require filling in: (1) The exact function signature and module location of `combine_by_coords` isn't specified, (2) The complete error message or stack trace isn't provided, and (3) The expected behavior after fixing isn't explicitly stated. Despite these gaps, an experienced engineer could reasonably infer that the function should handle non-monotonic coordinates in non-concatenated dimensions without raising errors, which aligns with the actual solution that modifies the monotonicity check to only apply to concatenation dimensions.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) Understanding the problem requires familiarity with xarray's coordinate system and how `combine_by_coords` works with multi-dimensional data, (2) The engineer needs to trace through the code to understand why the monotonicity check is failing and identify that it's incorrectly applied to all dimensions rather than just concatenation dimensions, (3) The solution involves modifying the logic in the `combine_by_coords` function to change from checking `concatenated.dims` to `concat_dims`, which requires understanding the difference between these two sets, (4) Testing the fix requires creating appropriate test cases with non-monotonic bystander dimensions. While the actual code change is small (a few lines), the conceptual understanding and debugging required puts this in the 1-4 hour range.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-constructed sample for a coding benchmark. The issue description provides sufficient context to understand the problem, the solution requires meaningful domain knowledge about data structures and coordinate systems, and the fix involves both understanding the existing logic and modifying it appropriately. The test case demonstrates clear validation criteria. The issue represents a realistic debugging scenario that tests both code comprehension and problem-solving skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-3677": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: merging a DataArray into a Dataset works with the top-level xr.merge() function but fails with the ds.merge() method. The issue includes a minimal, complete code example that demonstrates both the working case and the failing case, making it easy to reproduce the problem. It also explains the technical cause of the failure (DataArray lacks an items() method that the merge logic expects). The expected behavior is clear from the working example with xr.merge([ds, da]), so an engineer knows exactly what the ds.merge(da) method should accomplish. This gives sufficient information for an experienced engineer to understand the problem and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-defined and the solution is straightforward - the code needs to handle DataArray inputs by converting them to Datasets before proceeding with the existing merge logic. Looking at the patch, the fix is literally one line of code that checks if the input is a DataArray and converts it to a Dataset if so. An experienced engineer familiar with xarray would quickly understand that DataArrays can be converted to single-variable Datasets using to_dataset(), and the existing merge logic can handle the rest. The main time would be spent understanding the codebase structure to find where to add this check, but given the error message and stack trace, it would be relatively easy to locate the merge method in the Dataset class.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a clean, well-defined issue that would work well in a coding benchmark. The problem is clearly specified, has a straightforward solution, and the test case is simple and focused. The fix requires understanding of the xarray API (specifically that DataArrays can be converted to Datasets) but doesn't involve complex algorithmic thinking or deep architectural changes. An engineer working on this would need to understand the relationship between DataArrays and Datasets in xarray, which is fundamental domain knowledge for working with this library.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-4094": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A complete, runnable code example that reproduces the problem, (2) The exact error message that occurs (MergeError about conflicting values for variable 'y'), (3) Clear expected behavior (working roundtrip between stacked and unstacked), (4) A concise problem description explaining the context (need to stack/unstack variables), and (5) Complete version information. An experienced engineer has all the information needed to understand the problem and work toward a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides a clear reproduction case that immediately highlights the problem. The error message about conflicting values during merge gives a strong hint about the root cause. Looking at the actual fix, it's a simple one-line change adding `drop=True` to a `sel()` call in the `to_unstacked_dataset` method. An experienced engineer would need some time to trace through the stacking/unstacking logic and understand why coordinates conflict in the single-dimension case, but the fix itself is straightforward once the cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, has a specific reproduction case, and the expected outcome is unambiguous. The fix is localized and doesn't require deep architectural changes. This would make a good benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-4695": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a complete, reproducible code example showing the exact problem: when a DataArray dimension is named \"method\", calling .loc with that dimension key fails because xarray confuses it with the fill method parameter. The issue clearly states the expected behavior (dimension names should be irrelevant) and provides specific error context (ValueError because it expects \"pad\", \"backfill\", or \"nearest\"). The code sample is copy-pastable and demonstrates both working (D1) and failing (D2) cases, making the problem crystal clear.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified in the issue, and the solution shown in the patch is quite simple: changing from `self.data_array.sel(**key)` to `self.data_array.sel(key)` in the DataArray.__getitem__ method. The issue is that using **key unpacks the dictionary, causing the \"method\" dimension name to be interpreted as a keyword argument to sel(). The fix requires understanding this parameter unpacking behavior and making a one-line change. While it requires some understanding of Python's **kwargs mechanism and the xarray codebase structure, the actual implementation is straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - the problem is well-defined, reproducible, and the solution tests a developer's understanding of Python parameter passing mechanisms and debugging skills. The fix is clean and focused, making it suitable for evaluating coding ability.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-6599": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a complete, runnable example that demonstrates the exact problem, shows the expected output from the working version (v2022.3.0) versus the incorrect output from the latest version, and clearly describes what's wrong (dimensions swapped, unreasonably large values, timedelta64 coordinates being treated incorrectly). The user has followed good practices by providing an MVCE (Minimal, Complete, Verifiable Example) with all necessary imports and data. An experienced engineer could easily reproduce this issue and understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly localized to how timedelta64 data types are handled in the polyval function. Looking at the patch, it's a small but important change in the `_ensure_numeric` helper function where timedelta64 types (dtype.kind == 'm') need to be converted differently than datetime64 types (dtype.kind == 'M'). The fix involves splitting the existing condition and adding a simple `.astype(float)` conversion for timedeltas. An experienced engineer familiar with numpy dtypes and the codebase structure could identify and implement this fix relatively quickly after understanding the problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, a focused solution, and appropriate test coverage. The issue demonstrates good software engineering practices in problem reporting and would be suitable for evaluating coding ability.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-6721": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: accessing the `chunks` attribute on a zarr-backed xarray dataset loads the entire array into memory instead of just inspecting metadata. The issue provides a concrete reproduction example with a specific URL, explains the expected behavior (should inspect the `encoding` attribute on DataArrays), and includes detailed log output explaining that xarray tries to convert variables' data into NumPy arrays to check for a `.chunks` attribute, which forces Zarr to fetch all chunk files. The problem statement is unambiguous and the desired outcome is clear.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is remarkably simple: changing `v.data` to `v._data` in the `get_chunksizes` function in `xarray/core/common.py`. The issue is that accessing `v.data` triggers data loading, while `v._data` accesses the underlying data object without loading. An experienced engineer would need time to: 1) Understand the xarray codebase structure and locate where chunks are processed, 2) Identify that `get_chunksizes` is the problematic function, 3) Realize that `v.data` property triggers loading while `v._data` doesn't, and 4) Implement and test the fix. The conceptual understanding and code change are straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly described, the solution is focused and surgical, and the test case appropriately validates that chunks access doesn't trigger data loading. This would make a good benchmark item as it tests understanding of lazy loading concepts and property vs attribute access patterns in Python.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-6386": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the short option `-v` expects an argument while the long option `--verbose` works correctly without one. The expected behavior is explicitly stated - both should work the same way. The issue includes the exact command that fails (`pylint mytest.py -v`), the error behavior (displays usage and reports missing argument), the working command (`pylint mytest.py --verbose`), and version information. An experienced engineer would have all the information needed to understand that they need to make the `-v` flag behave like `--verbose` by not requiring an argument.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the patches, the solution involves: 1) Adding metavar parameter support to the argument classes, 2) Adding `-v` to the preprocessing options dictionary, 3) Modifying the preprocessing logic to handle short options, and 4) Adding metavar=\"\" to prevent argument expectations. While it touches multiple files (argument.py, arguments_manager.py, utils.py, base_options.py), the changes are relatively small and straightforward. The core logic change is adding `-v` to the preprocessing options and ensuring metavar is properly handled. An experienced engineer familiar with argparse and the codebase structure could identify and implement this solution within an hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. The problem is clearly defined, the solution scope is reasonable, and the test case provided validates the fix appropriately. This would be a good benchmark sample for evaluating coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "pylint-dev__pylint-6903": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear understanding of what needs to be fixed. The problem is clearly described: when running pylint with --jobs=0 in a Kubernetes Pod, the _query_cpu() function returns 0, which causes multiprocessing.Pool to crash because it requires at least 1 process. The issue provides specific details including the exact error, the problematic calculation (2/1024 = 0 when cast to int), the files being read (/sys/fs/cgroup/cpu/cpu.cfs_quota_us and cpu.shares), and even suggests a solution (\"append a ` or 1` at the end of this line\"). The expected behavior is clear: pylint should not crash and should handle the case where calculated CPU count is 0. The location of the problem is specified (pylint/lint/run.py line 60), making it straightforward to understand what needs to be implemented.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a simple fix that requires 15 minutes to 1 hour. The problem is well-defined: prevent the CPU count from being 0 by adding a fallback to 1. Looking at the gold patch, the solution is just a few lines of code that check if avail_cpu == 0 and set it to 1 if so. The logic is straightforward and the location is clearly identified in the issue. An experienced engineer familiar with the codebase would quickly understand the problem, locate the _query_cpu() function, and implement the simple conditional check. The most time would be spent understanding the context around CPU detection in containerized environments, but the actual code change is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained, the solution is straightforward, and the test coverage appears adequate. The issue represents a good benchmark sample as it tests understanding of edge cases in system resource detection and basic defensive programming practices.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-8898": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: pylint's `bad-names-rgxs` option incorrectly splits regular expressions on commas, which breaks regex patterns that legitimately contain commas (like quantifiers `{1,3}`). The issue provides a concrete example configuration that demonstrates the bug, shows the exact error that occurs (\"missing ), unterminated subpattern\"), and states the expected behavior clearly - that any valid regular expression should be expressible in this option. The reproduction steps are simple and clear. An experienced engineer would have all the information needed to understand that they need to modify how pylint parses comma-separated regex lists to handle commas within regex quantifiers properly.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task. While the problem is clearly defined, the solution requires: (1) Understanding how pylint's configuration parsing works, specifically the `_regexp_csv_transfomer` function in `pylint/config/argument.py`, (2) Designing a parser that can distinguish between commas that separate regex patterns vs commas that are part of regex syntax (like quantifiers), (3) Implementing the `_check_regexp_csv` function that correctly handles brace-delimited quantifiers, (4) Adding proper tests. The solution involves creating a stateful parser that tracks open/close braces, which requires careful thought about edge cases. The patch shows this touches multiple files and requires substantial new logic, making it more than a simple fix but not extremely complex.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, has a clear reproduction case, and the expected behavior is unambiguous. The solution space is constrained enough that different engineers would likely arrive at similar approaches. The test cases provided in the patch demonstrate that the solution can be objectively verified.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10051": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It clearly describes the problem: caplog.get_records() becomes decoupled from caplog.records after caplog.clear() is called, causing get_records() to freeze and not reflect new log entries. The issue provides a complete reproductive example that demonstrates the exact problem, shows the expected vs actual behavior, and even includes links to the specific lines of code in the codebase where the problem occurs. The root cause is explained clearly: caplog.clear() replaces the records list rather than clearing it, which breaks the connection with get_records(). An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides direct links to the problematic code in logging.py, clearly explains the root cause (records list being replaced instead of cleared), and includes a minimal reproduction case. The solution requires understanding that the handler.reset() method creates a new list instead of clearing the existing one, then implementing a new clear() method that properly clears the list in-place. The actual code change is small - adding a new clear() method to the handler and modifying one line in the LoggingPlugin.clear() method. Most of the time would be spent understanding the codebase structure and writing appropriate tests.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the problem is clearly specified, has a well-defined solution, includes a reproduction case, and tests the engineer's ability to understand object reference semantics and implement a targeted fix. The issue demonstrates good software engineering practices with clear problem description and links to relevant code.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10081": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a specific bug where unittest.TestCase.tearDown() is executed for classes marked with @unittest.skip when running pytest with --pdb flag. The issue provides: (1) A clear minimal reproducible example in test_repro_skip_class.py showing the problematic behavior, (2) Expected vs actual behavior - the test should be skipped entirely but tearDown is being called when --pdb is used, (3) Complete version information and environment details, (4) Reference to a similar previous issue (#7215) for context, and (5) Clear demonstration of the error (NameError from undefined 'xxx' in tearDown). An experienced engineer would understand exactly what needs to be fixed: prevent tearDown execution for skipped test classes when using --pdb, similar to how it already works for skipped test methods.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue is clearly localized to pytest's unittest handling logic, specifically around the --pdb flag behavior. Looking at the gold patch, the fix is quite straightforward: modify the condition in src/_pytest/unittest.py to check if either the test method OR the parent class is skipped before deciding whether to postpone tearDown. The change is only a few lines of code that adds a check for _is_skipped(self.parent.obj) in addition to the existing _is_skipped(self.obj) check. An experienced engineer familiar with the codebase could identify this logic relatively quickly by following the --pdb tearDown handling code and understanding the existing fix for method-level skips.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, good reproducibility, and a focused solution scope. The issue has good test coverage and the expected behavior is unambiguous.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5262": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that `_pytest.capture.EncodedFile` incorrectly advertises a mode of `rb+` (including the binary flag 'b') from its underlying stream, but its `write()` method only accepts text strings, not bytes. When youtube-dl checks the mode and sees 'b', it tries to write bytes, causing a TypeError. The issue provides a clear minimal reproduction case with specific package versions and a concrete test case. The root cause is clearly identified: the mode property should not include 'b' since the EncodedFile wrapper only handles text. The expected behavior is implicit but clear - the mode should not include 'b' so that external libraries don't attempt to write bytes to it.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified and the solution is straightforward: add a mode property to the EncodedFile class that returns the underlying buffer's mode with 'b' removed. Looking at the gold patch, it's just 4 lines of code adding a property method. An experienced engineer would need some time to understand the pytest capture system and locate the EncodedFile class, but once found, the fix is simple. The test is also straightforward - just verify that sys.stdout.mode doesn't contain 'b' when using capfd.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is well-documented with a clear reproduction case, the problem is well-defined, and the solution requires understanding the interaction between pytest's capture mechanism and external libraries. The fix is non-trivial enough to be meaningful but not so complex as to be unreasonable for a benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5631": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear problem statement - pytest 3.6.0 fails when collecting tests that use @patch with numpy arrays as the 'new' parameter, (2) A specific error message - ValueError about \"truth value of an array with more than one element is ambiguous\", (3) A concrete example showing the problematic code pattern: @patch(target='XXXXXX', new=np.array([-5.5, 3.0])), (4) Context about when it worked (pytest 3.1.3) vs when it fails (pytest 3.6.0), (5) Root cause analysis pointing to a specific commit that introduced the regression, and (6) Technical explanation of why the problem occurs - the check \"p.new in sentinels\" returns an array of booleans instead of a boolean when p.new is a numpy array. The issue provides sufficient detail for an engineer to understand both what needs to be fixed and why the current approach fails.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because: (1) The problem is clearly identified - the issue is in the sentinel comparison logic in num_mock_patch_args function, (2) The root cause is well-explained - using 'in' operator with numpy arrays causes truth value ambiguity, (3) The solution is straightforward - replace equality-based comparison with identity-based comparison (using 'is' instead of 'in'), (4) The fix involves modifying a single function in one file (src/_pytest/compat.py), changing maybe 5-10 lines of code, (5) No complex algorithm changes or architectural modifications are needed. An experienced engineer would quickly understand that they need to change the sentinel detection logic to use identity comparison rather than equality comparison to avoid triggering numpy's __eq__ method.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement, identifiable root cause, and straightforward solution path. The issue provides good reproducible context and the fix doesn't require deep architectural knowledge or complex design decisions. It's suitable for a coding benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5787": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly describes a problem where chained exceptions are not properly serialized when using pytest-xdist. The issue provides concrete test cases that demonstrate the problem, shows the expected behavior (full exception chain displayed without xdist) versus actual behavior (only final exception shown with xdist), and includes version information. The problem is that exception serialization in distributed testing should preserve the entire exception chain, not just the final exception. An experienced engineer would understand they need to modify the serialization/deserialization logic in pytest's reporting system to handle chained exceptions properly.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a complex issue requiring substantial research and implementation. The engineer needs to: 1) Understand pytest's internal reporting and serialization mechanisms, 2) Learn how Python's exception chaining works (both explicit 'from' chaining and implicit chaining), 3) Identify where in the codebase exception serialization happens for xdist, 4) Implement proper serialization/deserialization for ExceptionChainRepr objects, 5) Handle both types of chained exceptions correctly. The solution involves modifying core serialization logic in _pytest/reports.py, adding new imports, and creating substantial new functions (_report_to_json and _report_kwargs_from_json) with over 100 lines of new code. The patch shows this required deep understanding of pytest's internals and careful handling of complex nested data structures.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly described with reproducible test cases, the solution space is well-defined (fix exception serialization for distributed testing), and the provided patches demonstrate this is a legitimate bug fix rather than a feature request. The issue would be suitable for evaluating coding ability as it requires understanding both Python exception handling and pytest's architecture.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-5809": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies: (1) The problem: HTTP 400 errors when using lexer=python3 with certain content in the --pastebin feature, (2) The root cause: pytest output is not Python code but arbitrary text, so using python3 lexer is inappropriate, (3) The exact location in code: src/_pytest/pastebin.py lines 68-73, (4) The proposed solution: change lexer from \"python3\" to \"text\", and (5) References issue #5764 for additional context. The issue provides specific technical details including the URL endpoint, error type, and even mentions that the solution works when tested. An experienced engineer would have all the information needed to implement the fix.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix taking less than 15 minutes. The solution involves changing a single string literal from \"python3\" to \"text\" in the params dictionary. The exact file and location are specified (src/_pytest/pastebin.py), and the gold patch confirms it's literally a 1-line change removing the conditional logic and simply setting lexer to \"text\". No research, complex logic, or multiple file changes are required - it's a straightforward parameter change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. The problem is clearly defined, the solution is straightforward, and the test changes are minimal (updating expected values). This is an excellent example for evaluating an engineer's ability to understand a well-defined issue and implement a simple fix correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5840": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem: after upgrading pytest from 5.1.1 to 5.1.2, conftest.py fails to load because the project path is being converted to lowercase on Windows, causing import system conflicts. The issue mentions that \"the upgrade forced the project path into lowercase, so the import system looks for a top-level module named 'python' which doesn't exist.\" While the description gives a clear symptom and context, there are some gaps in the technical details - it doesn't specify exactly which part of the pytest codebase is causing the path case conversion, or the precise mechanism by which this breaks imports. However, the core problem (Windows case-insensitive filesystem issues with path handling) is identifiable and provides enough context for an experienced engineer to investigate and solve.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: (1) It requires understanding pytest's conftest loading mechanism and path handling across multiple files (config/__init__.py, pathlib.py), (2) The engineer needs to understand Windows case-insensitive filesystem behavior and how it interacts with Python's import system, (3) The solution involves replacing the custom unique_path function with pathlib.Path.resolve() and updating multiple parts of the codebase that handle conftest path resolution, (4) The fix touches both core configuration logic and path utilities, requiring careful testing to ensure no regressions. While not extremely complex, it requires system-level understanding and touches multiple components.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample because it represents a real-world platform compatibility issue that requires understanding of filesystem behavior, path handling, and import mechanisms. The problem has a clear failure mode that can be tested, and the solution involves both removing problematic code and replacing it with more robust alternatives. The test changes demonstrate that the solution can be validated programmatically.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6197": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a regression in pytest 5.2.3 where pytest inappropriately tries to import __init__.py files during test collection. The reporter provides: (1) A clear problem statement explaining the regression behavior, (2) Exact version information showing it worked in 5.2.2 but fails in 5.2.3, (3) A complete minimal reproduction case using tox with specific commands that can be run to reproduce the issue, (4) Clear expected behavior (should not import random __init__.py files) vs actual behavior (imports and fails on assert False). The issue gives enough context about the problem domain (test collection) and the specific failure mode (AssertionError during collection) for an experienced engineer to understand what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is clearly described, fixing it requires: (1) Understanding pytest's complex collection mechanism and how it changed between versions, (2) Debugging why __init__.py files are being inappropriately collected/imported, (3) Identifying the root cause in the collection logic, (4) Implementing a fix that prevents eager collection without breaking legitimate functionality, (5) Writing appropriate tests. The patch shows significant changes to core collection logic in PyobjMixin class, removing eager object mounting, and refactoring the Module class. This involves understanding subtle timing issues in when objects get imported during collection phases.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue sample is suitable for evaluating coding ability. It tests understanding of complex software behavior (test collection mechanisms), debugging skills (identifying regression causes), and implementation skills (fixing collection logic without side effects). The reproduction case is self-contained and the expected behavior is clear from the regression description.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6202": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) A clear description of the problem - '.['  being incorrectly replaced with '[' in test report headlines, (2) A concrete example showing the bug in action with \"test_boo[.[\" being displayed as \"test_boo[.[\" instead of the correct \"test_boo[.[]\", (3) Detailed source code analysis tracing the exact problem to line `return s.replace(\".[\", \"[\")` in the getmodpath() method in _pytest/python.py, (4) The exact file paths and line numbers where the problematic code exists, (5) A proposed solution to replace the problematic line with `return s`, and (6) Confirmation that this fix passes all tests. The issue author has done thorough debugging work and pinpointed the exact cause and solution.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix taking less than 15 minutes. The issue description provides the exact location of the problematic code (line in getmodpath() method) and the exact solution (remove the .replace(\".[\", \"[\") call). An experienced engineer would simply need to: (1) Navigate to src/_pytest/python.py, (2) Find the getmodpath() method, (3) Change `return s.replace(\".[\", \"[\")` to `return \".\".join(parts)`, and (4) Run tests to confirm the fix. The solution involves changing literally one line of code, and the issue author has already verified this fix works.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. This is actually an ideal benchmark sample - the problem is clearly described, the solution is straightforward, and it tests the engineer's ability to understand pytest internals and make a precise fix. The issue demonstrates good debugging skills in tracing through the codebase, and the fix is simple enough that success/failure would be clear-cut.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7205": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when running pytest with `--setup-show` and the `-bb` flag (which turns BytesWarnings into exceptions), pytest fails because it tries to implicitly convert bytes parameters to strings in the fixture reporting code at `src/_pytest/setuponly.py:69`. The issue provides a minimal reproducible example with the exact command that triggers the problem (`python3 -bb -m pytest --setup-show`) and suggests a specific solution approach (using `saferepr` instead of implicit `str()` conversion). The location of the problematic code is pinpointed, and the expected behavior is clear - the setup should work without raising BytesWarnings when displaying bytes parameters.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15-60 minutes. The problem is clearly identified at a specific line in setuponly.py where implicit string conversion happens. The solution involves importing saferepr and using it instead of the implicit str() conversion. Looking at the actual patch, it's a simple 2-line change: adding an import and replacing the format call with saferepr. The engineer would need some time to understand the pytest codebase structure and locate the exact issue, but once found, the fix is minimal and the approach is suggested in the issue description.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear reproduction case, specific error location, and straightforward solution. The issue description provides all necessary context, including the exact command line flags needed to reproduce the problem and the specific file/line where the issue occurs.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7236": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear minimal reproduction case with exact Python code, specific environment details (Python 3.6.10, pytest 5.4.2), and demonstrates the exact problem: when running pytest with --pdb, tearDown is unexpectedly executed on skipped tests, causing errors. The issue clearly states the expected behavior (tests should be skipped even with --pdb) and notes that this worked correctly in pytest 5.4.1 but broke in 5.4.2. The problem statement is concrete, reproducible, and unambiguous.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-isolated to the interaction between --pdb and skipped unittest tests. Looking at the gold patch, the solution involves adding a check for _is_skipped(self.obj) in the runtest method and creating a small helper function _is_skipped. The fix is conceptually straightforward - just need to avoid executing tearDown for skipped tests when --pdb is enabled. It requires understanding the pytest unittest integration code and the flow of test execution, but the actual code changes are minimal and targeted.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, has a minimal reproduction case, and the solution scope is well-bounded. The test case provided in the test patch also validates that the fix works correctly for both @unittest.skip and @pytest.mark.skip decorators, ensuring comprehensive coverage of the issue.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7324": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It provides a clear reproducer showing that `Expression.compile(\"False\")` causes a Python interpreter crash on debug builds for Python 3.8+, with an assertion failure in compiler_nameop. The core problem is identifiable: certain identifiers like \"False\" are being rejected in the compilation context. However, the issue description lacks some important details: (1) it doesn't explain what Expression.compile() is supposed to do or what the expected behavior should be, (2) it doesn't provide context about what the Expression class is or where it's located in the codebase, and (3) it only mentions \"False\" but doesn't clarify if other similar identifiers like \"True\" or \"None\" have the same issue. An experienced engineer could reasonably infer that the goal is to make Expression.compile() handle these Python reserved words without crashing, but some investigation would be needed to understand the full scope and intended behavior.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the core problem (handling reserved Python identifiers in expression compilation) is conceptually straightforward, the solution requires understanding how AST compilation works and identifying the root cause of the name rejection. The engineer needs to: (1) locate the Expression class and understand its purpose in the pytest codebase, (2) understand why certain identifiers cause compilation failures, (3) devise a strategy to handle reserved words (like prefixing them as shown in the patch), and (4) implement the solution across multiple functions (expression parsing and matching). The patch shows this involves modifying the AST generation to prefix identifiers and updating the matcher to handle the prefixed names. This requires moderate understanding of Python AST manipulation and careful consideration of how the change affects the broader system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant additional issues. The problem is technical but well-contained within the expression compilation system. The reproducer is clear and the crash behavior is definitive, making it easy to verify when a solution works. The linked bpo issue provides additional context about the Python interpreter behavior, though the main issue is solvable without deep knowledge of CPython internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7490": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear description of the regression between pytest versions: dynamically adding xfail markers using `request.node.add_marker(mark)` worked in pytest 5.x but fails in 6.0.0rc0. The issue includes a minimal reproducible test case in `test_foo.py`, shows the expected behavior (test marked as xfailed) versus actual behavior (test fails), and specifies exact versions. The problem is concrete: dynamic xfail markers should behave the same as static ones. An engineer would clearly understand they need to fix the dynamic marker evaluation logic in pytest's test execution flow.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is clearly defined, solving it requires understanding pytest's internal test execution flow, specifically how markers are evaluated and when xfail logic is applied. The engineer needs to trace through the skipping.py module to understand when `evaluate_xfail_marks()` is called versus when markers might be dynamically added. The solution involves modifying the hook execution order and adding logic to re-evaluate xfail marks after test execution, which requires careful consideration of pytest's plugin architecture and test lifecycle. The patch shows changes across multiple functions in the same file, indicating moderate complexity.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, has a clear test case, and the solution can be verified objectively. The test patch provides good coverage for both the failing and passing scenarios with strict mode. This is an excellent candidate for a coding benchmark as it tests understanding of plugin systems, test execution lifecycle, and regression fixing skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7521": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a regression in pytest 6.0.0rc1 where capfd.readouterr() incorrectly converts carriage return characters (\\r) to newline characters (\\n). The issue provides: (1) A concrete failing test case from borgbackup showing the expected vs actual behavior, (2) A minimal reproducer that demonstrates the exact problem, (3) Clear comparison showing it worked in pytest 5 but fails in pytest 6.0.0rc1, and (4) Specific expected behavior (carriage returns should be preserved, not converted to newlines). The problem is unambiguous - the capture functionality should preserve the original line endings exactly as they were written.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly a regression in text capture behavior, and the provided patch shows it's a simple one-line fix adding newline=\"\" parameter to TextIOWrapper in the capture.py file. An experienced engineer would need to: (1) Understand the capture mechanism in pytest (moderate complexity), (2) Identify that TextIOWrapper is performing universal newline translation by default, (3) Recognize that adding newline=\"\" parameter disables this translation. The actual code change is minimal, but requires understanding how Python's text I/O handles newline translation and locating the right place in the capture code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a straightforward regression fix with clear expected behavior, good test coverage, and minimal code changes required. The issue is well-suited for evaluating coding ability as it tests understanding of Python I/O behavior and pytest internals.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10297": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes that RidgeClassifierCV doesn't accept the store_cv_values parameter despite documentation claiming it should. The issue provides: (1) A complete reproducible code example showing the TypeError when using store_cv_values=True, (2) Clear expected vs actual results, (3) Reference to existing documentation that mentions cv_values_ attribute should be available when store_cv_values=True, (4) Version information. The problem is unambiguous: the RidgeClassifierCV constructor needs to accept the store_cv_values parameter and implement the functionality to store cross-validation values, matching the behavior already present in RidgeCV.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that would take 15-60 minutes. Looking at the patches, the solution involves: (1) Adding store_cv_values parameter to RidgeClassifierCV.__init__(), (2) Passing it to the parent class constructor, (3) Updating documentation to match RidgeCV's format. The functionality already exists in the parent _BaseRidgeCV class, so no new implementation is needed. The engineer would need to examine RidgeCV to see how store_cv_values is implemented, then replicate the same pattern in RidgeClassifierCV. This is a simple parameter addition and documentation update rather than complex algorithmic work.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with clear requirements and a straightforward solution pattern already established in the codebase. The test coverage shows the expected behavior clearly.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10908": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: CountVectorizer's get_feature_names() method raises a NotFittedError even when a vocabulary parameter is provided at initialization, which is inconsistent behavior since transform() works fine with a provided vocabulary without fitting. The issue provides concrete code examples showing the problem scenario, demonstrates the current behavior with hasattr() checks, and explains the root cause (that transform() calls _validate_vocabulary() which sets vocabulary_ but get_feature_names() doesn't). The expected behavior is clearly stated: get_feature_names() should not raise NotFittedError when vocabulary is provided, similar to how transform() behaves.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The solution is straightforward: add a call to _validate_vocabulary() in the get_feature_names() method before the existing _check_vocabulary() call, similar to how transform() handles it. The code change is minimal (3 lines as shown in the patch), and the logic is clear from the issue description and existing codebase patterns. An experienced engineer would quickly identify that they need to follow the same pattern used in transform() method.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean, well-documented bug with a clear solution pattern already established in the codebase. The test cases appropriately verify both the fix and the existing functionality, ensuring the solution maintains backward compatibility while fixing the inconsistency.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-12585": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear description of the problem: the `clone` function fails when parameters are estimator types (classes) rather than instances. The issue includes a minimal, reproducible example showing exactly how to trigger the bug: `clone(StandardScaler(with_mean=StandardScaler))`. The expected behavior is clearly stated (no error should occur), and the actual behavior is described (TypeError from calling `get_params` on a class). Most importantly, the issue author even provides a specific proposed fix with the exact line number and code change needed in `base.py` line 51. The explanation of why this happens (attempting to call `get_params` on a class instead of an instance) demonstrates a clear understanding of the root cause.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue author has already identified the exact problem location (base.py line 51) and provided the specific code change needed: adding `or isinstance(estimator, type)` to the condition. The fix is a single line change that adds a type check to prevent the clone function from attempting to call `get_params` on class types. An experienced engineer would need to understand the clone function logic, verify the proposed fix makes sense, and add appropriate tests. The gold patch confirms this is indeed a minimal change - just adding the isinstance check and a simple test case.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, has a straightforward solution, and includes both a minimal reproduction case and a proposed fix. The test added is simple and directly tests the reported functionality. The change is localized and doesn't require deep architectural understanding.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13135": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: KBinsDiscretizer with strategy='kmeans' fails because kmeans can produce unsorted cluster centers, which causes np.digitize to fail since it requires sorted bins. The issue provides a complete reproducible example with specific code that demonstrates the problem, explains the root cause (unsorted bin_edges being fatal for np.digitize), and specifies the expected behavior (no error should be thrown). The description includes all necessary technical details including the specific function (np.digitize) that requires sorted input, making it clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Once you understand that the problem is unsorted cluster centers from kmeans, the solution is straightforward - just sort the centers after clustering. The fix involves adding a single line of code (centers.sort()) in the _discretization.py file. While it requires understanding the KBinsDiscretizer workflow and the relationship between kmeans centers and bin_edges, the actual code change is minimal and the logic is simple. The hardest part might be locating the right place in the code to add the sort, but the error traceback and understanding of the kmeans strategy would guide you there quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent example for a coding benchmark - it has a clear problem statement, reproducible example, and requires understanding both the immediate technical issue (unsorted arrays) and the broader context of how KBinsDiscretizer works with different clustering strategies. The solution is simple but not trivial, requiring the engineer to understand the root cause and implement an appropriate fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13142": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when n_init>1 is set in GaussianMixture, fit_predict(X) and predict(X) return different results, which is incorrect behavior since they should be consistent. The issue provides a complete reproducible code example that demonstrates the bug, shows the expected vs actual results, and even explains why the existing unit test doesn't catch this (because it doesn't set n_init). The problem statement is unambiguous - the two methods should return identical results but don't when n_init>1. An experienced engineer would clearly understand that they need to ensure fit_predict(X) and predict(X) produce consistent results regardless of the n_init parameter value.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, the fix involves moving a few lines of code (the final e-step) to occur after setting the best parameters rather than before. The core issue is in the timing of when the final e-step is executed in the fit_predict method. An experienced engineer would need to: 1) Understand the GaussianMixture algorithm and the role of the e-step, 2) Trace through the fit_predict method to see why results differ when n_init>1, 3) Realize that the final e-step needs to use the best parameters found during initialization rather than potentially different parameters. The actual code change is minimal (moving ~5 lines of code), but requires understanding the algorithm flow, making it more than a trivial fix but not requiring substantial research or rewriting.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-suited sample for the benchmark. The issue is clearly defined, has a specific reproducible test case, and the solution involves understanding algorithmic flow rather than just syntactic changes. The test patch also properly validates the fix by ensuring fit_predict and predict return identical results when n_init>1. This tests both understanding of the problem and ability to implement the correct solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13328": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: HuberRegressor fails when given boolean input data, throwing a TypeError. The issue includes complete reproducible code showing exactly what works (float arrays) and what fails (boolean arrays), along with the expected behavior (it should work like LinearRegression by casting booleans to floats). The actual error behavior is explained - the optimizer tries to negate boolean arrays with the '-' operator, which NumPy doesn't allow. The solution requirement is clear: boolean arrays should be automatically cast to float, just like in LinearRegression.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a straightforward fix that takes 15 minutes to 1 hour. The solution involves adding a single parameter `dtype=[np.float64, np.float32]` to the `check_X_y` call in the HuberRegressor.fit method. This ensures boolean inputs are automatically converted to float types. An experienced engineer would need to: 1) Understand the error comes from boolean data not being converted to float, 2) Locate the input validation in HuberRegressor.fit(), 3) Add the dtype parameter to force conversion. The fix is only 2 lines of code change and follows standard sklearn patterns for handling data type conversion.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the problem is clearly defined, the solution is straightforward but requires understanding of sklearn's input validation patterns, and the test case properly validates the fix. The issue demonstrates good software engineering practices with clear reproduction steps and expected vs actual behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13779": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear description of the problem: the VotingClassifier fails when sample_weight is passed and an estimator is set to None. The issue includes a complete, reproducible code example that demonstrates the exact failure scenario. The problem is clearly explained - when an estimator is set to None via set_params(), the code still tries to call fit() on it when sample_weight is provided, causing an AttributeError. The root cause is identified: the code doesn't check for None estimators in the sample_weight support validation loop. The expected behavior is clear - the voting estimator should handle None estimators gracefully when sample_weight is passed, just as it does when no sample_weight is provided.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a very straightforward fix that should take less than 15 minutes. The issue description clearly identifies the problem location (sample_weight support checking) and the solution is evident from the provided patch: simply add a None check before validating sample_weight support. The fix involves adding just 2 lines of code (if step is None: continue) in the existing loop in the fit method of the voting estimator. No complex logic, algorithm changes, or deep understanding of the codebase is required - just a simple guard clause to skip None estimators.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the issue is clearly described with a reproducible example, the solution is straightforward but requires understanding the codebase structure, and the test validates the fix properly. The problem demonstrates a real-world scenario where an estimator might be dynamically disabled by setting it to None, which is a legitimate use case that should be supported.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14053": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear title describing the problem: \"export_text returns IndexError when there is a single feature\", (2) Complete, runnable code that reproduces the issue with specific imports and dataset usage, (3) Clear explanation of what goes wrong: \"it tries to access a feature name at an index that doesn't exist in the provided list, causing an out-of-range error\", and (4) The specific function and scenario that triggers the bug (export_text with single feature and feature_names parameter). An experienced engineer would have all the information needed to understand the problem, reproduce it, and develop a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue is a straightforward IndexError bug in the export_text function. Looking at the gold patch, the solution requires adding a simple conditional check to handle the case where tree_.feature contains TREE_UNDEFINED values (which happens for leaf nodes). The fix is just 2 lines of code - changing a list comprehension to include a conditional that checks if the feature index is valid before accessing the feature_names list. An experienced engineer would quickly identify this as a bounds-checking issue and implement the guard clause.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. The sample is well-suited for the benchmark as it has clear requirements, a specific bug scenario that can be reproduced, and the solution can be validated through the provided test case that checks the fix works for single-feature trees with custom feature names.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14087": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: an IndexError is thrown when using LogisticRegressionCV with refit=False. The issue includes complete, reproducible code that demonstrates the exact problem, including specific parameter values (cv=5, solver='saga', tol=1e-2, refit=False). The error message is clearly described (\"too many indices for array\" when attempting to average coefficient paths across CV folds). The expected behavior is straightforward (no error should be thrown). An experienced engineer has all the information needed to identify that this is an indexing issue in the LogisticRegressionCV implementation when refit=False, and can work to fix the coefficient path averaging logic.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution, this is a 15 min - 1 hour fix. The patch shows three main changes: (1) fix a variable reference from self.multi_class to multi_class, (2) add a conditional check for elasticnet penalty before accessing l1_ratios_, and (3) handle the l1_ratio_ attribute properly for non-elasticnet penalties. These are relatively small, targeted fixes that address specific indexing and attribute access issues. While an engineer would need to understand the LogisticRegressionCV code structure and trace through the coefficient averaging logic, the actual code changes are minimal and localized to one function. The debugging process to identify the root cause might take some time, but once identified, the fixes are straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - it has a clear, reproducible bug with a well-defined expected behavior. The solution requires understanding the codebase logic but doesn't involve overly complex algorithmic changes. The test coverage is comprehensive, testing multiple parameter combinations (penalty types, multi_class settings) which would help validate any proposed solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14629": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem: AttributeError when using cross_val_predict(method='predict_proba') with MultiOutputClassifier. The issue provides a specific hypothesis about the root cause (the use of estimator.classes_ in _validation.py line 857-866), includes complete reproducible code that demonstrates the exact error, shows expected vs actual results, and even suggests the correct approach (using mo_clf.estimators_[i].classes_ instead). The author has done thorough debugging and points to the exact location in the codebase where the problem occurs. An experienced engineer would have all the information needed to understand and fix this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue clearly identifies the problem and its location. Looking at the actual solution, it involves adding a classes_ attribute to MultiOutputClassifier during the fit method by collecting classes_ from each individual estimator. This is a straightforward implementation that requires understanding the MultiOutputClassifier structure but doesn't involve complex algorithmic changes or extensive refactoring. The fix is localized to one method in one class, making it a relatively quick implementation for someone familiar with scikit-learn's architecture.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described, has a focused scope, and the solution is verifiable through the provided test case. The problem demonstrates understanding of scikit-learn's multioutput architecture and attribute consistency across different estimator types. The reproducible example makes it easy to verify that any proposed solution actually works.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14710": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear error scenario: when calling `gbrt.fit(X, y)`, the early-stopping routine fails because it tries to compare strings and floats, resulting in a TypeError. The problem occurs because the accuracy scorer receives integer-encoded true labels but original string class predictions. While the description doesn't explicitly state what the fix should be, there's a sensible interpretation - the labels need to be properly decoded/converted before being passed to the scorer. The error message and context make it reasonably clear that this is a data type mismatch issue in the early stopping validation process.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, the solution is relatively straightforward: add checks for classifiers and convert integer-encoded labels back to string labels using `self.classes_[y.astype(int)]` in two places within the `_check_early_stopping_scorer` method. The fix requires understanding the scikit-learn classifier architecture and how label encoding works, but the actual code changes are minimal (4 lines added). An experienced engineer familiar with the codebase could identify the issue location and implement this fix within 15 minutes to 1 hour after understanding the problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for the benchmark. The test patch provides a clear regression test that verifies the fix works with string targets and early stopping. The problem is specific enough to have a targeted solution, and the error is reproducible with the provided context.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14894": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides a clear problem statement (ZeroDivisionError in _sparse_fit), complete reproduction code that demonstrates the exact issue, expected vs actual results, and a technical explanation of the root cause (division by zero when n_class is zero with empty support vectors). The code example is runnable and clearly shows the difference between dense and sparse matrix behavior. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue provides clear reproduction steps and explains the root cause (division by zero). The solution involves adding a simple conditional check for the edge case when n_SV is zero, and creating an empty sparse matrix instead of performing the division. The fix is localized to one function (_sparse_fit in sklearn/svm/base.py) and requires minimal code changes - just a few lines to handle the edge case appropriately.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. This is a well-documented bug with clear reproduction steps, straightforward fix, and appropriate test coverage. The issue is suitable for evaluating coding ability as it tests understanding of edge cases, error handling, and sparse matrix operations in a machine learning context.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25747": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem statement: when using `transform_output=\"pandas\"`, a ValueError occurs because the transformer output has 4 rows but scikit-learn's pandas output wrapper tries to assign the original 96-row index to it, causing a length mismatch. While the description is somewhat brief and lacks specific code examples or stack traces that would make it perfectly clear, there is a sensible interpretation of what's required - the pandas output wrapper should not override the index when the data is already a DataFrame. The gold patch confirms this interpretation by removing the index assignment for DataFrame inputs in the `_wrap_in_pandas_container` function.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves a small, targeted change to the `_wrap_in_pandas_container` function in `sklearn/utils/_set_output.py`. The fix simply removes 2 lines of code that were incorrectly overriding the index when the input is already a DataFrame. While an engineer would need some time to understand the codebase structure and locate the relevant function, the actual fix is straightforward once the problem is understood. The test case also shows this is a focused change that doesn't require extensive modifications across multiple files.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within the pandas output functionality, and the solution is clean and focused. The test case provides good coverage for the regression scenario.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-25931": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the bug: when fitting an IsolationForest with a pandas DataFrame and a non-default contamination parameter, a warning is incorrectly emitted about invalid feature names during the fit() call. The issue provides a minimal reproducible example showing exactly how to trigger the bug, specifies the expected behavior (no warning), describes the actual behavior (warning emitted), and includes complete version information. An experienced engineer would have all the information needed to understand and resolve this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves creating a private _score_samples method that skips input validation (which removes feature names) and calling this from fit() instead of the public score_samples method. The core insight is that input validation during fit() removes feature names, causing the warning. The fix requires understanding the codebase structure and creating a simple wrapper method, but doesn't require extensive research or major architectural changes. The solution touches only one file and adds about 10-15 lines of code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, has a reasonable difficulty level that tests both debugging skills and understanding of scikit-learn's validation patterns, and the solution is well-contained. The test also properly validates the fix by ensuring no warnings are emitted when fitting with contamination parameter and pandas DataFrame.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25973": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. The user provides a complete, reproducible code example that demonstrates the bug. They explain that SequentialFeatureSelector should accept an iterable of splits for the cv parameter (as documented), but currently fails when passed splits from a cross-validator like LeaveOneGroupOut(). The expected behavior is clearly stated (should run without errors), and the actual behavior is described (IndexError due to empty scoring results list). The issue includes version information and references to documentation, making it clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves: 1) Adding an import for check_cv and is_classifier, 2) Calling check_cv() once in the fit method to properly validate/convert the cv parameter, and 3) Passing the validated cv parameter through to the helper method. This is a small, focused change that requires understanding how cross-validation works in scikit-learn and knowing about the check_cv utility function, but doesn't require substantial rewriting or complex logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly described with a complete reproducible example, the fix is straightforward but requires some domain knowledge of scikit-learn's cross-validation utilities, and the test adequately verifies the fix by reproducing the exact scenario from the issue report.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-10323": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified but requires some interpretation. The user describes a problem with Sphinx's literalinclude directive where prepended content doesn't maintain proper indentation alignment with the included code. They provide a clear example showing the current behavior (misaligned indentation) versus the expected behavior (properly aligned indentation). The core problem is that the prepend filter is applied after dedent, causing indentation inconsistencies. While the issue description doesn't explicitly state the technical solution (reordering filters), it provides enough context about the expected behavior and includes a workaround attempt that hints at the underlying problem. An experienced engineer could reasonably deduce that the filter order needs adjustment.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a simple reordering of filters in the sphinx/directives/code.py file - moving dedent_filter before prepend_filter and append_filter. The fix is only 3 lines changed. Once an engineer understands that the issue is caused by filter ordering, the solution is straightforward. The main time would be spent understanding the codebase structure, locating the relevant code (LiteralIncludeReader class), and understanding how the filters work together. With a few hours to familiarize with the codebase, this would likely take 15 minutes to 1 hour to implement and test.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear problem description and straightforward solution. The test case shows the expected behavior clearly. The issue demonstrates good software engineering practices - the user provided minimal reproduction cases, expected behavior, and even attempted workarounds. This would be a good benchmark sample as it tests understanding of filter pipelines and order-of-operations in text processing systems.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-10673": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly describes a concrete problem: when users try to add standard Sphinx-generated pages (genindex, modindex, search) to a toctree directive, Sphinx raises warnings about nonexistent documents. The issue provides specific examples of the problematic syntax, references to Stack Overflow questions showing this is a common user need, and clearly states the desired solution - that the provided toctree directive should work without raising errors. The problem statement is unambiguous: Sphinx should recognize these as valid built-in documents rather than treating them as missing files.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours of work. The solution involves understanding Sphinx's toctree parsing mechanism and modifying multiple files (sphinx/directives/other.py, sphinx/environment/adapters/toctree.py, sphinx/environment/collectors/toctree.py). The engineer needs to: 1) Understand how Sphinx tracks document names and validates toctree entries, 2) Identify that genindex/modindex/search are \"generated documents\" stored in the std domain's initial_data['labels'], 3) Modify the toctree parsing logic to recognize these generated documents as valid, 4) Update the toctree resolution logic to handle references to these documents properly. While not extremely complex, it requires understanding Sphinx's internal architecture and making coordinated changes across multiple components.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with clear success criteria, the solution is testable, and it addresses a legitimate user need. The test patch shows comprehensive validation of the fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7440": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with some minor gaps. It clearly describes the problem: Sphinx treats glossary terms case-insensitively, causing \"mysql\" and \"MySQL\" to be seen as duplicates, leading to build failures. The expected behavior is implied - that case should matter for glossary terms. The issue provides reproduction steps, environment details, and points to specific files. However, it doesn't explicitly state what the desired solution should look like (whether to make terms case-sensitive or handle duplicates differently), though the context strongly suggests case-sensitivity is wanted. An experienced engineer could reasonably infer the requirements.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The patches show it involves removing lowercase normalization in just two places in sphinx/domains/std.py: removing `.lower()` call in make_glossary_term function and removing `lowercase=True` parameter from XRefRole. Once an engineer understands that Sphinx is inappropriately case-folding glossary terms, the fix is straightforward - just preserve the original case. The main time would be spent understanding the Sphinx codebase structure and locating where case normalization occurs, but the actual code changes are minimal and conceptually simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within the Sphinx documentation domain handling, the solution is clean and focused, and the test changes appropriately verify case-sensitive behavior. This is a good benchmark sample as it tests understanding of case handling in text processing systems and requires locating the right normalization points in the codebase.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7462": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when using an empty tuple type annotation `Tuple[()]`, Sphinx crashes with an \"IndexError: pop from empty list\" error in the `unparse` method of `python.py`. The issue provides a complete reproduction case with specific code (`def foo() -> Tuple[()]:`) and clear steps to reproduce the bug. The expected behavior is also clearly stated - docs should build successfully with valid type annotations. The error occurs because the code attempts to pop from an empty list when processing empty tuple annotations, which is a specific and well-defined problem that can be understood and fixed without ambiguity.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is a straightforward edge case handling problem. The error message \"pop from empty list\" immediately points to the root cause - the code assumes there are elements to pop but doesn't handle empty tuples. Looking at the gold patch confirms this: it's a simple conditional check to handle empty tuples differently. The fix involves adding an `if node.elts:` condition in two locations and providing appropriate handling for the empty case. An experienced engineer familiar with the codebase could identify and implement this fix relatively quickly once they locate the relevant `unparse` methods.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - the issue is clear, the problem is well-defined, and the solution requires understanding the codebase structure and implementing proper edge case handling. The fix is localized and testable.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-7985": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly states the problem: linkcheck doesn't check local (internal) links. The user provides a concrete reproducible example with index.rst content showing both a broken external link and a broken local link. The current behavior is described (only external link is checked) and the expected behavior is clear (local links should also be checked). The problem statement, reproduction steps, current results, and expected results are all provided, making it easy to understand what needs to be implemented.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because it requires: (1) Understanding the existing linkcheck.py architecture and how URI checking currently works, (2) Implementing new logic to distinguish between different URI schemes using regex, (3) Adding file system checking for local references using path operations, (4) Integrating ignore pattern matching for local links, (5) Modifying the control flow to properly categorize local links as working/broken/ignored. The patch shows about 15-20 lines of new code but requires careful integration with existing logic and understanding of the codebase structure.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is clear, the solution is implementable, and the test cases properly validate the functionality. This appears to be a good benchmark sample for evaluating coding ability as it requires understanding existing code architecture, implementing file system operations, and properly integrating new functionality.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-8269": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear about what needs to be fixed. It describes a specific bug where the linkcheck command incorrectly reports \"Anchor not found\" errors when the real issue is an HTTP error (404, 500, etc.) from the server. The issue provides a clear reproduction case using sphinx-quickstart with a non-existent URL (https://google.com/test.txt#test), shows the current incorrect behavior, and specifies the expected behavior with a concrete example of the desired output format. The problem is that when linkcheck_anchors=True, HTTP errors are masked by anchor-checking logic, but the user wants HTTP errors to be reported just like when linkcheck_anchors=False. The expected solution is clearly stated: report HTTP errors (like \"404 Client Error: Not Found\") instead of anchor errors when the server returns an error status code.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it's literally adding one line (response.raise_for_status()) to check HTTP status codes before proceeding to anchor checking. The fix is in sphinx/builders/linkcheck.py in the check_uri function. An experienced engineer would need to: 1) Understand the linkcheck flow and locate where HTTP requests are made, 2) Identify that the issue occurs because anchor checking happens regardless of HTTP status, 3) Add the raise_for_status() call to catch HTTP errors before anchor processing. The logic is straightforward - if there's an HTTP error, it should be raised and reported instead of proceeding to check anchors. This requires minimal code change and basic understanding of HTTP status handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, a simple solution, and the test case provided shows exactly how to verify the fix works correctly. The issue has good reproduction steps and clear expected behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-8551": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a bug where `:type:` and `:rtype:` fields in Sphinx documentation generate false ambiguous class lookup warnings. The issue provides a concrete, reproducible example with RST code that demonstrates the exact problem. It shows that when using `py:currentmodule:: mod.submod`, unqualified references to \"A\" should resolve to `mod.submod.A` but instead generate ambiguity warnings and incorrectly link to `mod.A`. The expected behavior is clearly stated: no warnings should be emitted and the references should resolve correctly to the most specific match in the current module context. The environment information is also provided. An experienced engineer would have enough information to understand that this is about Sphinx's Python domain cross-reference resolution mechanism and that implicit xrefs from info fields behave differently than explicit xref roles.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The engineer needs to understand Sphinx's domain system, specifically how the Python domain handles cross-references and module context. They need to trace through the code to understand why `:type:` and `:rtype:` fields don't respect the current module context like explicit `:py:class:` roles do. Looking at the gold patch, the solution involves modifying two files: adding module/class context to cross-references in `sphinx/domains/python.py` and passing the environment to the `make_xref` call in `sphinx/util/docfields.py`. While the changes are relatively small (6 lines), debugging cross-reference resolution in a complex documentation system like Sphinx requires understanding the interaction between domains, directives, and the reference resolution mechanism. This requires substantial domain expertise and careful investigation of the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with clear reproduction steps and expected behavior. The gold patch shows a clean solution that addresses the core issue. The test patch demonstrates proper verification of the fix. This is a good candidate for evaluating coding ability as it requires understanding of a complex system's architecture and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-9711": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes that the `needs_extensions` check in Sphinx performs string-based version comparison instead of proper semantic version comparison, leading to incorrect version ordering (e.g., '0.6' > '0.10' as strings but '0.6' < '0.10' as versions). The issue provides a concrete reproduction case with sphinx-gallery versions 0.9 vs 0.10, shows the exact error behavior, and clearly states the expected behavior. The problem is in the `verify_needs_extensions` function in `sphinx/extension.py` where version comparison needs to be updated to use proper semantic versioning instead of string comparison.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-isolated to the version comparison logic in the `verify_needs_extensions` function. The solution involves importing a proper version comparison library (like packaging.version) and replacing the simple string comparison with semantic version comparison, while handling edge cases like 'unknown version' and invalid version strings. The patch shows this is a focused change to one function with proper error handling, requiring some thought about edge cases but not extensive research or major architectural changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with clear reproduction steps, expected behavior, and a focused solution area. The issue description provides sufficient context for an engineer to understand and fix the problem. The test patch also demonstrates that the fix can be properly validated.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13372": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, reproducible example showing the problem: `Mul(Max(0, y), x, evaluate=False).evalf()` raises an UnboundLocalError because variables `reprec` and `imprec` are referenced before assignment. The issue description explains the root cause - evalf tries to look up a handler for Max, gets a KeyError, but then the multiplication evaluation routine fails to initialize local variables properly. The reporter even suggests a specific solution: adding `else: raise NotImplementedError` clauses where `reprec` and `imprec` should be defined. The problem is in the sympy/core/evalf.py file in the multiplication evaluation logic, and the expected behavior is clear from the working case `Mul(x, Max(0, y), evaluate=False).evalf()`.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides a clear reproduction case, identifies the root cause (uninitialized variables in specific code paths), and even suggests the solution. The fix involves adding two simple `else: raise NotImplementedError` statements in the evalf.py file where `reprec` and `imprec` should be initialized but aren't. Looking at the gold patch confirms this - it's just adding 4 lines of code (2 else clauses with NotImplementedError). An experienced engineer would need some time to understand the evalf system and trace through the code path, but the specific problem and solution are well-defined.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly described, reproducible, and the solution is straightforward. The test case added confirms the fix works correctly by ensuring the problematic expression now evaluates without error and returns the expected symbolic form.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13480": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear, reproducible code example showing exactly how to trigger the bug, (2) The specific error that occurs (NameError in hyperbolic.py at line 590), (3) The exact cause of the error (variable 'cotm' is referenced but never defined), (4) Multiple test cases showing which values fail (2, 3, 5, 6, 8, 9, 11, 12, 13, 15, 18, etc.), and (5) The specific file and line number where the bug occurs. An experienced engineer would have all the information needed to locate and fix this typo bug without any ambiguity about what needs to be done.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a simple typo fix that should take less than 15 minutes. The issue clearly states that 'cotm' is referenced but never defined, and looking at the gold patch confirms this is just changing 'cotm' to 'cothm' on line 590. An engineer would: (1) Navigate to hyperbolic.py line 590, (2) See the undefined variable 'cotm', (3) Notice that 'cothm' is defined a few lines above, (4) Realize it's a typo and fix it. The context makes it obvious what the correct variable name should be since 'cothm = coth(m)' is defined just above the error line.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is crystal clear, the fix is straightforward but requires actually understanding the code context, and the test cases verify the fix works. It tests basic debugging skills and attention to detail without being trivially obvious from the issue description alone.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13877": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is reasonably well-specified but requires some interpretation. It clearly demonstrates a problem where computing matrix determinants with symbolic entries fails for certain matrix sizes (f(5) returns nan, f(6) fails with a TypeError about invalid NaN comparison). The code example shows the exact failing case and mentions that the Bareiss algorithm is encountering NaN values during coefficient comparison. However, there are some gaps: the issue doesn't explicitly state what the expected behavior should be (should it return a symbolic result, 0, or handle the NaN differently?), and the question about whether Bareiss algorithm is valid for non-integer matrices adds some uncertainty. An experienced engineer would need to investigate the algorithm's applicability and determine the appropriate fix, but the core problem is clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. From the gold patch, we can see the solution involves modifying the _eval_det_bareiss method to use a different zero-testing function (_is_zero_after_expand_mul) and replacing a simple pivot finding approach with _find_reasonable_pivot. This requires: (1) Understanding the Bareiss algorithm implementation and why it's failing with symbolic entries, (2) Recognizing that the issue is in zero detection/pivot selection when dealing with symbolic expressions, (3) Implementing a more robust zero-testing function using expand_mul, and (4) Modifying the pivot selection logic. The solution touches core matrix computation logic and requires understanding of symbolic mathematics, making it more complex than a simple bug fix but not requiring extensive research or major architectural changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is reproducible with a clear test case, the solution is focused and well-defined, and the test patch provides appropriate validation. The issue represents a legitimate bug in symbolic matrix determinant computation that would be suitable for benchmark evaluation.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-16792": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: the cython backend for autowrap fails when array arguments don't appear in the wrapped expression. The issue includes a minimal reproducible example showing the exact failure case, explains the root cause (incorrect C function signature where `x` should be `double *` instead of `double`), shows the actual vs expected behavior, and even provides a working counterexample when the expression does depend on the argument. The author clearly states the business case for why this matters (interfacing with external libraries that need predefined signatures). All the information needed to understand and fix the problem is present.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This would take 1-4 hours to solve. While the problem is clearly identified, the solution requires understanding the codegen module's logic for handling argument sequences, particularly how it determines metadata for array vs scalar arguments. The engineer needs to trace through the code to understand why array arguments that don't appear in expressions lose their array metadata, then modify the argument processing logic to preserve array type information. The fix involves understanding the distinction between symbols found in expressions vs those explicitly provided in argument_sequence, and ensuring proper metadata is applied to both cases. This requires moderate familiarity with the codebase and some debugging time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified with minimal reproducible examples, the problem domain is well-defined, and the solution requires understanding moderate complexity code generation logic without being overly esoteric. The test case provided in the patch gives clear success criteria.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17139": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproducible example showing exactly what command triggers the error (simplify(cos(x)**I)). The error message is explicitly stated (\"Invalid comparison of complex I\"), and the root cause is clearly explained - Sympy attempts to compare the complex number I with zero during simplification, which Python doesn't support. The issue identifies the specific problematic behavior (ordering comparison on complex values) and the context where it occurs (during the simplification process). An experienced engineer would immediately understand that the solution needs to handle complex exponents differently to avoid the comparison operation that triggers the TypeError.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified (comparison of complex numbers), and looking at the gold patch, the solution is quite straightforward - adding a simple check `if not rv.exp.is_real: return rv` before the problematic comparison `if (rv.exp < 0) == True`. The fix requires understanding where the comparison happens in the code and adding an early return for non-real exponents. While it requires some familiarity with SymPy's structure to locate the right function in fu.py, the actual code change is minimal and the logic is straightforward once you understand the issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue description is clear and actionable, the problem is well-defined with a specific reproducible case, and the solution requires understanding of both the error cause and the appropriate fix location. The test cases properly verify that complex exponents are handled without crashing, making this suitable for automated testing of solutions.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17318": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly describes: (1) The specific input expression that causes the problem: (3 \u2013 sqrt(2)*sqrt(4 + 3*I) + 3*I)/2, (2) The exact error that occurs: IndexError \"tuple index out of range\" in radsimp._split_gcd when trying to access the first element of an empty tuple, (3) The expected behavior: if an expression cannot be denested, it should be returned unchanged, and (4) Shows both the old erroneous result (IndexError) and the new correct result (expression returned unchanged). The issue provides enough technical detail for an engineer to understand what's happening and what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the patch, the core issue is a missing validation check in the _sqrt_match function. The fix adds a condition to check that squared arguments are not only rational but also positive (avoiding complex numbers that cause the empty tuple). The change is small but requires understanding the mathematical context of why complex numbers cause issues in the denesting algorithm. An experienced engineer would need time to trace through the code, understand the sqrtdenest algorithm, and identify where the validation should be added, but the actual code change is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - it has a clear problem statement, a specific reproducible error, and requires understanding both the codebase structure and the mathematical domain logic. The fix involves adding proper input validation, which is a common software engineering task.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17630": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides clear reproduction steps with concrete code examples, demonstrates the exact error that occurs (AttributeError when accessing 'cols' attribute), and identifies the root cause (intermediate zero blocks become Zero instead of ZeroMatrix). The expected behavior is clear - the multiplication should work consistently without type conversion issues. The issue even includes diagnostic information showing the type inconsistency. An experienced engineer could understand exactly what needs to be fixed: ensuring that zero blocks maintain their ZeroMatrix type throughout block multiplication operations.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve. While the problem is clearly identified, fixing it requires understanding SymPy's matrix expression system, particularly how BlockMatrix operations are processed and how zero elements are handled during matrix arithmetic. Looking at the patch, the solution involves modifying the _postprocessor function in matexpr.py to handle MatAdd operations properly, ensuring ZeroMatrix objects are preserved. This requires understanding the internal architecture of SymPy's matrix expressions, testing the fix across different scenarios, and ensuring it doesn't break existing functionality. The code change itself is small (2 lines), but arriving at the correct solution requires substantial investigation into the matrix expression processing pipeline.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the reproduction case is clear and self-contained, and the fix can be validated through the provided test cases. The issue represents a good test of understanding SymPy's matrix expression internals and type consistency requirements.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17655": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproducible example showing that `point1 + point2 * sympy.sympify(2.0)` works but `point1 + sympy.sympify(2.0) * point2` fails. The issue description explains the exact problem: when attempting to add the expression `2.0*Point2D(1, 1)` to `Point(0, 0)`, Sympy's addition logic cannot handle the generic multiplication object (Mul) and raises a GeometryError. The expected behavior is clear - both multiplication orders should work equivalently. The provided code patch confirms this by implementing the `__rmul__` method to handle right-side multiplication, which is exactly what's needed to fix the asymmetric behavior described in the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a classic Python operator overloading issue that should take 15 minutes to 1 hour for an experienced engineer. The problem is well-understood: when you write `2.0 * point`, Python calls `point.__rmul__(2.0)` since the left operand (float) doesn't know how to multiply with a Point. The solution is straightforward - implement `__rmul__` to delegate to `__mul__`. The existing `__mul__` method already handles the multiplication logic correctly. An experienced engineer familiar with Python's data model would quickly recognize this pattern and implement the 4-line solution. The only time needed would be to understand the codebase structure, locate the Point class in sympy/geometry/point.py, and write the simple delegation method.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - it has a clear problem statement, demonstrates expected vs actual behavior with concrete examples, provides good test coverage in the patch, and tests a fundamental programming concept (operator overloading) that any competent Python developer should understand. The solution is elegant and follows Python best practices.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-18211": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: calling Eq(n*cos(n) - 3*sin(n), 0).as_set() raises NotImplementedError instead of returning a ConditionSet. The expected behavior is explicitly shown with the exact format of the ConditionSet that should be returned: ConditionSet(n, Eq(n*cos(n) - 3*sin(n), 0), Reals). The issue provides a concrete example with specific mathematical expressions, making it clear what needs to be fixed. An experienced engineer would understand that the as_set() method in the relational module needs to handle cases where solve_univariate_inequality fails and return an appropriate ConditionSet instead of allowing the NotImplementedError to propagate.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution is relatively straightforward: wrap the existing solve_univariate_inequality call in a try-catch block and return a ConditionSet when NotImplementedError is raised. The patch shows this is a localized change in sympy/core/relational.py in the _eval_as_set method, adding about 8 lines of code including imports and comments. An experienced engineer familiar with the codebase would need some time to understand the relational module structure and the ConditionSet class, but the actual implementation is simple once the approach is clear.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined bug with a clear expected behavior and a straightforward solution approach. The test cases provided confirm the expected behavior and would serve well for validation in a benchmark setting.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-20428": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when calling `clear_denoms()` on a complicated constant polynomial that evaluates to zero, the resulting polynomial prints as \"Poly(0, x, domain='EX')\" but behaves inconsistently - `is_zero` returns False while `as_expr().is_zero` returns True. The issue provides a complete reproducible example with the exact input polynomial, shows the problematic behavior with specific method calls, and explains the root cause: an unstripped leading 0 in the DMP representation `DMP([EX(0)], EX, None)` instead of the correct `DMP([], EX, None)`. The issue also explains how this causes downstream problems like IndexError in `terms_gcd()` and ZeroDivisionError in `primitive()`. The expected behavior is clearly implied - the polynomial should behave consistently as a zero polynomial.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that takes 15 minutes to 1 hour. The solution involves changing a single line in the `__bool__` method of the ExpressionDomain class from `return f.ex != 0` to `return not f.ex.is_zero`. This is a small conceptual change that requires understanding the difference between `!= 0` and `is_zero` in SymPy's expression system. An experienced engineer would need some time to trace through the code to understand how `clear_denoms()` creates the problematic DMP representation and how the `__bool__` method affects polynomial behavior, but the actual fix is minimal once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-documented with clear examples, the expected behavior is obvious (zero polynomial should behave consistently), and the solution space is constrained. The test case provided in the patch confirms that the fix resolves the specific issue described. This is a good benchmark sample for evaluating debugging and problem-solving skills in symbolic mathematics libraries.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-20438": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some clarity but requires interpretation. It shows several code examples demonstrating problems with `is_subset` behavior and an AttributeError when simplifying equality between ProductSet and FiniteSet objects. However, the issue text is somewhat fragmented and doesn't clearly state what the expected behavior should be. The examples show: 1) `b.is_subset(c)` returns None instead of True/False, 2) `c.is_subset(b)` returns True (which may be incorrect), and 3) `Eq(b, c).simplify()` raises AttributeError. While an experienced engineer can infer that the problem involves incorrect subset checking and missing equals method for Complement objects, the exact requirements need to be deduced from the examples rather than being explicitly stated.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: 1) Understanding the SymPy sets architecture and multiple dispatch system, 2) Implementing subset checking logic for ProductSet vs FiniteSet combinations, 3) Adding type checks in relational simplification to prevent AttributeError, 4) Modifying comparison handlers for proper equality checking between different set types. The solution touches multiple files (relational.py, comparison.py, issubset.py) and requires understanding of how ProductSet expansion works and how SymPy's dispatch system handles set operations. While not extremely complex, it requires domain knowledge of the codebase and careful implementation across several interconnected components.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is solvable from the given information, the test cases provide good validation coverage, and the solution involves core programming concepts (type checking, method implementation, logical operations) that are appropriate for evaluating coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20590": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clearly stated: Symbol instances unexpectedly have a __dict__ attribute in version 1.7 when they shouldn't, given that Symbol uses __slots__. The issue explains that in version 1.6.2, accessing __dict__ on a Symbol raised an AttributeError (expected behavior), but in 1.7 it returns an empty dict (unexpected). The author correctly suspects this violates the purpose of __slots__ and suggests it's likely due to a parent class no longer defining __slots__. While the exact technical solution isn't spelled out, an experienced engineer would understand they need to identify which parent class is missing __slots__ and add it. The behavioral expectation is clear: Symbol instances should not have __dict__.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided solution, it only requires adding `__slots__ = ()` to the Printable class in _print_helpers.py - a 6-line change including comments. The main work involves: (1) Understanding the __slots__ mechanism and inheritance behavior (5-10 minutes for an experienced engineer), (2) Tracing the Symbol class inheritance hierarchy to find which parent class is missing __slots__ (10-20 minutes), (3) Adding the missing __slots__ declaration (2 minutes), and (4) Testing the fix (5-10 minutes). The concept is straightforward - when a class with __slots__ inherits from a class without __slots__, instances get __dict__. The solution is a simple one-line addition to fix the inheritance chain.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained technical problem with a clear behavioral change between versions. The issue demonstrates good understanding of Python's __slots__ mechanism and provides specific version information. The test case appropriately verifies the fix by checking that Symbol instances don't have __dict__ and can't have attributes assigned. This makes for a good benchmark problem as it tests understanding of Python's object model and inheritance behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-21379": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The reporter provides a clear minimal working example (MWE) that demonstrates the problem: a PolynomialError occurs when using subs({1: 1.0}) on an expression containing exp(sinh(Piecewise(...)/z)) with real symbols, but only under specific conditions (after clearing cache, with real symbols, etc.). The reporter also provides helpful debugging information about when the error does and doesn't occur (works with cosh/tanh, fails without division by z, etc.). However, there are some gaps - the exact root cause isn't identified, and the reporter admits uncertainty about where the issue lies. An experienced engineer would need to investigate the interaction between caching, real symbol assumptions, and the polynomial operations, but the reproduction case is clear enough to work from.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the fix itself is relatively small (adding a try-catch block around gcd operations in sympy/core/mod.py), understanding the problem requires significant investigation. The engineer needs to: (1) reproduce the specific PolynomialError with the given expression, (2) trace through the sympy codebase to understand why gcd operations fail with piecewise expressions containing real symbols, (3) understand the interaction between caching, symbol assumptions, and polynomial operations, and (4) determine that catching PolynomialError and defaulting to G=S.One is the appropriate solution. The debugging information provided helps narrow down the conditions, but the root cause analysis still requires substantial code investigation and understanding of sympy's internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a suitable sample for the benchmark. The issue is reproducible with a clear MWE, the expected behavior is obvious (the substitution should work without throwing PolynomialError), and the test cases verify the fix appropriately. The issue represents a real-world problem that could occur in mathematical computation libraries, and solving it requires both debugging skills and understanding of symbolic mathematics code.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-22714": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It provides: (1) Exact reproduction steps with minimal code (`import sympy as sp; with sp.evaluate(False): sp.S('Point2D(Integer(1),Integer(2))')`), (2) Clear error description (\"Imaginary coordinates are not permitted\"), (3) Context explanation of why this is a problem (the error occurs inappropriately in unevaluated context), and (4) Specific location where the error originates (`sympy/geometry/point.py`). An experienced engineer would have sufficient information to understand that the issue is about the Point2D constructor incorrectly rejecting real coordinates when evaluate=False, and would be able to investigate the coordinate validation logic in the geometry module.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-isolated to a specific validation check in `sympy/geometry/point.py`. The reproduction case is simple and the error message points directly to the problematic code. The fix involves understanding how SymPy's `evaluate(False)` context affects expression evaluation and modifying a single condition from `im(a)` to `im(a).is_zero is False` to handle unevaluated expressions properly. While it requires understanding SymPy's evaluation system, the actual code change is minimal and the logic is straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample is well-suited for the benchmark as it has clear reproduction steps, a specific error to fix, and the solution can be verified by running the provided test case. The issue demonstrates good software engineering practices with proper error reporting and minimal reproduction cases.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23824": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is extremely well-specified. It clearly identifies the specific problem: the kahane_simplify() function incorrectly reverses the order of leading uncontracted gamma matrices. The issue provides a complete, runnable test case that demonstrates the bug with concrete examples showing both expected and actual behavior. The description explains that \u03b3^\u03bc \u03b3_\u03bc \u03b3^\u03c1 \u03b3^\u03c3 and \u03b3^\u03c1 \u03b3^\u03c3 \u03b3^\u03bc \u03b3_\u03bc should both simplify to 4 \u03b3^\u03c1 \u03b3^\u03c3, but the second case incorrectly produces 4 \u03b3^\u03c3 \u03b3^\u03c1. The author even identifies the root cause: \"the leading matrices are removed at the beginning of the function and then inserted at the start of the product at the end of the function, and the insertion loop is just backward.\" This level of detail makes it very clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue author has already identified the exact root cause and location of the bug in the kahane_simplify() function. The problem is simply that a loop that inserts leading matrices is running backwards. Looking at the provided patch, the fix is indeed very simple - it changes a few lines in the insertion logic to preserve the correct order. An experienced engineer would need some time to understand the gamma matrices context and verify the fix works correctly, but the actual coding change is minimal and straightforward once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is clearly described, has a specific reproducible test case, involves a well-defined mathematical operation, and has a clean, focused solution. The domain knowledge required (gamma matrices in high-energy physics) might be unfamiliar to some engineers, but the mathematical behavior is clearly explained in the issue description. The test cases provided make it easy to verify correctness without deep domain expertise.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23950": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly explains: (1) the current incorrect behavior with a concrete example showing Contains(x, Reals).as_set() returning Contains(x, Reals), (2) why this is wrong (Contains is a boolean, not a set), (3) the downstream failure it causes with a specific error trace showing the AttributeError in Piecewise.eval when as_relational is called, and (4) the logical expectation that as_set() should return the actual set. The problem statement includes the exact file location (sympy/sets/contains.py), method name (as_set), and the chain of calls that leads to the failure. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The problem is clearly identified and localized to a single method (as_set in Contains class). The solution is conceptually straightforward - instead of raising NotImplementedError, return the set that the element should belong to (self.args[1]). The fix requires changing only one line of code in sympy/sets/contains.py. An experienced engineer familiar with the codebase would quickly understand that Contains objects store the element and set as args[0] and args[1] respectively, making the solution obvious. The main time would be spent understanding the Contains class structure and testing the fix, but the core change is trivial.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-isolated bug with a clear problem statement and straightforward solution. The issue provides good context about why the current behavior is wrong and what the expected behavior should be. The fix is localized and doesn't require deep architectural changes.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-24066": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproducible example that demonstrates the exact problem: SI._collect_factor_and_dimension() incorrectly handles dimensionless quantities within function arguments (specifically exp()). The issue shows that while `expr = units.second / (units.ohm * units.farad)` is correctly identified as dimensionless, when this same expression is used as an argument to `exp()` in `100 + exp(expr)`, the system fails to recognize that the exponent should be dimensionless and throws a ValueError. The expected behavior is clear: expressions inside functions like exp() that are dimensionless should be treated as such, and the function should not raise an error. The code example is complete and runnable, making it easy to reproduce and verify the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves modifying the Function handling case in _collect_factor_and_dimension() method in unitsystem.py. The fix is relatively straightforward: when processing function arguments, check if each dimension is dimensionless using is_dimensionless() and convert it to Dimension(1) if so. The change is localized to a few lines of code in a single method and doesn't require extensive refactoring. An experienced engineer familiar with the codebase could identify that the issue is in the Function branch of _collect_factor_and_dimension(), understand that dimensionless arguments need special handling, and implement the fix within this timeframe.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is well-defined, has a clear reproduction case, and the solution space is appropriately constrained. The test case provided in the issue maps well to the test that was added in the PR, indicating good alignment between the problem description and verification approach.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-24443": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some domain knowledge to fully understand. The problem is clearly identified: `_check_homomorphism` is broken on PermutationGroups, specifically when inverted generators are involved in the `_image()` function. The issue provides a concrete code example that reproduces the ValueError, points to the exact location in the codebase (line 336-337 in homomorphisms.py), and explains that the problem occurs when `r[i]` is an inverted generator and the `in gens` test fails. However, understanding what constitutes a \"valid homomorphism\" and the mathematical concepts involved requires some background in group theory and combinatorics. The statement \"I think the whole thing can be greatly simplified\" gives a hint about the solution direction but leaves the specific implementation details to be figured out.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty because: (1) It requires understanding mathematical concepts (group theory, homomorphisms, permutation groups) which takes time for familiarization, (2) The engineer needs to analyze the existing `_image()` function logic and understand why it fails with inverted generators, (3) The solution involves substantially rewriting the `_image()` function with a cleaner approach using array_form and symbol mapping, (4) The fix requires understanding the relationship between generators, their symbols, and how to properly map them in the homomorphism check, (5) While the core issue is localized to one function, the mathematical domain knowledge and the need to completely rethink the implementation approach makes this more than a simple 15-minute fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is reproducible with the provided code example, the solution area is clearly identified, and the test patch shows that the fix should allow creating identity homomorphisms on PermutationGroups. The mathematical domain knowledge requirement is reasonable for a coding benchmark targeting experienced engineers.",
            "q2_5_confidence": 4
        }
    }
]