[
    {
        "astropy__astropy-13033": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when attempting to remove a required column from a TimeSeries object, the exception message is misleading rather than informative. The issue provides a complete reproducible example showing exactly how to trigger the problem, including the specific code that sets up a TimeSeries with required columns and attempts to remove one. The expected behavior is clearly stated - users should get an informative exception about missing required columns. The issue also includes system details for context. An engineer would have all the information needed to understand the problem and implement a solution that improves the exception messaging when required columns are violated.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the provided patch, the solution involves modifying the _check_required_columns method in astropy/timeseries/core.py to improve error message formatting. The fix adds a helper function as_scalar_or_list_str to properly format column names in error messages, and updates the error message template to use this formatting. This is a relatively straightforward change that doesn't require deep architectural understanding - just identifying where the error messages are generated and improving their clarity. The main work involves understanding the existing error handling logic and making the messages more user-friendly when multiple required columns are involved.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with clear requirements and a straightforward solution focused on improving user experience through better error messaging. The issue includes good reproduction steps and the patch shows it's a localized change to error handling code.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-13977": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The author clearly describes the problem: when implementing a duck type of `astropy.units.Quantity`, they encounter issues with reflected arithmetic operators when the left operand has different but compatible units. They provide a concrete minimal working example showing the problematic behavior and reference the numpy documentation suggesting that `NotImplemented` should be returned instead of raising an error. The specific request is to modify `Quantity.__array_ufunc__()` to return `NotImplemented` when inputs are incompatible, rather than raising `ValueError`. While some implementation details need to be figured out (like exactly when to return `NotImplemented` vs raising exceptions), the core requirement is clear enough for an experienced engineer to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The solution requires understanding the numpy `__array_ufunc__` protocol, the astropy codebase structure, and the unit system. The patch shows that the fix involves wrapping the existing logic in a try-catch block and implementing specific logic to determine when to return `NotImplemented` vs re-raising exceptions. This requires understanding when other types might handle the operation (checking for custom `__array_ufunc__` implementations) and modifying the core `__array_ufunc__` method behavior. The implementation also needs comprehensive tests covering various duck typing scenarios. While not trivial, it's not an extremely complex architectural change - it's a thoughtful modification to error handling logic that requires good understanding of the numpy protocol and astropy's design.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The issue provides a clear problem statement with working examples, references relevant documentation (numpy docs), and the provided patches show both the implementation approach and comprehensive test coverage. The duck typing example helps illustrate the exact use case that needs to be supported. This appears to be a suitable benchmark sample.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14096": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified with a clear, minimal reproducible example that demonstrates the problem. The user provides exact code showing how to subclass SkyCoord, create a property that accesses a non-existent attribute, and what the current misleading behavior is. The expected behavior is also clear - the error message should reference 'random_attr' (the actual missing attribute) rather than 'prop' (the property name). The problem is specific to attribute error handling in subclassed SkyCoord objects, and an experienced engineer would easily understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that requires understanding Python's attribute access mechanism (__getattr__ vs __getattribute__). The solution involves changing how the SkyCoord.__getattr__ method handles failed attribute access - instead of raising a generic AttributeError, it should call __getattribute__ to get the proper exception. The actual code change is minimal (replacing a few lines), but it requires understanding the subtle difference between these magic methods and how they interact with properties. An experienced engineer familiar with Python's object model could solve this in 15 minutes to 1 hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is testable, and it represents a realistic debugging scenario that tests understanding of Python's attribute access mechanisms. The provided test clearly validates the fix by checking that the correct attribute name appears in the error message.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14182": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly demonstrates the desired functionality with concrete examples showing: (1) Current behavior of RestructuredText output without header_rows parameter, (2) Working example of header_rows with fixed_width format, and (3) The desired outcome for header_rows with RestructuredText format. The request is unambiguous: add support for the header_rows parameter to the RestructuredText format, similar to how it works in the fixed_width format. The examples show exactly what the output should look like, making the requirements crystal clear.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours of work. Looking at the gold solution, it involves: (1) Modifying the RST class __init__ method to accept header_rows parameter, (2) Updating the write() method to handle variable header row indices, (3) Adding a read() method to properly set start_line based on header rows, (4) Removing hardcoded start_line from SimpleRSTData class. While not extremely complex, it requires understanding the inheritance structure (RST inherits from FixedWidth), how the writing/reading pipeline works, and careful coordination between multiple components. The engineer needs to trace through the codebase to understand how header_rows works in other formats and adapt it for RST.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for evaluation. The functionality request is clear, the expected behavior is demonstrated with examples, and the solution requires meaningful code changes across multiple methods while building on existing patterns in the codebase. The test patch shows comprehensive testing of the new functionality.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14309": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It describes that cron tests in HENDRICS are failing due to changes in astropy's `identify_format` function, specifically related to a commit that changed how the function handles filepaths without FITS extensions. The issue provides a clear reproduction case showing that `identify_format(\"write\", Table, \"bububu.ecsv\", None, [], {})` is now behaving differently. However, the issue doesn't explicitly state what the expected vs actual behavior should be - it requires the engineer to infer from the context that the function should return False/None for non-FITS files rather than executing `isinstance(args[0], ...)` when the filepath doesn't have a FITS extension. The provided patches help clarify that the issue is in the `is_fits` function in `astropy/io/fits/connect.py` where the logic flow was incorrect.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is a straightforward logic flow problem in the `is_fits` function. Looking at the gold patch, the fix involves changing an if-statement to a direct return statement, removing unnecessary nesting. The function was checking if the filepath had a FITS extension and returning True if it did, but then falling through to check `isinstance(args[0], ...)` for non-FITS files. The fix simplifies this to directly return the boolean result of the extension check. An experienced engineer familiar with the codebase could identify this as a simple control flow issue and implement the fix relatively quickly. The test case is also straightforward - just verifying that `is_fits` returns False for a non-FITS file.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests the ability to understand a regression issue, trace through code logic, and implement a clean fix. The issue provides sufficient context about the problem domain and includes a reproducible test case.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14365": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the ascii.qdp Table format in astropy assumes QDP commands must be uppercase (like \"READ SERR 1 2\") when QDP itself is case-insensitive and accepts lowercase commands (like \"read serr 1 2\"). The issue provides a concrete example of the failing case with a minimal test file containing \"read serr 1 2\" that should work but doesn't. The expected behavior is clearly stated - the file should read into a Table with errors rather than crashing. The reproduction steps are complete and specific, including the exact commands to run and the expected vs actual behavior. An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that requires adding case-insensitive matching to the QDP parser. Looking at the provided patch, the solution involves: 1) Adding re.IGNORECASE flag to the regex compilation in _line_type() function, and 2) Converting \"NO\" values to uppercase before comparison. The changes are minimal (2 lines modified) and localized to the astropy/io/ascii/qdp.py file. An experienced engineer familiar with the codebase could identify the parsing logic, understand that regex matching needs to be case-insensitive, and implement the fix within 15-60 minutes. The solution doesn't require deep algorithmic thinking or extensive refactoring - it's a clear case of making existing string matching case-insensitive.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, straightforward implementation, and good test coverage. The issue demonstrates a real-world problem (hand-created QDP files often use lowercase commands) and the solution is backwards compatible. The test patch shows comprehensive verification including both uppercase and lowercase variants.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14508": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: the `io.fits.Card._format_float()` function creates unnecessarily long string representations of floats, which causes comment truncation. The issue includes a concrete, reproducible example showing the exact problem - when creating a Card with value 0.009125, it gets formatted as \"0.009124999999999999\" instead of the more concise \"0.009125\". The reporter provides the exact location of the problematic code (line 1300-1302 in astropy/io/fits/card.py), explains the root cause, and even suggests a potential solution approach. The expected behavior is clearly stated: being able to create any valid FITS Card via `io.fits.Card`. All necessary context is provided including version information and a working example that demonstrates both the problem and the desired outcome.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic function `_format_float()` and even suggests the solution approach. The fix involves changing the float formatting logic from using `f\"{value:.16G}\"` to first trying `str(value)` and only using the more verbose formatting if needed. The gold patch shows this is exactly what was implemented - a relatively simple change to the formatting logic. While understanding the FITS format requirements and ensuring the fix doesn't break edge cases requires some thought, the core change is straightforward and localized to a single function. The test patch also shows simple test cases that verify the fix works correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, good test coverage, and a focused solution. The issue provides excellent reproduction steps and the expected behavior is unambiguous. An experienced engineer should be able to implement and test this fix effectively within the given time constraints.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14995": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear information about what needs to be fixed. It describes a specific problem in v5.3 where NDDataRef mask propagation fails when one operand doesn't have a mask, particularly with `handle_mask=np.bitwise_or`. The issue includes a complete reproducible example showing the exact operations that fail, the expected behavior (mask should be copied over to output when one operand lacks a mask), and mentions it worked correctly in v5.2. The code examples clearly demonstrate the problematic scenarios (mask * no mask operations) and show exactly where the failure occurs. All necessary context is provided including version information and the specific function/parameter involved.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is a simple one-line change in the `_arithmetic_mask` method in `astropy/nddata/mixins/ndarithmetic.py`. The bug is in line 523 where `elif operand is None:` should be `elif operand.mask is None:`. The issue is that the code was checking if the entire operand was None rather than checking if the operand's mask was None. This is a straightforward logical error that requires minimal code change but does require understanding the mask propagation logic to identify the root cause. An experienced engineer would need some time to trace through the code and understand how mask handling works, but once identified, the fix itself is trivial.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, has a clear reproduction case, includes comprehensive test coverage in the patch, and represents a realistic debugging scenario that tests both problem identification and understanding of mask propagation logic in scientific computing libraries.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7336": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes that the `units.quantity_input` decorator fails when used on constructors that have a type hinted return value of `-> None`. The problem is that the decorator tries to call `.to()` on `None`, which doesn't have this attribute. The issue includes a minimal reproducer that demonstrates the exact problem, shows the error scenario, mentions a workaround (removing the type hint), and even suggests a potential fix (checking explicitly for None return values). An experienced engineer would have all the information needed to understand the problem and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is a simple one-line change in the decorator logic: changing the condition from checking if return annotation is not `inspect.Signature.empty` to checking if it's not either `inspect.Signature.empty` or `None`. The fix requires understanding how the decorator works and recognizing that `None` should be treated as a special case like an empty signature. While it requires some thought to understand the decorator logic, the actual code change is minimal and straightforward once the problem is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for a coding benchmark. The issue is clearly defined with a good reproducer, the solution is focused and requires understanding of Python decorators and type annotations, and the test changes are appropriate to verify the fix works correctly. The problem tests both debugging skills (understanding why None.to() fails) and knowledge of Python's typing system and decorator patterns.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7606": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear code example showing the problematic behavior: `x = u.Unit('asdf', parse_strict='silent')` followed by `x == None` which raises a TypeError instead of returning False. The expected behavior is explicitly stated in the comment \"Should be False\". The issue title also clearly indicates this is about \"Unit equality comparison with None raises TypeError for UnrecognizedUnit\". An experienced engineer would understand that they need to fix the equality comparison method for UnrecognizedUnit objects to handle comparison with None (and likely other non-unit objects) gracefully by returning False instead of raising an exception.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly localized to the equality comparison method of UnrecognizedUnit class. The solution involves modifying the __eq__ method to handle None and other invalid comparison targets by returning NotImplemented (which Python converts to False for equality comparisons) instead of letting exceptions propagate. Looking at the gold patch, it's a small change adding proper exception handling around the Unit() constructor call and returning NotImplemented when it fails. This requires understanding Python's comparison protocol and basic exception handling, but is not complex from an implementation standpoint.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the expected behavior is clear, and the solution is straightforward. The test patch confirms the fix works correctly by testing equality comparisons with None and ensuring they don't raise exceptions. This is a good benchmark sample as it tests understanding of Python's comparison protocol and proper exception handling in equality methods.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7671": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It provides concrete examples showing that minversion function fails when comparing version strings with development indicators (like '1.14dev') due to a bug in LooseVersion from distutils. The issue demonstrates the exact failure case with reproducible code examples, shows that it works without the patch version number ('.3'), and references that pkg_resources.parse_version (which was previously used) handles this correctly. The problem is that PR #7647 changed the implementation to use LooseVersion, which has this known bug. A successful solution needs to handle version strings with development indicators properly in the minversion function.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified (LooseVersion bug with dev versions), the location is specified (minversion function affected by PR #7647), and the solution approach is hinted at (the regex from PEP440). The fix involves adding a regex pattern to extract only the numeric version part before comparison, which is a small, focused change to the minversion function. An experienced engineer would need some time to understand the version comparison logic and implement the regex-based solution, but it's not a complex algorithmic problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. It has a clear problem statement, concrete examples, and a focused solution scope. The provided test case addition appropriately verifies the fix handles the problematic case ('0.12dev'). An engineer would be able to understand and solve this with the given information.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-10554": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with a clear reproduction case showing the problem. The issue describes a bug where using `.order_by()` and `.values_list()` on a union queryset causes the original queryset to break when accessed again. The reproduction steps are concrete: create a union of filtered Dimension objects, apply ordering, call `.order_by().values_list()`, then try to access the original queryset again which fails. However, there are some minor gaps - the exact error message when it \"breaks\" isn't shown, and the model structure (what the 'order' field is) isn't fully detailed. But the core problem is clear enough for an experienced Django developer to understand and work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's ORM internals, specifically how union querysets work with ordering and column selection. The bug involves the SQL compiler logic where ORDER BY terms need to match columns in the result set. Looking at the patch, it required modifying both the SQL compiler (django/db/models/sql/compiler.py) and query classes (django/db/models/sql/query.py) to properly handle cases where ORDER BY columns aren't already in the SELECT clause. The solution adds logic to automatically include missing ORDER BY columns in the SELECT statement and adds a new method `add_select_col`. This level of ORM internals work typically requires substantial understanding of Django's query compilation process and would take an experienced developer several hours to research, understand, and implement correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The reproduction case is clear and testable, the problem affects a specific but realistic use case (union querysets with ordering), and the test patch provides good coverage of the fix including multiple ordering scenarios. This is a suitable benchmark issue for evaluating Django ORM debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11087": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some blanks to fill in. The core problem is clear: Django's .delete() operation is fetching unnecessary fields during cascade deletion, which causes UnicodeDecodeError when those fields contain problematic data. The issue specifically identifies that the problem occurs in the text_log_error.line field that \"is not actually needed for the .delete()\" operation. However, the issue description doesn't specify exactly which fields should be considered \"required\" vs \"unnecessary\" for deletion operations, leaving some interpretation to the implementer. The gold patch shows this involves selecting only referenced fields (foreign key fields) during cascade deletion unless signals are connected. While the high-level goal is clear (optimize .delete() to use only required fields), the specific implementation details about what constitutes \"required fields\" and how to determine them would need to be figured out by the engineer.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours of work for several reasons: 1) Understanding Django ORM deletion mechanics and cascade behavior requires domain knowledge, 2) The solution involves modifying the core deletion logic in django/db/models/deletion.py, which is a critical and complex part of Django, 3) The implementation requires understanding relationships between models and determining which fields are actually referenced during deletion, 4) The solution must account for signal listeners (pre_delete/post_delete) which change the requirements for field selection, 5) The patch shows substantial logic changes including imports, new helper methods, and complex field selection logic with itertools.chain and get_candidate_relations_to_delete, 6) Writing comprehensive tests requires understanding the interaction between model relationships, deletion cascades, and signal handling. While not the most complex Django issue possible, this definitely requires substantial understanding of Django internals and careful implementation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-documented with clear reproduction steps, the solution has a reasonable scope, and the test coverage demonstrates the fix works correctly. The issue represents a legitimate optimization problem in Django that would benefit from the proposed solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11265": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. It clearly demonstrates a bug where using `exclude()` on a queryset with annotated `FilteredRelation` causes a `FieldError`. The reporter provides a concrete example showing the difference between using `filter()` (which works) and `exclude()` (which fails) on the same queryset with `book_alice__isnull=False`. They also point to the likely problematic function `split_exclude()` and mention that \"A new query is created without all extra datas from the original query.\" However, the issue lacks some details like the exact error message, stack trace, or deeper explanation of why this happens. An experienced engineer would need to investigate the Django ORM internals to understand the root cause, but the core problem and expected behavior are clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's ORM internals, specifically how `FilteredRelation` annotations work and how the `split_exclude()` method processes queries. Looking at the gold patch, the solution involves two key parts: (1) copying `_filtered_relations` from the original query to the new query in `split_exclude()`, and (2) modifying the `trim_start()` method to avoid trimming INNER JOINs from filtered relations. This requires deep knowledge of Django's query compilation, join handling, and the relationship between filtered relations and query exclusion. An engineer would need to trace through the ORM code, understand why the filtered relation data is lost during exclude operations, and implement the fix while ensuring it doesn't break other functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability. It tests knowledge of Django ORM internals, debugging skills, and the ability to understand complex object-relational mapping concepts. The test case provided clearly validates the expected behavior, making it easy to verify if a solution works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11299": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a Django bug where CheckConstraint with OR operator generates incorrect SQL on SQLite and Oracle. The problem is that Django incorrectly includes fully qualified field names (e.g. \"my_table\".\"my_field\") in part of the check constraint when there's a combination of OR and AND clauses. The issue provides a concrete example with a TestConstraint model containing field_1 and flag fields, shows the problematic generated SQL that includes \"new__app_testconstraint\".\"field_1\" and \"new__app_testconstraint\".\"flag\" in the AND clause but just \"flag\" in the OR clause, and shows exactly what the correct SQL should look like. The root cause is identified as AND clause items using Col while OR clause uses SimpleCol. This gives a developer everything needed to understand and fix the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively simple fix requiring only one line of code change in django/db/models/sql/query.py. The fix involves passing the simple_col parameter to a recursive call in the _add_q method. While understanding the Django ORM query building system requires some familiarity with the codebase, the actual solution is straightforward once you trace through how CheckConstraints are processed and identify that the simple_col parameter needs to be propagated through recursive calls. An experienced engineer familiar with Django's internals could identify and implement this fix within 15 minutes to 1 hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This issue is excellent for a coding benchmark. It provides clear reproduction steps, explains the root cause, shows expected vs actual behavior, and has a precise solution. The fix requires understanding Django's query building internals but is implementable with a single line change. The provided tests adequately verify the fix works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11532": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The problem is clearly described: when the computer hostname contains non-ASCII characters (like \"\u6b63\u5b97\") and email encoding is set to a non-unicode encoding like iso-8859-1, Django crashes when trying to create email messages. The issue provides specific context including the failing test location (tests/mail/tests.py#L368), the problematic code location (django/core/mail/message.py#L260), clear steps to reproduce (set hostname to non-ASCII value and run mail tests), and even suggests a fix (convert domain name to punycode). A test case is also provided showing the expected behavior. While an engineer might need to explore the codebase to understand the full context and implementation details, the core problem and solution direction are clear enough for a meaningful attempt.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because it requires: (1) Understanding the email encoding system in Django and how Message-ID headers are generated, (2) Identifying all locations where domain names are used that could contain non-ASCII characters, (3) Creating a centralized punycode function in django.utils.encoding, (4) Modifying multiple files (message.py, utils.py, validators.py, html.py) to use this new function consistently, and (5) Writing appropriate tests. The solution touches multiple modules and requires understanding the email RFC standards and punycode encoding. While not extremely complex, it requires careful analysis to ensure all edge cases are covered and the solution is applied consistently across the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is technical but well-bounded, the solution approach is clear, and the test coverage demonstrates the fix works as expected. This would be a good benchmark sample for evaluating coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11551": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) A clear reproduction case with specific code examples showing the problem, (2) Detailed explanation of the root cause including the specific commit that introduced the regression, (3) A comprehensive truth table showing all the logical cases that need to be handled, (4) Complete proposed solution code for the `_check_list_display_item` function in `django/contrib/admin/checks.py`. The issue author has done thorough analysis and provides everything needed to understand both the problem and the correct solution approach.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because: (1) The issue provides the exact solution code, making implementation straightforward, (2) It's a single function modification in one file (`_check_list_display_item` in `django/contrib/admin/checks.py`), (3) The logic change is well-explained with a truth table showing all cases, (4) Most time would be spent understanding the Django admin validation flow and testing the fix, rather than figuring out what to implement. The core change involves restructuring the conditional logic to remove the `hasattr(model, item)` check and properly handle the try/except blocks.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is actually an excellent benchmark sample because it requires understanding complex conditional logic, exception handling, and Django's admin validation system. The issue is about a regression fix that broke other functionality, which is a realistic software engineering scenario. The provided test case and truth table make it easy to verify correctness.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11734": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with concrete test cases that demonstrate the problem. The description shows that OuterRef() works correctly with filter() but crashes when used with exclude() or ~Q(). The test case clearly shows the specific Django ORM operations that fail: using OuterRef('pk') within Item.objects.exclude() and Item.objects.filter(~Q()). However, there are some blanks to fill in - the issue doesn't explicitly explain what the expected behavior should be (should it work the same as filter?) or provide details about the crash/error messages. An experienced engineer would need to infer that the goal is to make exclude() and ~Q() work consistently with filter() when using OuterRef.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's ORM internals, specifically how OuterRef is handled differently in various query contexts. The solution involves changes across multiple files (fields/__init__.py, related_lookups.py, and sql/query.py) and requires understanding the query compilation process. The engineer needs to trace through how OuterRef objects are processed in exclude() vs filter() operations, identify why the model resolution fails, and implement proper handling in the split_exclude method. While not extremely complex, it requires substantial knowledge of Django's query system and careful debugging to identify the root cause.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for the benchmark. The test case provides a clear way to verify the solution works correctly, and the problem is specific enough that a solution can be validated objectively. The domain knowledge required (Django ORM) is reasonable for a coding assessment.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11740": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The author clearly describes the problem: when changing a UUIDField to a ForeignKey in Django models, the generated migration doesn't include proper dependencies for the referenced model (App2). The example code shows the before/after states of the models, making it clear what transformation is happening. However, there are some gaps in specification: (1) The exact expected behavior isn't fully detailed - what should the migration look like with proper dependencies? (2) The description mentions \"missing any dependencies for App2\" but doesn't specify exactly what dependencies should be present or where they should appear in the migration file. (3) While the problem is clear (missing dependencies), the solution requirements could be more explicit about what constitutes a \"correct\" migration with dependencies. Despite these gaps, an experienced Django developer familiar with the migration system could reasonably interpret that the migration should include dependency declarations to ensure App2 migrations run before this AlterField operation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the gold patch, the solution involves modifying the Django migration autodetector in django/db/migrations/autodetector.py. The fix requires: (1) Understanding Django's migration dependency system and how autodetection works, (2) Identifying where in the generate_altered_fields method to add dependency logic, (3) Adding a dependencies list and calling _get_dependencies_for_foreign_key for new foreign key fields, (4) Passing these dependencies to the AlterField operation. This requires substantial knowledge of Django internals, specifically the migration autodetector system. An engineer would need to understand how Django tracks field changes, generates migrations, and manages dependencies between apps. The code change itself is relatively small (adding ~3 lines), but finding the right place to make the change and understanding the existing dependency system would take considerable time. The test also shows this affects cross-app dependencies, adding complexity to the analysis.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues that would prevent using this sample for evaluation. The problem is a legitimate Django migration system bug with a clear technical solution. The issue demonstrates good software engineering skills including understanding of ORM relationships, migration systems, and dependency management. The fix requires both understanding existing code architecture and implementing the correct solution, making it a good test of debugging and problem-solving abilities.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11815": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified with clear problem description, concrete example, and expected solution. The user provides a specific models.py example showing how Status enum with translated values causes migration problems, shows the problematic generated migration code, explains why it fails (translation changes the enum values), and proposes the exact fix needed (use Status['GOOD'] instead of Status('Good')). The technical details are precise - they identify that the issue is in Django's migration serialization of enum objects, where it should use the enum name rather than the value to avoid translation-related breakage.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution involves modifying the EnumSerializer.serialize() method in django/db/migrations/serializer.py to use enum name instead of value. The change is straightforward: replace the complex value serialization logic with a simple format string using self.value.name and bracket notation. The core logic change is only about 5 lines, but requires understanding Django's migration serialization system and enum handling. An experienced engineer would need some time to locate the right file and understand the serialization flow, but the actual fix is quite simple once identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with a clear technical solution that can be properly tested and validated.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11964": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when using TextChoices/IntegerChoices with Django model fields, the returned value should be a string/integer (the underlying type) but instead returns an enum object. The issue provides a complete, concrete example with a model definition using TextChoices, test cases that demonstrate the expected behavior (asserting the field value should be a str with value \"first\"), and clearly states what the expected results should be. An engineer would understand exactly what needs to be fixed: ensure that accessing a model field with choices returns the underlying value type rather than the enum object.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward issue that should take 15 minutes to 1 hour to solve. Looking at the provided solution, it only requires adding a __str__ method to the Choices class in django/db/models/enums.py that returns str(self.value). The problem is well-localized to the enum implementation, and the fix is a simple 5-line method addition. An experienced engineer familiar with Django would quickly identify that the issue stems from enum behavior and that overriding __str__ to return the value instead of the enum representation is the appropriate solution.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, has a focused scope, and has a clean, minimal solution that can be easily verified through the provided test cases. The problem domain (Django model enums) is well-understood, and the solution approach is straightforward for someone familiar with Python enums and Django.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12155": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: docutils reports an error when rendering docstrings where the first line is not empty (i.e., where text starts immediately after the opening triple quotes). The issue identifies the exact problematic code in the `trim_docstring` function in `django/contrib/admindocs/utils.py`, specifically the line that calculates indentation: `indent = min(len(line) - len(line.lstrip()) for line in lines if line.lstrip())`. It explains that the problem occurs because the first line's indentation is 0, which breaks the minimum calculation. The issue even provides a proposed solution: skip the first line when calculating indentation by changing to `lines[1:]`. The description includes a clear example of the problematic docstring format and explains why current Django docstrings work (they have an empty first line) while standard Python docstrings don't.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location and provides a specific solution approach. However, looking at the actual patch, the solution is different from what was suggested - instead of modifying the `trim_docstring` function, the developers replaced it entirely with Python's built-in `inspect.cleandoc` function. This requires some understanding that `cleandoc` handles the docstring trimming problem correctly, and involves removing the custom `trim_docstring` function and updating all its usages. An experienced engineer would need time to verify that `cleandoc` is the appropriate replacement and ensure all call sites are updated properly, but the overall change is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution space is well-bounded, and the expected behavior is unambiguous. The issue provides enough context for an engineer to understand both the technical problem and implement a proper fix. The test patch shows that the solution should handle docstrings without leading line feeds without producing errors, which is a reasonable and testable requirement.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12262": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: custom template tags with keyword-only arguments that have defaults are raising TemplateSyntaxError inappropriately. The issue provides two concrete code examples that demonstrate the exact failure scenarios: (1) a tag with keyword-only argument with default value that should accept a custom value, and (2) a tag receiving duplicate keyword arguments. The examples show the specific template tag definitions using @register.simple_tag and the corresponding template usage that fails. The issue mentions this affects both simple tags and inclusion tags, and notes the problem exists since version 2.0. An experienced engineer would have sufficient information to understand what needs to be fixed and could identify that this is likely related to parameter parsing logic in Django's template library.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides clear examples of what's broken, and the problem is localized to template tag parameter parsing logic. Looking at the actual patch, it's a one-line change in django/template/library.py where 'params' is changed to 'kwonly' in a conditional check. An experienced engineer would need to: (1) understand Django's template tag parsing mechanism, (2) locate the parse_bits function, (3) identify that keyword-only parameters aren't being handled correctly in the validation logic, and (4) make the simple fix to check against kwonly parameters instead of regular params. The logic is straightforward once you understand the parameter parsing flow.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with clear reproduction steps and a straightforward fix. The issue provides good examples and the problem domain (Django template tag parsing) is well-documented. An experienced engineer should be able to solve this efficiently.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12325": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a problem with Django's Multi-Table Inheritance (MTI) where the order of OneToOneField declarations affects behavior, specifically when there are multiple OneToOne references. The code examples show two versions of a Picking class - one that doesn't work (with document_ptr first) and one that works (with origin first). The issue mentions that \"order seems to matter\" and questions whether this is correct behavior, noting that there should be an explicit parent_link marker. While the exact expected behavior isn't completely explicit, a reasonable interpretation is that the order shouldn't matter and the parent_link=True should properly identify the parent relationship regardless of field declaration order. The problem is concrete enough with code examples to understand what needs fixing.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: 1) It requires understanding Django's complex Multi-Table Inheritance implementation and how OneToOneField parent links are processed during model metaclass creation, 2) The solution involves modifying core Django ORM code in base.py and options.py files, 3) Looking at the patch, it requires understanding the relationship between parent_link detection logic and field processing order, 4) The fix involves both changing the parent link detection logic (adding field.remote_field.parent_link check) and removing validation that was incorrectly enforcing parent_link requirements, 5) This type of ORM internals issue requires substantial knowledge of Django's model system and careful consideration of edge cases.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's model system, has clear reproduction cases, and the solution doesn't require external dependencies or complex setup. The issue focuses on core ORM behavior which is a good test of Django internals knowledge.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12663": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It provides a clear description of the regression (SimpleLazyObject with nested subquery annotation fails), includes concrete model definitions (classes A, B, C), provides a complete test case that reproduces the bug, references the specific commit that introduced the regression (35431298226165986ad07e91f9d3aca721ff38ec), and gives a working code example that should pass but currently fails. An experienced engineer would have all the information needed to understand the problem and work towards a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly a regression with a specific root cause (the referenced commit), and the solution is a small, targeted change. The patch shows it's just a 2-line modification in the output_field property to handle the 'target' attribute properly. An experienced engineer familiar with Django's ORM would need some time to trace through the query building logic and understand why SimpleLazyObject fails with subqueries, but the fix itself is straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample is well-suited for the benchmark as it tests understanding of Django's ORM internals, specifically how query annotations and lazy objects interact. The issue has a clear reproduction case and the solution is verifiable through the provided test.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12708": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear problem scenario: migration crashes when deleting index_together if there's a unique_together on the same fields. The steps to reproduce are specific (create models with 2 fields, add to both unique_together and index_together, then delete index_together). The user also explains the context - they're trying to refactor to use Django 1.11's Options.indexes feature. However, there are some gaps: the exact error message/crash details aren't provided, and the technical specifics of why this conflict occurs aren't explained. An experienced engineer would need to investigate the Django migration system and understand how index_together and unique_together interact, but the core problem is clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's migration system internals, specifically how index_together and unique_together constraints interact during deletion. The engineer needs to: 1) Reproduce the issue by understanding Django's schema editor and migration operations, 2) Investigate why deleting index_together fails when unique_together exists on same fields, 3) Identify that the _delete_composed_index method needs to distinguish between index and unique constraints, 4) Modify the method to specify {'index': True, 'unique': False} to avoid conflicts. The solution involves understanding database constraints and Django's introspection system. The patch is relatively small but requires deep knowledge of Django's ORM internals and careful testing with the migration system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a solid benchmark issue because it tests understanding of Django's migration system, database constraints, and ORM internals. The solution requires both debugging skills to understand the crash and architectural knowledge to implement the fix correctly. The test patch shows comprehensive coverage including edge cases with multiple constraint types.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12754": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with a clear example showing the problem. It describes a Django migration issue where moving a field from a base model to a subclass in the same migration step causes the migration to fail during execution (though makemigrations works). The issue provides concrete before/after code examples showing a `Readable` model with a `title` field being refactored into a `Readable` base class and `Book` subclass with the `title` field moved to the subclass. The problem is that the auto-generated migration creates operations in the wrong order (CreateModel then RemoveField) when it should be RemoveField then CreateModel. While some implementation details are left to figure out (like exactly where in the Django migration autodetector code to make changes), the core problem and expected behavior are clear enough for an experienced engineer to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because it requires understanding Django's migration autodetector system, which is a fairly complex piece of code. The engineer would need to: 1) Understand how Django's migration autodetector generates migration operations, 2) Identify where the dependency ordering logic exists, 3) Understand the specific case where moving fields between inheritance hierarchies creates ordering problems, 4) Implement logic to detect when a field is being moved from base to subclass and create appropriate dependencies. Looking at the patch, it adds about 10 lines of code to handle dependency detection for removed base fields, but requires understanding the intricate migration autodetector logic and model state management. The test also shows this requires understanding Django's ModelState system and migration testing patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This issue is well-suited for the benchmark. It has a clear problem description with concrete examples, the solution is non-trivial but achievable for an experienced engineer, and the test case clearly validates the fix. The issue demonstrates both problem-solving skills (understanding the dependency ordering issue) and Django framework knowledge (migration autodetector mechanics). The patch shows a targeted fix that doesn't require massive code changes, making it a good benchmark for assessing coding ability in a real-world scenario.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12858": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It describes a specific problem where models.E015 error is incorrectly raised when ordering uses lookups like '__isnull' which should be valid. The issue provides a concrete example showing that the ordering 'supply__product__parent__isnull' works fine in practice but triggers a validation error. The reporter mentions this was fine until #29408 was implemented and provides the model relationship structure. While the exact nature of models.E015 and the validation logic isn't explicitly detailed, an experienced Django developer familiar with the codebase would understand this refers to model validation checks that incorrectly flag valid lookup-based ordering as invalid. The core requirement is clear: fix the validation to allow lookups (not just transforms) in ordering specifications.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minutes to 1 hour fix. Looking at the gold patch, it's a very small change to django/db/models/base.py - just adding an additional condition to check for lookups alongside transforms in the validation logic. The fix involves adding 'and fld.get_lookup(part) is None' to an existing conditional. An experienced Django developer would need to: 1) Locate the models.E015 validation code (likely in model checking), 2) Understand that the current validation only checks for transforms but not lookups, 3) Add the lookup check to allow valid lookups like '__isnull'. The accompanying test is also straightforward - just adding a test case that verifies ordering with '__isnull' lookup doesn't trigger validation errors. While some familiarity with Django's internal validation and lookup/transform system is needed, this is a small, targeted change once the validation code is located.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is specific to Django's model validation system, provides a clear reproduction case, and has a well-defined solution. The fix is small but requires understanding of Django internals around model validation and the distinction between lookups and transforms. It tests both debugging skills (finding the validation code) and Django framework knowledge (understanding when lookups should be allowed in ordering). The test case clearly validates the fix works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13012": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when an ExpressionWrapper contains a constant expression (like Value(3)), Django incorrectly includes that constant in the GROUP BY clause, causing PostgreSQL errors. The issue provides concrete examples showing both the problematic behavior (with ExpressionWrapper) and the correct behavior (without ExpressionWrapper). It shows the exact SQL queries generated in both cases, making it crystal clear what the expected vs. actual behavior is. The root cause is evident - ExpressionWrapper is not properly delegating the group_by behavior to its wrapped expression, so constant expressions aren't being filtered out of GROUP BY clauses as they should be.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is quite straightforward - adding a single method `get_group_by_cols` to the ExpressionWrapper class that delegates to the wrapped expression. The issue is well-isolated to the ExpressionWrapper class in django/db/models/expressions.py. An experienced engineer would need to understand how Django's ORM handles GROUP BY clauses and recognize that ExpressionWrapper needs to delegate this behavior to its wrapped expression. The actual code change is minimal (3 lines), but requires understanding the Django ORM's expression system and GROUP BY handling logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear, focused solution. The problem is specific to Django's ORM expression handling, and the fix involves proper delegation of GROUP BY behavior. The test cases are also straightforward and verify both the empty group_by case (constant expressions) and non-empty case (non-constant expressions). This would make an excellent benchmark sample as it tests understanding of object-oriented design patterns (delegation) within a real-world framework context.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13028": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified but has some gaps. The user provides clear model definitions and shows the exact error case, plus identifies a workaround that confirms the root cause (naming conflict with 'filterable' field). However, the issue lacks the actual error message/traceback that would help pinpoint exactly where the NotSupportedError is raised. The user mentions \"Error happened when filtering\" but doesn't show the full stack trace. Despite this, an experienced engineer could reasonably deduce that this is related to Django's internal filterable attribute checking mechanism, especially given that renaming the field fixes it. The gold patch confirms this is in django/db/models/sql/query.py's check_filterable method.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This would take 1-4 hours for an experienced engineer. The issue requires understanding Django's internal query processing, specifically how the check_filterable method works and when it's called. The engineer would need to: 1) Reproduce the issue with the given models, 2) Trace through Django's query building to find where NotSupportedError is raised, 3) Understand that model instances have a 'filterable' attribute that conflicts with Django's expression filtering logic, 4) Modify the check_filterable method to distinguish between Django expressions and model instances. The fix itself is small (adding a hasattr check for 'resolve_expression'), but requires deep understanding of Django's ORM internals and the distinction between database expressions and model instances.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is reproducible, the solution is clear from the patch, and it represents a legitimate bug in Django's ORM filtering logic. The test case also properly validates the fix by creating a model with filterable=False and confirming it can be used in queries.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13112": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides concrete information about the problem: it crashes during makemigrations with mixed-case app names in Django 3.1b1, includes complete model definitions showing ForeignKey relationships, and specifies it worked in Django 3.0. However, it doesn't explicitly state what the crash error message is or the exact command that fails. The title mentions \"makemigrations crashes\" but the description says \"migrate\" which creates some ambiguity. An experienced engineer could reasonably infer this is about the migration system handling mixed-case app names in ForeignKey references, but some details need to be filled in.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django's migration system and how ForeignKey deconstruction works, (2) The engineer needs to trace through the codebase to find where mixed-case app names are being incorrectly handled, (3) The solution involves modifying the deconstruct method in django/db/models/fields/related.py to properly handle app_label.model_name format with mixed-case app labels, (4) It requires understanding the difference between how string references and model references are handled. While the actual code change is small (about 6 lines), finding the right location and understanding the issue requires significant investigation into Django's internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's ORM system, the test case clearly demonstrates the expected behavior, and the solution is verifiable through the migration system. The issue represents a real-world scenario that developers might encounter when using mixed-case app names.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue has some blanks to fill in but provides a sensible interpretation. The title \"make temporal subtraction work without ExpressionWrapper\" combined with the code example clearly indicates the desired functionality - temporal field subtraction using F() expressions should work directly. While the issue doesn't explicitly state what's currently broken or provide error messages, the context suggests that the shown code doesn't work as expected and requires ExpressionWrapper. The gold patch and test changes confirm this interpretation by showing removal of ExpressionWrapper calls from temporal subtraction operations. An experienced engineer could reasonably infer that the goal is to enable direct temporal arithmetic with F() expressions.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it involves understanding Django's ORM expression system and moving complex logic from as_sql() to resolve_expression() method. The patch shows significant refactoring - moving 30+ lines of temporal type checking logic between methods, handling edge cases for different database backends, and ensuring proper expression resolution. It requires understanding when expressions are compiled vs resolved, database feature detection, and the inheritance hierarchy of expression classes. While not extremely esoteric, it demands solid knowledge of Django's internals and careful testing across different temporal field combinations.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-scoped to Django ORM expressions, has a clear functional requirement, and the test changes provide good validation criteria. An engineer could implement a solution and verify it works using the temporal subtraction test cases.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13297": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some inference. The reporter clearly describes a breaking change between Django 3.0 and 3.1 where kwargs values in TemplateView.get_context_data() are now SimpleLazyObjects instead of the expected types. They provide working example code showing the problem and their workaround (explicit string conversion). The core issue is that SimpleLazyObjects cause crashes when used with get_object_or_404 filtering. However, the description doesn't explicitly state what the \"crash\" is or provide the actual error message. An experienced engineer would reasonably understand that the solution needs to ensure kwargs values have the correct types when passed to filtering operations, but some details about the exact failure mode need to be inferred.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Understanding Django's lazy object system and how SimpleLazyObject differs from the lazy() function, (2) Identifying the specific location in TemplateView where kwargs are being wrapped incorrectly, (3) Understanding the deprecation warning system context, and (4) Figuring out that lazy() with type preservation is the correct replacement for SimpleLazyObject. The actual code change is small (replacing SimpleLazyObject with lazy(access_value, type(value))()), but requires navigating Django's generic view system and understanding the subtle differences between lazy object implementations. The fix involves modifying the _wrap_url_kwargs_with_deprecation_warning function in django/views/generic/base.py.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly a regression that needs fixing, the example code is realistic and demonstrates the issue well, and the provided test adequately verifies that the fix works with filtering operations. The issue is suitable for a coding benchmark as it tests understanding of Django internals, lazy evaluation, and type preservation.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13406": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: when pickling and unpickling Django queryset queries that use values()/values_list(), the reconstructed queryset incorrectly returns model instances instead of dictionaries. The issue includes a complete minimal reproducible example with the models.py file defining the Toy model and the exact crashing code that demonstrates the problem. The expected behavior is clearly stated (should return dictionaries like the original queryset) versus the actual behavior (returns broken model instances). The issue also provides helpful context about following Django's official documentation on pickling querysets, making it clear this should be supported functionality.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution is quite straightforward - it's essentially a 2-line fix in the query setter method of django/db/models/query.py. The problem is that when a pickled query with values_select is restored, the queryset loses track of what iterable class it should use to process results. The fix simply checks if the restored query has values_select and sets the appropriate iterable class. An experienced engineer familiar with Django's ORM would quickly identify that this is about the relationship between query state and result processing, and the solution follows a clear pattern in the Django codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. This is an excellent benchmark sample - the problem is clearly defined, the solution is focused and testable, and it represents a real-world scenario that developers encounter when working with Django's ORM. The issue involves understanding Django's internal architecture but doesn't require extensive domain knowledge beyond typical Django development experience.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13449": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A complete, minimal reproducible example with the exact model definition, query code, and generated SQL; (2) Clear identification of the problem - the CAST() statement is incorrectly placed around just LAG() instead of the entire window expression; (3) Specific context that this only affects DecimalField on SQLite; (4) A working workaround using output_field=FloatField(); (5) Demonstration that other field types work correctly. An experienced engineer has all the information needed to understand the root cause and implement a fix.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours of work. The engineer needs to: (1) Understand Django's ORM expression system and how Window functions work; (2) Investigate SQLite-specific numeric casting behavior and the SQLiteNumericMixin; (3) Understand how Django generates SQL for different database backends; (4) Implement a custom as_sqlite method that handles DecimalField casting correctly; (5) Write appropriate tests. The solution involves modifying the Window class inheritance and adding database-specific logic, which requires substantial understanding of Django's internals but isn't extremely complex.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained database backend compatibility issue with a clear problem statement and testable solution. The issue demonstrates good software engineering practices with minimal reproduction cases and proposed workarounds.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13568": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clear: Django's auth.E003 system check currently only looks for unique=True on the USERNAME_FIELD but should also accept UniqueConstraint definitions in the model's Meta.constraints. The issue explains why this matters (avoiding implicit *_like indexes on PostgreSQL) and provides a concrete example of a User model that should pass the check but currently doesn't. However, there are some implementation details left ambiguous, such as whether partial unique constraints (with conditions) should be accepted or only total unique constraints. The provided patches clarify that only total unique constraints should be considered, but this isn't explicitly stated in the issue description itself.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minutes to 1 hour fix. The solution involves modifying a single function (check_user_model in django/contrib/auth/checks.py) to add an additional condition that checks for UniqueConstraint objects in cls._meta.total_unique_constraints that match the USERNAME_FIELD. The logic is straightforward: extend the existing if condition to also check for model-level unique constraints in addition to field-level unique=True. An experienced engineer familiar with Django's codebase could understand the problem quickly by looking at the existing check and the provided example, then implement the fix by adding a few lines of code to check the constraints list.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a straightforward enhancement to Django's system checks that has clear requirements and a well-defined scope. The issue provides good context about the motivation (PostgreSQL index behavior) and includes a concrete example that helps understand the expected behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13807": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) Clear steps to reproduce: create a Model called \"Order\" (a SQL keyword), create fixtures, and use loaddata, (2) Specific root cause identification: missing proper quoting around table names in SQLite PRAGMA statements, (3) Exact file location and line numbers: django/db/backends/sqlite3/base.py line 327 in check_constraints function, (4) Concrete code snippets showing the problematic SQL statements like 'PRAGMA foreign_key_check(%s)' and 'PRAGMA foreign_key_list(%s)', (5) Clear explanation that the issue is due to missing back ticks/quotes around %s placeholders when table names are SQL keywords. The issue gives an engineer everything needed to understand the problem and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue pinpoints the exact problem location (line 327 in check_constraints) and root cause (missing quotes around table names). The solution involves replacing direct string formatting with proper name quoting using self.ops.quote_name(). An experienced engineer would need to: (1) Understand Django's database backend architecture and quote_name functionality (15-30 min), (2) Locate the specific lines mentioned and understand the PRAGMA statements (5-10 min), (3) Apply the fix by wrapping table_name and column_name variables with self.ops.quote_name() (10-15 min). The code change is straightforward and localized to a few lines in one function.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear reproduction steps and a specific technical solution. The issue provides sufficient context about SQL keywords causing problems in SQLite, and the solution (proper name quoting) is a standard database practice. The test provided validates the fix appropriately by creating a model with SQL keyword table and column names.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13810": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The reporter describes a specific problem with Django's middleware handling in ASGI contexts where MiddlewareNotUsed exceptions cause side effects. They identify the root cause in django/core/handlers/base.py line 58 where the handler variable is overwritten by adapt_method_mode() even when middleware is skipped due to MiddlewareNotUsed, causing the middleware chain to be \"poisoned\". The issue explains that this results in synchronous middleware behavior instead of expected async coroutines. While the technical explanation is detailed, some aspects require domain knowledge of Django's middleware system and ASGI to fully understand. The expected behavior is implied (middleware should be properly skipped without affecting subsequent middleware) rather than explicitly stated, but a competent Django developer could reasonably interpret what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) It requires understanding Django's middleware loading mechanism and async/sync adaptation logic, (2) The developer needs to trace through the middleware chain execution to understand how MiddlewareNotUsed affects handler state, (3) The fix involves careful refactoring of variable assignment logic to prevent state pollution when middleware is skipped, (4) The solution requires writing appropriate tests to verify the fix works in async contexts. While the actual code change is small (just a few lines), understanding the problem fully and ensuring the solution doesn't break existing behavior requires significant investigation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained bug with a clear technical solution that demonstrates good software engineering skills including debugging complex async/sync interaction issues, understanding framework internals, and careful state management. The issue provides sufficient context for someone familiar with Django to implement and test a fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14017": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It describes a specific problem where the & (and |) operators are not commutative between Q objects and Exists objects in Django's ORM. The issue provides a concrete example showing that `Exists(...) & Q(...)` works but `Q(...) & Exists(...)` raises a TypeError. The expected behavior is clearly stated - these operators should be commutative. The issue even suggests a potential root cause (missing `__rand__` definition), which helps guide the solution direction. The problem is reproducible with the provided minimal example.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly defined and the solution, as shown in the gold patch, involves a small but thoughtful change to the `_combine` method in `django/db/models/query_utils.py`. The fix changes a simple type check from `isinstance(other, Q)` to a more inclusive check that also accepts objects with a `conditional` attribute set to True. This requires understanding Django's Q object implementation and the Exists class behavior, but the actual code change is minimal (one line modification). An experienced engineer would need some time to understand the Django ORM structure and why Exists objects should be treated similarly to Q objects, but the solution itself is straightforward once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is well-documented with a clear problem statement and reproducible example. The solution requires understanding Django's ORM internals but is not overly complex. The test patch shows comprehensive testing of the fix including edge cases with empty Q objects. The issue demonstrates a real-world problem that developers might encounter when working with Django's query system.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14140": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Q objects with 1 child are treated differently during deconstruct, causing issues when deconstructing Q objects with non-subscriptable children like Exists objects. The issue provides concrete examples showing the inconsistent behavior between single-child and multi-child Q objects in their deconstruct() method output. It demonstrates the crash case with Q(Exists(...)).deconstruct() and explains that the special case for single-child Q objects assumes the child is a 2-tuple (field, value), which fails for boolean expressions like Exists. The solution direction is also clear: remove the special case so all Q objects deconstruct consistently into args rather than kwargs, or alternatively keep the special case but add explicit checking for 2-tuples.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-isolated to the deconstruct() method in django/db/models/query_utils.py. The issue clearly identifies the problematic code path and the solution is straightforward: remove the special case handling for single-child Q objects. Looking at the gold patch, it's a simple change - removing about 6 lines of conditional logic and replacing it with consistent handling. The change is localized to one method and doesn't require deep understanding of Django's internals beyond the Q object structure. An experienced engineer would need some time to understand the Q object implementation and verify the change doesn't break existing functionality, but the actual code change is minimal and clear.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of object deconstruction patterns, handling edge cases in API design, and making backward-compatible changes. The issue is well-contained, the solution is clear, and the tests adequately verify the fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14238": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but requires some Django knowledge to fully understand. It clearly states the problem (DEFAULT_AUTO_FIELD subclass check fails for subclasses of BigAutoField and SmallAutoField), provides a concrete example with code showing how to reproduce the issue, and even hints at the solution location (AutoFieldMeta.__subclasscheck__). However, an engineer would need to understand Django's metaclass system and how DEFAULT_AUTO_FIELD works to implement the fix. The issue doesn't explain exactly what \"fails\" means or what the expected behavior should be, but from context and the provided example, it's reasonable to infer that custom subclasses should be treated as valid AutoField types.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minutes to 1 hour fix. The issue provides a clear hint about where the problem is located (AutoFieldMeta.__subclasscheck__) and even suggests the solution approach (allowing subclasses in the _subclasses property). The actual fix is just changing one line from using 'in' operator to using 'issubclass()' function. An experienced engineer would need some time to understand Django's AutoField metaclass system and locate the relevant code, but once found, the fix is straightforward. The solution requires understanding the difference between checking membership vs inheritance, which is a common Python concept.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's ORM system, has a clear reproduction case, and the solution is localized to a specific method. The test cases provided verify both the basic functionality and the specific subclass scenario, making it a good benchmark sample for testing understanding of Python inheritance and Django's metaclass usage.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14351": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly demonstrates a regression between Django 2.2.5 and 3.2 where Q objects with OR conditions behave differently when using `agent__property_groups__in=property_groups` vs `agent__property_groups__id__in=property_groups.values_list(\"id\", flat=True)`. The author provides specific code examples showing what works vs what doesn't, includes the problematic SQL output, and even shows a hacky debug fix in `django/db/models/sql/query.py:233`. However, the issue description lacks clarity on the exact root cause and some technical details are buried in debugging output that may be hard to parse. The core problem is identifiable: when using `__in` with a queryset in OR'd Q objects, Django is incorrectly selecting all fields instead of just the primary key, but understanding the precise mechanism requires some interpretation of the provided debugging information.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The problem involves understanding Django's ORM query compilation, specifically how Q objects handle field selection when dealing with subqueries in OR conditions. The developer needs to: 1) Understand Django's query compilation internals, particularly the `get_group_by_cols` and `get_default_columns` methods, 2) Trace through how OR'd Q objects with `__in` lookups are processed differently than `__id__in` lookups, 3) Identify that the issue is in the `In` lookup class in `django/db/models/lookups.py`, and 4) Implement the `get_group_by_cols` method to properly handle field selection. The fix itself is relatively small (about 10 lines) but requires deep understanding of Django's ORM internals and the specific interaction between subqueries and group by clauses.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is technically sound and the provided test case clearly validates the fix. While the issue description could be cleaner, an experienced Django developer should be able to understand the core problem and work towards a solution. The provided patches show this is a legitimate regression that needs to be fixed in Django's ORM layer.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14493": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear reproduction case with specific code showing how to derive from ManifestStaticFilesStorage and set max_post_process_passes to 0. The issue describes exactly what happens (crash during collectstatic), identifies the root cause with a direct link to the problematic code (line 246-257 in storage.py), and explains that the problem occurs because the 'substitutions' variable is only set if the loop is entered at least once. The motivation for setting max_post_process_passes to 0 is also provided with a reference to a related Django ticket. An experienced engineer would have all the information needed to understand the problem and implement a fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem: the 'substitutions' variable is undefined when max_post_process_passes is 0 because it's only initialized inside the loop. Looking at the gold patch confirms this is a simple one-line fix - just initialize 'substitutions = False' before the loop. The engineer would need to locate the specific file and function (post_process method in ManifestStaticFilesStorage), understand the variable scoping issue, and add the initialization line. This requires minimal code change and straightforward debugging of a variable scope issue.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly described, the fix is straightforward but requires understanding of the code flow, and the test case properly validates that collectstatic works with max_post_process_passes = 0. The issue provides good context about why someone would want this configuration, making it realistic.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14580": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. The user provides a clear, complete example with specific model definitions in models.py and shows exactly what migration file Django generates. The problem is crystal clear: the generated migration file references 'models.Model' in the bases tuple but doesn't import 'models' from django.db, causing a NameError. The user explicitly states the expected behavior (Django should generate valid Python) vs actual behavior (missing import statement). The example is reproducible and the error location is obvious - in the bases=(app.models.MyMixin, models.Model) line where 'models' is undefined. This provides all necessary information for an engineer to understand and fix the issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that would take 15 minutes to 1 hour. The issue is clearly in the migration serializer module as the user suspects. An experienced engineer would need to: 1) Locate the serialization code for models.Model in django.db.migrations.serializer.py, 2) Find that the TypeSerializer class handles special cases including models.Model, 3) Observe that models.Model currently has an empty imports list, and 4) Add the required import statement to the imports list. The actual code change is minimal (adding one import string to a list), but it requires understanding the serialization framework and how imports are tracked. The provided patch confirms this is exactly a one-line fix in the special_cases list.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined bug with clear reproduction steps, expected vs actual behavior, and a straightforward solution. The issue demonstrates good software engineering practices in bug reporting and would make an excellent benchmark sample for testing coding ability.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14672": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear information about what needs to be fixed. It explains that in Django 3.2, an identity property was added to ForeignObjectRel objects to make them comparable and hashable. The problem is that when `through_fields` is a list in ManyToManyRel, the `make_hashable` call is missing, which causes issues specifically with proxy models. The issue provides a concrete minimal reproduction case with specific Django model code that demonstrates the problem, and explicitly states the solution: \"Add missing make_hashable call on self.through_fields in ManyToManyRel.\" The gold patch confirms this is exactly what's needed - a one-line change in the identity property of ManyToManyRel to wrap `self.through_fields` with `make_hashable()`. The issue description gives sufficient context about the underlying cause (identity property for comparison, hashing requirements) and the specific location where the fix is needed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem (missing make_hashable call), the specific location (ManyToManyRel.identity property), and provides a minimal reproduction case. The actual fix is literally a one-line change wrapping `self.through_fields` with `make_hashable()`. An experienced engineer would need some time to understand Django's ORM structure and locate the ManyToManyRel class in django/db/models/fields/reverse_related.py, understand why the identity property needs all elements to be hashable, and verify the fix works with the provided test case. However, once the location is found, the change itself is trivial - it's just adding the missing function call that's already used elsewhere in the same codebase for similar purposes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of Django's ORM internals, specifically the relationship between model inheritance (proxy models), field relationships, and hashability requirements. The issue has a clear problem statement, minimal reproduction case, and a precise solution that can be objectively verified by the provided tests.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14787": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: method_decorator() creates a partial object that lacks standard function attributes like __name__ and __module__, which breaks decorators that expect these attributes (like the logger decorator in the example). However, there are some issues with the example code - it calls Test().test_method() but the method is named hello_world(), and the logger variable is referenced but not defined as an actual logger instance. Despite these minor inconsistencies in the example, the fundamental issue is understandable: the method_decorator should preserve wrapper assignments so that the decorated function retains its metadata. Looking at the provided patch, the solution involves using @wraps to preserve function metadata when creating the bound_method.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that requires 15 minutes to 1 hour. The problem is well-understood in Python - when using functools.partial, function metadata is lost. The solution is to apply functools.wraps to preserve the original function's attributes. The actual code change is minimal (adding wraps() around the partial call), but requires understanding how method_decorator works internally and why the metadata is lost. An experienced engineer familiar with Python decorators and functools would quickly identify this as a common issue and implement the appropriate fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample demonstrates a common Python problem with function metadata preservation and has a clear, focused solution. The test case confirms the expected behavior and would be suitable for validating solutions.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15104": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: a KeyError occurs with Django's migration autodetector when using a custom ForeignKey field that hardcodes its 'to' argument and removes it from deconstructed kwargs. The issue includes a complete, self-contained reproduction test case that demonstrates the exact problem. The author also provides the specific line of code that needs to be changed and suggests the exact fix: changing `del deconstruction[2]['to']` to `deconstruction[2].pop('to', None)` in the autodetector. The problem is clearly in the `only_relation_agnostic_fields` method of the MigrationAutodetector class, making this a very well-defined issue with a clear solution path.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides the exact location of the problem and suggests the specific solution. The fix involves changing a single line of code from `del deconstruction[2]['to']` to `deconstruction[2].pop('to', None)` in the autodetector.py file. An experienced engineer would need some time to understand the Django migration system context, verify the reproduction case, and understand why the KeyError occurs when the 'to' key is missing from kwargs. However, once the context is understood, the actual implementation is trivial - just replacing a `del` statement with a `pop` method that handles missing keys gracefully.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample because: (1) it has a clear, reproducible problem with a provided test case, (2) the solution is well-defined but requires understanding the codebase context, (3) it tests knowledge of Python dict operations and error handling, and (4) the fix is small but meaningful for the Django framework functionality.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. It provides a clear code reproduction case showing that `qs1 | qs2` works but `qs1 | qs2` (in reverse order) raises an AssertionError. The issue description explains the technical root cause: during QuerySet OR operations, when the right-hand side Query has sequential aliases that exist in the left-hand side's table_map, Query.table_alias creates conflicting aliases, leading to intersection between change_map's keys and values, which violates the assertion in Query.change_aliases. The author also suggests a potential solution approach involving providing alias_map to Query.join/Query.table_alias and incrementing suffixes. However, some details are missing - the exact AssertionError message isn't provided, and an engineer would need to dig into the Django ORM internals to fully understand the Query.combine, Query.change_aliases, and alias management mechanisms. Still, there's enough information for an experienced Django developer to understand the problem and work toward a solution.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a 4+ hour fix because it requires deep understanding of Django's ORM query internals, specifically the Query class's alias management system. The issue involves complex interactions between Query.combine(), Query.change_aliases(), Query.table_alias(), and Query.bump_prefix() methods. An engineer would need to: 1) Understand Django's query combining mechanism and alias management, 2) Trace through the specific failure case where alias conflicts occur, 3) Design a solution that prevents alias collisions during query combination, 4) Implement changes to the bump_prefix method to accept an exclude parameter and modify the combine method to call it appropriately, 5) Add proper comments explaining the assertion logic. The patch shows this touches core ORM functionality with changes across multiple methods in django/db/models/sql/query.py, plus test additions. This level of ORM internals work typically requires substantial research and careful implementation to avoid breaking existing functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The reproduction case is clear and self-contained, the expected behavior is reasonable (QuerySet OR should be commutative), and the test patch provides good verification that the fix works correctly. This is a legitimate bug in Django's ORM that affects real-world usage scenarios.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15277": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is exceptionally well-specified. It provides: (1) Clear context about the performance problem in Value._resolve_output_field() when CharField() is instantiated without max_length, (2) Specific code examples showing the current behavior and timing benchmarks (8.1 \u00b5s baseline vs 5.86 \u00b5s improved), (3) Root cause analysis identifying that MaxLengthValidator is unnecessarily created even when max_length is None, (4) Exact proposed solution: modify CharField.__init__ to only add the validator when max_length is not None, (5) Reference to BinaryField.__init__ as precedent for this pattern, and (6) Evidence that the change passes existing tests. An experienced engineer would have everything needed to implement this micro-optimization.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward 15 minute to 1 hour fix. The solution involves adding a simple conditional check in CharField.__init__ to only append the MaxLengthValidator when self.max_length is not None. The exact change is clearly specified in the issue description and matches the actual patch. The most time-consuming part would be understanding the codebase context and running tests to ensure the change doesn't break anything, but the actual code change is trivial (adding an if statement around one line).",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained performance optimization with clear requirements, expected behavior, and implementation approach. The issue provides benchmarking data to verify the improvement and references existing patterns in the codebase for precedent.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15280": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) Complete, runnable code examples including model definitions for User and Profile classes, (2) A specific test case that demonstrates the expected behavior, (3) Clear description of what should happen (3 queries executed, then 0 queries for accessing user.profile.user.kind), (4) Actual observed behavior showing the problem (user.profile.user.get_deferred_fields() returns {'kind'} but still triggers a database query), (5) Technical explanation of the root cause (Django correctly evaluates deferred fields in the inner User queryset but instances inherit deferred field information from the outer User queryset), and (6) Additional context that the issue also occurs with ForeignKey relationships. An experienced engineer would have everything needed to understand and reproduce the problem.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: (1) It requires understanding Django's complex ORM internals, specifically how prefetch_related works with nested Prefetch objects and deferred field management, (2) The bug involves the interaction between multiple ORM components (queryset evaluation, field deferring, and prefetching), (3) The solution requires identifying the specific location in Django's codebase where the field caching logic needs modification, (4) Looking at the actual fix, it's a small but non-trivial change in related_descriptors.py that adds a condition to check if a field is already cached before setting it, (5) An engineer would need to trace through Django's prefetch execution flow to understand why deferred fields from the outer queryset are incorrectly inherited by inner querysets. While the actual code change is small (3 lines), the debugging and understanding required puts this in the 1-4 hour range.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined with reproducible examples, the expected vs actual behavior is explicit, and the test case provides a clear success criteria for any solution. This would make an excellent benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15315": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Field.__hash__() changes value when a field is assigned to a model class, which breaks the field's use in dictionaries. The issue includes a concrete code example showing exactly how to reproduce the problem, references the specific PR (#31750) that introduced the bug, and explains the root cause. The solution direction is also hinted at - reverting the __hash__ change from #31750. An experienced engineer would have all the information needed to understand the problem, locate the relevant code in django/db/models/fields/__init__.py, and implement a fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-understood and the solution is relatively straightforward. Looking at the gold patch, it involves changing the __hash__ method in Field class to only return hash(self.creation_counter) instead of a tuple that includes model information. The engineer needs to: 1) Understand that hash values should be immutable, 2) Locate the __hash__ method in django/db/models/fields/__init__.py, 3) Simplify it to remove the model-dependent parts that change when a field is assigned to a model. The code change is small (removing a few lines) and the logic is clear once you understand the immutability requirement for hash functions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is clear and testable, and it represents a realistic debugging scenario that tests understanding of Python's hash function requirements and Django's field implementation. The test patch also provides good coverage by checking hash immutability before and after field assignment to a model.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15375": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly demonstrates the problem: using the 'default' argument in aggregate functions after annotate() causes a crash, while the same operation works without the default argument. The issue provides concrete code examples showing both the failing case (Book.objects.annotate(idx=F(\"id\")).aggregate(Sum(\"id\", default=0))) and a working workaround (using Coalesce). The description mentions this is a new feature in Django 4.0 and provides the specific Django version (4.0.1) and database backends tested (PostgreSQL and SQLite). An experienced engineer would understand they need to fix the aggregate function handling when a default argument is provided in the context of annotated querysets.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves a small but crucial change in the resolve_expression method of the aggregates module. The fix adds two lines to preserve the is_summary attribute when wrapping with Coalesce. This requires understanding Django's ORM internals, specifically how aggregates work with annotations, but the actual code change is minimal and focused. An experienced engineer familiar with Django would need some time to trace through the aggregate resolution logic and identify that the is_summary attribute needs to be preserved, but once identified, the fix is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability. It demonstrates a real-world Django ORM bug with clear reproduction steps, and the solution requires understanding of Django's internal aggregate handling. The test cases provided show both the fix working and regression testing to ensure annotations still work properly. This is a good example of a focused bug fix that requires some domain knowledge but isn't overly complex.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15380": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides a sensible interpretation of what needs to be fixed. It clearly states that \"Migration autodetector crashes when renaming a model and field\" and shows the specific scenario where this happens. The description mentions it's a regression tied to a specific commit (aa4acc164d1247c0de515c959f7b09648b57dc42), which helps narrow down the problem area. While it doesn't provide the full error traceback or detailed steps to reproduce, an experienced Django developer would understand this refers to Django's migration system and the autodetector component that generates migrations. The crash scenario (renaming both model and field simultaneously) is specific enough to guide investigation, and the provided commit hash gives a clear starting point for understanding what changed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a one-line fix changing 'old_model_name' to 'model_name' in the generate_renamed_fields method of django/db/migrations/autodetector.py. The issue is in the logic for handling field renames when the model itself has also been renamed - the code was incorrectly referencing the old model name instead of the new one when looking up the new model state. An experienced engineer familiar with Django's migration system could identify this issue relatively quickly by: 1) Looking at the regression commit mentioned, 2) Understanding the autodetector logic for handling renames, 3) Tracing through the code path when both model and field are renamed simultaneously. The fix itself is trivial once the bug is located, making this a 15 min - 1 hour task for someone with Django experience.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue description provides enough context for an experienced Django developer to understand the problem domain, and the regression commit reference provides a valuable debugging clue. The test patch shows clear verification of the fix with a test case that reproduces the exact scenario described. The solution requires understanding Django's migration autodetector logic but doesn't involve overly complex domain knowledge beyond what would be expected for Django development work.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15503": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified with clear problem description, reproduction steps, and expected behavior. It provides: (1) A specific problem statement - JSONField has_key lookups fail with numeric keys on SQLite/MySQL/Oracle but work on PostgreSQL, (2) Complete code examples including database configuration, model definition, and test case that demonstrates the exact failure, (3) Clear expected behavior - the test should pass on all database backends, finding entries with both string keys ('foo') and numeric string keys ('1111'), (4) Version information and testing details showing it works on PostgreSQL but fails on SQLite. The issue gives sufficient context about Django's JSONField functionality and the cross-database compatibility problem that needs to be solved.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Understanding Django's JSONField implementation across multiple database backends, (2) Investigating how JSON path compilation works differently between PostgreSQL and SQLite/MySQL/Oracle, (3) The solution involves modifying the HasKeyLookup class and creating new classes (HasKeyOrArrayIndex) to handle numeric keys properly, (4) Changes span multiple methods and require understanding the subtle differences in how databases handle JSON key lookups, (5) The patch shows this isn't a trivial fix - it required refactoring the compile_json_path_final_key method and updating several lookup classes. While not extremely complex, it requires solid understanding of Django's ORM internals and database-specific JSON handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the test case properly validates the fix, and the solution is focused on a specific database compatibility issue. The provided test adequately covers the numeric key scenarios across different JSON structures (nested objects, arrays). This is a good benchmark sample as it tests understanding of ORM internals and cross-database compatibility concerns.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15525": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified. The reporter clearly describes the problem: loaddata fails on non-default databases when natural keys use foreign keys. They provide a complete, reproducible example with models (Author and Book with natural keys), fixture data in JSON format, and the exact command that fails. The error scenario is specific - it works on the default database but fails when using `--database other`. However, there are some minor gaps: the actual error message/traceback isn't shown, and the reporter mentions being \"relatively new\" to natural keys, which suggests they might not fully understand the underlying issue. But the core problem is clear enough that an experienced engineer could reproduce and diagnose it.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. Looking at the gold patch, the solution is relatively small (3 lines changed in django/core/serializers/base.py), but understanding why this fix is needed requires deeper knowledge of Django's serialization system, natural keys, and database routing. The bug occurs because when building an instance for natural key lookup, the object's database state isn't being set correctly, causing foreign key lookups to fail on non-default databases. An engineer would need to: 1) Reproduce the issue and understand Django's loaddata flow, 2) Trace through the serialization/deserialization code to find where the database context is lost, 3) Understand how Django's _state.db works for model instances, and 4) Implement the fix ensuring the object has the correct database state before calling natural_key(). This requires solid understanding of Django internals but isn't extremely esoteric.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's serialization system, the test case clearly verifies the fix works, and the issue represents a real bug that affects users working with multiple databases and natural keys. The fix is clean and targeted without broad side effects.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15695": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some inference. The core problem is clear: RenameIndex() crashes when re-applied after moving backward and forward on an unnamed index. The test snippet shows the crash occurs when calling database_forwards() again after database_backwards(). However, the description doesn't explicitly state what the expected behavior should be - it mentions \"should restore the old auto-generated name\" but this is somewhat vague. An experienced developer familiar with Django migrations would understand this means the operation should be idempotent and not crash when re-applied. The test file location and partial code provide enough context to understand the scenario.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The gold patch shows it's a simple 3-line addition with a guard clause checking if old_index.name == self.new_name and returning early if so. An experienced developer would need to: 1) Understand the Django migrations system and RenameIndex operation, 2) Identify that the crash happens because the operation tries to rename an index to the same name, 3) Add a simple guard clause. The solution is straightforward once the problem is understood - just prevent the operation from proceeding when old and new names are the same.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is context-specific to Django migrations but the issue description provides enough information, including the test file location and expected behavior. The solution involves understanding Django's migration system but is ultimately a simple fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15732": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The problem is clear: there's an erroneous unique_together constraint on a primary key field that cannot be dropped by a migration. The user provides specific details about having two constraints on the same field (primary key and unique_together) and mentions the database is PostgreSQL. However, there are some gaps - the exact model structure isn't fully specified, and the specific migration code that's failing isn't shown. An experienced engineer would need to infer that this is a Django ORM issue based on the migration context and unique_together terminology. The core requirement is clear: fix the migration system to handle dropping unique_together constraints when multiple constraints exist on the same field, but the implementation details would need to be figured out by examining the codebase.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix that requires substantial understanding of Django's migration system and database schema operations. Looking at the gold patch, the solution involves modifying the schema backend's constraint handling logic in multiple places: updating alter_unique_together to exclude primary key constraints, enhancing _delete_composed_index to handle multiple constraints by preferring the default unique_together constraint name, and refactoring the unique constraint naming logic. The engineer would need to understand Django's ORM internals, database constraint systems, and how migrations translate to SQL operations. The fix spans multiple functions and requires careful consideration of edge cases, making it a moderately complex problem that goes beyond a simple code change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is a legitimate technical issue in Django's migration system that requires real engineering skills to solve. The test cases are appropriate and verify the fix works correctly for both primary key and unique field scenarios. This would be a good benchmark sample for evaluating an engineer's ability to work with complex ORM internals and database constraint handling.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15814": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It provides: (1) A clear problem statement - QuerySet.only() after select_related() crashes on proxy models, (2) Complete reproducible code with specific Django models (CustomModel, ProxyCustomModel, AnotherModel) showing the exact relationship setup, (3) The exact command that triggers the error, (4) Environment details (Windows 10, Python 3.10, Django 4.0.5), (5) The specific location where the bug occurs (django/db/models/sql/query.py line 745), and (6) Even suggests a potential fix by replacing 'opts = cur_model._meta' with 'opts = cur_model._meta.concrete_model._meta'. An experienced engineer would have all the information needed to understand the problem, reproduce it, and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because: (1) The issue provides the exact location of the bug and even suggests the fix, (2) The solution involves a single line change in django/db/models/sql/query.py, specifically ensuring that proxy models are handled by getting their concrete model metadata, (3) The underlying problem is well-understood - Django's ORM needs to work with the concrete model when dealing with proxy models in certain query operations, (4) While some familiarity with Django's ORM internals and proxy model concepts is needed, the specific change is straightforward once the problem is understood, (5) Writing a test case would be straightforward following the provided reproducible example.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-documented bug report with clear reproduction steps and a targeted fix. The issue would work well in a coding benchmark as it tests understanding of Django ORM internals and proxy model behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15930": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It provides a concrete code example that reproduces the problem (a syntax error when using Case() with ~Q(pk__in=[])), explains the expected behavior (all rows should be annotated with True since they all match the condition), and provides context about why this is relevant (~Q(pk__in=[]) is used as a sentinel value in application code). The issue description gives sufficient information to understand that the problem is with how Django's Case expression handles conditions that evaluate to empty strings in SQL compilation, and that the fix needs to handle this edge case properly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution requires understanding how Django's Case expression compiles SQL conditions and recognizing that empty condition strings (which represent \"match everything\" filters) need special handling in CASE WHEN expressions. The actual fix is quite simple - just 5 lines of code that check if the condition_sql is empty and replace it with a Value(True). An experienced engineer familiar with Django's ORM would need some time to trace through the expression compilation process and understand the issue, but the solution itself is straightforward once the problem is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly described, the problem is reproducible, and the solution requires understanding Django's ORM internals but is not overly complex. The test case also clearly validates the fix by ensuring that the Case expression with ~Q(pk__in=[]) correctly annotates all rows as \"selected\" rather than causing a syntax error.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15957": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that require interpretation. The core problem is clearly stated: Prefetch() objects don't work with sliced querysets, and a specific code example is provided showing the failing case. The desired behavior is also clear from the use case description - they want to efficiently prefetch only a limited number of related objects (e.g., 3 example posts per category) rather than all related objects. However, there are some details left to interpretation: (1) What exactly \"doesn't work\" means - does it error, return wrong results, or ignore the slice? (2) The expected behavior when slicing is applied - should it respect the slice per parent object or globally? (3) How this should interact with ordering. The test patch suggests it should respect slicing per parent object while maintaining proper ordering, which is the most logical interpretation of the requirements.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a complex issue requiring 4+ hours to solve properly. The difficulty stems from several factors: (1) Deep understanding of Django's ORM internals, specifically the prefetch_related mechanism and QuerySet slicing behavior, (2) The solution requires implementing sophisticated SQL generation using Window functions with RowNumber(), partitioning, and proper ordering - this is advanced SQL that needs to work across different database backends, (3) The fix spans multiple files and requires creating a new helper function _filter_prefetch_queryset that handles the complex logic of converting sliced querysets into equivalent window function queries, (4) Understanding the subtle interactions between QuerySet slicing (low_mark/high_mark) and prefetch relationships, (5) Ensuring the solution works correctly for different relationship types (M2M forward/reverse, ForeignKey reverse) as evidenced by the comprehensive test cases. The patch shows this is not a simple fix but requires deep ORM knowledge and sophisticated SQL generation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is technically sound, the solution is well-implemented using appropriate Django ORM patterns, and the test coverage is comprehensive. This would be a good benchmark sample for evaluating advanced Django ORM knowledge and SQL generation capabilities.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15973": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified but requires some interpretation. The user describes a specific Django migration problem where defining a ManyToManyField with a \"through\" model in a separate app causes migration issues. They provide concrete code examples showing the model definitions across three apps (fonte, variavel, fonte_variavel) and the generated migration file. The error scenario is clear: when FonteVariavelModel is in a separate app from FonteModel, migrations fail, but when they're in the same app, it works. However, the issue description doesn't explicitly state what the expected behavior should be or what specific error occurs during migration. An experienced engineer would need to infer that the problem is likely related to Django's migration dependency resolution when handling cross-app references in ManyToManyField through models.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The problem requires understanding Django's migration autodetection system, specifically how it resolves dependencies for ManyToManyField relationships with through models across different apps. Looking at the gold patch, the fix is a one-line change in django/db/migrations/autodetector.py, changing `remote_field_model` to `field.remote_field.through` in the resolve_relation call. However, arriving at this solution requires: 1) Understanding Django's migration internals and dependency resolution, 2) Identifying that the issue is in the _get_dependencies_for_foreign_key method, 3) Recognizing that the wrong model reference is being used when resolving the through model's app/name, and 4) Understanding that field.remote_field.through should be used instead of remote_field_model for through model resolution. The extensive test case also shows this requires deep understanding of migration testing patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate Django framework bug that requires understanding of internal migration mechanics. The issue is reproducible and the solution is testable. While it requires Django-specific knowledge, it's a good test of debugging skills in a complex codebase and understanding framework internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16032": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The title \"__in doesn't clear selected fields on the RHS when QuerySet.alias() is used after annotate()\" provides the core problem description. The provided test case clearly demonstrates the expected behavior - a query that uses annotate() followed by alias() in a subquery with __in lookup should return specific publisher names. However, the issue description lacks explicit explanation of what \"doesn't clear selected fields on the RHS\" means or why this is problematic. An experienced Django developer familiar with ORM internals would likely understand this refers to query optimization issues where unnecessary SELECT fields aren't being cleared in subqueries, but the exact failure mode isn't described. The test case serves as the primary specification of expected behavior.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the gold patch, it involves understanding Django's ORM query optimization internals, specifically the relationship between has_select_fields property/method, clear_select_clause(), and set_values() methods. The solution requires: 1) Converting has_select_fields from a property to an attribute, 2) Modifying get_prep_lookup() to use set_values() instead of clear_select_clause() + add_fields(), 3) Setting has_select_fields=True in set_values(). This requires deep understanding of Django's query compilation process and how annotate/alias affect field selection in subqueries. An experienced engineer would need time to trace through the ORM code, understand the optimization logic, and identify why the current approach fails with alias() after annotate().",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The test case provides a clear success criterion, and while the issue description could be more explicit about the problem mechanism, it's sufficient for an experienced Django developer to understand and solve. The solution involves well-defined code changes in specific files with clear test validation.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16136": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: \"When defining a simple View subclass with only an async 'post' method, GET requests to this view cause an exception.\" The reporter includes a complete, minimal reproduction case with exact code (a basic View class with only async post method), URL configuration, and precise steps to reproduce (start dev server, access the URL in browser). The error message in the title \"object HttpResponseNotAllowed can't be used in 'await' expression\" directly indicates what's happening - there's a mismatch between async/await expectations and the HttpResponseNotAllowed response being returned. An experienced engineer would clearly understand that the issue is in Django's View class handling of HTTP method not allowed responses in async contexts.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified: when an async view doesn't support a particular HTTP method, Django returns an HttpResponseNotAllowed object directly, but the async context expects a coroutine. Looking at the gold patch, the solution is straightforward - modify the http_method_not_allowed method in django/views/generic/base.py to check if the view is async and wrap the response in a coroutine function if needed. The fix involves adding about 10 lines of code to handle the async case. An experienced engineer familiar with Django's async patterns would quickly identify this as a common async/sync mismatch issue and implement the wrapper solution.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample is well-suited for evaluating coding ability as it tests understanding of async/await patterns, Django's view system, and the ability to identify and fix async/sync compatibility issues. The reproduction case is clear and the expected fix is testable.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16429": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is well-specified and provides clear information about what needs to be fixed. It includes: (1) A specific error condition - timesince() crashes with USE_TZ=True and >1 month intervals, (2) A concrete test case that reproduces the issue, (3) A clear identification of the root cause - the pivot datetime object doesn't account for tzinfo, (4) A specific file location and line numbers where the problem occurs (django/utils/timesince.py#L93-L100), and (5) A suggested fix - adding tzinfo=d.tzinfo to the datetime.datetime call. An experienced engineer would have sufficient information to understand both the problem and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because: (1) The issue provides the exact location of the problem and suggests the fix, (2) The solution involves adding a single parameter (tzinfo=d.tzinfo) to an existing datetime.datetime() call, (3) The root cause is clearly explained - timezone information is being lost when creating the pivot datetime, (4) While some understanding of Django's timezone handling is helpful, the fix itself is straightforward once the problem is understood. The gold patch confirms this - it's literally a one-line addition.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good sample for evaluating coding ability. The issue is well-documented with a clear reproduction case, the problem is specific and technical (timezone handling), and the solution requires understanding of datetime objects and timezone information. The test case provided makes it easy to verify the fix works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16454": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Django's CommandParser subclass doesn't pass its custom error formatting arguments to subparsers created via add_subparsers().add_parser(), causing subparser errors to show as stack traces instead of user-friendly messages. The issue provides a concrete example showing the problematic behavior, explains what the expected behavior should be (clean error messages like the main parser), and even hints at the solution approach by mentioning that \"the subparser action returned by add_subparsers() copies the relevant arguments through to constructed subparsers.\" The problem scope is clear - it's specifically about the CommandParser class in django/core/management/base.py and its add_subparsers() method.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves overriding the add_subparsers() method in CommandParser to ensure that when subparsers are created, they inherit the called_from_command_line parameter that controls error formatting. Looking at the gold patch, it's about 9 lines of code that use functools.partial to wrap the parser_class with the needed parameter. An experienced engineer familiar with argparse and Django would need some time to understand the inheritance relationship and how argparse subparsers work, but the actual implementation is straightforward once the approach is clear.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is testable, and the scope is contained within Django's management command framework. The provided test cases verify both the fix works and that it doesn't break existing functionality with custom parser classes.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16485": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug description: \"floatformat() crashes on '0.00'\". The issue includes specific code examples showing exactly how to reproduce the crash: floatformat('0.00', 0) and floatformat(Decimal('0.00'), 0). The function name (floatformat from django.template.defaultfilters) and input parameters are clearly specified. The expected behavior is implicit but clear - the function should not crash on these inputs. Looking at the provided patch, it's a simple one-character fix changing \"p < 0\" to \"p <= 0\" on line 171, which indicates the issue was a boundary condition bug when precision is exactly 0. The test patch confirms the expected output should be \"0\" for both cases.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This would take 15 minutes to 1 hour to fix. The issue is straightforward - a crash in the floatformat function with specific inputs. An engineer would need to: 1) Reproduce the crash with the provided examples, 2) Examine the floatformat function in django/template/defaultfilters.py to understand the logic, 3) Identify that the condition \"p < 0\" doesn't handle the case when p=0 correctly, 4) Change it to \"p <= 0\" to include the boundary case. The actual code change is trivial (one character), but it requires understanding the function's logic and identifying the boundary condition issue, which takes some thought but not extensive research.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear reproduction steps, specific function location, and straightforward solution. The test cases are also clear and verify the expected behavior. This would be suitable for a coding benchmark as it tests debugging skills and understanding of boundary conditions.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16569": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the `add_fields()` method in Django formsets fails when `index` is `None` under specific conditions (`can_delete=True` and `can_delete_extra=False`). The issue provides a clear explanation of why this happens (line 493 in `django/forms/formsets.py` where `index` is compared to `initial_form_count` without checking if `index` is `None`), includes a specific fix suggestion (adding `index is not None` to the condition), and provides both a code example and a complete self-contained reproduction script. The error occurs when calling `FormSet.empty_form()`, which is a specific, well-defined scenario.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue clearly identifies the problematic line of code (line 493 in django/forms/formsets.py) and provides the exact fix needed. An experienced engineer would need to: 1) Understand the reproduction case, 2) Locate the specific line mentioned, 3) Apply the suggested fix by adding the null check, and 4) Write a simple test case. The actual code change is minimal (adding `index is not None and` to an existing condition), and the logic is straightforward - preventing comparison operations when index is None.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-scoped, the solution is clearly specified, and the reproduction steps are complete and self-contained. The issue includes proper context about Django formsets and the specific conditions that trigger the bug, making it suitable for a coding benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16667": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, reproducible example with a specific code snippet showing how to trigger the crash. The issue describes exactly what happens: a user can provide malicious input through URL parameters that causes an OverflowError when Django's SelectDateWidget tries to convert the user-controlled year value to a datetime.date object. The problem is clearly identified in the SelectDateWidget.value_from_datadict method where datetime.date(int(y), int(m), int(d)) is called without proper error handling. The issue includes a complete minimal example that can be run locally to reproduce the crash, making it extremely clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue is clearly identified in the SelectDateWidget.value_from_datadict method where an OverflowError can occur when creating a datetime.date object with user-controlled input. The solution is simply adding exception handling around the existing datetime.date() call to catch the OverflowError and return an appropriate fallback value. Looking at the provided patch, it's just adding 2 lines of code (try/except block). An experienced engineer would quickly understand that large integer values cause datetime.date() to raise OverflowError, and the fix is to catch this exception and return a safe default value like \"0-0-0\" that will later be caught by Django's validation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is a realistic security-related bug (potential DoS through server crashes), has a clear reproduction case, and requires understanding of both Django's form handling and Python's datetime limitations. The solution is simple but requires the engineer to understand the root cause and choose an appropriate error handling strategy. The test cases provided also verify that the fix works correctly and that invalid dates are properly rejected by Django's validation system.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16950": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified with clear reproduction steps, model definitions, and admin configuration. The user describes that when adding a SubThing inline alongside a new Thing in Django Admin, the UUIDField 'id' on Thing is being set to null instead of using its default value (uuid.uuid4). However, there are some minor gaps in specification - the exact error message isn't provided, and the expected behavior could be more explicitly stated. Despite these minor omissions, an experienced engineer could reasonably understand that the solution should ensure UUIDField defaults are properly used in inline formset scenarios.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. Looking at the provided patch, the solution involves modifying the add_fields method in django/forms/models.py to add conditional logic that prevents nulling out foreign key fields when form data is provided and the field is not the parent model's primary key. This requires understanding Django's formset internals, the relationship between inline forms and their parent instances, and how default values are handled. The engineer would need to trace through the formset creation process, understand when and why fields are being nulled, and implement the correct conditional logic. While not trivial, it's a focused change to a single method rather than a complete rewrite.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is reproducible with the provided models and admin configuration. The solution involves Django's form/model internals which is appropriate for testing coding ability. The test cases show clear validation criteria for determining if the fix works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-20859": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproduction case showing the exact problem: calling `subfig.legend()` on a SubFigure raises an error when it should work. The expected behavior is clearly stated - the legend should work on subfigures just like it does on regular figures. The issue even includes a specific suggestion for the fix (changing L437 to check against `FigureBase` instead of `Figure`), along with a direct link to the relevant code location. The bug report follows a structured format with clear sections for the problem, reproduction code, actual vs expected outcomes, and version information. An experienced engineer would have no ambiguity about what needs to be implemented.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue description literally points to the exact line that needs to be changed and provides the solution. Looking at the actual patch confirms this - it's a very simple change: import `FigureBase` instead of `Figure`, change the isinstance check from `Figure` to `FigureBase`, and update the error message accordingly. The core change is just replacing `Figure` with `FigureBase` in two places. An experienced engineer would need some time to understand the codebase structure and the inheritance relationship between Figure and FigureBase/SubFigure, plus write a test, but the actual code change is minimal and straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the problem is clearly defined, the solution is focused and testable, and it represents a real-world issue that developers might encounter. The fix requires understanding object-oriented inheritance concepts but is not overly complex.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-22719": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. The user provides a minimal reproduction case showing that a MatplotlibDeprecationWarning is issued when calling ax.plot([], []) on an axis with category units. They clearly state their expected outcomes: either continue producing artists with no data or provide a more accurate warning. The issue also includes helpful context about the API changes related to unit converters and suggests that the warning is being triggered inappropriately for empty data structures. The reproduction steps are concrete and the problem is well-defined.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves adding simple checks for `values.size` before issuing deprecation warnings in two locations in the category.py file. The fix is straightforward - it prevents the deprecation warning from being triggered when empty data is passed. An experienced engineer would need to: 1) Reproduce the issue, 2) Locate where the deprecation warnings are generated in the category unit converter code, 3) Add size checks to prevent warnings on empty data, and 4) Write a simple test. The changes are minimal (adding `and` conditions to existing if statements) and the logic is clear once you understand that empty data shouldn't trigger these warnings.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly defined, has a minimal reproduction case, and the fix is focused and testable. The solution requires understanding the matplotlib codebase structure for unit converters but doesn't require deep domain expertise. The test case is straightforward and verifies that no deprecation warning is emitted for the specific case.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-23299": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "This issue is very well-specified. It provides a clear bug summary stating that `matplotlib.get_backend()` removes all figures from `Gcf` if the first figure was created in an `rc_context`. The reproduction code is complete and runnable, showing exactly when the bug occurs and when it doesn't (with the commented alternatives). The expected vs actual outcomes are clearly stated - figures should remain in `Gcf.figs` after calling `get_backend()`, but they are being removed. The consequences are also explained (e.g., `plt.close(fig2)` doesn't work). An experienced engineer would have no ambiguity about what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because it requires understanding the interaction between `rc_context()`, `get_backend()`, and the figure management system (`Gcf`). Looking at the solution, the fix involves modifying the `rc_context()` function to exclude the 'backend' parameter from being reset when the context exits. This requires understanding that when `get_backend()` is called, it can trigger backend initialization that interacts poorly with the context manager's restoration of rcParams. The engineer would need to trace through the matplotlib backend system, understand why figures are being cleared, and realize that the backend parameter should be treated specially in rc_context. While the actual code change is small (3 lines), the investigation and understanding of the root cause would take considerable time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly reproducible, the solution is well-targeted, and the test case properly validates the fix. This is a good benchmark sample that tests understanding of matplotlib's internal architecture and parameter management systems.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-23476": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the bug: when a matplotlib figure is unpickled on M1 Mac, its DPI is doubled each time. The issue provides complete reproduction code that demonstrates the problem, shows the actual output (DPI doubling from 200.0 to 400.0 to 800.0, etc.), and shows the expected output (DPI should remain constant at 200.0). The environment details are provided (MacOSX backend, matplotlib 3.5.2, M1 Mac). The problem is specific and measurable - the DPI value should be preserved during pickle/unpickle operations, but it's being doubled instead. An engineer would clearly understand that they need to fix the pickling mechanism to preserve the original DPI value.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a 15 min - 1 hour fix. The solution involves adding just 3 lines of code to the `__getstate__` method in `lib/matplotlib/figure.py` to restore the original DPI value during pickling. The fix is straightforward once you understand that the issue is related to device pixel ratio changes affecting the DPI during pickling. An experienced engineer would need some time to trace through the pickling code and understand how device pixel ratios interact with DPI, but the actual code change is minimal and focused. The test patch is also simple, creating a figure with a specific DPI, setting device pixel ratio, and verifying the DPI is preserved after pickle/unpickle.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. This is a well-contained bug with clear reproduction steps, specific to a platform (M1 Mac), and has a clean solution. The issue would make a good benchmark sample as it tests understanding of Python's pickle mechanism, matplotlib internals, and device pixel ratio handling.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24026": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified with a clear problem statement, reproducible code example, and expected behavior. The user wants stackplot to accept CN color aliases (like 'C0', 'C1') just like other matplotlib functions do. The provided code example clearly demonstrates the inconsistency - ax.plot() and Rectangle() work with CN aliases, but stackplot throws a ValueError. The desired outcome is obvious: stackplot should handle CN color aliases without errors, maintaining color consistency across different plot types.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours because: (1) Understanding matplotlib's color cycling system and how CN aliases are resolved requires diving into the codebase architecture, (2) The engineer needs to investigate why stackplot behaves differently from other plotting functions, (3) The solution involves understanding the difference between set_prop_cycle() and _get_lines.get_next_color() methods, (4) The fix requires modifying the core logic of how colors are handled in stackplot, replacing the prop_cycle approach with itertools.cycle() and proper color resolution, (5) Testing would need to ensure the fix doesn't break existing functionality while adding CN alias support.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is focused on a specific functionality gap, and the test case appropriately validates that CN color aliases work with stackplot. This is a good benchmark sample as it tests understanding of matplotlib's color system and API consistency.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-24149": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides a clear bug summary explaining that ax.bar raises an exception with all-NaN data in matplotlib 3.6.1 (a regression from 3.6.0). The reproduction code is minimal and precise: `ax.bar([np.nan], [np.nan])`. The expected behavior is clearly stated - it should return a BarCollection with one Rectangle having nan for x and height, as it did in 3.6.0. Additional debugging information shows the issue is specifically with NaN x positions. The context about likely related release note changes provides helpful background. An experienced engineer would have all the information needed to identify this as a regression bug requiring proper NaN handling in the bar plotting logic.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly a regression bug with specific error conditions (all-NaN data). The provided patch shows it's a targeted fix adding StopIteration exception handling in the _convert_dx function, with fallback to safe_first_element when no finite elements are found. An experienced engineer familiar with the codebase would need to: 1) Locate the bar plotting logic, 2) Identify where the NaN handling fails (likely in coordinate conversion), 3) Add proper exception handling for the case when _safe_first_finite finds no finite elements. The fix is small and focused - just adding try/except blocks around existing function calls. The main time would be spent understanding the coordinate conversion logic and ensuring the fallback behavior matches the 3.6.0 behavior.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-documented regression bug with clear reproduction steps and expected behavior. The issue description provides sufficient context including the fact that this breaks seaborn's histogram function, showing real-world impact. The additional debugging information helps narrow down the root cause to NaN x position handling specifically. The fix appears straightforward based on the provided patch.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24970": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The bug report clearly identifies that NumPy 1.24 is causing deprecation warnings when using matplotlib's colormap functionality. The reproduction code is minimal and precise: `plt.get_cmap()(np.empty((0, ), dtype=np.uint8))`. The expected outcome is clear: no warnings should be produced. However, the issue lacks some important details: (1) What specific deprecation warnings are being shown? (2) What NumPy functionality is being deprecated that matplotlib is using? Despite these gaps, an experienced engineer can reasonably interpret this as a request to update matplotlib's code to avoid deprecated NumPy operations when working with unsigned integer arrays, particularly empty arrays with uint8 dtype.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a relatively straightforward fix that involves restructuring how numpy error states are handled in the colormap indexing code. The solution moves the `np.errstate(invalid=\"ignore\")` context manager to only wrap the `astype(int)` conversion rather than the entire float processing block. This change addresses the NumPy 1.24 deprecation warnings while maintaining the same functionality. An experienced engineer would need to: (1) reproduce the warnings to understand what NumPy operations are deprecated, (2) locate the relevant code in matplotlib/colors.py, (3) understand the colormap indexing logic, and (4) adjust the error state handling. This requires some investigation and understanding of NumPy's deprecation changes, but the actual code change is modest and localized to one function.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a solid issue for benchmarking coding ability. It represents a real-world maintenance task that developers frequently encounter - updating code to avoid deprecation warnings from dependencies. The issue requires understanding both the matplotlib codebase and NumPy's behavioral changes, making it a good test of debugging and adaptation skills. The solution involves careful analysis of when error state management is needed, which tests practical programming judgment.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25311": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug summary stating that figures with draggable legends cannot be pickled. The reproduction code is complete and minimal, showing exactly how to reproduce the problem: create a figure, add a plot with a legend, make the legend draggable with leg.set_draggable(True), and then attempt to pickle the figure with pickle.dumps(fig). The issue clearly states the expected outcome (successful pickling) and notes that removing the draggable line fixes the problem. An experienced engineer would have everything needed to understand the problem and work toward a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding the matplotlib codebase architecture, specifically how draggable functionality works and why it breaks pickling. The solution involves changing how the canvas reference is stored in the DraggableBase class - converting from a direct attribute to a property to maintain picklability. This requires understanding Python's pickle mechanism, object serialization constraints, and the relationship between artists, figures, and canvases in matplotlib. While the actual code change is small (removing one line, adding a property), finding the root cause and understanding why canvas references break pickling would take substantial investigation time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the reproduction case is clear, and the solution involves core programming concepts (object serialization, property methods) that are appropriate for evaluating coding ability. The issue tests understanding of both the specific domain (matplotlib) and general Python concepts (pickling).",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-25332": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear bug description: \"Unable to pickle figure after calling align_labels()\". The reproduction code is complete and runnable, showing exactly when the problem occurs (after calling fig.align_labels()). The expected outcome is clearly stated (pickling should be successful). The issue includes specific version information (Matplotlib 3.7.0) and operating system (Windows). An experienced engineer would have all the information needed to reproduce the bug and understand what needs to be fixed - the align_labels() function is breaking pickle serialization of the figure object.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve. The engineer needs to: (1) Reproduce the bug and understand why align_labels() breaks pickling, (2) Debug the internals of matplotlib to identify that the issue is in the Grouper class which uses weak references that can't be pickled, (3) Research how to properly implement pickling support for objects containing weak references, (4) Implement __getstate__ and __setstate__ methods to convert between weak and strong references during serialization, and (5) Write appropriate tests. While the code change itself is relatively small (~15 lines), understanding the root cause requires significant investigation into matplotlib's internal architecture and pickle serialization mechanics.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is technically sound, and the test case properly verifies the fix. This is a good example of a real-world bug that requires understanding of Python's serialization mechanisms and library internals.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-25479": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. The user provides a clear, minimal reproduction case showing the exact problem: creating a colormap with one name, registering it with a different name, and then being unable to use it via the registered name. They explain the root cause (double internal name lookup tables) and show that manual lookup works fine with cm.get_cmap(). The expected behavior is clear - users should be able to refer to colormaps by their registered names. The issue includes specific code examples, error descriptions, and explains the confusing user experience this creates.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding matplotlib's colormap registration system and how names are handled internally across multiple modules (cm.py and colors.py). (2) The solution involves changes to both the registration logic and colormap equality comparison, requiring careful consideration of backward compatibility. (3) An engineer would need to trace through the code to understand why plt.set_cmap() fails while cm.get_cmap() works, which involves understanding the interaction between pyplot interface and colormap internals. (4) The fix requires updating the colormap's name attribute and modifying equality logic, plus comprehensive testing. While not extremely complex, it requires solid understanding of the codebase architecture.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-documented bug with clear reproduction steps, expected behavior, and a reasonable scope for a coding assessment. The issue demonstrates understanding of matplotlib's colormap system and requires both problem-solving and implementation skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-26291": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The user provides a clear reproduction case using `inset_axes` from `mpl_toolkits.axes_grid1.inset_locator`, expecting to create an inset axes box in the top right of a subplot. However, they don't specify what actual error occurs - the \"Actual outcome\" section is empty. From the code patch, it's clear this is related to a renderer issue when bbox_inches=\"tight\" is used during saving, but this context isn't provided in the issue description. An engineer would need to run the code and investigate to discover the specific error condition and underlying cause.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves adding a simple 2-line null check in the `__call__` method of `inset_locator.py`: checking if renderer is None and using `ax.figure._get_renderer()` as a fallback. Once an engineer reproduces the issue and identifies it occurs during figure saving with bbox_inches=\"tight\", the fix is straightforward. The patch shows this is a targeted fix that doesn't require extensive code changes or deep architectural understanding, just handling a missing renderer edge case.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. While the reproduction case doesn't show the full context (saving with bbox_inches=\"tight\"), the core problem of inset_axes failing is reproducible from the provided code. The test case confirms this is specifically about the tight bbox scenario during figure saving.",
            "q2_5_confidence": 4
        }
    },
    {
        "psf__requests-1724": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It provides two concrete code examples that demonstrate the difference between working code (method='POST') and failing code (method=u'POST'), showing that unicode method names cause a UnicodeDecodeError in Python 2.7.2. The issue author correctly identifies the likely root cause in sessions.py:313 where method.upper() is called, and explains that unicode strings are \"infecting\" the headers when they should be byte strings. The problem is specific, reproducible, and the expected behavior (unicode method names should work the same as regular strings) is clear. An experienced engineer would understand they need to ensure method names are converted to byte strings before processing.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that requires 15 minutes to 1 hour. The issue clearly points to the problem location (sessions.py:313), and the solution is conceptually simple: convert unicode method names to byte strings before processing. Looking at the provided patch, the fix is just adding 'method = builtin_str(method)' before the existing method.upper() call, plus importing builtin_str. An experienced engineer would quickly identify that they need to handle unicode-to-string conversion for Python 2/3 compatibility, locate the appropriate utility function in the codebase, and apply the fix. The test case is also straightforward to write.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean, well-isolated bug with a clear reproduction case and solution. The issue demonstrates good software engineering practices with specific examples and root cause analysis. The fix is minimal and targeted, making it suitable for a coding benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "psf__requests-5414": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is well-specified and clear about what needs to be fixed. It explains that accessing a URL like \"http://.example.com\" currently raises a UnicodeError, but should instead raise an InvalidURL exception with the message \"URL has an invalid label.\" The issue provides a specific reproduction case with code (`requests.get(\"http://.example.com\")`), references existing similar handling in the codebase (line 401 in requests/models.py), mentions the expected result based on PR #774, and includes complete system information. The problem is clearly defined: catch UnicodeError exceptions and convert them to InvalidURL exceptions for URLs with invalid labels starting with a dot.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is very simple - just adding 'u'.' to the existing check that already handles URLs starting with '*'. The change is literally adding one condition to an existing elif statement in prepare_url method. The engineer would need to: 1) Understand that URLs starting with '.' are invalid, 2) Locate the existing validation logic for '*' URLs, 3) Extend it to also catch '.' URLs. The code change is minimal (one line modification) and the logic is straightforward once you see the existing pattern for handling invalid URL labels.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear problem statement, reproduction steps, expected behavior, and a simple fix. The test changes also show exactly what needs to be validated. This sample would work well for evaluating coding ability as it tests understanding of exception handling, URL validation logic, and the ability to extend existing patterns in code.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-3151": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear MCVE (Minimal Complete Verifiable Example) showing the exact problem: xr.combine_by_coords raises a ValueError when identical coordinates are non-monotonic (['a', 'c', 'b'] vs ['a', 'b', 'c']). The issue includes the expected behavior (should work without error), references the documentation stating that identical coordinate dimensions should be ignored, and provides complete reproducible code. The problem statement clearly explains that the current implementation incorrectly requires identical coordinate dimensions to be monotonic, contradicting the documented behavior. An engineer would have all the information needed to understand both the bug and the expected fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the gold patch, the solution is quite straightforward - changing the loop from iterating over all dimensions (`for dim in concatenated.dims:`) to only iterating over concat_dims (`for dim in concat_dims:`). This removes the monotonicity check for dimensions that aren't being concatenated (the \"bystander\" dimensions). The logic is clear: only dimensions being actively concatenated need to be checked for monotonicity, not all dimensions. The fix involves changing just a few lines in one function, requires minimal understanding of the codebase beyond the combine_by_coords function, and the test case directly validates the fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the issue is clearly described, the problem is well-understood, the solution is focused and testable, and it represents a real bug that needed fixing. The test case directly validates that the fix works by combining datasets with non-monotonic bystander coordinates, which should pass after the fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-3677": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the `ds.merge()` method fails when trying to merge a DataArray, while the top-level `xr.merge()` function works fine with the same inputs. The issue provides a complete, runnable code example showing both the working case (xr.merge([ds, da])) and the failing case (ds.merge(da)), along with the expected output. The problem statement is unambiguous - the dataset merge method should handle DataArray objects the same way the top-level merge function does. An experienced engineer would clearly understand that they need to modify the dataset.merge() method to accept DataArray inputs and convert them appropriately before processing.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is quite simple - just one line of code that converts a DataArray to a Dataset if needed before passing it to the existing merge logic. The fix is in xarray/core/dataset.py in the merge method, adding: `other = other.to_dataset() if isinstance(other, xr.DataArray) else other`. An experienced engineer would need to: 1) locate the dataset.merge() method, 2) understand that DataArrays need to be converted to Datasets before the existing merge logic, 3) add the type check and conversion. The logic is straightforward and the implementation is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the test case provided in the patch confirms the expected behavior. This would be a good benchmark sample as it tests understanding of method parity, type handling, and API consistency in xarray.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-4094": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear Minimal Complete Verifiable Example (MCVE) that reproduces the problem, explicitly states the expected behavior (\"A working roundtrip\"), and clearly describes what's broken (\"this doesn't work if the variables only have a single dimension\"). The code sample shows exactly how to reproduce the issue: creating a Dataset with single-dimension DataArrays, stacking them with to_stacked_array(), and then failing when trying to unstack with to_unstacked_dataset(). The problem statement is unambiguous - the roundtrip operation should work but currently fails for single-dimension variables. An experienced engineer would have no trouble understanding what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The actual solution shown in the patch is a one-line change in the to_unstacked_dataset method, adding a `drop=True` parameter to the `.sel()` call. The issue is likely that when selecting single-dimension variables, the coordinate dimension isn't being properly dropped, causing problems in the unstacking process. An experienced engineer would need to: 1) Run the MCVE to reproduce the error, 2) Debug the to_unstacked_dataset method to understand why it fails, 3) Identify that the issue is in the variable selection logic, and 4) Add the missing drop=True parameter. While some investigation is needed to understand the xarray codebase and debug the specific failure, the core fix is straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly documented with a reproducible example, the expected behavior is obvious (roundtrip should work), and the solution involves understanding xarray's internal coordinate handling logic. The fix requires both debugging skills and understanding of the library's API, making it a good test of practical coding ability. The test case also properly validates the fix by ensuring the roundtrip produces identical results.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-4695": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, reproducible code example showing the exact problem: when a DataArray dimension is named \"method\", calling .loc with a dictionary fails, while it works fine with other dimension names like \"dim2\". The issue includes complete working code that demonstrates both the working case (D1.loc) and the failing case (D2.loc), making it easy to reproduce the bug. The problem description correctly identifies that dimension names should be irrelevant and suggests the issue is related to unsanitized dimension names being passed to another method. The expected behavior is clear: both cases should work identically regardless of dimension name.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue is clearly a parameter collision where \"method\" conflicts with a keyword argument. Looking at the gold patch, the fix is a simple one-line change in the DataArray.__getitem__ method, changing from `self.data_array.sel(**key)` to `self.data_array.sel(key)`. This prevents the dictionary keys from being expanded as keyword arguments, avoiding the naming conflict. An experienced engineer would quickly identify this as a common Python gotcha with **kwargs expansion and fix it with minimal code change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is clearly described with a reproducible example, the problem is well-defined, and the solution requires understanding Python's argument passing mechanics. The test case provided in the patch confirms the fix works correctly and would serve as a good validation for any proposed solution.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-6599": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a regression bug where `polyval` with timedelta64 coordinates produces incorrect results in the latest version compared to v2022.3.0. The issue provides: (1) A complete, runnable example with specific input data including datetime64 arrays and polynomial coefficients, (2) Clear expected vs actual outputs showing the dramatic difference in results (correct values in the thousands vs incorrect values in the 10^30 range), (3) Specific version information showing when the regression was introduced, and (4) Complete environment details. The problem statement is unambiguous - the function should return the same results as the previous version but currently returns vastly different (wrong) values when using timedelta64 coordinates.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The provided solution shows this is a targeted change in the `_ensure_numeric` function in `xarray/core/computation.py`. The fix involves distinguishing between datetime ('M') and timedelta ('m') dtypes and handling them differently - timedeltas should be converted to float using `.astype(float)` rather than using the `datetime_to_numeric` function meant for datetimes. The logic is straightforward once you understand that timedelta64 values can be directly cast to float (representing nanoseconds as a number), while datetimes need special handling. An experienced engineer familiar with the codebase would need time to locate the relevant function and understand the dtype handling, but the actual fix is a simple conditional change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear regression, complete reproduction case, and the solution involves a logical fix to handle timedelta64 dtypes correctly. The test case addition is also straightforward and validates the fix properly.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-6721": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified. It clearly describes the problem (accessing chunks loads entire dataset into memory), provides a reproducible example with a specific zarr URL, and states the expected behavior (should only inspect encoding attribute). However, there are some blanks to fill in - the issue doesn't specify exactly where in the codebase the problem occurs or provide technical details about why this happens. An experienced engineer would need to investigate the codebase to understand that the issue is in the get_chunksizes function where v.data triggers data loading while v._data (the private attribute) would avoid it.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The actual code change is minimal (changing v.data to v._data), but requires understanding the xarray codebase structure and the difference between .data and ._data attributes. An experienced engineer would need to: 1) Reproduce the issue, 2) Trace through the code to find where chunks attribute access triggers data loading, 3) Understand that .data can trigger computation while ._data is the raw underlying data, 4) Make the one-line fix, 5) Add a regression test. The investigation and testing would take most of the time, not the actual fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - it has a clear problem description, the fix is non-trivial enough to require understanding of the codebase internals (difference between .data and ._data), and includes a proper regression test. The external URL dependency shouldn't be problematic for evaluation since the core issue is about the chunks attribute behavior in general, not specific to that dataset.",
            "q2_5_confidence": 4
        }
    },
    {
        "pylint-dev__pylint-6386": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the short option `-v` for verbose expects an argument while the long option `--verbose` works correctly without an argument. The issue provides a concrete example showing the problematic command `pylint mytest.py -v`, states the expected behavior should match the long option, and includes version information. The bug is straightforward - there's an inconsistency between how the short and long forms of the verbose option are handled. An experienced engineer can immediately understand that they need to modify the argument parsing configuration to make `-v` behave like `--verbose` (i.e., not expect an argument).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly defined and the solution involves modifying argument parsing configuration. Looking at the patches, the changes are relatively small - adding metavar handling and updating the preprocessing options to handle `-v` like `--verbose`. An experienced engineer would need to: 1) Understand how pylint's argument parsing works, 2) Locate where the verbose option is defined, 3) Identify why `-v` expects an argument while `--verbose` doesn't, and 4) Make the necessary configuration changes. The patches show this involves modifying a few files with small, targeted changes rather than major refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good sample for the benchmark. The issue is clearly described, has a concrete reproducible example, and tests a developer's ability to understand argument parsing systems and make targeted configuration changes. The solution requires understanding the codebase structure but doesn't involve complex algorithmic thinking.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-6903": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when running pylint with --jobs=0 in a Kubernetes Pod, the _query_cpu() function returns 0, which causes a crash because multiprocessing requires a value > 0. The issue provides specific details including the exact command used, the file paths being read (/sys/fs/cgroup/cpu/cpu.cfs_quota_us, /sys/fs/cgroup/cpu/cpu.cfs_period_us, /sys/fs/cgroup/cpu/cpu.shares), their values (-1, 100000, 2), and the calculation that leads to the problem (2/1024 = 0 when cast to int). The issue even suggests a potential solution (\"append a ` or 1` at the end of this line\") and points to the specific line of code in pylint/lint/run.py#L60 where the fix should be applied. The expected behavior is clear: pylint should not crash and the calculated CPU number should never be 0.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified and localized to the _query_cpu() function in pylint/lint/run.py. The solution is straightforward - add a check to ensure the calculated avail_cpu value is never 0, and if it is, set it to 1. Looking at the gold patch, this is exactly what was done: a simple if statement checking if avail_cpu == 0 and setting it to 1. The fix requires minimal code changes (about 7 lines including comments) and doesn't require deep understanding of the codebase architecture. The main time would be spent understanding the context of the CPU detection logic and writing appropriate tests.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, a specific reproduction scenario, and a straightforward solution. The issue provides all necessary context for an engineer to understand and fix the problem.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-8898": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: pylint's bad-names-rgxs option splits on commas without considering that commas can be part of valid regex syntax (like quantifiers {1,3}). The issue provides a concrete example that reproduces the crash, shows the expected behavior (any valid regex should work), includes version information, and demonstrates the problem with a minimal configuration. An experienced engineer would have no ambiguity about what needs to be fixed - the CSV parsing for regex options needs to be smarter about when to split on commas.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task. While the problem is clearly defined, the solution requires: (1) Understanding pylint's configuration parsing system and locating the relevant code, (2) Designing a parser that can distinguish between commas used as separators vs commas within regex quantifiers, (3) Implementing the logic to track brace context while parsing, (4) Integrating the new function into the existing codebase, and (5) Writing comprehensive tests. The solution involves creating a new function with non-trivial string parsing logic and modifying multiple files. An experienced engineer would need time to understand the codebase structure and implement a robust solution that handles edge cases correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is testable, and it represents a realistic bug that requires practical programming skills to solve. The issue would make a good benchmark question.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10051": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: `caplog.get_records()` becomes decoupled from actual caplog records when `caplog.clear()` is called, causing `get_records()` to freeze and not reflect new records or clearing. The issue provides a concrete reproductive example that demonstrates the expected vs actual behavior through assertions. It also includes technical details about the root cause - that `caplog.records` gets replaced rather than cleared in the `clear()` method, which diverges the two objects that were initially set to the same list. The links to specific lines in the codebase (lines 699 and 345 in logging.py) provide additional context about where the problem originates. An experienced engineer would have a clear understanding of what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-isolated to the logging module, specifically the interaction between `caplog.clear()` and `caplog.get_records()`. The solution involves modifying the `clear()` method to clear the existing list rather than replacing it, which maintains the reference consistency. Looking at the actual patch, it adds a new `clear()` method to the handler that calls `records.clear()` instead of replacing the list, and then calls this new method from the main `clear()` function. This requires understanding the codebase structure but is a relatively small, focused change that doesn't require extensive research or major architectural modifications.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is straightforward, and the provided test case clearly validates the expected behavior. This would make a good benchmark sample as it tests understanding of object reference semantics and the ability to maintain API consistency.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10081": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly describes the problem: when running pytest with --pdb, the tearDown() method of unittest.TestCase classes decorated with @unittest.skip at the class level gets executed when it shouldn't. The issue provides a minimal reproducible example (test_repro_skip_class.py) showing a class decorated with @unittest.skip that has setUp() and tearDown() methods. It demonstrates the expected behavior (test properly skipped normally) versus the problematic behavior (teardown executed with --pdb). The issue also references a similar previous issue (#7215) for context, noting this is the same problem but at class level rather than function level. The environment details and version information are provided, making it clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. Looking at the gold patch, the solution involves modifying the runtest() method in src/_pytest/unittest.py to check if either the test method OR the parent class is skipped before deciding whether to postpone tearDown. The fix requires understanding the existing logic around PDB and tearDown postponement, identifying that _is_skipped() was only checking the test method and not the parent class, and adding a simple check for _is_skipped(self.parent.obj). This is a small, focused change that requires some understanding of the pytest codebase structure but doesn't involve complex logic or extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement, reproducible example, and straightforward solution. The issue would be suitable for evaluating coding ability as it requires understanding pytest's unittest integration, the skip decorator behavior, and debugging logic flow.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5262": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem: _pytest.capture.EncodedFile incorrectly advertises its mode as 'rb+' (including the 'b' for binary), which causes youtube-dl to think it should write bytes instead of strings. However, the EncodedFile's write() method raises an exception when passed bytes. The issue provides a clear minimal reproduction case with test.py showing how to trigger the problem. The root cause is identified - the mode property should not include 'b' since the file object expects string input, not binary. The expected behavior is implicit but clear: the mode should not include 'b' so that libraries like youtube-dl will write strings instead of bytes.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-defined and the solution is straightforward once you understand the problem. Looking at the gold patch, it's just adding a mode property to the EncodedFile class that removes the 'b' from the underlying buffer's mode. The solution requires understanding the interaction between the EncodedFile wrapper and external libraries that check the mode, but the actual code change is minimal - just a few lines adding a property method. An experienced engineer would need some time to locate the relevant EncodedFile class in the codebase and understand how it works, but the fix itself is simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly described with a reproduction case, the solution is targeted and minimal, and the test case is straightforward. This is a good example of a well-contained bug fix that tests both understanding of the issue and ability to implement a clean solution in the existing codebase architecture.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5631": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: ValueError when collecting tests that patch an array using `@patch(target='XXXXXX', new=np.array([-5.5, 3.0]))`. It includes specific version information (works in pytest 3.1.3, fails in 3.6.0), a concrete code example that reproduces the issue, and identifies the root cause - the problematic commit and the specific technical issue where `p.new in sentinels` returns an array of booleans instead of a single boolean when `p.new` is a numpy array. The technical explanation is precise and actionable.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the root cause: the problem occurs in the `num_mock_patch_args` function where `p.new in sentinels` fails with numpy arrays because it returns an array of booleans instead of a single boolean. The solution shown in the patch is straightforward - replace the `in` operator with identity comparison using `is` to check against specific sentinel objects. The change is localized to one function in one file (src/_pytest/compat.py) and involves replacing a problematic equality/membership check with identity checks. An experienced engineer would need some time to understand the codebase structure and test the fix, but the core logic change is simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear reproduction steps, identified root cause, and a focused solution. The test case provided in the patch also gives clear validation criteria for any solution attempt.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5787": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The problem is clearly demonstrated: when running tests with pytest-xdist (-n auto), chained exceptions are not fully displayed in the output, whereas they are shown properly when running without xdist. The issue provides clear test case examples showing the expected vs actual behavior. However, there are some gaps that require filling in: 1) The issue doesn't explicitly state that this is about exception serialization/deserialization in distributed test execution, 2) It doesn't mention that the fix needs to be in the reports.py module, 3) The connection between \"xdist\" and serialization isn't immediately obvious without domain knowledge. An experienced engineer familiar with pytest internals could reasonably infer that xdist requires serializing test reports between processes, and that chained exceptions aren't being properly serialized/deserialized, but this requires some domain knowledge to connect the dots.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because: 1) The engineer needs to understand that xdist works by distributing tests across processes and serializing/deserializing test reports, 2) They must identify that the issue is in the reports.py serialization logic, specifically that ExceptionChainRepr objects aren't being handled, 3) The solution involves substantial refactoring - extracting existing serialization code into separate functions (_report_to_json and _report_kwargs_from_json), adding support for ExceptionChainRepr in both serialization and deserialization, and handling the chain attribute properly, 4) The fix spans ~150 lines of new/modified code including imports, new functions, and updated logic, 5) Understanding the existing serialization format and extending it to handle chained exceptions requires careful analysis. While not extremely esoteric, this requires solid understanding of pytest internals, exception handling, and serialization patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is a legitimate bug with a clear expected outcome, the solution is well-contained within the codebase, and the test cases provide good validation. The issue demonstrates both understanding of Python exception handling and pytest internals, making it suitable for evaluating coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-5809": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem: the `--pastebin` feature in pytest uses `lexer=python3` when submitting output to bpaste.net, which causes HTTP errors for certain content types. The issue provides a concrete example showing how to reproduce the problem with specific code and references the exact file and line numbers (src/_pytest/pastebin.py#L68-L73). It also provides a clear solution: change the lexer from \"python3\" to \"text\" since pytest console output is arbitrary text, not Python code. The reasoning is sound - pytest output contains test results, error messages, and other console text that isn't valid Python syntax, so treating it as Python code for syntax highlighting purposes is inappropriate and causes the HTTP errors.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified and the solution is straightforward - change a single parameter from \"python3\" to \"text\" in the pastebin.py file. The gold patch shows this is indeed a very simple change, removing the conditional logic that selected between \"python3\" and \"python\" lexers and replacing it with a hardcoded \"text\" lexer. An experienced engineer would need minimal time to understand the codebase context, locate the relevant function (create_new_paste), and make the change. The only time needed would be to verify the change works correctly and understand the surrounding code structure.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, a simple solution, and straightforward testing. The issue provides good context and the solution makes logical sense. The test changes are also minimal and appropriate, updating the expected lexer value from the conditional logic to the hardcoded \"text\" value.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5840": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks important details. The title mentions \"ImportError while loading conftest (windows import folder casing issues)\" and states that \"5.1.1 works fine. after upgrade to 5.1.2, the path was converted to lower case\", but the actual error message or stack trace is not provided. The command shown (`pytest --collect-only .\\PIsys -m smoke`) appears to be cut off and doesn't show any error output. While we can infer from the title that this is related to case-sensitivity issues on Windows file systems when loading conftest.py files, the specific problem and expected behavior are not clearly described. An engineer would need to make significant assumptions about what the actual issue is and what constitutes a proper fix.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the provided patch, this issue requires understanding pytest's conftest loading mechanism and how path resolution works on case-insensitive file systems. The solution involves modifying multiple files (config/__init__.py and pathlib.py), removing the `unique_path` function entirely, and replacing it with `Path().resolve()` for proper case-insensitive path handling. The engineer needs to understand: (1) how pytest loads conftest.py files, (2) the difference between py.path.local and pathlib.Path for case resolution, (3) how the _conftestpath2mod caching mechanism works, and (4) Windows/case-insensitive file system behavior. While not extremely complex, this requires substantial understanding of the codebase and careful changes across multiple files to ensure conftest loading works correctly on all platforms.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "This sample has significant issues for use in a coding benchmark. The main issue description is too vague - it doesn't provide the actual error message, stack trace, or clear reproduction steps. An engineer would need to guess what the specific problem is based only on the title. Additionally, this issue requires intimate knowledge of pytest's internal conftest loading mechanism and cross-platform file system behavior, which is quite specialized. The solution involves removing an entire utility function and replacing its usage across multiple files, which suggests this might be more of a design/architectural change rather than a straightforward bug fix. For a benchmark, this would be difficult to evaluate fairly since much of the challenge comes from understanding the poorly specified requirements rather than coding ability.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6197": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a regression bug in pytest 5.2.3 where pytest tries to collect/import random __init__.py files under the current directory, which breaks when those files cannot be imported (e.g., platform-specific packages). The issue provides: 1) A clear problem statement with context about the build server failure, 2) A complete minimal reproducible example using tox that demonstrates the exact difference in behavior between pytest 5.2.2 (working) and 5.2.3 (broken), 3) Specific error output showing the collection failure, and 4) Technical details about the root cause (pytest trying to import __init__.py files it shouldn't). The reproduction steps are detailed and the expected behavior is clear from the working 5.2.2 version.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: 1) It requires understanding pytest's collection mechanism and how it changed between versions, 2) The engineer needs to trace through the code to find where pytest decides to import __init__.py files during collection, 3) Looking at the patch, it involves removing the _mount_obj_if_needed() call from Package.collect() and modifying the PyobjMixin.obj property to be lazy, which requires understanding the object mounting/collection flow, 4) The solution touches multiple parts of the collection system and requires careful consideration of when objects should be mounted to avoid eager collection, 5) While not extremely complex, it requires substantial understanding of pytest internals and careful debugging to identify the root cause of the regression.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-documented regression with clear reproduction steps and a focused scope. The issue would work well in a benchmark setting as it tests the ability to understand collection mechanisms, debug regressions, and implement fixes that prevent eager loading while maintaining proper functionality.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-6202": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) A clear example showing the problem with `@pytest.mark.parametrize(\"a\", [\"..[\"])` where the test name gets incorrectly transformed from `\"test_boo[..[]\"` to `\"test_boo[.[]\"`, (2) Detailed investigation tracing the bug through multiple source files with specific line references, (3) The exact root cause identified in `src/_pytest/python.py` line with `return s.replace(\".[\", \"[\")`, (4) A clear proposed solution to replace it with `return s`, and (5) Confirmation that the fix works by running tests. An experienced engineer has everything needed to understand and implement the solution.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix taking less than 15 minutes. The issue author has already done all the detective work, identifying the exact problematic line of code and providing the solution. The fix involves removing a single `.replace(\".[\", \"[\")` call from one line in `src/_pytest/python.py`. An engineer would just need to: (1) locate the file and line mentioned, (2) verify the current code matches the description, (3) remove the problematic replacement, and (4) run tests to confirm. No research, design thinking, or complex implementation is required.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is actually an excellent benchmark sample. The issue is clearly described, the solution is straightforward but not immediately obvious without investigation, and it tests an engineer's ability to understand the codebase structure and make targeted fixes. The provided test patch also ensures the solution can be properly validated. The only minor consideration is that the issue author already provides the solution, but this doesn't disqualify it since engineers would still need to understand the problem and implement the fix correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7205": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The issue describes a BytesWarning that occurs when using `--setup-show` with bytes parameters in pytest. The user provides a minimal reproduction case with a parametrized test using `b'Hello World'` and mentions running with `python3 -bb -m pytest --setup-show`. They also suggest the solution direction by mentioning \"Shouldn't that be using `saferepr` or something rather than (implicitly) `str()`?\" This gives a clear hint about the problem (implicit string conversion of bytes objects) and the general approach needed (using saferepr). While the exact location of the bug isn't specified, an experienced engineer could reasonably identify that this relates to fixture setup display functionality and work from there.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the patch, this is a 15 min - 1 hour fix. The solution involves: 1) Adding an import for `saferepr` from `_pytest._io.saferepr`, 2) Changing one line in `src/_pytest/setuponly.py` from `tw.write(\"[{}]\".format(fixturedef.cached_param))` to `tw.write(\"[{}]\".format(saferepr(fixturedef.cached_param, maxsize=42)))`. The core change is very simple - replacing direct string formatting with saferepr to avoid BytesWarning. However, it requires understanding the pytest codebase structure to locate the setuponly.py file and the specific function `_show_fixture_action` where the issue occurs. The test changes show that the output format changes slightly (adding quotes around parameters), but this doesn't complicate the fix significantly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-contained, the reproduction case is clear, and the fix is straightforward. The issue has good signal-to-noise ratio and represents a realistic debugging scenario that tests both problem identification and knowledge of Python's representation functions.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7236": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproducing test case showing the exact problem: when running pytest with --pdb flag, the tearDown method is being executed on skipped tests, when it should not be. The issue includes specific version information (pytest 5.4.2 vs 5.4.1), demonstrates the expected vs actual behavior with concrete command outputs, and clearly states that tearDown should not run for skipped tests even with --pdb. The problem statement is unambiguous - skipped tests should remain completely skipped regardless of the --pdb flag.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves: 1) Adding a helper function `_is_skipped()` to check if an object is marked with @unittest.skip, 2) Modifying the condition in the `runtest()` method to check not just for --pdb flag but also ensure the test is not skipped before setting up explicit tearDown. The code changes are minimal (about 10 lines added/modified) and localized to one file. The main challenge would be understanding the pytest codebase structure and locating the right place where tearDown behavior is controlled, but the logic itself is straightforward once you understand that skipped tests should bypass tearDown execution entirely.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-constructed issue with a clear problem statement, reproduction case, and expected behavior. The solution requires understanding pytest internals but the fix itself is clean and targeted. The test case provided in the test patch also demonstrates good testing practices by verifying the fix works for both @unittest.skip and @pytest.mark.skip decorators.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7324": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks critical information needed for a meaningful solution attempt. While it mentions that \"Pytest crashes the interpreter on debug build for 3.8+\" and provides a minimal reproducer `Expression.compile(\"False\")`, it fails to specify several crucial details: (1) What the expected behavior should be instead of crashing, (2) What specific error or crash occurs, (3) Any context about what Expression.compile() is supposed to do, (4) What constitutes a successful fix. The reference to bpo-40870 suggests this is related to a broader Python issue, but without being able to follow that link, an engineer would be left guessing about the root cause and appropriate solution. The issue title mentions it's specific to debug builds and Python 3.8+, but there's no explanation of why this matters or what changed in those versions.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Based on the patch, this is a moderately complex issue requiring 1-4 hours to solve. The solution involves understanding that the crash occurs because certain Python keywords (True, False, None) are valid in pytest's mark expression syntax but become invalid Python identifiers when converted to AST. The fix requires: (1) Adding a prefix system to avoid identifier conflicts, (2) Modifying the AST generation to prefix identifiers, (3) Updating the matcher lookup to strip the prefix. This requires deep understanding of AST manipulation, pytest's expression parsing system, and the interaction between custom expression syntax and Python's identifier rules. An engineer would need time to trace through the expression parsing code, understand the AST conversion process, and devise the prefix-based solution.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "This sample has significant issues for benchmark use: (1) The issue description is too vague - an engineer couldn't reasonably arrive at the specific prefix-based solution from just \"crashes on Expression.compile('False')\" without extensive code exploration and potentially external research about the Python bug referenced, (2) The solution requires very specific domain knowledge about AST manipulation and pytest internals that wouldn't be apparent from the issue description, (3) The reproducer is minimal but doesn't indicate what the expected behavior should be, making it unclear what constitutes a successful fix versus just preventing the crash in an incorrect way.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7490": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a regression between pytest versions 5.x and 6.0.0rc0 where dynamically adding an xfail marker using `request.node.add_marker(mark)` no longer works as expected. The issue provides: (1) A concrete code example showing exactly how the feature was being used, (2) Clear demonstration of the expected behavior in pytest 5.4.3 (test shows as XFAIL), (3) Clear demonstration of the broken behavior in pytest 6.0.0rc0 (test shows as failed), (4) Specific version information for both working and broken versions. The requirement is unambiguous: restore the ability to dynamically add xfail markers that are respected during test execution, just as they were in pytest 5.x.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the gold patch, it involves understanding pytest's internal test execution flow across multiple functions (pytest_runtest_setup, pytest_runtest_call, pytest_runtest_makereport) in src/_pytest/skipping.py. The core issue is that xfail markers added dynamically during test execution weren't being re-evaluated after the test ran. The solution requires: (1) Understanding pytest's hook system and test execution lifecycle, (2) Identifying that xfail evaluation needs to happen after test execution to catch dynamically added markers, (3) Modifying the logic to re-evaluate xfail marks in pytest_runtest_call after the test yields. While not a massive code change (~20 lines modified), it requires solid understanding of pytest internals and careful consideration of when xfail evaluation should occur.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for the benchmark. The problem is clearly defined with a regression scenario, the solution requires understanding of pytest internals but is achievable, and the test cases provided in the test patch would effectively validate any solution. The behavioral change is specific and testable.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7521": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, minimal reproducer test case that demonstrates the exact problem: capfd.readouterr() in pytest 6.0.0rc1 converts carriage returns (\\r) to newlines (\\n), while pytest 5.4.3 preserved them correctly. The issue includes complete test output showing the behavior difference between versions, complete package lists for both environments, and a focused test case that fails in pytest 6 but passes in pytest 5. The expected behavior is clear: carriage returns should be preserved in captured output, not converted to newlines. An engineer has all the information needed to understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The gold patch shows it's a simple one-line change adding newline=\"\" parameter to TextIOWrapper in the capture.py file. An experienced engineer would need to: 1) Understand that the issue is about newline handling in captured output, 2) Locate the capture functionality in src/_pytest/capture.py, 3) Recognize that TextIOWrapper by default performs newline translation, and 4) Add the newline=\"\" parameter to preserve original line endings. The fix requires understanding Python's TextIOWrapper newline handling behavior, but once identified, it's a trivial code change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified with a perfect reproducer, the solution is straightforward but requires some domain knowledge about Python's text I/O handling, and the test case provided in the issue directly validates the fix. The problem represents a real regression between pytest versions that would be encountered in practice.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10297": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is well-specified with some minor gaps. The user clearly demonstrates the problem with reproducible code showing that RidgeClassifierCV doesn't accept the store_cv_values parameter despite documentation claiming it should. The expected behavior is clear from the documentation quote provided. However, the issue could be slightly clearer about exactly what error occurs (though this can reasonably be inferred - likely a TypeError about unexpected keyword argument). The core problem and desired solution are unambiguous: add support for the store_cv_values parameter to match the documented behavior.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward parameter addition task. The solution involves: (1) Adding store_cv_values parameter to RidgeClassifierCV.__init__ with default False, (2) Passing it to the parent class constructor, (3) Updating documentation formatting. The existing RidgeCV already implements this functionality, so it's mainly about ensuring RidgeClassifierCV accepts and forwards the parameter correctly. An experienced engineer familiar with the codebase could implement this in 15-60 minutes after understanding the inheritance structure and seeing how RidgeCV handles this parameter.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a clean, well-contained bug fix that adds missing parameter support to match documented API. The issue provides clear reproduction code, expected behavior, and the solution is straightforward to implement and test. It's suitable for coding ability evaluation.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-10908": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. The user provides clear code examples demonstrating the inconsistent behavior between transform() and get_feature_names() methods when a vocabulary parameter is provided. They show that transform() works without fitting because it calls _validate_vocabulary(), while get_feature_names() raises NotFittedError. The expected behavior is explicitly stated: get_feature_names() should not raise NotFittedError when vocabulary is provided, following the same pattern as transform(). The class name (CountVectorizer), method names (get_feature_names, transform, _validate_vocabulary), and the specific error (NotFittedError) are all clearly identified.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified and the solution follows an existing pattern in the codebase. Looking at the gold patch, the fix involves adding just 3 lines of code to check if vocabulary_ exists and call _validate_vocabulary() if it doesn't, mirroring what transform() already does. An experienced engineer would need some time to understand the codebase structure, locate the get_feature_names() method in sklearn/feature_extraction/text.py, understand how _validate_vocabulary() works by examining the transform() method, and write appropriate tests. The solution is straightforward once the pattern is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue demonstrates good software engineering practices - identifying inconsistent API behavior, providing clear reproduction steps, and suggesting a logical solution. The fix requires understanding existing code patterns and applying them consistently, which is a common real-world programming task. The problem is well-scoped and the solution is clean and minimal.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-12585": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the `clone` function fails when parameters are estimator types (classes) rather than instances. The issue provides a concrete reproduction case with `clone(StandardScaler(with_mean=StandardScaler))`, states the expected result (no error), and even suggests a specific fix by modifying line 51 in `base.py` to add `or isinstance(estimator, type)` to the condition. The author explains their use case (storing wrapped estimators as classes in a wrapper), making the motivation clear. All necessary information is provided for an engineer to understand and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue provides the exact location of the problem (line 51 in base.py) and suggests the specific code change needed. Looking at the gold patch, it's indeed a one-line change adding `or isinstance(estimator, type)` to an existing condition. The logic is straightforward: when an estimator parameter is a type (class) rather than an instance, it should be handled differently in the clone function. An experienced engineer would need minimal time to understand the codebase context, verify the fix, and implement it. The test addition is also simple, creating a test case that verifies the clone works with estimator types.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clear, the reproduction case is simple, the fix is well-defined, and the test verifies the expected behavior. The problem domain (sklearn's clone function) is well-understood, and the solution doesn't require deep architectural changes or complex logic.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13135": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: KBinsDiscretizer with strategy='kmeans' fails because the k-means centers (and consequently bin_edges) can be unsorted, which breaks np.digitize that expects sorted bin edges. The issue provides a complete code example that reproduces the problem, explains the root cause (unsorted centers from k-means), and shows the specific error scenario. The expected behavior (no error) and actual behavior (error due to unsorted bin_edges) are clearly stated. An experienced engineer would understand exactly what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified: k-means centers can be unsorted, causing bin_edges to be unsorted, which breaks np.digitize. The solution is straightforward - sort the centers after k-means clustering. Looking at the gold patch confirms this: it's literally a 2-line addition (centers.sort() + comment) in the _discretization.py file. An experienced engineer familiar with the codebase would quickly locate the k-means logic in KBinsDiscretizer, understand that centers need to be sorted before creating bin_edges, and implement the fix. The hardest part would be understanding the codebase structure, but the fix itself is trivial once the location is found.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, has a straightforward solution that can be verified with tests, and represents a realistic bug that could occur in practice. The reproducible example makes it easy to test solutions, and the fix requires understanding both the problem domain (discretization/binning) and the specific implementation details.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13142": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when n_init>1 is set in GaussianMixture, fit_predict(X) and predict(X) return different results, which violates the expected behavior that these should be consistent. The issue provides a complete, runnable code example that reproduces the bug, shows the expected vs actual behavior, and even identifies that the existing unit test doesn't catch this because it doesn't test with n_init>1. The problem statement is unambiguous - these two methods should return the same cluster assignments for the same data.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) Understanding the problem requires familiarity with how GaussianMixture works, particularly the interaction between n_init (multiple random initializations) and the fit_predict vs predict methods. (2) The solution involves understanding the code flow in the base mixture class and identifying that the final e-step was happening too early in the process, before the best parameters were set. (3) The fix itself is relatively simple (moving a few lines of code), but diagnosing why the methods disagree and understanding the timing of when the final e-step should occur requires substantial domain knowledge and code analysis. (4) Writing appropriate test cases to verify the fix also requires understanding the expected behavior.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear problem statement, reproducible example, and the solution can be verified through the provided test cases. The issue demonstrates good software engineering practices in bug reporting.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13328": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear description of the problem: HuberRegressor throws a TypeError when fitting with boolean predictors, while other regressors like LinearRegression handle this gracefully by converting boolean arrays to float. The issue includes a complete, reproducible code example showing exactly what fails (X_bool) and what works (X and X_bool_as_float). The expected behavior is clearly stated - boolean arrays should be automatically converted to float dtype just like in LinearRegression. The solution requirements are unambiguous: modify HuberRegressor to accept boolean input arrays and convert them to float internally.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is very straightforward: adding dtype=[np.float64, np.float32] parameter to the check_X_y call in the HuberRegressor.fit method. This is a simple one-line change that leverages existing sklearn validation infrastructure. An experienced engineer would need to: 1) Understand that check_X_y is the validation function responsible for input processing, 2) Recognize that adding the dtype parameter allows automatic type conversion, 3) Add the appropriate test case. The hardest part would be familiarizing themselves with sklearn's validation patterns and locating the right file, but the actual code change is minimal and follows established patterns in the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, straightforward solution, and good test coverage. The issue demonstrates good software engineering practices with reproducible examples and clear expected behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13779": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified with a clear problem statement, concrete code example that reproduces the bug, and specific context about what fails. The issue states that \"Voting estimator will fail at fit if weights are passed and an estimator is None\" and provides a complete code snippet showing exactly when the failure occurs - after setting an estimator to None via set_params() and then calling fit() with sample_weight. The problem is in the sklearn/ensemble/voting.py file where the code doesn't check for None estimators before checking sample_weight support. An experienced engineer would understand they need to add a None check in the fit method's sample_weight validation loop.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix requiring 15 minutes to 1 hour. The problem is clearly identified in the issue description, and the solution involves adding a simple None check in the sample_weight validation loop in the VotingClassifier/VotingRegressor fit method. Looking at the provided patch, it's just 2 lines of code: checking if the estimator step is None and continuing to the next iteration if so. An engineer would need to locate the relevant code in sklearn/ensemble/voting.py, understand the existing sample_weight validation logic, and add the missing None check. The fix is conceptually simple and doesn't require deep algorithmic changes or extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, a specific reproduction case, and a straightforward solution. The test case provided in the patch confirms the fix works correctly and would serve as good validation for any solution attempt. The issue is representative of real-world software engineering tasks where edge cases need to be handled properly.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14053": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It states that `export_text` does not handle the single-feature case properly, provides a complete, runnable code example that demonstrates the issue, and includes system/version information. The code example shows exactly how to reproduce the problem: creating a decision tree with only one feature (sepal_length) and calling export_text with feature_names. While the issue doesn't explicitly state what the expected vs actual output should be, the problem statement \"does not handle the single-feature case properly\" combined with the reproducible example gives enough context for an experienced engineer to understand that there's likely an indexing or feature name handling bug when dealing with trees that have only one feature.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that takes 15 minutes to 1 hour. The solution involves adding a conditional check in the feature_names_ list comprehension to handle the case where tree_.feature[i] might be TREE_UNDEFINED (which happens in single-feature trees). The fix is a small, targeted change to one line in the export_text function in sklearn/tree/export.py. An experienced engineer familiar with scikit-learn would likely be able to: 1) reproduce the issue, 2) identify that the problem is in the feature name mapping logic, 3) understand that TREE_UNDEFINED needs special handling, and 4) implement the fix within an hour. The change requires understanding the tree structure and how feature indexing works, but doesn't require major architectural changes or extensive research.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with clear reproduction steps and a targeted fix. The test patch shows exactly what the expected behavior should be, validating that this would work well in a benchmark setting. The issue tests a specific edge case (single feature trees) which is valuable for assessing debugging skills and understanding of tree data structures.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14087": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly describes the problem: an IndexError is thrown when using LogisticRegressionCV with refit=False. The issue provides a complete, reproducible code example that demonstrates the exact error condition, specifies the expected result (no error should be thrown), and includes detailed version information. The problem statement is unambiguous - the code should run without throwing an error when refit=False is used with LogisticRegressionCV. An experienced engineer would have all the information needed to understand what needs to be fixed and what constitutes a successful solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve. While the problem is clearly defined (IndexError with refit=False), solving it requires: (1) Understanding the LogisticRegressionCV implementation and how the refit parameter affects the code flow, (2) Identifying why an IndexError occurs specifically when refit=False (likely related to array indexing when coefficients aren't being refit), (3) Tracing through the cross-validation logic to find where the error originates, and (4) Implementing a fix that handles the refit=False case properly. The gold patch shows this involves modifying conditional logic around multi_class handling and l1_ratio computation, plus adding proper null handling for elasticnet penalty cases. This level of debugging and understanding the interaction between multiple parameters (refit, multi_class, penalty type) in a complex ML algorithm implementation would take a few hours for an experienced engineer.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with clear reproduction steps, the expected behavior is obvious (no error should occur), and the gold patch shows a reasonable solution. The test patch appropriately expands test coverage to prevent regression. This is a suitable benchmark sample.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14629": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the bug: `cross_val_predict(method='predict_proba')` fails with `MultiOutputClassifier` due to an AttributeError when trying to access `estimator.classes_`. The issue provides a specific location in the code (line 857-866 in _validation.py), explains the root cause (MultiOutputClassifier needs `mo_clf.estimators_[i].classes_` instead of `estimator.classes_`), includes a complete reproducible code example, specifies expected results, and provides version information. An experienced engineer would have all the information needed to understand and fix this problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The issue clearly identifies the problem location and root cause. Looking at the gold patch, the solution is straightforward: add a `classes_` attribute to MultiOutputClassifier's fit method that aggregates the classes from individual estimators. The code change is minimal (adding about 20 lines including docstring) and the logic is simple. The main time would be spent understanding the codebase structure and testing the fix, but the actual implementation is quite direct once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly defined, has a specific reproducible test case, and the solution requires understanding both the immediate problem (missing classes_ attribute) and the broader context of how MultiOutputClassifier works with scikit-learn's cross-validation utilities. The fix is non-trivial enough to test coding ability while being achievable within a reasonable timeframe.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14710": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: HistGradientBoostingClassifier fails when using string targets with early stopping enabled because the scorer receives mismatched data types (integer y_true vs string y_pred). The issue provides a complete reproducible example showing the exact error scenario, explains the root cause (y_true being integer while y_pred are original string classes), and even includes a detailed potential resolution with specific code changes in the _check_early_stopping_scorer method. The expected behavior is clearly stated (no error should be thrown), and the technical details about encoding y_true are sufficient for an experienced engineer to understand and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue is well-documented with a clear root cause and even includes a suggested solution. The fix involves adding just a few lines of code to convert integer-encoded targets back to their original string classes before scoring. An experienced engineer would need some time to understand the gradient boosting codebase structure and verify the solution works correctly, but the actual code change is minimal and straightforward. The main effort would be in understanding the early stopping flow and ensuring the fix doesn't break other functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, has a focused scope, includes a reproducible test case, and the solution is well-defined but requires understanding of the codebase. The provided test patch confirms the fix works correctly and covers the edge case properly.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14894": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear description of the problem: a ZeroDivisionError occurs in _sparse_fit for SVM when support_vectors_ is empty. The issue includes complete reproducible code that demonstrates the problem, showing that dense data works but sparse data fails. It specifies the expected behavior (no error, with self.dual_coef_ = sp.csr_matrix([])) versus the actual behavior (ZeroDivisionError). The code example is minimal and focused, making it easy to understand the exact conditions under which the bug occurs. The version information is also provided, giving context for the environment.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue is localized to the _sparse_fit method in sklearn/svm/base.py. Looking at the provided patch, the solution involves adding a simple conditional check for when n_SV (number of support vectors) is 0, and handling that case by creating an empty sparse matrix instead of attempting the division that causes the error. The fix requires understanding the sparse matrix construction and recognizing that the division operation fails when there are no support vectors, but the logic is straightforward once you locate the problematic code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear reproduction case, specific expected behavior, and a focused solution. The test case provided in the patch confirms the fix works correctly and prevents regression. This sample would work well for evaluating coding ability as it tests understanding of sparse matrices, error handling, and edge cases in scientific computing libraries.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25747": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides all necessary information for a successful solution. It clearly describes: (1) The problem: FeatureUnion fails when using pandas transform output with custom transformers that aggregate data, (2) A complete, reproducible code example showing the exact failure case, (3) The expected behavior: no error should be thrown when using pandas transform output, (4) The actual behavior: an error occurs (though the specific error message isn't shown, it's implied), and (5) System/version information. The code example demonstrates that the same operation works with default numpy output but fails with pandas output, making the root cause clear. An experienced engineer would understand they need to fix the pandas transform output handling in FeatureUnion to properly handle cases where transformers change the DataFrame structure (like aggregation operations).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided solution, this is a 15 min - 1 hour fix. The issue is in the `_wrap_in_pandas_container` function in `sklearn/utils/_set_output.py`. The problem is that when a DataFrame is passed to this function, it was incorrectly overriding the index even when the transformer intentionally changed it (like in aggregation operations). The fix is simple: remove the lines that override the index when the input is already a DataFrame. This requires understanding the pandas transform output mechanism and recognizing that preserving the original DataFrame's structure (including any index changes made by transformers) is the correct behavior. The change is only 2 lines removed, but requires some thought about the semantics of pandas transform output.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the test cases adequately verify the fix. The issue represents a realistic bug that could occur in practice when using sklearn's pandas transform output feature with custom transformers that modify DataFrame structure.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25931": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes that IsolationForest raises an unexpected warning about feature names when fitted with a pandas DataFrame and a non-default contamination parameter. The issue provides: 1) A clear description of the bug with context about when it occurs, 2) Exact reproduction code showing the problem, 3) Expected vs actual behavior, 4) A hypothesis about the root cause (the estimator calls predict on training data internally), 5) A specific link to the problematic line of code in the sklearn repository. An experienced engineer has all the information needed to understand the problem: the warning is triggered because when contamination != \"auto\", the fit method calls score_samples() which performs input validation that strips feature names, causing the inconsistency warning.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue is well-understood and the solution is straightforward once you trace through the code. Looking at the provided patch, it involves: 1) Creating a private _score_samples method that skips input validation, 2) Having the public score_samples method call this private method after doing validation, 3) Modifying the fit method to call _score_samples instead of score_samples when setting offset_. This avoids the double validation that was stripping feature names. The change is localized to one file, involves moving existing code into a new private method, and requires minimal new logic. An experienced engineer familiar with sklearn patterns would recognize this as a common solution to avoid double validation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified, has a well-defined reproduction case, and the solution requires understanding the codebase structure and sklearn's validation patterns. It tests the ability to trace through method calls, understand when input validation should and shouldn't be applied, and implement a clean solution following existing code patterns. The test case is also straightforward - it checks that no warning is raised when fitting with a DataFrame and non-default contamination.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25973": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear description of the bug (SequentialFeatureSelector cannot accept splits from a cross-validator despite documentation suggesting it should), includes a complete reproducible code example showing the exact failure case, states the expected behavior (should run without errors), and even mentions the user has done similar things with other scikit-learn classes. The problem is unambiguous: the cv parameter should accept an iterable of splits but currently fails when provided one.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution requires understanding that scikit-learn has a standard pattern for handling cv parameters using the check_cv utility function. An experienced engineer would need to: (1) recognize this is a common pattern in scikit-learn, (2) locate the check_cv function, (3) add the import and call to check_cv, and (4) thread the validated cv object through the method calls. The actual code changes are minimal (adding import, one line to call check_cv, updating method signature and call), but it requires some familiarity with scikit-learn's conventions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample - it has a clear bug description, straightforward reproduction case, and the solution follows established patterns in the codebase. The test case properly validates that the fix works by ensuring no exception is raised when passing cv splits.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-10323": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a problem with the Sphinx literalinclude directive where leading whitespace is removed from prepended content, causing poor indentation in code examples. The issue provides: (1) A complete reproduction case with specific files (index.rst and pom.xml), (2) The exact current output showing the indentation problem, (3) The expected behavior with properly indented output, (4) Context about the Sphinx environment and extensions used, and (5) Even mentions a potential workaround involving dedent. An experienced engineer would have all the information needed to understand that the prepend/append filters are being applied before the dedent filter, when they should be applied after to preserve proper indentation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution is straightforward: reorder the filter execution in the LiteralIncludeReader.read() method so that dedent_filter runs before prepend_filter and append_filter. This only requires moving one line in the filters list to change the execution order. The core insight is understanding that dedent should be applied to the included content before adding prepend/append content. While it requires some understanding of how the filtering pipeline works, it's a simple reordering change rather than complex logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample is well-suited for a coding benchmark as it tests understanding of execution order in a processing pipeline and requires minimal code changes but thoughtful analysis of the problem. The test case also properly validates the fix works correctly with the combination of dedent, prepend, and append options.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-10673": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. Users want to add references to 'genindex', 'modindex', and 'search' in their toctree directive, but currently this raises errors because Sphinx treats these as nonexistent documents. The issue provides concrete examples of the desired syntax and references multiple Stack Overflow questions showing this is a common user need. The solution requirement is clear: allow these special generated documents to be referenced in toctree without errors. The issue provides a specific code example of what should work, making the expected behavior unambiguous.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Sphinx's internal document handling system and modifying multiple files. The engineer needs to: 1) Understand how Sphinx tracks documents vs generated content like indexes, 2) Modify the toctree directive parsing logic to recognize generated docnames, 3) Update the toctree resolution logic to handle these special references, 4) Ensure figure numbering doesn't break with generated docs. The solution touches 3 different files (sphinx/directives/other.py, sphinx/environment/adapters/toctree.py, sphinx/environment/collectors/toctree.py) and requires understanding the distinction between regular documents and generated content like genindex/modindex/search. While not extremely complex, it requires familiarity with Sphinx's architecture and careful handling to avoid breaking existing functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution scope is clear, and the test coverage appears adequate. The issue represents a legitimate feature enhancement that improves user experience without breaking existing functionality. The provided test cases validate the core functionality properly.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7440": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is vague and ambiguous. While it mentions \"glossary duplicate term with a different case\" in the title, the actual problem description is unclear. The reproduction steps show how to build documentation, but don't clearly explain what goes wrong. The \"Expected behavior\" section just asks \"MySQL != mysql term right ?\" without explaining the current vs. expected behavior. The issue references external links and Travis CI logs that we cannot access. An engineer would need to infer from the title and minimal context that this is about case sensitivity in glossary terms, but the specific requirements for a solution are not clearly stated.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the actual fix in the patches, this is a relatively simple change - removing the lowercase conversion in two places in the Sphinx code (removing `.lower()` call and `lowercase=True` parameter). Once an engineer understands that the issue is about preserving case sensitivity in glossary terms, the solution involves identifying where case conversion happens and removing it. This requires some familiarity with the Sphinx codebase to locate the relevant files (sphinx/domains/std.py), but the actual code changes are minimal and straightforward. The fix involves only 2 lines of changes across the same file.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The main issue is that the problem description is too vague and relies heavily on external context (Travis CI logs, external repository links) that wouldn't be available in the benchmark setup. An engineer working from just this issue text would struggle to understand what specific behavior needs to be implemented. The issue would require significant clarification about what exactly constitutes the \"duplicate term\" problem and what the expected case-handling behavior should be. Additionally, the reproduction steps reference a different repository (phpmyadmin) rather than the Sphinx codebase where the fix needs to be applied.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7462": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It clearly describes a problem where Sphinx crashes with \"IndexError: pop from empty list\" when processing empty tuple type annotations like `Tuple[()]`. The bug report includes a minimal reproduction case, environment details, and expected behavior (docs should build successfully). However, the issue description doesn't explicitly state that this is a crash/exception - you have to infer this from the error message in the title. An experienced engineer familiar with Sphinx's architecture would understand that this is likely an AST parsing issue in the type annotation handling code, and the goal is to prevent the crash by properly handling empty tuples. The reproduction steps are clear and the expected outcome is straightforward.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The error \"pop from empty list\" is a classic Python error that immediately indicates the problem: code is trying to remove elements from an empty list. Looking at the patch, the fix involves adding a simple conditional check in two files (sphinx/domains/python.py and sphinx/pycode/ast.py) to handle empty tuples before attempting to pop from the list. The logic is straightforward: if the tuple has elements, process them normally and remove the trailing comma; if empty, return appropriate punctuation. An experienced engineer would quickly identify this as an edge case handling issue in AST unparsing logic. The fix requires minimal code changes and follows a clear pattern.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained, the solution is evident from the error message, and the test cases clearly verify the fix. This is a good benchmark sample as it tests both debugging skills (understanding the IndexError) and code comprehension (finding the right place to add the conditional check). The issue involves understanding AST processing which is a realistic software engineering challenge.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7985": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified. The user clearly explains what they want (linkcheck to also check local links), provides a concrete reproduction case with code examples, and states the expected behavior. However, there are some implementation details left unspecified - for example, exactly how local links should be resolved (relative to what directory), what should happen with different types of local references, or how errors should be reported. The core requirement is clear but an engineer would need to make some reasonable assumptions about the specifics of how local link checking should work.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task. The engineer needs to: (1) Understand the existing linkcheck.py code structure and how URI checking currently works, (2) Distinguish between different URI schemes to identify local vs external links, (3) Implement file existence checking for local references relative to the source directory, (4) Integrate this with the existing status reporting system, and (5) Add appropriate test cases. The solution touches the core checking logic and requires understanding Sphinx's file handling, but it's not extremely complex - around 20 lines of new code plus tests.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained feature request with a clear scope. The existing infrastructure for link checking can be extended logically to handle local files. The test setup is straightforward and the expected behavior is reasonable to implement.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-8269": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when linkcheck_anchors=True, Sphinx reports \"Anchor not found\" even when the real issue is an HTTP error (404, 500, etc.). The reproduction steps are concrete and actionable, showing exactly how to reproduce the bug with specific commands. The expected behavior is clearly contrasted with the actual behavior using concrete examples of the output messages. An experienced engineer would have no trouble understanding that they need to modify the linkcheck functionality to report HTTP errors before attempting anchor checking.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that should take 15 minutes to 1 hour. The core issue is clear: HTTP errors should be raised before attempting anchor validation. Looking at the solution, it's just adding `response.raise_for_status()` after the HTTP request in sphinx/builders/linkcheck.py. An experienced engineer would need some time to locate the relevant code in the linkcheck builder, understand the flow of how anchors are checked, and determine where to insert the status check. The logic is simple but requires understanding the codebase structure and the requests library behavior.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, straightforward reproduction steps, and a focused solution. The test coverage in the patch is appropriate and the fix is minimal and targeted. This would make a good benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-8551": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is largely well-specified. It clearly describes the bug (inconsistent class lookup behavior between explicit xref roles and implicit :type:/:rtype: fields), provides concrete reproduction steps with RST code examples, and states the expected behavior. However, there are minor gaps: it doesn't specify exactly which modules/files in the codebase handle this functionality, and some implementation details about how the lookup should work are left implicit. An experienced engineer would need to explore the codebase to understand Sphinx's cross-reference resolution mechanism, but the core problem and desired outcome are clear.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. The engineer needs to: 1) Understand Sphinx's cross-reference resolution system and how info fields work, 2) Locate the relevant code (likely in domains/python.py and util/docfields.py), 3) Debug why implicit xrefs behave differently than explicit ones, 4) Implement context propagation so implicit xrefs have proper module/class scope, and 5) Write tests. The fix involves understanding domain-specific code and cross-reference mechanics, but the actual code changes are relatively small (adding context information to xref nodes). This requires moderate familiarity with Sphinx internals but isn't extremely complex.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for the benchmark. It has clear reproduction steps, expected behavior, and the solution can be objectively verified through the provided test cases. The core problem is well-defined even though some implementation details need to be discovered through codebase exploration.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-9711": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that the `needs_extensions` function in Sphinx performs string comparison for version checking instead of proper semantic version comparison. This causes incorrect behavior where version \"0.10\" is considered less than \"0.6\" when compared as strings. The issue provides a clear reproduction case with specific commands, shows the expected behavior (sphinx-gallery 0.10.0 should be accepted when 0.6 is the minimum), and identifies the root cause. An experienced engineer would understand they need to modify the version comparison logic in the `needs_extensions` function to use proper version parsing instead of string comparison.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified in the `verify_needs_extensions` function where string comparison is used instead of proper version comparison. The solution involves importing a version parsing library (like packaging.version) and modifying the comparison logic. The code change is relatively small - adding imports and changing the version comparison logic with proper error handling for invalid versions. The hardest part might be understanding the existing codebase structure and ensuring proper error handling, but the core fix is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests the ability to identify a common software engineering problem (improper version comparison), requires understanding of semantic versioning concepts, and involves making a targeted fix with appropriate error handling. The issue is well-documented with clear reproduction steps and expected behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13372": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It clearly demonstrates a bug: `Mul(Max(0, y), x, evaluate=False).evalf()` throws an UnboundLocalError while `Mul(x, Max(0, y), evaluate=False).evalf()` works fine. The problem is that the order of multiplication arguments affects the behavior, which shouldn't happen. The issue author provides a helpful hint about the root cause being in elif clauses that define reprec and imprec variables, suggesting they should have an `else: raise NotImplementedError` clause. While the exact location and implementation details aren't spelled out, an experienced engineer could reasonably interpret this as: (1) there's an UnboundLocalError happening in evalf() when certain argument orders are used, (2) the bug is related to uninitialized reprec/imprec variables in elif chains, and (3) the fix involves adding proper else clauses. The provided code example gives a clear reproduction case.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problematic code location (elif clauses defining reprec and imprec) and suggests the solution (adding else: raise NotImplementedError). The actual fix shown in the patch is indeed very simple - just adding two \"else: raise NotImplementedError\" blocks to existing elif chains in sympy/core/evalf.py. An experienced engineer would need to: (1) reproduce the error (straightforward with the given example), (2) locate the evalf() function and find the elif chains mentioned, (3) identify which variables are causing UnboundLocalError, and (4) add the missing else clauses. The debugging and implementation should be quick since the issue author provided clear guidance about the root cause.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample provides a clear reproduction case, has a straightforward fix that's well-suited for benchmarking coding ability, and the test case properly verifies the fix works. The issue demonstrates good software engineering practices - identifying a bug, providing reproduction steps, and suggesting a solution approach.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-13480": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear, reproducible example showing that `coth(log(tan(x)))` fails when substituting certain integral values like 2, 3, 5, etc. While it doesn't explicitly state what \"fails\" means (likely an error or exception), the example code is specific enough that an engineer could run it, reproduce the problem, and understand that the expected behavior is for the substitution to work without error. The pattern of failing values gives additional context. However, there are some blanks to fill in regarding the exact expected output or error behavior, making this a category 1 rather than 0.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a simple typo fix - changing `cotm` to `cothm` on a single line. Once an engineer reproduces the issue and traces through the code execution, they would likely find this variable name error fairly quickly since it's a clear typo (cotm vs cothm). The fix requires understanding the coth function implementation and the _peeloff_ipi helper, but the actual code change is minimal. This would take somewhere between 15 minutes to 1 hour - time to reproduce the issue, trace through the code, identify the typo, and verify the fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined enough to be reproducible, the solution space is constrained to fixing the substitution behavior, and the patch shows this is a straightforward bug fix rather than a complex algorithmic change. The test cases provided give good guidance on expected behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-13877": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear, reproducible example showing exactly when the problem occurs (determinant calculation for n=5 returns nan instead of a valid result). The code example is complete and demonstrates the progression from working cases to the failing case. The user even provides a reasonable hypothesis about the root cause (Bareiss algorithm limitations with symbolic matrices). An experienced engineer would have enough information to reproduce the bug, understand that it's related to determinant computation with symbolic entries, and begin investigating the matrix determinant implementation, particularly the Bareiss algorithm.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) The bug reproduction is straightforward, but understanding why it happens requires diving into the matrix determinant algorithms, specifically the Bareiss method implementation. (2) The engineer needs to understand the mathematical concepts behind the algorithm and why it fails with symbolic entries. (3) The solution involves modifying the pivot-finding logic to use a more appropriate zero-testing function (_is_zero_after_expand_mul instead of the default), which requires understanding how symbolic expressions are handled in SymPy. (4) While the actual code changes are relatively small (mainly in one function), finding the right approach requires some research into the algorithm's limitations and SymPy's symbolic handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it represents a real algorithmic bug with symbolic computation, has a clear reproduction case, and tests the engineer's ability to debug mathematical algorithms in a symbolic computation library. The fix requires both understanding the problem domain and making the appropriate code modifications.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-16792": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: autowrap with cython backend fails when array arguments don't appear in the wrapped expression. The issue includes a minimal reproducible example showing the exact failure case, demonstrates the incorrect C function signature that's generated (showing `double x` instead of `double *x`), explains why this happens, provides a working counterexample where the expression depends on the argument, gives context for why this matters in real-world usage (interfacing with external libraries), and even mentions that the author has identified the problem location in codegen. All the information needed to understand and solve the problem is present.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve. While the problem is clearly described, solving it requires: understanding the sympy codegen module architecture, tracing through how argument metadata is handled for array vs scalar types, understanding how argument_sequence interacts with symbol detection from expressions, and implementing logic to preserve array metadata even when symbols don't appear in expressions. The solution involves modifying the `routine` method in `codegen.py` to handle cases where symbols are provided in argument_sequence but don't appear in the expression atoms. This requires careful analysis of the existing code flow and implementing a helper function to extract dimensions, making it a moderate complexity task.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark candidate. The issue is clearly specified with minimal reproducible examples, has a focused scope within the codegen module, requires understanding of both Python and the sympy codebase architecture, and has a clear pass/fail criterion via the provided test case. The solution demonstrates good software engineering practices including proper metadata handling and edge case consideration.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17139": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description shows a specific problem with the simplify function when applied to cos(x)**I, where I is the imaginary unit. The code snippet demonstrates the exact input that causes the issue, and the error message \"Invalid comparison of complex I\" clearly indicates what's going wrong. Looking at the gold patch, it's clear that the issue is in the fu.py file where complex exponents are not being handled properly - the code tries to compare a complex number which is invalid. While the issue description is brief, it provides enough information to understand that the simplify function should handle complex exponents gracefully without throwing an error. The expected behavior (based on the test patch) is that expressions like cos(x)**I should remain unchanged rather than causing an error.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that would take 15 minutes to 1 hour. The gold patch shows that the solution is simply adding a check for `if not rv.exp.is_real: return rv` in the _f function in fu.py. An experienced engineer would need to: 1) Reproduce the error with the given code snippet, 2) Trace through the simplify function to find where the invalid comparison occurs, 3) Identify that the issue is in the trigonometric simplification logic that doesn't handle complex exponents, and 4) Add the simple check to return early when the exponent is not real. The fix is only 2 lines of code and the logic is straightforward - if the exponent is complex, don't try to apply the simplification that would lead to invalid comparisons.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with a clear reproduction case, the expected behavior is reasonable (complex exponentials should be left unsimplified), and the fix is straightforward. The test cases also clearly demonstrate the expected behavior for both simple complex exponents (I) and more complex ones (2*I + 1).",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17318": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides a clear enough understanding of what needs to be fixed. It clearly states that sqrtdenest raises an IndexError for a specific input, and that the expected behavior is to return the expression unchanged when it cannot be denested. The specific input case is provided, along with expected old vs new behavior. While it doesn't explain exactly where in the code the IndexError occurs or the precise technical details of why denesting fails, an experienced engineer could reasonably work with this information to identify the root cause and implement a fix. The core requirement is clear: handle the IndexError gracefully and return the input unchanged when denesting is not possible.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the provided patches, this issue requires understanding the mathematical logic in sqrtdenest.py and specifically the _sqrt_match function. The fix involves modifying a condition to check that squared terms are both Rational AND positive, not just Rational. This requires: 1) Understanding how the sqrtdenest algorithm works, 2) Identifying where the IndexError occurs (in the _sqrt_match function), 3) Understanding why negative rationals cause issues, and 4) Implementing the additional positivity check. While the actual code change is small (adding 'and sq.is_positive'), it requires mathematical understanding and careful debugging to identify the root cause. This would likely take 1-4 hours for an experienced engineer to fully understand, debug, and implement correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-structured mathematical bug fix with clear test cases. The issue provides sufficient context for an engineer familiar with symbolic mathematics to understand and solve the problem. The test patch shows comprehensive coverage of the fix, including both the specific failing case and related edge cases.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17630": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear reproduction case with exact code examples showing: (1) how to create a BlockMatrix with ZeroMatrix blocks, (2) that single multiplication works fine, (3) that double multiplication fails with an exception, and (4) identifies the root cause - that zeros in the result are `Zero` objects instead of `ZeroMatrix` objects. The problem is clearly defined: block multiplication should work consistently for multiple operations, and the type inconsistency between `ZeroMatrix` and `Zero` is preventing this. An experienced engineer would understand exactly what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is clearly identified, the solution requires understanding SymPy's internal matrix expression system and how block matrix operations work. The fix involves modifying the `_postprocessor` function in `matexpr.py` to handle `MatAdd` operations properly, ensuring type consistency. The engineer needs to: (1) understand the BlockMatrix and MatAdd interaction, (2) trace through the multiplication logic to see where `Zero` vs `ZeroMatrix` type inconsistency occurs, (3) implement a fix that maintains proper matrix types during addition operations, and (4) ensure the fix doesn't break other functionality. The actual code change is small (2 lines) but requires substantial understanding of the matrix expression framework.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-documented with clear reproduction steps, the solution is focused and specific, and the test cases properly verify the fix. The issue demonstrates good software engineering practices with minimal, targeted changes that address the core type consistency problem.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-17655": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides clear, concrete examples showing the problem: `point1 + point2 * sympy.sympify(2.0)` works fine, but `point1 + sympy.sympify(2.0) * point2` raises an exception. The expected behavior is explicitly stated - both expressions should give the same result. The issue demonstrates a classic Python operator precedence problem where multiplication should be commutative (a*b should equal b*a), but the Point class is missing the `__rmul__` method to handle right multiplication. The code examples are complete and reproducible, making it clear what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward 15-60 minute fix. The issue is a classic Python magic method problem - the Point class has `__mul__` but is missing `__rmul__` for right multiplication. An experienced engineer would quickly recognize this pattern and understand that adding `__rmul__` that delegates to `__mul__` is the standard solution. The gold patch confirms this - it's literally a 4-line addition of the `__rmul__` method. The solution requires minimal code familiarization since it's a well-known Python operator overloading pattern, and the fix is localized to a single method in one file.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - the problem is clearly defined, the solution is straightforward but requires understanding Python operator overloading, and the test cases properly verify the fix. The issue demonstrates good software engineering principles around operator commutativity.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-18211": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. The problem is that `solveset` (called via `Eq().as_set()`) raises a `NotImplementedError` when it encounters an equation it cannot solve, but it should instead return a `ConditionSet` that represents the unsolved condition. The issue provides a concrete example with `Eq(n*cos(n) - 3*sin(n), 0).as_set()` and shows exactly what the expected output should be: `ConditionSet(n, Eq(n*cos(n) - 3*sin(n), 0), Reals)`. The behavior change is straightforward - catch the `NotImplementedError` and return a `ConditionSet` instead. The code patch confirms this understanding by showing a try-except block that does exactly this in the `_eval_as_set` method of the relational module.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution requires understanding where the `NotImplementedError` is being raised (in `solve_univariate_inequality`) and adding a try-except block to catch it and return a `ConditionSet` instead. The patch shows this is accomplished with about 8 lines of code changes in a single method (`_eval_as_set` in `sympy/core/relational.py`). An experienced engineer would need some time to locate the right method, understand the code flow from `as_set()` to `solve_univariate_inequality`, and implement the try-catch logic, but this is not a complex algorithmic change - just error handling with a fallback to a different return type.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. The problem is well-defined, the solution is straightforward, and the test cases clearly verify the expected behavior. This would be a good benchmark sample as it tests understanding of error handling, symbolic mathematics concepts (ConditionSet), and codebase navigation skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-20428": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It provides a specific example showing how clear_denoms() returns a polynomial that prints as \"Poly(0, x, domain='EX')\" but has inconsistent behavior - is_zero returns False while as_expr().is_zero returns True. The issue identifies the root cause: an unstripped leading 0 in the DMP representation ([EX(0)] instead of []). The expected behavior is clearly demonstrated by comparing with a properly constructed zero polynomial. The issue provides concrete reproduction steps, shows the problematic behavior with multiple examples, and identifies the underlying data structure issue that needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively simple fix that changes one line in the __bool__ method of the ExpressionDomain class. The change is from \"return f.ex != 0\" to \"return not f.ex.is_zero\". While understanding the issue requires some familiarity with SymPy's polynomial representation and the interaction between different zero-checking methods, the actual fix is straightforward once the root cause is identified. An experienced engineer familiar with the codebase could identify that the issue stems from inconsistent zero-checking behavior and implement this fix within 15 minutes to 1 hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the reproduction case is clear, and the fix is straightforward. The test case properly verifies that the problematic polynomial now behaves consistently as a zero polynomial. This would be a good benchmark sample for testing understanding of symbolic mathematics libraries and debugging inconsistent object behavior.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-20438": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified. It provides a concrete code example that demonstrates the problem: `b.is_subset(c)` should return True but doesn't, while `c.is_subset(b)` correctly returns True. The mathematical relationship is clear - both sets represent the same Cartesian product {1,2} \u00d7 {1,2}. However, there are some gaps: the expected output for some operations is missing (like what `b.is_subset(c)` currently returns), and the issue description lacks explicit statement of what the correct behavior should be. An experienced engineer familiar with set theory would understand that equivalent sets should have symmetric subset relationships.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Understanding SymPy's set system architecture and multiple dispatch pattern, (2) Identifying that the issue spans multiple files (issubset handlers, comparison handlers, and relational operations), (3) Implementing proper ProductSet-FiniteSet comparison logic that checks containment of all ProductSet elements, and (4) Fixing related equality comparison edge cases. The patch shows changes across 3 different files with about 20 lines of modifications, requiring knowledge of SymPy's internal APIs and fuzzy logic system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within SymPy's set operations, has clear test cases, and the mathematical correctness is verifiable. The solution requires domain knowledge of SymPy but is a legitimate coding challenge suitable for benchmarking.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20590": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The reporter clearly identifies that Symbol instances unexpectedly gained a __dict__ attribute in version 1.7, which contradicts the purpose of __slots__ (memory optimization). They provide concrete examples showing the behavior in version 1.6.2 (no __dict__) vs 1.7 (has __dict__). The hypothesis that \"some parent class accidentally stopped defining __slots__\" gives a clear direction for investigation. However, the issue doesn't explicitly state what the desired behavior should be - though it's reasonable to infer that Symbol instances should not have __dict__ to maintain the memory efficiency that __slots__ provides. An experienced engineer could work with this information to identify and fix the root cause.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a 15-minute to 1-hour fix. The solution is simply adding `__slots__ = ()` to the Printable class in sympy/core/_print_helpers.py. The issue stems from a mixin class (Printable) not defining __slots__, which causes subclasses that use __slots__ to inadvertently gain a __dict__. Once an engineer understands that Symbol inherits from Printable and that mixin classes should define empty __slots__ when used with slotted classes, the fix is straightforward - just a few lines of code with a comment explaining the rationale. The research to identify the inheritance chain and understand the __slots__ behavior would take some time, but the actual code change is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark issue. It tests understanding of Python's __slots__ mechanism, multiple inheritance patterns, and debugging skills to trace through a class hierarchy. The issue is concrete with clear before/after behavior, and the solution demonstrates knowledge of Python internals. The test patch appropriately verifies both the absence of __dict__ and immutability behavior. An engineer would need to understand the relationship between __slots__ and __dict__, investigate the inheritance chain, and implement the correct fix in the mixin class.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-21379": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The reporter provides a clear minimal working example (MWE) that demonstrates the problem: a PolynomialError occurs when calling `.subs({1: 1.0})` on a specific expression involving `exp(sinh(Piecewise(...)))` with real symbols, but only under certain cache conditions. The reporter also provides helpful debugging information about what conditions trigger the error (real symbols, specific functions, cache state). However, there are some gaps: the exact error message/traceback isn't provided, and the root cause analysis is incomplete. An experienced engineer would need to reproduce the issue and debug further to understand why the PolynomialError occurs, but the MWE gives a clear starting point and success criteria (the subs operation should work without throwing an exception).",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the fix itself is relatively small (wrapping a gcd call in a try-except block), getting there requires significant investigation. The engineer needs to: (1) Reproduce the issue using the provided MWE, (2) Trace through the sympy codebase to understand why PolynomialError is being raised during a subs operation, (3) Identify that the issue occurs in the Mod.eval method when gcd() is called on complex expressions, (4) Understand that catching PolynomialError and defaulting to G=1 is the appropriate solution. The debugging process involves understanding sympy's internal workings, the interaction between different symbolic math operations, and cache behavior. The actual code change is small but finding the right location and solution requires substantial investigation time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample provides a clear reproducible case with specific conditions that trigger the bug. The test cases check that the problematic expression can be created and the subs operation works without throwing an exception. This is suitable for evaluating coding ability as it requires both debugging skills and understanding of mathematical libraries.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-22714": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified with a clear problem statement, reproducible code example, and expected behavior. The issue shows that `with sp.evaluate(False):` causes an unexpected crash when creating a Point2D object, while the same code works fine without the evaluate context or when passing evaluate=False as a parameter. The error message \"Imaginary coordinates are not permitted\" is clearly identified as incorrect behavior since the coordinates (1,2) are real integers. The test case and expected behavior are unambiguous.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is in the Point2D validation logic where `im(a)` (imaginary part) is being evaluated incorrectly under `evaluate(False)` context. Looking at the gold patch, the fix is a simple one-line change in sympy/geometry/point.py line 155, changing `if any(a.is_number and im(a) for a in coords):` to `if any(a.is_number and im(a).is_zero is False for a in coords):`. This requires understanding how SymPy's evaluation context affects expression evaluation, but the actual code change is minimal and straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue has a clear reproduction case, the problem is well-defined, and the solution requires understanding SymPy's evaluation context behavior. The fix demonstrates knowledge of how boolean expressions should be properly evaluated in SymPy when evaluation is disabled.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-23824": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It clearly describes the problem: the kahane_simplify() function incorrectly reverses the order of leading uncontracted gamma matrices. The issue provides a concrete example with expected vs actual behavior, showing that \u03b3^\u03bc \u03b3_\u03bc \u03b3^\u03c1 \u03b3^\u03c3 and \u03b3^\u03c1 \u03b3^\u03c3 \u03b3^\u03bc \u03b3_\u03bc should both simplify to 4 \u03b3^\u03c1 \u03b3^\u03c3, but the second case incorrectly produces 4 \u03b3^\u03c3 \u03b3^\u03c1 due to the bug. The author even identifies the root cause: leading matrices are removed at the beginning and re-inserted at the end, but the insertion loop runs backward. The test code demonstrates the exact failure case with specific function calls and expected assertions. A developer has all the information needed to understand the problem, reproduce it, and implement a fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue description clearly identifies the root cause (backward insertion loop) and provides the exact location of the bug in the kahane_simplify() function. Looking at the gold patch, the fix is indeed simple - replacing a backward loop that inserts elements at position 0 with a list comprehension that prepends the free positions correctly. The change is only 2 lines: removing the problematic loop and replacing it with a single line that concatenates the lists in the correct order. An experienced engineer familiar with the codebase would need minimal time to understand gamma matrix simplification, locate the kahane_simplify function, identify the problematic loop mentioned in the issue, and implement the straightforward fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clearly specified with concrete examples, the bug is well-isolated to a specific function, and the fix is straightforward once the problem is understood. The test cases provided in the issue description directly translate to the test patch, making it easy to verify the solution. The domain knowledge required (gamma matrix algebra) is explained sufficiently in the context, and the actual programming fix is a simple list manipulation issue that any experienced Python developer could handle.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23950": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It shows that `Contains(x, Reals).as_set()` incorrectly returns `Contains(x, Reals)` instead of the actual set `Reals`. The issue explains that Contains is a boolean function, not a set, and this incorrect behavior causes failures in other parts of the code (like with Piecewise) because Contains doesn't have the expected set methods like `as_relational`. The expected behavior is clear: `as_set()` should return the set that the element is being tested for containment in (the second argument of Contains), not the Contains object itself.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The problem is clearly identified and the solution is straightforward - the `as_set()` method in the Contains class needs to return `self.args[1]` (the set argument) instead of raising NotImplementedError or returning self. Looking at the gold patch confirms this: it's literally a one-line change from `raise NotImplementedError()` to `return self.args[1]`. An experienced engineer would need some time to understand the Contains class structure and verify that args[1] is indeed the set argument, but the conceptual fix is simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is clear and testable, and it represents a meaningful coding task that tests understanding of class methods and argument handling in Python.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-24066": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It provides a complete reproduction case showing exactly what should work but doesn't. The code example demonstrates that `expr = units.second / (units.ohm * units.farad)` should be dimensionless (and the assertion confirms this), but when used as an argument to `exp()` in `buggy_expr = 100 + exp(expr)`, the `_collect_factor_and_dimension()` method fails to handle it properly. The expected behavior is clear: mathematical functions like `exp()` of dimensionless quantities should return dimensionless results. The issue provides specific function names (`SI._collect_factor_and_dimension()`) and demonstrates the exact problem scenario.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is localized to a specific method (`_collect_factor_and_dimension`) and the bug is in how it handles Function expressions. Looking at the patch, the fix is relatively straightforward: instead of blindly returning all dimensions from function arguments, it needs to check if each dimension is dimensionless and convert it to `Dimension(1)` if so. The change is only a few lines and the logic is clear once you understand that mathematical functions like exp() should preserve dimensionlessness. An experienced engineer familiar with the codebase could identify the Function branch in the method, understand the issue with dimension handling, and implement the fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for a coding benchmark. It has a clear reproduction case, the problem is well-defined, and the solution requires understanding both the physics/units domain and the specific implementation details of dimension tracking. The test case provided in the patch would effectively validate whether a solution works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-24443": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The description provides a clear code example that demonstrates the problem, points to the specific file and lines where the issue occurs (homomorphisms.py lines 336-337), and explains the root cause: \"When `r[i]` is an inverted generator, the `in gens` test fails.\" The author also mentions that \"the whole thing can be greatly simplified,\" giving a hint about the solution direction. However, the issue doesn't explicitly state what the expected behavior should be or provide a detailed specification of how the fix should work. An experienced engineer would need to understand the mathematical concepts of group homomorphisms and examine the existing code to determine the correct implementation approach. The provided code example and error location give enough context to understand and solve the problem, though some domain knowledge and code analysis would be required.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve for several reasons: (1) It involves understanding mathematical concepts of group homomorphisms and permutation groups, which requires domain expertise in combinatorics; (2) The engineer needs to analyze the existing `_image()` function in the homomorphisms.py file to understand how it currently works and why it fails with inverted generators; (3) The solution involves substantially rewriting the `_image()` function and parts of `_check_homomorphism()` function, as shown in the gold patch which replaces a complex 30+ line implementation with a much simpler 4-line approach; (4) The engineer needs to understand the relationship between generator symbols, array forms, and the mapping between domain and codomain; (5) Testing and validation would be needed to ensure the new implementation handles all cases correctly. While not extremely esoteric, this requires significant analysis and refactoring of core logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem has a clear reproducible test case, points to the specific problematic code, and the solution can be validated with the provided test. The mathematical domain knowledge required (group theory/combinatorics) is reasonable for a coding benchmark as it tests both algorithmic thinking and ability to work with specialized mathematical libraries.",
            "q2_5_confidence": 4
        }
    }
]