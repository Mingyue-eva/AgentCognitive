[
    {
        "astropy__astropy-13033": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified. It clearly describes the problem: when a TimeSeries object has additional required columns beyond 'time', removing a required column produces a misleading error message. The expected behavior is clearly stated (an exception that informs users that required columns are missing), and the actual confusing behavior is demonstrated with a specific example. However, there are some minor gaps - the issue doesn't provide a minimal reproducible code example showing exactly how to trigger the problem, and doesn't specify exactly what the improved error message should look like. But these gaps can be reasonably filled in by an experienced engineer examining the codebase and understanding the intent.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly localized to error message formatting in the TimeSeries validation logic. The solution requires: (1) understanding how the current error message is constructed, (2) modifying the string formatting to properly handle lists of required columns vs single columns, and (3) ensuring the error message shows both what was expected and what was actually found. The patch shows this involves adding a helper function for formatting and updating the error message template - a straightforward change requiring some thought about edge cases (single vs multiple columns) but not complex algorithmic work.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements and a straightforward solution path. The issue has good reproducibility potential and tests can verify the fix works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-13977": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some blanks to fill in. The author clearly describes the problem: when trying to implement a duck type of astropy.units.Quantity, they encounter a ValueError instead of getting NotImplemented returned from __array_ufunc__(), which would allow for proper fallback to reflected operators like __radd__. They provide a specific error traceback showing the problem occurs in astropy/units/quantity.py line 617 and astropy/units/core.py line 1042 in the _condition_arg function. The core request is clear: change Quantity.__array_ufunc__() to return NotImplemented instead of raising ValueError when inputs are incompatible, which aligns with numpy documentation. However, some details need to be inferred, such as exactly which specific error conditions should trigger NotImplemented vs continued exception raising, and how to distinguish between \"incompatible inputs that should return NotImplemented\" vs \"genuine errors that should still raise exceptions\".",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty level task. Looking at the gold patch, the solution requires: (1) Understanding the numpy __array_ufunc__ protocol and when NotImplemented should be returned, (2) Wrapping the existing logic in a try-catch block to catch TypeError and ValueError exceptions, (3) Adding logic to determine whether to return NotImplemented or re-raise the exception based on whether any inputs/outputs have custom __array_ufunc__ implementations, (4) Writing comprehensive tests with multiple duck typing test classes. The patch shows ~40 lines of changes to the main function plus ~120 lines of new tests. This requires understanding both the astropy codebase and numpy protocols, plus careful consideration of edge cases, making it a substantial but not extremely complex change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-motivated, the solution approach is clear from the numpy documentation, and the provided error traceback gives good guidance on where to make changes. The issue involves important protocol compliance with numpy's __array_ufunc__ behavior, making it a legitimate and valuable coding challenge.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14096": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified. It clearly describes the problem: when a subclassed SkyCoord has a custom property that tries to access a non-existent attribute, the error message incorrectly identifies the property as missing rather than the actual missing attribute. The issue provides a concrete example with the expected error traceback, showing that 'prop' is reported as missing when 'random_attr' should be reported as missing. However, there are some blanks to fill in - the developer would need to understand the codebase structure and locate the relevant __getattr__ method in sky_coordinate.py to implement a fix. The solution approach isn't explicitly stated, requiring some investigation into how Python's attribute access mechanism works.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-defined and the solution is relatively straightforward once you understand Python's attribute access mechanism. The fix involves changing just a few lines in the __getattr__ method of SkyCoord to call __getattribute__ instead of raising a custom AttributeError. An experienced developer would need to: (1) locate the __getattr__ method in sky_coordinate.py, (2) understand that the current implementation masks the original AttributeError, and (3) replace the custom error with a call to __getattribute__ which preserves the original error context. The solution is elegant and requires minimal code changes - just replacing 3 lines with 1 line.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark problem as it tests understanding of Python's attribute access mechanism and error handling in object-oriented programming. The issue is realistic, the solution is clean, and the test case clearly verifies the fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14182": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the RestructuredText (RST) output format in the astropy library does not support the `header_rows` parameter, which causes a TypeError when attempting to use it. The issue provides a concrete example showing the exact error message and traceback, making it clear what functionality is missing. The desired behavior is evident - the user wants to be able to specify custom header rows when writing tables in RST format, similar to other table formats. The error message specifically shows that `RST.__init__()` doesn't accept the `header_rows` keyword argument, which pinpoints exactly what needs to be implemented.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because it requires understanding how the astropy table writing system works, particularly the RST format implementation and how header_rows are handled in other formats. Looking at the patches, the solution involves: (1) modifying the RST.__init__() method to accept header_rows parameter, (2) updating the write() method to handle variable header row counts, (3) adding a read() method that adjusts start_line based on header rows, and (4) removing a hardcoded start_line value. While not extremely complex, it requires understanding the existing codebase structure, the FixedWidth base class, and how header rows affect both reading and writing operations. The solution touches multiple methods and requires careful consideration of the table format structure.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined feature request with clear requirements and a straightforward implementation approach. The issue provides good context about why this feature would be useful (autogenerated documentation), and the error message clearly indicates what needs to be fixed. The solution is testable and the test case provided in the patch demonstrates the expected functionality well.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14309": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clear: there's an IndexError occurring in the `is_fits` function in `astropy/io/fits/connect.py` at line 72 when `args[0]` doesn't exist (tuple index out of range). The error traceback shows the exact location and the issue description references a specific commit that likely caused the regression. The problem is that when `filepath` is a string without a FITS extension, the function now tries to execute `isinstance(args[0], ...)` but `args` might be empty. However, there are some details left to interpretation about the exact conditions that trigger this and what the expected behavior should be in edge cases.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is straightforward - there's an IndexError when accessing `args[0]` when the args tuple might be empty. Looking at the gold patch, the solution is simple: change the logic to return the result of the file extension check directly instead of falling through to the `isinstance` check when the filepath doesn't have a FITS extension. The fix involves modifying just a few lines in one function (`is_fits`) to handle the case where args might be empty. An experienced engineer would need some time to understand the codebase context and the intended behavior of the `is_fits` function, but the actual code change is minimal.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it represents a realistic debugging scenario where a recent change introduced a regression. The issue has a clear error message, traceback, and the solution requires understanding the logic flow in the function. The test case is also straightforward and tests the specific edge case that was causing the problem.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-14365": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly states that the ascii.qdp Table format incorrectly assumes QDP commands must be uppercase (like \"READ SERR 1 2\") when QDP itself is case-insensitive and should accept lowercase commands (like \"read serr 1 2\"). The problem is demonstrated with a specific error traceback showing \"ValueError: Unrecognized QDP line: read serr 1 2\". The expected behavior is clearly described - the parser should accept both uppercase and lowercase commands since QDP files are often created by hand and case sensitivity is unnecessarily restrictive. The solution requirement is unambiguous: make the QDP parser case-insensitive.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution involves making two small changes: (1) adding re.IGNORECASE flag to the regex compilation in _line_type() function, and (2) changing the \"NO\" string comparison to use v.upper() == \"NO\" for case-insensitive matching. These are straightforward changes that require understanding regex flags and basic string comparison methods. An experienced engineer familiar with the codebase could identify the relevant parsing logic in the qdp.py file and implement these changes relatively quickly. The changes are localized and don't require extensive refactoring or complex logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for evaluating coding ability. The issue is clearly defined with a specific error message and expected behavior. The solution requires understanding regex patterns, case sensitivity handling, and careful consideration of where case-insensitive comparisons are needed throughout the parsing logic. The test changes show that comprehensive testing is needed to verify the fix works for both lowercase and uppercase commands. This tests practical debugging skills, regex knowledge, and attention to detail in ensuring all relevant code paths handle case insensitivity correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14508": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: `io.fits.Card._format_float()` produces unnecessarily long string representations of floats (e.g., `0.009124999999999999` instead of `0.009125`), which causes comment truncation in FITS cards. The issue provides specific details including the exact function and line numbers causing the problem, a concrete example of the problematic behavior, and even suggests a potential solution approach (using `str(value)` first, then falling back to formatting only if the string exceeds 20 characters). The expected behavior is clearly stated: being able to create any valid FITS Card via `io.fits.Card`. An experienced engineer would have enough information to understand and solve this issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue pinpoints the exact problematic function (`_format_float()`) and even suggests the solution approach. Looking at the actual patch, the fix involves replacing the existing formatting logic with a simpler approach: use `str(value)` first, then only apply truncation if needed. The core change is straightforward - replacing complex formatting logic with simpler string conversion. While some thought is needed to handle edge cases (scientific notation, truncation logic), the overall change is relatively small and localized to a single function. An experienced engineer familiar with the codebase could implement this in under an hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with a clear solution. The issue provides good technical detail, the fix is localized to a specific function, and the test cases demonstrate the expected behavior clearly. This would make for a good benchmark sample as it tests understanding of string formatting, precision issues with floating point numbers, and the ability to write cleaner, more efficient code while maintaining backward compatibility.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-14995": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: mask propagation fails in v5.3 when one operand lacks a mask, causing a TypeError when using `handle_mask=np.bitwise_or`. The error message \"TypeError: unsupported operand type(s) for |: 'int' and 'NoneType'\" clearly indicates the problem. The expected behavior is also stated - when one operand has no mask, the existing mask should be copied to the output. However, the issue lacks a minimal reproducible example showing exactly how to trigger this error, which would help understand the specific use case and method calls involved. Despite this gap, an experienced engineer could reasonably infer that this involves NDDataRef arithmetic operations and work from the error description and expected behavior.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively simple fix requiring 15 minutes to 1 hour. The issue is in the `_arithmetic_mask` method in `astropy/nddata/mixins/ndarithmetic.py`. The bug is a simple logic error on line 523: `elif operand is None:` should be `elif operand.mask is None:`. The method was incorrectly checking if the entire operand is None rather than if the operand's mask is None. This is a small, focused change that requires understanding the conditional logic flow but doesn't involve complex algorithmic changes or extensive refactoring. An experienced engineer familiar with the codebase could identify and fix this relatively quickly once they understand the mask propagation logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within a single method, the fix is straightforward, and the test cases provided in the patch clearly demonstrate the expected behavior for various combinations of masked and unmasked operands. The issue would work well for evaluating debugging and code comprehension skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "astropy__astropy-7336": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the `units.quantity_input` decorator fails when used with constructors that have a type hint return value of `-> None`. The issue provides a complete error traceback showing that `'NoneType' object has no attribute 'to'` when the decorator tries to call `return_.to(wrapped_signature.return_annotation)` on a None value. The issue includes specific version information, a clear workaround (omitting the return type hint), and even suggests a potential fix (explicitly checking for None return values). The problem is unambiguous - the decorator should handle the case where the return annotation is None and not attempt unit conversion on it.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue is well-understood from the description and traceback. Looking at the gold patch, the solution is a simple one-line change in `astropy/units/decorators.py` line 223, changing the condition from checking only `inspect.Signature.empty` to also excluding `None`. The fix involves adding `None` to the condition check: `if wrapped_signature.return_annotation not in (inspect.Signature.empty, None):`. This requires minimal understanding of the codebase - just locating the decorator logic and understanding that None return annotations shouldn't be processed for unit conversion. The test changes are more extensive but straightforward, mainly removing the dynamic code execution wrapper and converting to direct test functions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the test coverage demonstrates the fix works correctly. This is an excellent benchmark sample as it tests understanding of Python decorators, type annotations, and defensive programming practices while remaining accessible to experienced engineers.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7606": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear error message with a complete stack trace showing that comparing an UnrecognizedUnit with None raises a TypeError instead of returning False as expected. The traceback shows exactly where the error occurs (line 1701 in core.py) and what the expected behavior should be (the comment \"Should be False\" makes the desired outcome explicit). The error occurs because the __eq__ method tries to convert None to a Unit, which fails. This gives a developer all the information needed to understand the problem and implement a fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified in the stack trace - the __eq__ method in UnrecognizedUnit class doesn't handle None properly. Looking at the gold patch, the solution involves adding try/except blocks around Unit conversion calls and returning NotImplemented instead of continuing with failed conversions. An experienced developer would need to: 1) understand that equality comparisons should return NotImplemented when they can't handle the comparison (Python convention), 2) identify that both the base Unit class and UnrecognizedUnit class need similar fixes, and 3) add appropriate exception handling. The changes are small but require understanding Python's comparison protocol.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is straightforward once you understand Python's comparison conventions, and the test cases clearly verify the expected behavior. This is a good benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "astropy__astropy-7671": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some blanks to fill in. The problem is clearly described: the minversion function fails when comparing version strings like '1.14dev' due to a TypeError in LooseVersion comparison. The error traceback is provided, showing the exact failure point in introspection.py line 146-148. The root cause is identified as a known bug in LooseVersion (https://bugs.python.org/issue30272). However, while the problem is clear, the specific solution approach isn't explicitly stated - an engineer would need to determine how to handle version strings with non-numeric components like 'dev' to make them compatible with LooseVersion comparisons. The reference to PR #7647 suggests this is a regression, but the specific change that caused it isn't detailed in the issue text.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is well-defined (LooseVersion can't compare versions with mixed int/string components), the solution requires: (1) Understanding the minversion function and its role in version comparison, (2) Researching how to handle version strings with development tags like 'dev', 'rc', etc., (3) Implementing a preprocessing solution that extracts only the numeric portion of version strings before passing to LooseVersion, (4) Understanding PEP 440 version specification format to create an appropriate regex. The actual code change is relatively small (adding regex preprocessing), but requires domain knowledge about version numbering standards and careful regex crafting to avoid breaking existing functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within a single function, the error is reproducible, and the solution approach (preprocessing version strings) is reasonable. The test changes confirm the expected behavior and provide good validation criteria.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-10554": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear error traceback showing \"ORDER BY position 4 is not in select list\" and mentions it's related to \"Union queryset with ordering breaks on ordering with derived querysets\". While the title gives context about union querysets and ordering, the actual code snippet that reproduces the issue is minimal - just showing that evaluating `qs` causes the error while `[dim.id for dim in qs]` works. There are some blanks to fill in about the exact queryset construction that leads to this issue, but the core problem (SQL ORDER BY referencing a column not in SELECT) is clear from the traceback, and there's a sensible interpretation that the solution should handle cases where ORDER BY columns aren't included in the SELECT clause for union queries.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Django's ORM query compilation, specifically how ORDER BY clauses are handled in union queries in the django/db/models/sql/compiler.py file. (2) The solution involves modifying the get_order_by method to detect when an ORDER BY column isn't in the SELECT list and automatically add it, plus adding a new method add_select_col to the Query class. (3) While the changes are relatively small (about 15-20 lines), they require deep understanding of Django's SQL compilation process, query structure, and the interaction between SELECT and ORDER BY in union scenarios. (4) The engineer needs to understand the SQL standard requirement that ORDER BY columns must be in the SELECT list for UNION queries and implement a solution that automatically adds missing columns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for a coding benchmark. The problem is well-defined (SQL error with clear cause), the solution pattern is established in Django (automatic query modification), and the test cases in the patch provide good coverage of the expected behavior. An experienced engineer should be able to work from the error message and codebase to arrive at a similar solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11087": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps that require interpretation. The core problem is clearly described: a UnicodeDecodeError occurs during Django's .delete() operation when upgrading from Python 2.7 to 3.6. The issue identifies two specific problems: (1) mysqlclient-python behavior differences between Python versions, and (2) Django fetching unnecessary fields during deletion that aren't needed for the operation. The second issue is the focus for optimization - the title \"Optimize .delete() to use only required fields\" clearly indicates what needs to be solved. The stack trace, SQL queries, and specific mention that text_log_error.line field \"is not actually needed for the .delete()\" provides sufficient context. However, some implementation details are left to the engineer to figure out, such as exactly which fields are \"required\" and how to determine this programmatically.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix that requires substantial understanding of Django's ORM deletion mechanism. The solution involves modifying the deletion collector in django/db/models/deletion.py to optimize field selection by: (1) Adding a helper method to check for signal listeners, (2) Implementing logic to determine which fields are actually referenced/required during deletion, and (3) Using .only() to limit field selection when safe to do so. The engineer needs to understand Django's foreign key relationships, deletion cascades, signal system, and queryset optimization. The patch shows this requires careful consideration of edge cases like select_related queries and signal listeners, making it a non-trivial change that touches core ORM functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-constructed issue that would make a good benchmark sample. The problem is clearly technical, requires deep Django ORM knowledge, and has a concrete optimization goal. The solution demonstrates sophisticated understanding of Django internals and the test coverage validates the fix properly. The issue represents a realistic performance optimization scenario that experienced Django developers might encounter.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11265": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some domain knowledge to fully understand. The description clearly identifies that using `exclude()` on a queryset with annotated FilteredRelation causes a FieldError, and provides a detailed stack trace showing the error occurs in the `split_exclude()` function. The author correctly identifies that \"A new query is created without all extra datas from the original query.\" However, an engineer would need to understand Django ORM concepts like FilteredRelation, annotations, and the Q object to fully grasp what should happen versus what's happening. The specific annotation name 'book_alice' and the error message \"Cannot resolve keyword 'book_alice' into field\" make it clear that the annotation is not being properly carried over to the new query context created during exclude operations.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's complex ORM internals, specifically how FilteredRelation annotations are handled during query exclusion operations. The engineer needs to: 1) Understand the Django query building system and how split_exclude() works, 2) Identify that _filtered_relations are not being copied to the new query object, 3) Understand the trimming logic and why INNER JOINs from filtered relations should not be trimmed. The actual code changes are relatively small (adding one line and modifying a condition), but finding the right solution requires deep understanding of Django's query compilation process and the interaction between annotations, joins, and query exclusion logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained Django ORM bug with a clear reproduction case and clean solution. The test case provided is simple and directly verifies the fix. An experienced engineer familiar with Django internals should be able to solve this within the estimated timeframe. The issue demonstrates good software engineering practices with detailed error reporting and proper identification of the problematic function.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11299": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that require interpretation. The core problem is clearly described: Django is incorrectly including fully qualified field names in CheckConstraints when there are combined OR and AND clauses, causing migration failures. The specific error message is provided: \"malformed database schema (app_testconstraint) - no such column: new__app_testconstraint.field_1\". The technical detail about AND clauses using Col while OR clauses use SimpleCol provides a helpful clue about the root cause. However, an experienced engineer would need to fill in some details about the exact structure of constraints that trigger this issue and determine the precise fix needed. The issue provides enough context to understand what's broken and why, making it solvable with reasonable interpretation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour complexity issue. The solution involves understanding Django's query building system, specifically how CheckConstraints are processed with mixed OR/AND clauses. The engineer needs to: 1) Understand the difference between Col and SimpleCol usage in different clause types, 2) Trace through the query building logic in django/db/models/sql/query.py, 3) Identify where the simple_col parameter needs to be passed through recursively in the _add_q method. The actual fix is small (passing one additional parameter), but requires substantial understanding of Django's ORM internals and query processing. The migration system and constraint handling adds complexity that would require research into the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is specific to Django's ORM implementation, the error is reproducible, and the fix is testable. The issue description provides enough technical detail to guide an experienced engineer toward the solution, even though some investigation would be required to understand the exact code paths involved.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11532": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly describes the problem: when the computer hostname contains non-ASCII characters (like \"\u6b63\u5b97\"), Django's email functionality crashes when using non-Unicode encodings like iso-8859-1. The issue provides specific details including: (1) The exact test that fails (tests/mail/tests.py#L368), (2) The specific location where the problem occurs (django/core/mail/message.py#L260 in the Message-ID header generation), (3) A complete traceback showing the encoding error, (4) The root cause (Python trying to convert Unicode domain names to iso-8859-1 encoding), and (5) A suggested fix (convert domain names to punycode before using). The issue references specific files and line numbers, making it easy to locate the problematic code.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding Unicode/encoding concepts and punycode conversion, (2) The solution involves creating a new utility function (punycode) and modifying multiple files (message.py, utils.py, validators.py, encoding.py, html.py), (3) It requires understanding Django's email system and how domain names are handled in various contexts, (4) The engineer needs to identify all places where domain encoding occurs and ensure consistent punycode conversion, and (5) Writing appropriate tests to verify the fix works correctly. While the core concept is straightforward, implementing it properly across the entire Django codebase requires careful analysis and testing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined bug with a clear reproduction case, specific error traceback, and a reasonable scope for a coding assessment. The solution requires understanding of encoding concepts which is appropriate for evaluating an engineer's technical skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11551": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. The author provides a detailed explanation of the problem: Django admin validation is incorrectly raising admin.E108 errors for fields accessible only via instance (like PositionField from django-positions library) after upgrading from Django 2.0.7 to 2.2.1. The issue includes: 1) The specific commit that caused the regression (47016adbf54b54143d4cf052eeb29fc72d27e6b1), 2) A comprehensive truth table showing the behavior before/after the change and what the correct behavior should be, 3) The exact function affected (_check_list_display_item), 4) A complete proposed solution with corrected code, 5) Clear explanation of why hasattr(model, item) returning False shouldn't prevent attempting model._meta.get_field(item). The issue provides enough detail that an engineer could implement the fix without ambiguity.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix because: 1) The issue provides the exact location of the bug (django/contrib/admin/checks.py, _check_list_display_item function), 2) The author includes a complete proposed solution showing exactly what needs to be changed, 3) The fix involves restructuring the conditional logic in one function - removing the hasattr(model, item) check and reorganizing try/except blocks, 4) While the logic requires careful thought to understand the different cases (as shown in the truth table), the actual code changes are relatively straightforward once the problem is understood, 5) An experienced engineer familiar with Django would be able to implement this fix within an hour after understanding the validation logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample because it tests understanding of Django's admin validation system, conditional logic restructuring, and regression fix implementation. The issue is clearly documented with a specific reproduction case and comprehensive analysis.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-11734": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague - it only provides an error message without showing the actual code that produces this error, the expected behavior, or a reproducible example. An experienced engineer would need to understand Django's ORM internals and deduce from the error message that this involves OuterRef handling in exclude() operations. While the error message gives a clue about the problem domain (subquery/outer query references), there's significant ambiguity about what specific scenario triggers this and what the correct behavior should be. The patches show this involves complex query processing logic across multiple files, suggesting the issue is more nuanced than the brief description indicates.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) It involves understanding Django's complex ORM query processing, specifically how OuterRef objects are handled in exclude operations and related lookups, (2) The fix spans multiple files (fields/__init__.py, related_lookups.py, sql/query.py) indicating it's not a simple localized change, (3) The solution requires understanding the interaction between query splitting, field preparation, and expression resolution - this is fairly deep ORM knowledge, (4) The logic changes involve subtle distinctions in how different expression types (OuterRef vs F objects) are processed, requiring careful analysis to avoid breaking other functionality. While not the most complex Django issue possible, it's definitely beyond a quick fix and requires substantial domain knowledge.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The test patch provides a clear test case that demonstrates the expected functionality, which would help validate any solution attempt. While the issue description is brief, the error message and domain (Django ORM) provide enough context for an experienced engineer to investigate and develop a solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11740": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clear: when changing a UUIDField to a ForeignKey, Django's migration system fails to create necessary dependencies, causing a runtime error \"Related model 'testapp2.App2' cannot be resolved.\" The user provides specific context (Django 2.2, PostgreSQL) and describes the expected vs actual behavior. However, some details are missing - we don't see the actual model definitions, the exact migration that was generated, or a minimal reproducible example. An experienced engineer would need to infer the specific technical requirements from the general description and error message, but the high-level problem is clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the solution, it requires understanding Django's migration autodetector system, specifically how field alterations are handled and how dependencies are tracked. The fix involves modifying the `generate_altered_fields` method to properly detect when a field change introduces foreign key dependencies and add those to the migration. This requires: (1) Understanding Django's migration internals and dependency system, (2) Identifying where in the autodetector logic the dependency detection is missing, (3) Implementing the fix to call `_get_dependencies_for_foreign_key` for altered fields, (4) Writing appropriate tests. The code change itself is relatively small but requires deep understanding of Django's migration system architecture.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained Django framework bug with a clear fix. The issue demonstrates a specific gap in Django's migration dependency detection logic, and the solution appropriately addresses it by ensuring that field alterations that introduce foreign key relationships properly track dependencies. The test case validates the fix comprehensively.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11815": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is fairly well-specified but requires some domain knowledge to fully understand. The problem is clearly described: when using an Enum object as a default value for a CharField, Django migrations serialize the enum's value instead of its name, causing issues with translated enum values. The issue provides a concrete error example \"ValueError: 'Good' is not a valid Status\" and suggests the solution should use the enum's name rather than its value. However, someone unfamiliar with Django migrations, enum serialization, or internationalization might need to do some research to understand the full context. The core problem and desired outcome are clear enough for an experienced Django developer to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding Django's migration serialization system, specifically the EnumSerializer class. The engineer needs to: 1) Locate the EnumSerializer in django/db/migrations/serializer.py, 2) Understand how it currently serializes enum values using .value, 3) Modify it to use .name instead with the correct syntax (Enum['name']), and 4) Update all related tests. The solution itself is conceptually simple but requires navigating Django's codebase, understanding the serialization framework, and ensuring the change works across different enum types (TextEnum, BinaryEnum, IntEnum, etc.). The patch shows this touches multiple test cases and requires careful attention to the serialization format.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with a clear technical solution. The issue description provides enough context about the problem (translation causing enum value mismatches) and the desired fix (use enum names instead of values). The gold patch confirms this is a focused change to the EnumSerializer class with comprehensive test coverage.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-11964": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear from the test failure: when using TextChoices/IntegerChoices with Django model fields, calling str() on the field value returns the enum representation (e.g., 'MyChoice.FIRST_CHOICE') instead of the actual value ('first'). The test shows the expected behavior should return the underlying value. However, there are some gaps: the issue doesn't explicitly show the model definition or enum class structure, and the explanation about \"freshly created vs retrieved instances\" could be clearer. An experienced developer can reasonably infer that the solution involves overriding the __str__ method in the Choices class to return the enum value instead of the default enum representation.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The solution is straightforward once you understand the problem: override the __str__ method in the Choices class to return str(self.value) instead of the default enum string representation. The fix involves adding just a few lines of code to django/db/models/enums.py. An experienced developer familiar with Python enums would quickly recognize this pattern. The main time would be spent understanding the Django codebase structure and locating the correct file, but the actual implementation is trivial.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with using this sample. The problem is well-contained, the solution is clean and focused, and the test clearly validates the expected behavior. This is a good example of a focused bug fix that tests a specific aspect of Django's enum functionality.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12155": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem (docutils error when first line of docstring is not empty), provides the specific error message, pinpoints the exact problematic code in the trim_docstring function, explains why the current implementation fails (first line indentation is 0), and even suggests a concrete solution (skip the first line in the indentation calculation). The issue includes the specific line of code causing the problem and the proposed fix, making it clear what needs to be changed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue clearly identifies the problematic line of code and provides the exact solution. However, looking at the actual patch, the solution was more comprehensive - instead of just modifying the trim_docstring function as suggested, the developers replaced it entirely with Python's built-in cleandoc function from the inspect module. This requires understanding that cleandoc already handles the docstring trimming correctly and making the decision to refactor rather than patch. The change spans multiple files but is conceptually straightforward once you understand that cleandoc is the better solution.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution approach is clear, and the test case verifies the fix works correctly. An engineer could reasonably arrive at either the suggested fix (modifying the indentation calculation) or the actual solution (replacing with cleandoc), both of which would resolve the issue.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-12262": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem with Django's custom template tags: when using keyword-only arguments with defaults, the system incorrectly raises TemplateSyntaxError for unexpected keyword arguments, and provides wrong error messages for duplicate keyword arguments. The description provides concrete examples of the error messages being raised. However, there are some gaps - it doesn't specify exactly what \"simple tags\" are in Django's context, doesn't provide a minimal code example to reproduce the issue, and uses vague language like \"supplying any other variable.\" While an experienced Django developer could reasonably interpret this as referring to template tag registration and the parse_bits function, someone unfamiliar with Django's template system might need to do additional research to understand the context fully.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a one-line fix in django/template/library.py that changes 'unhandled_kwargs' to 'kwonly' in a conditional check within the parse_bits function. The fix is quite straightforward - it's correcting a variable reference to check against keyword-only parameters instead of unhandled kwargs. An experienced engineer familiar with Django would need to: 1) Understand how Django's template tag parsing works, 2) Locate the parse_bits function, 3) Understand the logic flow for handling keyword arguments, and 4) Identify that the wrong variable is being checked. This would likely take 15 minutes to 1 hour, including time to understand the template tag system, write a test case to reproduce the issue, and implement the fix.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is technically sound, the fix is clean and targeted, and the test cases adequately cover the scenarios described in the issue. This would be a good benchmark sample for evaluating understanding of Django's template system and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12325": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: Django is incorrectly requiring parent_link=True on a OneToOneField that shouldn't need it when there are multiple OneToOneField references to the same model. The issue provides a concrete example showing the error message \"Add parent_link=True to appname.Picking.origin\" and demonstrates that changing field order fixes it. However, there are some ambiguities: (1) The exact conditions that trigger this bug aren't fully specified - it's unclear if this affects all cases with multiple OneToOneFields to the same parent model or only specific scenarios, (2) The expected behavior isn't explicitly stated - while the workaround shows what should work, it doesn't clearly articulate the principle that Django should follow, (3) The mention of \"shouldn't it look from top to bottom like it does with managers and other things\" suggests there may be broader consistency issues but doesn't elaborate. Despite these gaps, an experienced Django developer familiar with model inheritance and OneToOneField behavior could reasonably infer that the issue is about Django's parent link detection logic being confused by multiple references and incorrectly flagging non-parent-link fields.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix based on several factors: (1) The issue requires understanding Django's model inheritance system, particularly how parent_link detection works in the metaclass and model options, (2) Looking at the patch, the fix involves modifying logic in django/db/models/base.py and django/db/models/options.py - core Django ORM files that require careful consideration, (3) The solution requires identifying that the parent link detection logic in __new__ method needs to filter for actual parent links (field.remote_field.parent_link), and removing the incorrect validation logic in options.py, (4) This isn't a trivial one-line fix but requires understanding the relationship between model metaclass creation, field processing, and validation, (5) The developer would need to trace through Django's model creation process to understand why the order dependency exists and how to fix it properly, (6) Writing appropriate tests to cover the edge cases would also take some time. While not extremely esoteric, this requires solid Django internals knowledge and careful analysis of the model creation flow.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. This is a legitimate Django ORM bug with a clear reproduction case and well-defined fix. The issue tests core Django functionality (model inheritance and OneToOneField behavior) which is appropriate for a coding benchmark. The solution requires understanding Django internals without being overly esoteric, making it a good test of Django framework knowledge and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12663": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly states the problem: \"Using SimpleLazyObject with a nested subquery annotation fails\" and provides a detailed stack trace showing the error occurs when Django tries to convert a SimpleLazyObject to an int in the get_prep_value method. The issue mentions this is a regression from a specific commit (35431298226165986ad07e91f9d3aca721ff38ec). However, the issue lacks a minimal reproducible example showing the exact queryset construction that triggers this error. While the stack trace provides valuable debugging information pointing to the specific failure in django/db/models/fields/__init__.py line 968, an experienced engineer would need to infer the exact scenario from the context and stack trace.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. The issue requires understanding Django's ORM internals, specifically how SimpleLazyObject interacts with query field processing. The stack trace points to the exact location of failure, but the engineer needs to: 1) Understand how SimpleLazyObject works and why it previously worked, 2) Trace through the query building process to see where the type conversion fails, 3) Find the appropriate place to handle SimpleLazyObject evaluation or add compatibility. Looking at the provided solution, it's a focused change in the output_field property of the Query class, involving understanding the difference between select.field and select.target attributes. The fix itself is small (2-3 lines), but requires domain expertise in Django ORM internals to identify the root cause and appropriate solution location.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a solid issue for benchmarking coding ability. It tests understanding of Django internals, debugging skills using stack traces, and knowledge of lazy object evaluation patterns. The issue is specific to Django ORM behavior and would be appropriate for evaluating mid-level to senior Django developers. The provided test case also gives good insight into the expected behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12708": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. It clearly describes: (1) the crash occurs in Django 1.11.10, (2) the specific error location (django/db/backends/base/schema.py, line 378, in _delete_composed_index()), (3) the root cause (ValueError: Found wrong number (2) of constraints), and (4) the context (refactoring to use Django 1.11's Options.indexes feature). The issue explains that the problem happens when there's both a unique_together and index_together on the same fields, and the migration system finds 2 constraints instead of the expected 1. However, an experienced engineer would need to investigate the codebase to understand exactly how _delete_composed_index works and why it's finding multiple constraints. The two suggested solution directions (making index_together deletion coherent with unique_together, and avoiding index re-creation during declaration moves) provide helpful guidance but leave implementation details to be figured out.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue because: (1) The engineer needs to understand Django's migration system and schema editing internals, particularly how _delete_composed_index works and how it distinguishes between different constraint types. (2) Looking at the gold patch, the solution involves modifying the _delete_composed_index call in alter_index_together to include {'index': True, 'unique': False} instead of just {'index': True}, which is a small but non-trivial change requiring understanding of the constraint filtering logic. (3) The issue requires debugging why the function finds 2 constraints instead of 1, and understanding how to properly filter for only index constraints when unique constraints exist on the same fields. (4) While the code change is small (adding 'unique': False to the filter), arriving at this solution requires investigating the database introspection code and understanding how constraints are represented and filtered. This goes beyond a simple fix and requires meaningful investigation of the Django ORM's schema management.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is well-defined with a clear error case, the solution space is constrained to Django's migration system, and the test coverage shows the expected behavior clearly. This would make a good benchmark problem for evaluating understanding of ORM internals and debugging skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12754": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific scenario where creating a model subclass and moving a field onto it in the same migration causes a FieldError during migration execution, even though makemigrations works fine. The error message is clear: \"Local field 'title' in class 'Book' clashes with field of the same name from base class 'Readable'\". However, there are some blanks to fill in - the issue doesn't provide the exact model definitions or migration steps that reproduce the problem. An experienced engineer would need to infer the likely scenario: a base model 'Readable' with a 'title' field is being refactored so that the 'title' field is removed from 'Readable' and a new subclass 'Book' is created with its own 'title' field. The core problem is clear - there's a dependency ordering issue in migrations where the field removal and model creation need to be properly sequenced.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: (1) It requires understanding Django's migration system and autodetector logic, which is complex, (2) The engineer needs to trace through the migration generation code to understand why the dependency ordering is incorrect, (3) The solution involves modifying the autodetector.py file to add proper dependency tracking between field removal and model creation operations, (4) The fix requires understanding model inheritance relationships and how Django tracks field dependencies across model hierarchies, (5) While not requiring >100 lines of code changes, it's a non-trivial change to core Django functionality that requires careful consideration of edge cases and potential side effects.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for the benchmark. The problem is well-defined within Django's migration system, the error is reproducible, and the solution involves understanding and modifying core Django code. The test case clearly validates the expected behavior (RemoveField operation should come before CreateModel operation). An experienced engineer familiar with Django would be able to work on this issue effectively.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-12858": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear error message (models.E015) and the specific field path that's causing the problem ('supply__product__parent__isnull'). The user explains the data model structure showing Stock.supply -> Supply.product -> Product.parent relationships, and mentions this worked until a specific PR (#29408) was implemented. While the exact solution approach isn't specified, there's enough information to understand that the issue is with Django's model ordering validation incorrectly flagging 'isnull' as a nonexistent field when it should be recognized as a valid lookup. The error context and field relationships provide sufficient detail for an experienced Django developer to understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively straightforward fix that adds a condition to check for lookups in addition to transforms in the _check_ordering method. The change is minimal (adding `and fld.get_lookup(part) is None` to an existing condition) and localized to a single location in django/db/models/base.py. An experienced Django developer would need to understand the validation logic in the _check_ordering method and recognize that lookups like 'isnull' should be treated as valid alongside transforms. The debugging process would involve understanding the error path and the distinction between field transforms and lookups in Django's ORM, which should take 15 minutes to 1 hour for someone familiar with Django internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained Django ORM issue with a clear error message and straightforward fix. The issue demonstrates good software engineering practices with specific error reproduction and model relationship explanation. The solution requires understanding Django's field lookup system but doesn't involve complex architectural changes or extensive research.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13012": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some gaps. It clearly describes the problem: constant expressions in ExpressionWrapper objects are incorrectly placed in GROUP BY clauses, causing PostgreSQL errors. The specific error message is provided, giving concrete evidence of the issue. However, the description lacks some important details: (1) it doesn't provide a minimal reproducible example showing how to trigger this bug, (2) it doesn't explain the expected behavior (that constant expressions should not appear in GROUP BY), and (3) it doesn't mention the ExpressionWrapper class by name in the title or early description, requiring inference from context. An experienced engineer could reasonably interpret that the solution should prevent constant expressions from being included in GROUP BY clauses, but would need to investigate the codebase to understand the mechanism.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The solution is quite straightforward - adding a single method `get_group_by_cols` to the ExpressionWrapper class that delegates to the wrapped expression. The fix is only 3 lines of code. An experienced Django developer would likely recognize that this is a missing method issue fairly quickly by examining how GROUP BY clauses are constructed and noting that ExpressionWrapper lacks the `get_group_by_cols` method that other expression classes have. The main time would be spent understanding the Django ORM expression system and locating the right place to add the method, but this shouldn't take more than an hour for someone familiar with Django's codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined enough to solve, the solution is straightforward, and the test cases clearly verify the expected behavior. This would be a good benchmark sample as it tests understanding of Django's ORM expression system and the ability to identify missing method implementations.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13028": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The reporter provides a clear reproduction case showing that when a Django model has a field named \"filterable\", querying that model raises a NotSupportedError. They show the complete error traceback pointing to django/db/models/sql/query.py line 1131 in the check_filterable method. The reporter demonstrates that renaming the field from \"filterable\" to \"filterable_test\" fixes the issue, indicating a naming conflict. However, there are some details missing: the exact query being performed isn't shown (just \"for p in ProductMetaData.objects.filter(\"), and the relationship between the field name and Django's internal filterable attribute checking mechanism isn't explicitly explained. An experienced engineer could reasonably infer that Django's check_filterable method is incorrectly treating model fields named \"filterable\" as expressions with filterable=False attributes, but would need to investigate the codebase to understand the exact mechanism and develop an appropriate fix.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides a clear error traceback pointing directly to the problematic method (check_filterable in django/db/models/sql/query.py). The solution involves adding a condition to check if the expression has a resolve_expression method before checking the filterable attribute, which distinguishes between actual Django expressions and model instances that happen to have a \"filterable\" field. This is a small, targeted change requiring minimal code modification but some understanding of Django's ORM internals to distinguish between expressions and model instances.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is reproducible, the solution is straightforward, and the test case demonstrates the fix works correctly. The issue represents a real-world Django ORM bug that would be valuable for evaluating debugging and problem-solving skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13112": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified. It clearly shows the error message, provides the relevant code snippets (models.py, settings.py, apps.py), and demonstrates the specific case where Django's makemigrations fails. However, there are some gaps: the issue doesn't explicitly explain that the problem is with case sensitivity in app names, and doesn't provide the exact steps to reproduce the error. An experienced engineer would need to infer that the issue is about Django incorrectly lowercasing the entire app.model reference instead of just the model name. The provided code and error message give enough context to understand and solve the problem, but some detective work is required.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The engineer needs to: 1) Understand Django's migration system and how ForeignKey references work, 2) Trace through Django's codebase to find where the problematic lowercasing occurs (in related.py), 3) Understand the difference between app labels and model names in Django's internal model referencing, 4) Design a solution that handles the dot-separated app.model format correctly. The fix itself is relatively small (5 lines), but requires understanding Django's internals and the specific case handling for string-based model references. The solution involves parsing the model reference string and applying different case transformations to app vs model components.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark issue. It tests understanding of Django's migration system, string parsing, and the subtle distinction between app labels and model names. The issue is specific enough to have a clear solution, but requires meaningful investigation into Django's codebase to understand the root cause and implement the fix correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13128": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and ambiguous. While it mentions \"make temporal subtraction work without ExpressionWrapper\" and provides an error message, it doesn't explain what specific operation should work, what the current behavior is, or what the expected behavior should be. The error message gives a hint about mixed types requiring output_field specification, but without context about what code produces this error or what the desired outcome should be, an engineer would need to do significant investigation. Looking at the test patches, it's clear the issue is about allowing direct subtraction of datetime/date/time fields using F() expressions without wrapping them in ExpressionWrapper with explicit output_field specification, but this intent is not clearly communicated in the issue description itself.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because: (1) The engineer needs to understand Django's expression system, particularly how CombinedExpression, TemporalSubtraction, and DurationExpression work together; (2) The solution involves moving logic from as_sql() to resolve_expression() method, which requires understanding the expression compilation pipeline; (3) The fix touches multiple expression classes and requires careful handling of edge cases like checking for native duration field support; (4) While the core change is relocating existing logic rather than writing entirely new functionality, it requires deep understanding of when expressions are resolved vs compiled and the implications of that timing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate Django ORM enhancement that improves the API by removing the need for explicit ExpressionWrapper usage in temporal operations. The solution is well-contained within the expressions module and the test cases clearly demonstrate the expected behavior. This would be a reasonable coding challenge for someone familiar with Django's internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13297": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: SimpleLazyObject instances in TemplateView kwargs cause database parameter binding errors. The user provides a specific error message \"Error binding parameter 0 - probably unsupported type\" from django/db/backends/sqlite3/operations.py and mentions their workaround of converting to string. However, there are some blanks to fill in: (1) The exact reproduction steps aren't provided - we don't see the specific TemplateView code that triggers this, (2) The mention of \"offer_slug (coming in from kwargs.get)\" suggests a specific use case but doesn't show the full context, (3) The connection between TemplateView.get_context_data() and get_object_or_404 filtering isn't explicitly demonstrated. Despite these gaps, an experienced Django developer would likely understand that this relates to URL kwargs being wrapped in SimpleLazyObject for deprecation warnings, and that these objects don't serialize properly for database queries. The solution direction (converting SimpleLazyObject to proper types) is reasonably inferrable.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve properly. While the surface problem (SimpleLazyObject causing database errors) might seem straightforward, the actual solution requires understanding Django's internal deprecation warning system and the specific implementation in TemplateView. Looking at the gold patch, the fix involves: (1) Understanding that _wrap_url_kwargs_with_deprecation_warning uses SimpleLazyObject incorrectly, (2) Knowing that lazy() with type hints is the proper alternative, (3) Modifying the closure-based warning system while preserving the deprecation behavior. An engineer would need to trace through Django's generic view system, understand how URL kwargs flow through to context data, reproduce the database error, and then find the specific function responsible. The debugging process alone could take 1-2 hours, and arriving at the elegant lazy(access_value, type(value))() solution requires good knowledge of Django's functional utilities. This is not a trivial fix but also not extremely esoteric.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is a legitimate Django framework bug with a clear technical solution. The test demonstrates the issue well by showing that URL parameters can be used for database filtering after the fix. This is suitable for a coding benchmark as it tests understanding of Django internals, debugging skills, and knowledge of Python's functional programming utilities.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13406": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is well-specified with some minor gaps to fill in. It clearly describes the problem: queryset pickling/unpickling breaks when using values()/values_list() with annotate(). The issue provides a detailed error traceback, explains the expected vs actual behavior, and references the relevant Django documentation. However, it doesn't provide a minimal reproducible code example, which would help clarify the exact steps to reproduce. The description gives enough context about the Django ORM behavior and pickling functionality that an experienced engineer could reasonably understand what needs to be fixed - the queryset's internal state isn't being properly restored when recreated from a pickled query that uses values selection.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly scoped to Django's queryset pickling mechanism, specifically around values()/values_list() functionality. The stack trace points directly to the problem area in the ORM. Looking at the actual solution, it's just a 2-line addition to the query setter method that checks for values_select and sets the appropriate iterable class. An experienced Django developer familiar with the ORM internals could identify that the issue is about the queryset not preserving its _iterable_class when reconstructed from a pickled query, and the fix is straightforward once the root cause is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Django's ORM, has a clear error manifestation, and the solution is testable. The test patch shows comprehensive test cases that verify the fix works for both values() and values_list() scenarios. This would make a good benchmark problem as it tests understanding of Django ORM internals, pickling behavior, and requires connecting the error symptoms to the underlying cause.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13449": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It provides: (1) A specific error message \"OperationalError: near 'OVER': syntax error\" that occurs when using Lag() with DecimalField on SQLite, (2) The exact generated SQL query showing the problematic CAST placement, (3) A clear diagnosis that \"the CAST() statement ends after LAG whereas it should be around the whole statement\", (4) A working example with regular fields vs failing example with DecimalField, and (5) A workaround using output_field=FloatField(). The issue identifies that this is SQLite-specific and relates to how Django's ORM generates SQL for window functions with DecimalFields. An experienced engineer would have sufficient information to understand that they need to fix the SQL generation for Window expressions containing Lag functions with DecimalField output on SQLite.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Understanding Django's ORM expression system and how Window functions generate SQL, (2) Diagnosing SQLite-specific SQL generation issues with CAST placement, (3) Understanding the SQLiteNumericMixin pattern used elsewhere in Django for similar database-specific fixes, (4) Implementing a custom as_sqlite method that handles DecimalField casting correctly, and (5) Writing comprehensive tests. The solution involves modifying the Window class in django/db/models/expressions.py to inherit from SQLiteNumericMixin and override SQL generation behavior specifically for SQLite when DecimalFields are involved. While not extremely complex, it requires solid understanding of Django internals and database-specific SQL generation patterns.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained database-specific bug with a clear reproduction case, specific error symptoms, and a targeted solution. The issue provides good technical detail and the solution follows established Django patterns for handling database-specific SQL generation differences.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13568": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some domain knowledge to fully understand. The core problem is clear: the auth.E003 system check requires USERNAME_FIELD to be unique via unique=True, but this creates unwanted implicit indexes on PostgreSQL. The proposed solution is to also check for UniqueConstraints in Model._meta.constraints. However, understanding this requires familiarity with Django's authentication system, system checks, and the difference between field-level unique=True and model-level UniqueConstraints. The mention of PostgreSQL-specific index behavior adds technical context but isn't essential to the core fix. An experienced Django developer could reasonably implement a solution based on this description.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15min-1hour fix. The solution involves modifying a single condition in django/contrib/auth/checks.py to add an additional check for total_unique_constraints. The logic is straightforward: extend the existing uniqueness check to also consider UniqueConstraints, not just the field's unique property. Most time would be spent understanding Django's constraint system and locating the right check function, but the actual code change is minimal - just adding an 'and not any(...)' clause to an existing if condition.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a solid benchmark candidate that tests understanding of Django's authentication system, system checks, and model constraints. The fix requires both domain knowledge and careful implementation to handle the constraint checking correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-13807": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: loaddata crashes on SQLite when table names are SQL keywords like \"order\". The issue provides a concrete error message showing the SQLite syntax error \"near 'order': syntax error\", identifies the root cause (missing back ticks around table names in SQL statements), points to the exact file and function (django/db/backends/sqlite3/base.py line 327, check_constraints function), and shows the problematic code with PRAGMA foreign_key_check(%s) statements. The issue also confirms it affects specific Django versions (3.1.0 and 3.1.2). An engineer would clearly understand they need to properly quote table names when they are SQL keywords in the SQLite backend.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the root cause and the exact location of the problem. The solution involves adding proper quoting around table names using self.ops.quote_name() in the check_constraints function. Looking at the patch, it's a straightforward change that replaces direct string formatting with proper name quoting in multiple places within the same function. An experienced engineer would need some time to understand Django's database backend architecture and locate the quote_name method, but the core fix is relatively simple once the pattern is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly described, the solution approach is evident from the error description, and the test case provided shows how to reproduce the issue with a model using SQL keywords. This is an excellent benchmark sample as it tests understanding of SQL injection prevention, proper identifier quoting, and Django's database backend patterns.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-13810": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some domain knowledge to fully understand. The reporter clearly describes: (1) The problem - a TypeError when using ASGI with middleware that has async_capable=False, (2) The root cause - the handler variable gets overwritten by adapt_method_mode() output even when MiddlewareNotUsed is raised, causing subsequent middleware to receive the wrong handler type, (3) The location - django/core/handlers/base.py line 58 in the load_middleware method. However, understanding the full implications requires knowledge of Django's ASGI/WSGI handling, middleware chains, and async/sync adaptation. An experienced Django developer could reasonably interpret what needs to be fixed from this description.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly localized to the load_middleware method in django/core/handlers/base.py. The solution shown in the patch is quite simple - store the adapted handler in a temporary variable and only assign it to the main handler variable if no MiddlewareNotUsed exception occurs. This requires understanding the control flow and the async/sync adaptation logic, but once understood, the fix is straightforward. The main time would be spent understanding the middleware loading mechanism and testing the solution.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained, has a clear technical explanation, and the solution is focused on a specific piece of logic. The test case provided also demonstrates the expected behavior clearly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14017": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It provides a concrete problem: Q(...) & Exists(...) raises a TypeError while Exists(...) & Q(...) works fine. The issue includes a complete traceback showing the exact error location in django/db/models/query_utils.py line 92 in __and__ method, and line 73 in _combine method. The problem is clearly identified as a lack of commutativity in the & and | operators for Q-Exists pairs. The issue author even provides a hypothesis about the root cause: \"I think there's a missing definition of __rand__ somewhere.\" This gives enough information for an engineer to understand that the problem is in the operator implementation and that the solution involves making these operators commutative.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-localized to a specific method (_combine) in a specific file (query_utils.py). The traceback points directly to the problematic code, and the solution shown in the patch is quite minimal - just changing one condition in the _combine method from checking isinstance(other, Q) to also allowing objects with conditional=True attribute. An experienced engineer would need some time to understand Django's query system and verify that Exists objects have the conditional attribute, but the actual code change is small and the logic is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clear, well-specified, and the solution requires understanding Django's ORM internals but doesn't require extensive research or massive code changes. The test case provided also clearly validates the fix by testing both directions of the & and | operations with Q and Exists objects.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14140": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some gaps to fill in. It clearly identifies the problem: Q objects with 1 child are treated differently during deconstruct, causing a TypeError when the child is a non-subscriptable object like 'Exists'. The issue provides a specific error traceback showing the exact failure point in the deconstruct method where it tries to access child[0] and child[1] on an 'Exists' object. It also mentions a proposed patch that removes the special case for single-child Q objects. However, there are some details that require interpretation: the exact behavior expected after the fix isn't explicitly stated, though it's implied that single-child Q objects should deconstruct into args instead of kwargs. The mention of backward compatibility considerations suggests the author understands the implications but doesn't specify which approach to take.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is localized to a specific method (deconstruct in django/db/models/query_utils.py) and the problem is clearly identified in the traceback. The solution involves removing a special case that assumes single children are always subscriptable tuples, which is a straightforward code change. Looking at the gold patch confirms this - it's a simple removal of conditional logic (about 8 lines removed, 4 lines added). An experienced engineer familiar with Django's codebase could quickly understand that the special case for single-child Q objects is causing the issue and that the fix is to treat all cases uniformly by putting children in args rather than trying to unpack them into kwargs.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained, the error is reproducible, and the solution area is clearly identified. The test patches show comprehensive coverage of the fix, testing both the specific case mentioned (Q objects with boolean expressions) and ensuring the general deconstruct behavior works correctly. This is a good candidate for a coding benchmark as it tests understanding of Python object behavior (subscriptable vs non-subscriptable) and Django's internal query construction mechanisms.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14238": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some domain knowledge to fully understand. The issue describes a problem where Django crashes with a ValueError stating that a custom AutoField subclass \"must subclass AutoField\" when used as DEFAULT_AUTO_FIELD. The error message, stack trace, and proposed solution location (AutoFieldMeta.__subclasscheck__ method) provide good guidance. However, there are some gaps: the issue doesn't show the actual custom field definition (MyBigAutoField) that's causing the problem, and the explanation of why the current _subclasses property approach fails could be clearer. An experienced Django developer could reasonably infer that the issue is about the metaclass's subclass checking logic not properly handling inheritance chains for BigAutoField and SmallAutoField subclasses.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The solution involves changing a single line in the AutoFieldMeta.__subclasscheck__ method, replacing a direct membership check with issubclass() to properly handle inheritance. The error message and stack trace clearly point to the problematic code location. An experienced developer would need to: (1) understand that the issue is about metaclass subclass checking, (2) locate the AutoFieldMeta class in django/db/models/fields/__init__.py, (3) recognize that the current implementation uses 'subclass in self._subclasses' which only checks direct membership rather than inheritance, and (4) change it to use issubclass() for proper inheritance checking. The fix is conceptually straightforward once the inheritance issue is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of Python metaclasses, inheritance checking, and Django's field system. The fix is surgical and well-contained, making it suitable for evaluation.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14351": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. It clearly describes a specific problem: Q object OR operations cause different behavior when using `agent__property_groups__id__in` vs `agent__property_groups__in`, leading to a \"subquery must return only one column\" error in Django 3.2. The error message and stack trace provide concrete evidence of the problem. However, the issue lacks a minimal reproducible example showing the exact code that triggers this behavior, and doesn't specify the expected behavior clearly - it only mentions it \"worked in 2.2.5\" without describing what the correct output should be. An experienced engineer could reasonably infer that the solution should make both query patterns behave consistently and not cause the ProgrammingError, but some details about the exact models and relationships would need to be figured out from the codebase.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because it requires understanding Django's ORM internals, specifically how Q objects handle aliasing and subquery generation. The engineer needs to: 1) Understand the difference in behavior between the two query patterns, 2) Trace through Django's query compilation process to find where the extra columns are being added, 3) Understand the relationship between lookups, subqueries, and GROUP BY clauses, and 4) Implement a fix that ensures subqueries only return the necessary columns. The provided patch shows it's a focused change to the `get_group_by_cols` method in `lookups.py`, but finding this solution requires deep knowledge of Django's ORM architecture and debugging skills to trace the query generation process.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The test case provided shows a clear way to verify the fix works, and the problem is specific enough to Django's ORM that it represents a legitimate coding challenge for someone working with Django internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-14493": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a crash in ManifestStaticFilesStorage when max_post_process_passes is set to 0, provides the complete error traceback showing an UnboundLocalError for the variable 'substitutions', and even includes a direct link to the problematic code in the repository. The issue explains that the variable 'substitutions' is only set if a loop is entered at least once, making it clear that this is a logic error where the variable is referenced before being initialized when the loop doesn't execute. The motivation for setting max_post_process_passes to 0 is also provided, giving context for why this edge case matters. An experienced engineer would have all the information needed to understand and fix this bug.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a very simple fix that should take less than 15 minutes. The issue is a classic UnboundLocalError where a variable is referenced before assignment. Looking at the gold patch confirms this - the fix is literally adding a single line \"substitutions = False\" before the for loop to initialize the variable. Once an engineer understands the problem from the clear error message and description, the solution is obvious: initialize the variable before it's potentially referenced. This requires no complex logic, no deep understanding of the Django staticfiles system, and involves adding just one line of code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. This is a clean, well-documented bug with a straightforward fix that would be suitable for a coding benchmark. The issue description provides clear reproduction information, the error is unambiguous, and the solution is simple enough that it tests basic debugging skills without requiring extensive domain knowledge.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14580": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. The user provides a concrete error message showing that Django generates a migration file with a missing import statement for 'models'. The error occurs in a generated migration file at line 23 where 'models.Model' is referenced but 'models' is not imported. The actual vs expected behavior is clearly stated: Django should generate migration files with the necessary import statements included. The error traceback shows the specific location where the failure occurs, and the user correctly identifies that this is likely related to django.db.migrations.writer module. Combined with the provided patches, it's clear that the solution involves ensuring the TypeSerializer properly includes the required imports when serializing models.Model references in migrations.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-defined and the solution involves a simple one-line change in the TypeSerializer class. An experienced engineer would need to: 1) Understand that the problem is in migration serialization where models.Model references aren't including the necessary import, 2) Locate the TypeSerializer class in django/db/migrations/serializer.py, 3) Identify that the special_cases list needs to include the import statement for models.Model, and 4) Add the missing import to the tuple. The change is minimal but requires understanding Django's migration serialization system and how imports are tracked during the serialization process. The accompanying test also shows this is a straightforward fix that can be validated easily.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues with this sample. The problem is clearly described, the solution is focused and testable, and it represents a realistic bug that could occur in Django development. The error message and context provide sufficient information for an engineer to understand and solve the issue.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14672": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: a missing call to `make_hashable` on `through_fields` in the `ManyToManyRel` class, which causes a TypeError when `through_fields` is a list (unhashable type). The issue provides a complete stack trace showing exactly where the error occurs (in the `__hash__` method of `reverse_related.py` line 140), explains the root cause (through_fields can be a list but needs to be made hashable for the identity tuple), and even provides the exact solution needed. The context about Django 3.2's identity property addition and the relationship to proxy models gives helpful background. An experienced engineer would have all the information needed to locate the exact file, function, and line that needs modification.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides the exact location of the problem (ManyToManyRel class in reverse_related.py) and the exact solution (wrap self.through_fields with make_hashable call). From the gold patch, we can see it's literally a one-line change replacing `self.through_fields,` with `make_hashable(self.through_fields),` in the identity property. An experienced engineer would need some time to understand the Django ORM internals, locate the specific file and method, understand why make_hashable is needed (to ensure all tuple elements are hashable), and verify the fix, but the actual code change is trivial. The stack trace points directly to the problematic line, making debugging straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is clearly defined, the solution is straightforward, and the test case demonstrates the exact scenario that triggers the bug. This is an excellent benchmark sample as it tests understanding of Python hashability concepts, Django ORM internals, and the ability to follow a clear bug report to its resolution.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-14787": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides a sensible interpretation of what needs to be fixed. The core problem is clear: method_decorator() creates a partial object that lacks standard function attributes like __name__ and __module__, causing AttributeError exceptions. The title \"method_decorator() should preserve wrapper assignments\" gives additional context about what the solution should achieve. While the description doesn't specify exactly how to fix it or mention functools.wraps, an experienced engineer familiar with Python decorators would understand that wrapper assignments (like __name__, __module__) need to be preserved, which is typically done using functools.wraps. The gold patch confirms this interpretation by showing wraps(method) being applied to the partial object.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-understood in the Python decorator ecosystem: when creating wrapper functions or using functools.partial, you need functools.wraps to preserve function metadata. The solution involves a single line change in django/utils/decorators.py, wrapping the partial object with wraps(method). An experienced engineer would quickly identify this as a standard decorator preservation issue and apply the conventional fix. The main time would be spent understanding the Django codebase structure and locating the method_decorator implementation, rather than figuring out the solution approach.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-understood, the solution follows standard Python patterns, and the test clearly validates the fix. This would be a good benchmark sample for testing knowledge of Python decorator best practices.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15104": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description with a complete stack trace showing the exact error (KeyError: 'to' in django/db/migrations/autodetector.py line 99). The issue author explains the root cause: a custom ForeignKey field that hardcodes its 'to' argument and removes it from deconstructed kwargs. Most importantly, they provide the exact solution: changing \"del deconstruction[2]['to']\" to \"deconstruction[2].pop('to', None)\" which safely handles the case where the 'to' key doesn't exist. The file path, line number, and specific code change are all clearly identified.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides the exact location (django/db/migrations/autodetector.py line 99) and the precise solution (change del to pop with default None). The fix is a simple one-line change that makes the code more defensive by using pop() instead of del to avoid KeyError when the 'to' key is missing. An experienced engineer would need minimal time to understand the problem, locate the file, and implement the change. The main time would be spent understanding the Django migration system context and writing appropriate tests.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for the benchmark. The issue is clearly described with concrete symptoms, provides a complete stack trace for debugging, identifies the root cause, and even suggests the exact solution. The fix is straightforward but requires understanding Python dictionary operations and defensive programming practices. The test case shows how to create a custom ForeignKey that triggers the bug, making it easy to verify the solution works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15128": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is reasonably well-specified but has some gaps. It clearly identifies the problem: an AssertionError in Query.change_aliases when combining QuerySets with OR operations. The reporter provides specific technical details about the root cause - that sequential aliases (T4, T5) can create conflicting mappings during query combination where change_map keys intersect with values. They also suggest a solution direction involving modifying alias_map handling in Query.join and Query.table_alias. However, the description lacks a concrete code example to reproduce the issue, which would make it easier for an engineer to understand the exact scenario. The technical explanation is detailed enough for someone familiar with Django's ORM internals to understand what's happening, but might be challenging for those less familiar with the query combination logic.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours to solve because: (1) It involves deep understanding of Django's ORM query combination logic, specifically the alias management system during OR operations, (2) The solution requires modifying the bump_prefix method to accept an exclude parameter and updating the combine method to call bump_prefix early with proper exclusions, (3) Multiple files need to be edited (query.py for the main fix, plus test files for verification), (4) The engineer needs to understand the intricate relationship between alias prefixes, table mapping, and query combination to avoid breaking existing functionality, (5) Writing appropriate tests requires creating a scenario with the specific alias conflict pattern described. While not the most complex Django issue, it requires substantial understanding of internal ORM mechanics.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is a legitimate bug in Django's ORM with a clear technical cause and solution path. The fix involves modifying internal query logic which is a reasonable test of advanced Django/ORM knowledge.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15277": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when using `annotate(x=Value('test'))`, the `CharField.__init__` method unconditionally adds a `MaxLengthValidator` even when `max_length` is `None`, causing a runtime TypeError when the validator tries to compare an int with NoneType. The issue provides the exact error message, pinpoints the problematic code in `CharField.__init__`, and suggests the specific solution: adding a null check before appending the validator. The author even references `BinaryField.__init__` as precedent for this pattern. The problem, cause, and solution are all clearly articulated with sufficient technical detail.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15-60 minutes. The problem is clearly identified (unconditional validator addition), the location is specified (CharField.__init__), and the solution is simple (add a null check). An experienced engineer would need minimal time to: 1) locate the CharField class in django/db/models/fields/__init__.py, 2) find the problematic line that adds MaxLengthValidator, 3) wrap it in an if statement checking max_length is not None. The fix is a simple 2-line change (wrapping one line in an if statement), and the author already confirmed all existing tests pass.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is clear and simple, and it's a good test of basic debugging and defensive programming skills. The provided test case appropriately verifies that various value types can be processed without throwing exceptions when the output field is resolved.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15280": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some gaps. It clearly describes the problem: when using prefetch_related with nested relationships, deferred fields are incorrectly handled when following prefetches back to the \"parent\" object, causing unnecessary database queries. The issue provides concrete models (User and Profile), shows the specific error (AssertionError: 1 != 0), and includes the actual SQL query being executed. However, the issue description is missing the actual test code that demonstrates the problem - it mentions \"The second assertNumQueries actually fails\" but doesn't show the complete test setup. From the test patch, we can infer this involves prefetch_related with nested Prefetch objects and only() calls, but an engineer would need to piece together the exact scenario from the description and error message. The core problem is clear enough that an experienced Django developer could understand what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The problem requires understanding Django's ORM internals, specifically how prefetch_related works with deferred fields and caching mechanisms. The solution involves modifying the get_prefetch_queryset method in related_descriptors.py to check if a field is already cached before setting it, which is a relatively small code change (3-4 lines). However, arriving at this solution requires: (1) Understanding the Django ORM's prefetch mechanism and how it interacts with deferred fields, (2) Tracing through the caching logic to identify why deferred field information is being incorrectly inherited, (3) Understanding the relationship between forward and reverse relationships in prefetching, (4) Writing appropriate test cases to verify the fix. The fix itself is simple, but the investigation and understanding required to identify the root cause and ensure the solution doesn't break other functionality would take several hours.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate Django ORM bug with a clear fix. The test case provided adequately verifies the solution, and the issue would be suitable for evaluating an engineer's ability to understand complex ORM behavior and implement targeted fixes in a large codebase.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15315": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is mostly well-specified but has some ambiguity. It clearly identifies the core problem: Field.__hash__() changes value when assigned to a model class, which breaks dictionary usage. The issue references the specific PR (#31750) that introduced the bug and suggests a solution (reverting the hash change). However, the description lacks specific details about what the current problematic implementation looks like or exactly how the hash should be computed. An experienced engineer would need to investigate the codebase to understand the current __hash__ implementation and determine the appropriate fix, but the high-level requirement is clear.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is well-defined (hash immutability) and the solution is straightforward once you locate the Field.__hash__ method. The gold patch shows it's just simplifying the hash calculation from a tuple of multiple values to just the creation_counter. The main time would be spent: 1) Understanding Django's field system and locating the __hash__ method in django/db/models/fields/__init__.py, 2) Understanding why the current implementation is problematic, 3) Implementing the simple fix, and 4) Writing a test to verify immutability. The code change itself is minimal (removing a few lines), making this a relatively quick fix for an experienced engineer.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking coding ability. It tests understanding of Python hash semantics (immutability requirement), Django's model field system, and the ability to write appropriate tests. The solution requires both identifying the problematic code and understanding why hash immutability is important for objects used in dictionaries. The reference to PR #31750 provides helpful context without making the solution too obvious.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15375": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified with a clear problem description, specific error message, malformed SQL output, and a minimal reproduction case. The description clearly states: (1) the sequence of operations (annotate then aggregate with default), (2) the specific error that occurs (OperationalError with syntax error), (3) the malformed SQL that's generated, and (4) mentions it affects both PostgreSQL and SQLite with Django 4.0.1. An experienced engineer would understand that they need to fix the SQL generation when using aggregate with default after annotate operations, and the provided SQL shows exactly what's wrong (missing SELECT clause).",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours because: (1) Understanding the problem requires knowledge of Django's ORM internals, specifically how annotate() and aggregate() interact and how SQL is generated, (2) The bug involves the subtle interaction between aggregate wrapping with Coalesce and the is_summary attribute that affects SQL generation, (3) Finding the root cause requires tracing through Django's query compilation process to understand why the SELECT clause is missing, (4) The fix itself is small (3 lines) but requires understanding that is_summary needs to be preserved when wrapping aggregates, (5) Writing appropriate tests requires understanding the different scenarios where this bug could manifest.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the fix is focused and specific, and the test cases adequately cover the functionality. This is a good benchmark sample that tests understanding of ORM internals without being overly complex.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15380": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue provides a clear stack trace showing that the Django migration autodetector crashes with a KeyError when renaming both a model and field simultaneously. The traceback points to line 823 in autodetector.py where it tries to access 'mymodel' but the key doesn't exist. While the issue doesn't provide the exact reproduction steps or model definitions, the stack trace is specific enough to understand the problem: the code is looking for a model with the old name in the new state, but it should be looking for the new model name. The regression commit reference also helps pinpoint when this broke. An experienced engineer could reasonably deduce that this is about handling simultaneous model and field renames.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward bug fix. The stack trace clearly identifies the problematic line (823 in autodetector.py), and the solution is a simple one-line change from `old_model_name` to `model_name` when looking up the new model state. The logic error is clear: when both model and field are renamed, the code incorrectly uses the old model name to look up the new state instead of the new model name. An experienced engineer familiar with Django's migration system could identify and fix this within 15-60 minutes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The stack trace provides sufficient information to locate and understand the bug, and the fix is straightforward. The test case also clearly validates the specific scenario of renaming both model and field simultaneously, making this a good benchmark sample.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15503": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some minor gaps to fill in. The core problem is clearly stated: JSONField has_key lookups with numeric keys fail on SQLite (and mentioned MySQL/Oracle in title) but work on PostgreSQL. The issue provides specific version information, a test case failure with traceback, and mentions it's been tested on both SQLite and PostgreSQL backends. However, there are some details that need interpretation: (1) The title mentions MySQL and Oracle but the description only tests SQLite vs PostgreSQL, (2) The exact scope of affected lookups (has_key, has_keys, has_any_keys) is mentioned in title but description focuses on has_key, (3) What constitutes a \"numeric key\" could be interpreted as string representations of numbers vs actual numeric types. Despite these gaps, an experienced engineer could reasonably infer that the issue is about ensuring consistent behavior across database backends for JSON key lookups involving numeric keys.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: (1) It requires understanding Django's JSONField implementation across multiple database backends, particularly the differences in how SQLite/MySQL/Oracle vs PostgreSQL handle JSON path compilation, (2) The solution involves modifying the HasKeyLookup class and creating a new HasKeyOrArrayIndex class, affecting multiple methods and files, (3) The engineer needs to understand JSON path compilation, the difference between object keys and array indices, and how json.dumps() vs compile_json_path() handle numeric keys differently, (4) Multiple classes need modification (HasKeyLookup, KeyTransformIsNull, KeyTransformRegex) and comprehensive tests need to be written, (5) The fix requires deep understanding of Django ORM internals and database-specific JSON handling. While not extremely esoteric, this goes beyond a simple function change and requires substantial understanding of the codebase architecture.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is a legitimate bug with a clear technical solution, the test case provided gives a good verification method, and the solution is testable and well-bounded. The issue represents a good benchmark for testing understanding of Django internals, database abstraction layers, and cross-platform compatibility concerns.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15525": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clear: loaddata fails on non-default databases when natural keys use foreign keys, while it works fine on the default database. The issue provides a complete stack trace showing the exact error (DoesNotExist: Author matching query does not exist), complete model definitions with natural key implementations, and demonstrates the one-to-many relationship between Book and Author models. However, there are some gaps - the issue doesn't provide the actual fixture data being loaded or the exact loaddata command being used. An experienced engineer would need to infer that the problem likely relates to database routing or state management when deserializing objects with natural key dependencies across different databases. The error trace gives strong hints that the issue occurs during natural key resolution when the related Author object isn't found in the target database context.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the actual code change is small (3 lines in the gold patch), understanding the problem requires deep knowledge of Django's serialization system, database routing, and object state management. An engineer would need to: 1) Understand how Django's loaddata command works with natural keys, 2) Trace through the serialization/deserialization process in django/core/serializers/base.py, 3) Understand that the issue occurs because when Model(**data) creates an object, it doesn't have the correct database state set, so natural_key() queries look in the wrong database, 4) Realize that setting obj._state.db = db before calling natural_key() ensures queries use the correct database. The solution involves understanding Django's internal object state management (_state.db), which is not commonly known. Writing comprehensive tests also requires understanding Django's multi-database testing setup.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate Django bug with a clear reproduction case and well-defined expected behavior. The issue demonstrates good software engineering practices with proper model definitions, natural key implementations, and dependency declarations. The fix is targeted and doesn't introduce breaking changes.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15695": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides enough context to understand the core problem. It specifies that RenameIndex() crashes when an unnamed index for unique_together is moving backward and forward, with the specific error being 'relation \"new_pony_test_idx\" already exists' on PostgreSQL. While the description is brief, it clearly identifies the problematic operation (RenameIndex), the scenario (unnamed index moving backward/forward), and the expected behavior (should restore old auto-generated name). The gold patch confirms this interpretation - it's a simple guard clause to prevent renaming when old and new names are the same. An experienced developer familiar with Django migrations would understand this refers to the database schema migration system and the need to handle index renaming operations correctly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that would take 15 minutes to 1 hour. The gold patch shows it's just a 3-line addition: a simple conditional check to return early if old_index.name == self.new_name. Once an engineer understands the Django migration system and locates the RenameIndex operation in django/db/migrations/operations/models.py, the solution is quite obvious - prevent unnecessary renaming operations that would cause database conflicts. The error message clearly points to the issue (relation already exists), and the fix is a standard defensive programming pattern. Most of the time would be spent understanding the codebase structure rather than implementing the actual solution.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major concerns. This is a well-contained issue with a clear problem statement and straightforward solution. The test patch demonstrates the exact scenario being tested (reapplying the operation), making it suitable for benchmark evaluation. The fix requires understanding Django's migration system but doesn't involve complex algorithms or extensive refactoring.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15732": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The user describes having an erroneous unique_together constraint on a model's primary key field (unique_together = (('id',),)) that cannot be dropped by a migration. They explain that the migration fails because it finds two unique constraints on the same column - the primary key constraint and the unique_together constraint - but expects only one. The specific error scenario is clear: the migration system cannot handle dropping a unique_together constraint when there are multiple unique constraints on the same field. However, some details need to be inferred, such as the exact migration operation being attempted and the specific error message/behavior. An experienced Django developer would understand this is about the alter_unique_together migration operation and the constraint detection logic in Django's schema editor.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the solution, it requires understanding Django's migration system, specifically the schema editor's constraint detection and deletion logic. The fix involves modifying the _delete_composed_index method to handle cases where there are multiple constraints on the same fields by adding logic to prefer the constraint with the default unique_together name when multiple constraints exist. It also requires adding a new _unique_constraint_name helper method and updating the _create_unique_sql method. The solution touches multiple parts of the schema editor code and requires understanding of database constraint naming conventions. While not extremely complex, it requires significant understanding of Django's ORM internals and careful consideration of edge cases.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a legitimate Django ORM bug that affects migration operations when there are conflicting unique constraints. The issue description provides enough context about the problem scenario, and the solution is a reasonable enhancement to Django's constraint handling logic. The test cases show clear before/after behavior that validates the fix works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15814": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear problem statement - QuerySet.only() after select_related() crashes on proxy models with a specific ValueError, (2) Complete reproduction case with exact model definitions (CustomModel, ProxyCustomModel, AnotherModel) and the failing query code, (3) Full stack trace showing the exact error location in django/db/models/query.py at line 2243, (4) Environment details (Windows 10, Python 3.10, Django 4.0.5), and (5) A suggested fix at line 745 in django/db/models/sql/query.py to use concrete_model._meta instead of just _meta. The issue even identifies the root cause - the problem occurs when working with proxy models because the code doesn't account for the concrete model underneath. An experienced engineer would have all the information needed to understand the problem, reproduce it, and implement a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The issue provides the exact location of the problem (django/db/models/sql/query.py line 745) and even suggests the specific fix (using cur_model._meta.concrete_model._meta instead of cur_model._meta). The actual code change is minimal - just one line modification. The gold patch confirms this is indeed a simple fix. The main time would be spent understanding Django's proxy model system and verifying that the suggested fix doesn't break other functionality, but the core problem and solution are clearly identified in the issue description.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - it has a clear problem statement, complete reproduction case, specific error location, and requires understanding of Django's ORM internals (specifically proxy models). The solution is straightforward once the problem is understood, making it a good test of both debugging skills and Django framework knowledge.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-15930": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear error message showing a SQL syntax error (\"syntax error at or near 'THEN'\") and identifies the specific problematic code pattern: `~Q(pk__in=[])` used within a `Case()` expression in Django ORM. The error message shows the malformed SQL output with an empty WHEN clause. While the description is brief, it provides the essential information: (1) the specific error being encountered, (2) the exact code pattern that triggers it, (3) the resulting SQL syntax issue, and (4) context that this is a sentinel value used in application code. An experienced Django developer familiar with the ORM would understand this is about handling edge cases in CASE WHEN expressions where the condition evaluates to an empty string, requiring a fix to generate valid SQL.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is localized to Django's expression handling, specifically in the Case/When SQL generation code. The error message clearly points to the problem: empty WHEN clauses in CASE expressions. Looking at the patch, the solution is straightforward - detect when condition_sql is empty and replace it with a condition that always evaluates to True. The fix involves adding just 5 lines of code with a simple conditional check in the as_sql method of django/db/models/expressions.py. An experienced developer would quickly identify this as a SQL generation edge case and implement the appropriate fallback logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained Django ORM issue with a clear reproduction case and targeted fix. The test case demonstrates the expected behavior, and the solution is elegant and minimal. This would be suitable for a coding benchmark as it tests understanding of SQL generation edge cases and Django ORM internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15957": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It clearly states that prefetch() objects don't work with sliced querysets and provides the specific error message. The use case is explained well - wanting to display categories with a few example objects efficiently. However, the exact technical requirements for the solution aren't fully detailed. An experienced Django developer would understand this refers to the prefetch_related() ORM functionality and that the solution needs to handle sliced querysets in prefetch operations, but some domain knowledge is required to fully grasp what needs to be implemented.",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a highly complex issue requiring 4+ hours to solve. The solution involves deep Django ORM internals including: (1) Understanding prefetch_related mechanism and related descriptors, (2) Working with Django's query compilation and database backends, (3) Implementing Window functions with RowNumber for pagination-like behavior, (4) Handling query slicing, limits, and ordering correctly, (5) Modifying multiple descriptor classes (ForwardManyToOneDescriptor, ManyToManyDescriptor), (6) Creating a sophisticated helper function that manages complex query predicates. The patch spans multiple files and requires intimate knowledge of Django's database abstraction layer. This is clearly an advanced ORM feature that touches core Django functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for a coding benchmark. While complex, it has a clear problem statement, specific error condition, and well-defined expected behavior. The comprehensive test suite shows various scenarios (M2M forward/reverse, FK reverse, ordering) that validate the solution works correctly. An experienced engineer with Django knowledge could reasonably attempt this given sufficient time.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-15973": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly describes: (1) the specific scenario that triggers the bug (defining a through model in a separate app), (2) the exact error message and full traceback, (3) that it works when the through model is in the same app but fails when in a separate app. The traceback pinpoints the exact location where the error occurs (field.remote_field.through._meta.auto_created), making it clear that the issue is with Django trying to access _meta on a string instead of a model class. An experienced Django developer would understand this is a model resolution issue during migrations.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The traceback clearly points to the problematic line in schema.py where field.remote_field.through._meta is accessed. The issue is that through is a string rather than a resolved model class when the through model is in a different app. Looking at the patch, the fix involves changing one parameter in the _get_dependencies_for_foreign_key function from remote_field_model to field.remote_field.through. This requires understanding Django's model resolution system and migration dependency handling, but the actual code change is minimal (1 line). An experienced Django developer familiar with the codebase could identify and implement this fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-defined Django bug with a clear reproduction case and targeted fix. The issue has sufficient context for someone to understand the problem, and the solution involves core Django migration/model resolution logic that would be valuable to test in a coding benchmark.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16032": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks crucial context. It simply states \"__in doesn't clear selected fields on the RHS when QuerySet.alias() is used after annotate().\" and shows an error message \"django.db.utils.OperationalError: sub-select returns 10 columns - expected 1\". However, there's no code example showing how to reproduce the issue, no explanation of what the expected behavior should be, and no clear description of the problem scenario. An engineer would need to understand Django's ORM internals, guess at the specific use case that triggers this error, and infer what the correct behavior should be. The error message suggests a column count mismatch in a subquery, but without seeing the problematic query or understanding the expected outcome, it's difficult to know what constitutes a \"successful solution.\"",
            "q2_1_difficulty": 3,
            "q2_2_explanation": "This is a 4+ hour issue because it requires deep understanding of Django's ORM internals, specifically the QuerySet.alias() and annotate() methods and how they interact with __in lookups. The engineer would need to: 1) Reverse-engineer what scenarios trigger this bug from the sparse description, 2) Understand the complex interaction between annotations, aliases, and subquery field selection in Django's SQL generation, 3) Navigate through multiple interconnected files (related_lookups.py, query.py) to understand the root cause, 4) Implement a solution that changes the fundamental behavior of how QuerySet handles field selection (switching from add_fields to set_values and modifying has_select_fields logic). The patch shows this touches core ORM functionality and changes both property definitions and method calls, indicating significant architectural implications that require substantial research and testing.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The issue description is severely underspecified, making it unsuitable for a coding benchmark. Without a reproducible example or clear problem statement, an engineer cannot reliably understand what needs to be fixed. The issue would require extensive investigation to even understand the problem scenario, let alone implement a solution. Additionally, this appears to be a complex ORM internals issue that requires very specialized Django knowledge, making it more of a research/debugging task than a straightforward coding problem. A benchmark should provide clear requirements that allow for deterministic evaluation of solutions.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16136": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It provides a clear problem description: when defining a View subclass with only an async \"post\" method, GET requests cause a TypeError because HttpResponseNotAllowed can't be used in an 'await' expression. The issue includes a complete stack trace showing the exact error, identifies the specific scenario that triggers it (async view with only POST method receiving GET request), and the error message clearly indicates the root cause. An experienced engineer would understand that the framework is trying to await a non-awaitable HttpResponseNotAllowed object in the context of an async view, and would know they need to modify the http_method_not_allowed method in django/views/generic/base.py to handle async views properly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified in the stack trace pointing to the http_method_not_allowed method. The solution requires understanding async/await patterns and modifying one method to conditionally return a coroutine when dealing with async views. The actual code change is relatively small (adding a conditional check for view_is_async and wrapping the response in an async function), but requires some thought about Django's async view handling patterns. An experienced engineer familiar with Django would need some time to understand the async view architecture and implement the proper solution.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly defined with a reproducible error, the solution requires understanding of both Django's class-based views and async/await patterns, and the fix demonstrates proper async handling in a web framework context. The test patch also provides good validation that the solution works correctly for both sync and async views.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16429": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem description: calling timesince() with a datetime object that's one month or more in the past while USE_TZ=True results in a TypeError about mixing offset-naive and offset-aware datetimes. The issue includes the specific error message, the exact commit hash where the problem was introduced, a direct link to the problematic code in django/utils/timesince.py (lines 93-100), and even proposes a specific solution (adding d.tzinfo argument to the datetime.datetime call). The reporter demonstrates good understanding of the codebase and provides actionable information for fixing the bug.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location and suggests the exact solution. The fix involves adding a single parameter (tzinfo=d.tzinfo) to a datetime constructor call in django/utils/timesince.py. While an engineer would need some time to understand the Django timezone handling and verify the fix is correct, the actual code change is minimal and straightforward. The provided patch confirms this - it's literally a one-line addition.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample - it has a clear bug description, specific reproduction conditions, points to the exact problematic code, and has a clean, focused solution. The test addition also properly covers the timezone-aware scenario that was missing.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16454": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and provides clear requirements for a successful solution. It identifies a specific problem: Django's CommandParser class has special error formatting arguments that are not being passed to subparsers created via add_subparsers().add_parser(). The issue includes: (1) A concrete example showing the problematic behavior with a stack trace instead of proper error formatting, (2) Clear explanation of the root cause - the CommandParser's extra arguments for error formatting aren't copied to subparsers, (3) A specific suggested solution direction - ensure that subparser actions copy relevant arguments to constructed subparsers, (4) Reference to the relevant class (CommandParser) and method (add_subparsers()), and (5) Context about what the correct behavior should be (human-facing usage messages instead of stack traces). An experienced engineer would have sufficient information to understand what needs to be fixed and how to approach the solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-defined and the solution is relatively straightforward. Looking at the gold patch, it involves: (1) Adding a functools import, (2) Overriding the add_subparsers method in CommandParser, (3) Using partial to ensure the called_from_command_line argument is passed to subparsers when they use CommandParser. The solution is contained within a single method (about 10 lines of code) and doesn't require complex logic or extensive codebase changes. An experienced engineer familiar with argparse and Django's command structure could identify this solution approach and implement it within an hour. The main work involves understanding how argparse subparsers work and how Django's CommandParser extends the base functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with clear requirements, a focused solution, and appropriate test coverage. The issue demonstrates good software engineering practices with a concrete example, clear problem statement, and suggestions for the fix direction. It would serve well as a benchmark item.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16485": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is quite vague and lacks important context. It only states \"floatformat() crashes on '0.00'\" with \"Both throw ValueError: valid range for prec is [1, MAX_PREC]\" but doesn't specify: (1) what \"both\" refers to - there's only one example given, (2) what the second parameter should be when calling floatformat(\"0.00\", ?), (3) what the expected behavior should be instead of crashing, (4) any context about what floatformat() is supposed to do or where it's located in the codebase. From the patch, I can infer this relates to Django's template floatformat filter and the issue occurs when precision is 0, but this context is not provided in the issue description itself.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Once the problem is understood (floatformat crashes when precision is 0), the solution is quite straightforward. The patch shows it's a simple one-character change from \"p < 0\" to \"p <= 0\" in the condition check. An experienced engineer familiar with the codebase could identify this issue and implement the fix within 15-60 minutes. The logic is simple: the function should handle precision of 0 the same way it handles negative precision values, rather than throwing an error.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "This sample has significant issues for a coding benchmark: (1) The issue description is too vague to work from without additional context, (2) An engineer would need to guess what the second parameter should be and what the expected behavior is, (3) The phrase \"both throw\" suggests there should be two examples but only one is provided, making it unclear what scenarios need to be tested, (4) Without knowing this is Django's template system, an engineer might not even know where to look for the floatformat function in the codebase.",
            "q2_5_confidence": 4
        }
    },
    {
        "django__django-16569": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is extremely well-specified. It provides: (1) Clear reproduction scenario - when FormSet has can_delete=True and can_delete_extra=False, calling add_fields() with index=None fails; (2) Specific example of when this occurs - FormSet.empty_form(); (3) Exact error message - \"TypeError: '<' not supported between instances of 'NoneType' and 'int'\"; (4) Root cause analysis - points to line 493 in django/forms/formsets where index is compared to initial_form_count without null checking; (5) Suggested fix - add \"index is not None\" check before the comparison. The issue even provides the exact code change needed, making it crystal clear what needs to be done.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The issue description provides the exact location of the problem (line 493 in django/forms/formsets), explains the root cause (comparing None to an integer), and suggests the specific fix needed (adding a null check). An experienced engineer would need minimal time to understand the codebase context, locate the problematic line, implement the suggested fix, and verify it works. The fix is a simple one-line change adding a null check before the comparison.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal sample for the benchmark - the issue is clearly described, the solution is straightforward, and the test patch confirms the fix works by checking that empty_form.fields doesn't contain a DELETE field when can_delete_extra=False. The issue demonstrates a common programming mistake (not handling None values in comparisons) and the solution is clean and minimal.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16667": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem: SelectDateWidget crashes with OverflowError when user input values are too large. The issue provides the exact location of the problematic code (django/forms/widgets.py, line 1160, specifically the line \"date_value = datetime.date(int(y), int(m), int(d))\"), includes a stack trace showing the OverflowError, and explains that y, m, and d are user-controlled values. An experienced engineer would understand that they need to add exception handling around the datetime.date() constructor to catch OverflowError and return an appropriate fallback value. The problem is specific, reproducible, and has clear success criteria.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified with the exact line of code causing the issue. The solution is straightforward: add exception handling around the datetime.date() constructor to catch OverflowError. Looking at the patch, it's just adding 2 lines of code (except OverflowError: return \"0-0-0\"). An experienced engineer would quickly understand that when datetime.date() receives values that are too large, it throws OverflowError, and the fix is to catch this exception and return a sensible fallback. The most time would be spent understanding the existing codebase structure and writing appropriate tests, but the core fix is simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement, specific location, and straightforward solution. The issue demonstrates good software engineering principles around input validation and error handling. It would be suitable for evaluating an engineer's ability to understand exception handling, read stack traces, and implement defensive programming practices.",
            "q2_5_confidence": 5
        }
    },
    {
        "django__django-16950": {
            "q1_1_is_well_specified": 2,
            "q1_2_explanation": "The issue description is vague and lacks essential details for understanding the problem. While it mentions \"Django Admin with Inlines not using UUIDField default value,\" it doesn't provide model definitions, specific code examples, or clear reproduction steps. The description references an external link for exception details that cannot be accessed, and uses generic placeholders like \"Thing\" and \"subthing\" without concrete examples. An engineer would struggle to understand the specific scenario, what models are involved, how the inlines are configured, or what the expected behavior should be versus what's actually happening.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a moderately complex Django forms issue requiring 1-4 hours to solve. The bug involves understanding Django's inline formset internals, specifically how default primary keys are handled when form data is provided. The fix requires modifying the add_fields method in django/forms/models.py with conditional logic to differentiate between parent model primary keys and alternate keys. An engineer would need to understand Django's formset architecture, foreign key relationships, and UUID field behavior. The solution involves adding a multi-line conditional check, which requires careful analysis of the existing codebase and understanding of edge cases.",
            "q2_3_other_issues": 1,
            "q2_4_other_notes": "The issue description relies heavily on an external link (dpaste.com) to provide crucial exception details that would be necessary to understand and reproduce the problem. In the benchmark setup where only the main issue text is provided, this external dependency makes the issue unsuitable since engineers wouldn't have access to the actual error information. Additionally, the vague description with placeholder terms makes it difficult to create a proper test case or understand the specific models and relationships involved.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-20859": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: adding a legend to a SubFigure doesn't work and throws a TypeError. The issue provides a complete error traceback showing the exact error message \"Legend needs either Axes or Figure as parent\". It also includes a minimal reproducer (subfig.legend()) and points to the exact location in the code that needs to be changed (line 437 in legend.py), suggesting to check against FigureBase instead of Figure. The issue includes all necessary context including matplotlib version, operating system, and backend. An experienced engineer would have all the information needed to understand that SubFigure inherits from FigureBase but not Figure, and the legend code needs to accept FigureBase instances as valid parents.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that would take 15 minutes to 1 hour. The issue clearly identifies the problem location and suggests the exact fix needed. An engineer would need to: 1) understand that SubFigure is a subclass of FigureBase but not Figure, 2) change the import from Figure to FigureBase, 3) update the isinstance check from Figure to FigureBase, and 4) update the error message accordingly. The actual code changes are minimal (3-4 lines changed) and the logic is straightforward once you understand the class hierarchy. The most time would be spent understanding the matplotlib codebase structure and verifying that FigureBase is the correct parent class to check against.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent benchmark sample. The issue is clear, the fix is well-defined, and the test case validates the specific functionality. The problem involves understanding class inheritance hierarchies which is a fundamental programming concept. The suggested solution in the issue description actually matches the implemented fix, making this a realistic scenario where a developer might identify both the problem and solution approach from debugging.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-22719": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: a confusing deprecation warning is being triggered when passing empty data structures to matplotlib plotting functions on axes with category units. The user provides a detailed traceback showing the error occurs in `matplotlib/category.py` when `ax.plot([], [])` is called on an axis with string unit converters. They correctly identify that this seems like a \"false alarm\" warning since empty data shouldn't trigger deprecation warnings about passing numbers through unit converters. The issue includes reproduction steps and identifies the specific warning message. However, an engineer would need to investigate the codebase to understand exactly where the logic needs to be modified to handle the empty data case properly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is quite straightforward: adding `values.size and` checks before triggering deprecation warnings in two places in `lib/matplotlib/category.py`. The fix involves understanding that empty arrays (`values.size == 0`) should not trigger deprecation warnings about numeric conversions. An experienced engineer would need to: 1) Locate the relevant code in the category.py file based on the traceback, 2) Understand that the warning logic needs to check for non-empty arrays before warning about numeric data, 3) Add the simple size checks. The patch shows this is just adding `values.size and` and `data.size and` conditionals, which is a small, targeted change requiring minimal code modification.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-documented with clear reproduction steps, the solution is targeted and minimal, and the test case provided validates the fix appropriately. This is a good example of a bug report that includes sufficient technical detail for resolution.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-23299": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides a clear bug report with a specific problem: calling `matplotlib.get_backend()` removes all figures from `Gcf` if the first figure was created in an `rc_context`. The issue includes a concrete code example showing the AssertionError with before/after states of `Gcf.figs` (showing it goes from containing a figure to being empty). While the exact reproduction steps aren't provided, the bug description is specific enough that an experienced engineer could understand what's happening and work towards a solution. The core issue is that `rc_context` is somehow interfering with figure management when `get_backend()` is called, which is a well-defined problem even if some details need to be filled in through investigation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "Looking at the solution patch, this requires understanding the interaction between `rc_context`, backend management, and figure lifecycle in matplotlib. The fix involves modifying the `rc_context` function to exclude the 'backend' parameter from being reset, which requires understanding that backend changes can trigger figure cleanup. An engineer would need to: 1) Reproduce the issue and understand why `get_backend()` clears figures, 2) Trace through the rc_context implementation and figure management code, 3) Understand that backend resets are causing the figure clearing, 4) Implement the solution to prevent backend from being reset in rc_context. This involves understanding multiple interconnected systems and requires careful debugging, making it a 1-4 hour task.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is well-suited for the benchmark. The bug is specific and reproducible, the solution requires meaningful debugging and understanding of matplotlib internals, and the test case clearly validates the fix. An engineer would need to demonstrate understanding of context managers, backend management, and figure lifecycle to solve this properly.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-23476": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a specific bug: when a matplotlib figure is unpickled on M1 Mac, the DPI value doubles each time. The issue provides:\n\n1. A clear bug summary stating the core problem\n2. Detailed reproduction information showing how DPI increases exponentially (200\u2192400\u2192800\u21921600...)\n3. The exact error that occurs (OverflowError in backend_macosx.py)\n4. Complete environment details (matplotlib 3.5.2, macOS 12.4, M1 Mac, Python 3.9.12)\n5. Platform specificity (only happens on M1 MacBooks)\n\nThe expected behavior is implicitly clear: DPI should remain constant after unpickling. An experienced engineer would understand they need to prevent DPI doubling during the pickle/unpickle process, specifically in the figure's state management methods (__getstate__/__setstate__).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The problem is well-isolated to the pickle/unpickle mechanism, and the solution shown in the gold patch is quite straightforward - just 3 lines of code added to the __getstate__ method in figure.py. \n\nAn experienced engineer would need to:\n1. Understand that M1 Macs have device pixel ratio scaling that affects DPI\n2. Locate the figure's __getstate__ method where pickle state is prepared\n3. Realize that the scaled DPI should be reverted to original DPI before pickling\n4. Add the simple logic to restore _original_dpi value\n\nThe fix doesn't require deep architectural changes or complex logic - it's a targeted fix to preserve the original DPI value during serialization. The hardest part would be understanding the device pixel ratio concept and locating the right place in the code, which shouldn't take more than an hour for someone familiar with the codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained platform-specific bug with a clear reproduction case and straightforward solution. The issue would work well in a coding benchmark as it tests understanding of pickle serialization, platform-specific behaviors, and matplotlib's internal state management. The test case provided is also simple and verifies the core functionality effectively.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24026": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified. The problem is clear: stackplot doesn't support CN color aliases while other matplotlib functions do, causing a ValueError. The error message and traceback clearly show the issue occurs in stackplot.py line 73 when calling axes.set_prop_cycle(color=colors). The desired behavior is implied - stackplot should handle CN aliases like other matplotlib functions. However, there are some minor gaps: the exact desired API isn't explicitly stated (should colors parameter accept CN aliases directly, or should it work differently?), but the expectation is reasonable to infer from the context of wanting consistency with other matplotlib functions.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is localized to the stackplot function in stackplot.py. Looking at the provided patch, the solution involves: 1) Removing the problematic axes.set_prop_cycle(color=colors) call, 2) Converting colors to an itertools.cycle when provided, 3) Using axes._get_lines.get_next_color() as fallback when no colors provided, 4) Updating the fill_between calls to use next(colors) instead of axes._get_lines.get_next_color(). The logic is straightforward - instead of setting a property cycle that doesn't support CN aliases, directly iterate through colors and resolve CN aliases through the normal color system. An experienced engineer familiar with matplotlib's color handling would recognize this pattern quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, a reproducible error case, and the solution doesn't appear to require extensive domain knowledge beyond understanding matplotlib's color system and basic itertools usage.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-24149": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It provides a clear reproduction case (`ax.bar([np.nan], [np.nan])[0].get_x()`), a complete stack trace showing the exact error (StopIteration in `cbook._safe_first_finite(x0)`), and explains the context (breaks seaborn's histogram function). The user identifies that it's related to x position handling and mentions a relevant bullet point from release notes. An experienced engineer would understand they need to handle the case where `_safe_first_finite` finds no finite values and raises StopIteration, which is exactly what the provided patch does by adding try-except blocks around the calls to `_safe_first_finite` in the `_convert_dx` method.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue provides a clear stack trace pointing to the exact problem in `_convert_dx` method at line 2182 where `cbook._safe_first_finite(x0)` raises StopIteration. The solution is straightforward: add exception handling for StopIteration and fall back to using the first element unconditionally. The patch shows this is a small, localized fix adding just a few lines of exception handling code. An experienced engineer familiar with the codebase could quickly identify the problem and implement the solution.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the test case provided in the patch demonstrates the expected behavior. The issue would be suitable for evaluating coding ability as it requires understanding exception handling and fallback logic in data processing functions.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-24970": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem: NumPy 1.24 introduces deprecation warnings in matplotlib/colors.py at lines 730-732 due to out-of-bound integer conversions to uint8 arrays. The issue provides exact error messages with file paths, line numbers, and specific values (257, 256, 258) being converted to uint8. The deprecation warnings even suggest the recommended fix: using np.array(value).astype(dtype) for the old overflow behavior. The environment details (Python 3.10.9, matplotlib 3.6.2, NumPy 1.24) are also provided, making it clear this is a compatibility issue that needs to be addressed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is clearly identified with specific line numbers and the solution is straightforward - handling the NumPy deprecation warnings by adjusting how out-of-bound integer conversions are handled. Looking at the gold patch, the solution involves moving the np.errstate context manager and ensuring proper casting for both floats and unsigned integers. The changes are localized to a single function in colors.py, requiring understanding of NumPy array operations but not extensive codebase knowledge. An experienced engineer familiar with NumPy would quickly recognize this as a common deprecation issue with a well-established fix pattern.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal benchmark sample - the problem is clearly defined, the solution requires some technical knowledge of NumPy but is achievable, and success can be objectively measured by whether the deprecation warnings are eliminated while maintaining correct functionality. The test patch appropriately verifies the fix works across different data types including uint8.",
            "q2_5_confidence": 5
        }
    },
    {
        "matplotlib__matplotlib-25311": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clear: figures with draggable legends cannot be pickled, resulting in a \"TypeError: cannot pickle 'FigureCanvasQTAgg' object\". The user provides the specific error message and mentions it also affects draggable annotations. However, there are some missing details: no minimal reproducible code example is provided, the matplotlib backend is not specified (though the error suggests Qt), and there are no steps to reproduce the issue. Despite these gaps, an experienced engineer could reasonably infer that they need to make draggable legends (and likely draggable annotations) picklable by addressing the canvas reference issue. The solution approach becomes clearer when looking at the patch, which shows the problem is a direct canvas attribute reference that prevents pickling.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The engineer needs to: (1) understand how Python pickling works and why certain objects can't be pickled, (2) trace through the matplotlib codebase to find where draggable functionality stores canvas references, (3) understand the DraggableBase class in offsetbox.py and how it's used by draggable legends, (4) devise a solution that maintains functionality while enabling pickling (converting attribute to property), and (5) write appropriate tests. The actual code change is small (removing one line, adding a property), but requires significant investigation and understanding of matplotlib's architecture, draggable implementation, and pickling constraints. The solution is non-trivial as it requires recognizing that a property can be used instead of a stored attribute reference.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is technical but well-defined, the solution space is constrained (must make draggable objects picklable), and the test adequately verifies the fix. The issue represents a legitimate software engineering challenge suitable for a coding benchmark.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25332": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clear: calling `align_labels()` on a matplotlib figure makes it unpickleable, resulting in a \"TypeError: cannot pickle 'weakref.ReferenceType' object\". The issue provides a minimal error trace showing the problem occurs when calling `pickle.dumps(fig)` after `align_labels()`. However, there are some gaps: no minimal reproducible code example is provided, and the user doesn't specify which specific align_labels method they're calling (align_xlabels, align_ylabels, or both). An experienced engineer would reasonably interpret this as needing to fix the pickling issue for figures that have had their labels aligned, which matches what the actual solution does by implementing proper serialization methods for the Grouper class that handles weakref objects.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The problem requires understanding several complex concepts: (1) How matplotlib's align_labels functionality works internally and why it creates unpickleable objects, (2) Python's pickling mechanism and why weakref objects cannot be pickled, (3) How to implement custom __getstate__ and __setstate__ methods to handle serialization of objects containing weak references. The solution involves implementing proper pickle support by converting weak references to strong references during serialization and back to weak references during deserialization. This requires careful thought about object lifecycle management and understanding the internal structure of the Grouper class. While the actual code change is relatively small (about 15 lines), the debugging and research time to understand the root cause and design the appropriate solution would likely take 1-4 hours for an experienced engineer.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is a legitimate bug with a clear expected behavior (figures should remain pickleable after align_labels), and the test case properly validates the fix by calling align_ylabels() and ensuring the complete test figure can be pickled successfully.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-25479": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The description shows a clear problem: when a colormap is registered with a name different from its internal name attribute, matplotlib fails to find it during lookup. The error traceback shows that `some_cmap_name` (the colormap's internal name) is being looked up instead of `my_cmap_name` (the registered name). However, the issue description lacks the actual code that demonstrates the problem setup - we can infer from the error message and explanation that someone created a colormap, registered it with one name, but the colormap object itself had a different internal name. An experienced engineer can reasonably deduce that the solution involves ensuring name consistency between registered names and internal colormap names during the registration process.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour issue because: (1) Understanding the problem requires diving into matplotlib's colormap registration and lookup mechanisms in the `cm.py` module, (2) The engineer needs to trace through how `get_cmap()` works and why it's using the colormap's internal name instead of the registered name, (3) The solution involves modifying the `register()` method to update the colormap's internal name to match the registered name, and (4) Understanding the implications requires knowledge of colormap copying, equality comparisons, and the overall colormap system. While the actual code change is relatively small (a few lines), the investigation and understanding phase would take substantial time for someone not already familiar with matplotlib's internals.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is technically sound and the solution involves core matplotlib functionality that would be valuable to test. The issue demonstrates good understanding of object-oriented design principles and internal state management.",
            "q2_5_confidence": 4
        }
    },
    {
        "matplotlib__matplotlib-26291": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. The bug report includes a complete error traceback that shows exactly where the failure occurs (AttributeError: 'NoneType' object has no attribute '_get_renderer' in the inset_locator.py file at line 399). The issue provides specific context about what the user was trying to do (create inset axes following the first example on the matplotlib website), includes the full error stack trace, and provides relevant system information (OS, matplotlib version, backend, etc.). The error traceback clearly points to the root cause: the renderer parameter is None when get_window_extent() tries to access self.figure._get_renderer(). This gives a developer sufficient information to understand both what went wrong and where to look for a solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward bug fix that should take 15 minutes to 1 hour. The error traceback clearly points to the exact line where the issue occurs (line 399 in offsetbox.py where renderer is None). Looking at the provided patch, the solution is simple: add a null check for the renderer parameter and get the renderer from the axes figure if it's None. The fix is only 2 lines of code and follows a common pattern in matplotlib codebase. An experienced engineer would quickly identify that the issue is a missing null check and implement the same solution by following the existing pattern used elsewhere in the codebase where renderer might be None.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the test case provided in the patch demonstrates how to verify the fix works. This is a good example of a well-documented bug with a clean, minimal solution that would be suitable for evaluating coding ability.",
            "q2_5_confidence": 5
        }
    },
    {
        "psf__requests-1724": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear, reproducible error with complete stack trace showing UnicodeDecodeError when using unicode method names like u'POST', (2) Specific diagnosis pointing to the root cause in sessions.py:313 where method.upper() is called without proper string conversion, (3) Version information and confirmation the issue exists in current source. The reporter demonstrates understanding of the problem by identifying that the unicode method name \"infects\" the headers with unicode instead of keeping them as strings. An experienced engineer can clearly see that the fix requires converting the unicode method parameter to a regular string before the .upper() call, which is exactly what the provided patch does with builtin_str(method).",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the problem location (sessions.py line 313) and the root cause (unicode method names not being converted to strings). The solution is straightforward: add a string conversion before the .upper() call. An engineer would need to: (1) Understand Python 2/3 unicode compatibility (a few minutes with the codebase), (2) Locate the builtin_str utility function in the compat module (evident from existing imports), (3) Add the one-line fix. The patch shows this is indeed a simple change requiring only importing builtin_str and adding method = builtin_str(method). Most time would be spent understanding the codebase structure and testing, not solving the core problem.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an excellent benchmark sample: clear problem description, straightforward solution, realistic scenario (Unicode handling in Python 2/3 compatibility), and the test verifies the exact issue described. The solution requires understanding both the immediate problem and the codebase's compatibility patterns, making it a good test of practical debugging skills.",
            "q2_5_confidence": 5
        }
    },
    {
        "psf__requests-5414": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly describes: (1) the specific input that causes the problem (`http://.example.com`), (2) the current incorrect behavior (UnicodeError being raised), (3) the expected behavior (should raise InvalidURL instead), (4) provides the exact error message, (5) references existing similar handling in the codebase at a specific line, and (6) mentions this follows the pattern established for similar issues. The problem and desired solution are unambiguous - URLs with hostnames starting with a dot should be caught and converted to InvalidURL exceptions rather than letting UnicodeError bubble up.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix taking less than 15 minutes. The issue provides a direct reference to the relevant code location (requests/models.py line 401). Looking at the gold patch, the solution is simply adding '.' to an existing tuple check that already handles '*'. The fix requires changing one line of code: `elif host.startswith(u'*'):` becomes `elif host.startswith((u'*', u'.')):`. An experienced engineer would quickly locate the prepare_url method, understand the existing pattern for invalid URL detection, and make this minimal change. The test patch also shows this follows an established testing pattern.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. This is a well-contained bug fix with clear requirements, minimal code changes, and straightforward testing. The issue description provides sufficient context and the solution follows existing patterns in the codebase.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-3151": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but has some gaps that require reasonable interpretation. The core problem is clear: combine_by_coords incorrectly validates monotonicity for coordinates that don't vary between datasets, contradicting its own documentation. The user provides a specific error message and explains the expected behavior based on documentation. However, the issue lacks a complete minimal reproducible example - while it mentions \"running the example with yCoord = ['a', 'c', 'b']\", it doesn't show the full code that generates this error. An experienced engineer would need to infer the structure of the datasets being combined, but this is reasonably straightforward given the context and the test patch shows exactly what the example should look like.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-isolated to a specific function in combine.py where the monotonic validation logic is too broad. The fix requires understanding that the check should only apply to dimensions that actually vary between datasets (concat_dims) rather than all dimensions. The code change itself is small - just modifying the loop condition and removing an unnecessary conditional check. An experienced engineer familiar with xarray would quickly identify that the issue is in the combine_by_coords function and understand that identical coordinates across datasets shouldn't be subject to monotonicity requirements.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a solid benchmark candidate with a clear functional bug, straightforward fix, and good test coverage. The issue demonstrates understanding of library behavior and documentation inconsistencies.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-3677": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides a clear problem statement: the Dataset.merge() method fails when trying to merge a DataArray, while the top-level merge() function works fine. The issue includes a complete error traceback showing exactly where the failure occurs (/home/tegn500/Documents/Work/Code/xarray/xarray/core/merge.py, line 394, in coerce_pandas_values) and the specific error (AttributeError: 'DataArray' object has no attribute 'items'). The expected behavior is also clear - the Dataset.merge() method should work the same way as the top-level merge() function. An experienced engineer would easily understand that the problem is in the dataset_merge_method or merge_core functions not properly handling DataArray inputs, and the solution would involve converting the DataArray to a Dataset before processing.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The problem is clearly identified in the traceback - the code tries to call .items() on a DataArray object in the coerce_pandas_values function. The solution is straightforward: convert the DataArray to a Dataset before processing it. Looking at the gold patch, the fix is literally one line of code: `other = other.to_dataset() if isinstance(other, xr.DataArray) else other`. An experienced engineer would quickly identify that the merge method needs to handle DataArray inputs by converting them to Datasets first, similar to how the top-level merge function likely handles this case. The fix requires minimal code changes and no complex algorithmic thinking.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for a coding benchmark. The issue is clearly defined, the problem is easily reproducible, and the solution is straightforward but requires understanding of the codebase structure. The test case provided also clearly validates the fix by ensuring that ds.merge(da) produces the same result as xr.merge([ds, da]). An engineer would need to understand the xarray library structure and the merge functionality, but the fix itself is simple and well-contained.",
            "q2_5_confidence": 5
        }
    },
    {
        "pydata__xarray-4094": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides sufficient information to understand the problem. It clearly states that to_unstacked_dataset is broken for single-dimension variables, provides a specific error message (MergeError), and explains the use case (stacking and unstacking variables). While the description could be more detailed about the exact circumstances that trigger the bug, the error message and context provide enough information for an experienced engineer to understand what needs to be investigated. The problem is specific to single-dimension variables in the unstacking process, which narrows down the scope significantly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward bug fix that would take 15 minutes to 1 hour. The fix involves adding a single parameter (drop=True) to an existing method call in the to_unstacked_dataset function. An experienced engineer would need to: 1) Understand the error message and identify it's related to dimension handling in the sel() method, 2) Locate the to_unstacked_dataset method in the codebase, 3) Understand that the issue occurs when selecting single-dimension variables without dropping the dimension, and 4) Add the drop=True parameter. The solution is a one-line change that addresses a common pattern in xarray operations.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-contained within xarray's functionality, the fix is minimal and focused, and the test case clearly demonstrates the expected behavior. This is a good example of a targeted bug fix that can be used to evaluate debugging and problem-solving skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-4695": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: when a dimension is named \"method\", calling `.loc` throws a ValueError about invalid fill methods. The error message provides a strong hint that the dimension names are being passed to another method in \"unsanitized form.\" However, the issue lacks a complete minimal reproducible example - it doesn't show the exact code that triggers the error or the full stack trace. From the test patch, we can see this relates to DataArray.loc with dimensions named \"method\" causing parameter collision with sel() method parameters. An experienced engineer could reasonably infer that this is about keyword argument collision where dimension names conflict with method parameter names, but some detective work would be needed to fully understand the scope and create a proper reproduction case.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The gold patch shows it's a simple one-line change in the DataArray.__getitem__ method, changing from `self.data_array.sel(**key)` to `self.data_array.sel(key)`. The issue is that when dimensions have names that collide with method parameters (like \"method\"), using **key expansion passes them as keyword arguments to sel(), causing the conflict. The fix is straightforward - pass the dictionary directly instead of expanding it. An experienced engineer familiar with Python's keyword argument unpacking would quickly identify this as a parameter collision issue and find the problematic line. The main time would be spent understanding the codebase structure and locating where .loc is implemented, but the actual fix is trivial once found.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. While the original issue description could have been more detailed with a complete reproduction example, the core problem is identifiable and the solution is clean and focused. The test case appropriately verifies the fix by using a dimension named \"method\" which was the specific case mentioned in the issue. This is a good example of a subtle but important bug fix that tests understanding of Python argument passing semantics.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-6599": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides a reasonable foundation for understanding the problem. It clearly states that `polyval` with timedelta64 coordinates produces wrong results in the latest version compared to version 2022.3.0, and expects both versions to return the same results. However, the issue lacks crucial details: no code example showing the problematic behavior, no demonstration of what \"wrong results\" means specifically, and no clear specification of expected vs actual outputs. Despite these gaps, an experienced engineer familiar with xarray could reasonably infer that this is about polynomial evaluation functionality not handling timedelta64 data types correctly, and the provided patches confirm this interpretation - the fix involves properly converting timedelta64 to float in the `_ensure_numeric` function.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves a small, targeted change to the `_ensure_numeric` function in `xarray/core/computation.py`. Looking at the patch, the fix separates the handling of datetime (\"M\") and timedelta (\"m\") dtypes - previously they were both handled with `x.dtype.kind in \"mM\"`, but timedeltas need different treatment (simple `astype(float)` conversion vs the more complex `datetime_to_numeric` for datetimes). An experienced engineer would need time to: (1) reproduce the issue with timedelta64 coordinates, (2) locate the `polyval` function and trace through its execution, (3) identify that `_ensure_numeric` is incorrectly handling timedeltas, and (4) implement the dtype-specific logic. The change itself is only a few lines, but requires understanding the difference between datetime and timedelta handling.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues that would prevent using this sample. While the issue description could be more detailed with examples, the core problem is clear enough for an experienced engineer to work with. The fix is well-scoped and the test patch provides good validation of the solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "pydata__xarray-6721": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. The user reports that accessing the `chunks` attribute on a zarr-backed xarray dataset loads the entire array into memory, which is problematic for large datasets. The stack trace clearly shows the issue: in `get_chunksizes()` function in `xarray/core/common.py` line 1815, accessing `v.data` triggers loading data into memory via the `Variable.data` property, which calls `Variable.values` and ultimately `np.asarray()`. The expected behavior is that checking for chunks should not load data. The stack trace provides exact file paths, function names, and line numbers making it clear where the problem occurs. An experienced engineer can understand this is a performance issue where metadata access inadvertently triggers data loading.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The problem is clearly identified in the stack trace - line 1815 in common.py accesses `v.data` which triggers data loading. The solution shown in the gold patch is a simple one-line change: replace `v.data` with `v._data` to access the underlying data object without triggering the loading mechanism. This requires understanding the difference between the public `data` property (which loads data) and the private `_data` attribute (which gives direct access to the underlying array object). An experienced engineer familiar with the codebase would quickly identify this pattern and implement the fix. The test is also straightforward - create a dataset that would fail if data is loaded and verify chunks can be accessed without triggering the load.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly described with a detailed stack trace, the problem is well-understood (performance regression), and the solution requires understanding the codebase API patterns. The fix is simple but requires proper understanding of when data loading is triggered versus when metadata can be accessed without loading. This tests both debugging skills (following stack traces) and understanding of lazy loading patterns in scientific computing libraries.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-6386": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the short option `-v` for verbose expects an argument when it shouldn't, while the long option `--verbose` works correctly without expecting an argument. The bug report includes the exact error message (\"argument --verbose/-v: expected one argument\"), shows the expected behavior (should work like the long option), and provides version information. The problem is concrete and unambiguous - both options should behave consistently by not requiring arguments.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a relatively straightforward fix that involves: 1) Adding the short option `-v` to the preprocessing options mapping, 2) Adding metavar=\"\" to prevent argument expectations, and 3) Updating the argument parsing logic to handle short options starting with \"-\" instead of just \"--\". The changes span multiple files but are small and focused. An experienced developer familiar with argparse and the codebase structure should be able to identify and implement this fix within 15-60 minutes after understanding how the option parsing works in this codebase.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clearly defined, the solution requires understanding the codebase's argument parsing mechanism, and the fix involves making logical changes across multiple related files. The test case validates the fix appropriately by checking that the -v flag works without throwing an error. No major concerns for using this in a coding evaluation.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-6903": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: pylint crashes when running with `--jobs=0` in Kubernetes Pods because the `_query_cpu()` function returns 0, which causes multiprocessing.Pool to fail since it requires a value > 0. The issue provides specific details including: the exact function causing the problem (`pylint.run._query_cpu()`), the specific line of code where the calculation happens (line 60 with `2/1024` cast to int), the exact error message from multiprocessing, and even suggests a potential solution (`or 1` at the end of the calculation). The root cause is clearly identified: in Kubernetes environments with CPU limits, the calculation can result in 0 when a fraction of a CPU is allocated.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified and the solution is straightforward: ensure that `_query_cpu()` never returns 0 by adding a fallback to 1. Looking at the actual patch, it's exactly what was needed - just a few lines to check if `avail_cpu == 0` and set it to 1 in that case. The fix requires minimal code changes (about 7 lines added) and the logic is simple. An experienced engineer would need some time to understand the codebase context around CPU detection and multiprocessing, write the fix, and potentially add tests, but it's not a complex algorithmic change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is clear and testable, and the fix addresses a real-world scenario (Kubernetes deployments). The issue includes good reproduction context and the expected behavior is clearly stated.",
            "q2_5_confidence": 5
        }
    },
    {
        "pylint-dev__pylint-8898": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a bug where the `bad-names-rgxs` option in pylint incorrectly splits regular expressions on commas, even when those commas are part of regex quantifiers (like `{1,3}`). The issue provides a clear example of the problem, shows the exact error traceback that occurs when commas in regex quantifiers are incorrectly split, and demonstrates that the current CSV parsing logic doesn't account for commas within regex syntax. The bug report includes specific technical details like the exact error message \"missing ), unterminated subpattern\" and points to the root cause being that pylint splits on commas without considering regex syntax rules. A developer would clearly understand they need to implement a CSV parser that's aware of regex syntax to avoid splitting on commas that are part of quantifier expressions.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour task because it requires implementing a new parsing function that understands regex syntax. The developer needs to: (1) understand the existing CSV parsing logic in `_csv_transformer`, (2) design a new parser that tracks brace-delimited contexts to avoid splitting on commas within quantifiers like `{1,3}`, (3) implement the `_check_regexp_csv` function with proper state management using data structures like deque, (4) update the import/export structure in utils/__init__.py, and (5) modify the `_regexp_csv_transfomer` function to use the new parser. While not extremely complex, this requires careful thought about regex syntax rules, state tracking for nested braces, and proper integration across multiple files. The solution shown involves about 30-40 lines of new code with non-trivial logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is well-defined, the solution scope is clear, and the test cases provided demonstrate both the fix and edge cases. This is a suitable coding challenge that tests understanding of parsing logic, state management, and regex syntax knowledge.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10051": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that `caplog.get_records()` becomes decoupled from actual caplog records when `caplog.clear()` is called, resulting in frozen records that don't get cleared or updated with new records. The description provides specific technical details including references to the exact code locations (lines 699 and 345 in logging.py) and explains the root cause: that `caplog.records` gets replaced rather than cleared in `caplog.clear()`, which diverges the two objects. The expected behavior is clear - after calling `caplog.clear()`, `caplog.get_records()` should be empty and should capture new records going forward. Environment details are also provided with specific package versions.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue clearly identifies the root cause: in the `clear()` method, `caplog.records` gets replaced with a new empty list rather than being cleared in-place, which breaks the connection with `get_records()`. The solution involves modifying the `clear()` method to use `.clear()` on the existing list instead of replacing it. Looking at the patch, it adds a new `clear()` method to the handler that calls `self.records.clear()` instead of `self.records = []`, and updates the caplog's clear method to use `self.handler.clear()` instead of `self.handler.reset()`. This is a straightforward fix that requires understanding the problem and implementing a simple solution with minimal code changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is testable, and the patch shows a clean implementation. The test case provided clearly validates that the fix works correctly by checking that records are properly cleared and new records are captured after calling clear(). This is a good benchmark sample.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-10081": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a specific bug: when running pytest with --pdb flag, the tearDown() method of unittest.TestCase classes decorated with @unittest.skip at the class level gets executed when it shouldn't. The issue provides a concrete example showing the exact error (NameError: name 'xxx' is not defined) and mentions it's identical to issue #7215 but for class-level skipping rather than function-level. The expected behavior is clear: tearDown() should not be called for skipped test classes, just like it already works for skipped test functions. The reproduction case and error output make it easy to understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves modifying the runtest() method in src/_pytest/unittest.py to check if either the test method OR the parent class is skipped before deciding whether to postpone tearDown(). The fix is only 4 lines of code changes: adding a check for _is_skipped(self.parent.obj) in addition to the existing _is_skipped(self.obj) check. The logic is straightforward once you understand the existing pattern from issue #7215. An experienced engineer would need some time to locate the relevant code and understand the existing skip handling mechanism, but the actual implementation is quite simple.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with a clear problem statement, reproduction case, and targeted solution. The test patch shows comprehensive testing for both unittest.skip and pytest.mark.skip decorators, ensuring the fix works correctly. The issue is suitable for benchmarking coding ability as it tests understanding of object hierarchies, conditional logic, and debugging skills within a real codebase.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5262": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: `_pytest.capture.EncodedFile` incorrectly advertises a mode of 'rb+' (binary mode) when its `write()` method only accepts strings, not bytes. This causes an exception when external libraries like youtube-dl check the mode attribute to determine whether to write bytes or strings. The issue provides a complete stack trace showing exactly where the problem occurs, explains the root cause (youtube-dl looks for 'b' in `out.mode` to decide data type), and includes reproduction steps with specific package versions. The expected behavior is clear: the mode should not include 'b' since the EncodedFile only accepts string input to its write method.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-defined and the solution is straightforward: add a `mode` property to the `EncodedFile` class that returns the underlying buffer's mode with 'b' removed. The gold patch shows this is exactly what was done - a simple 4-line property method. An experienced engineer would need some time to understand the codebase structure, locate the `EncodedFile` class in `src/_pytest/capture.py`, understand why the mode mismatch occurs, and implement the property override. The solution requires understanding the interaction between pytest's capture mechanism and external libraries, but the actual code change is minimal and focused.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. This is a well-contained bug with a clear reproduction case, specific error message, and straightforward solution. The issue provides sufficient context about the interaction with external libraries and the expected behavior. The test case also validates the correct behavior by ensuring 'b' is not in sys.stdout.mode when using capfd.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5631": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The problem is clearly described: a ValueError occurs when collecting tests that patch an array in pytest 3.6.0 (but not 3.1.3). The error traceback points to the specific problematic line in `/usr/local/lib/python3.6/dist-packages/_pytest/compat.py:94` where `p.new in sentinels` returns an array of booleans instead of a boolean when `p.new` is an array. The issue author correctly identifies the root cause: when using @patch with `new` as an array, the equality check fails because arrays can't be evaluated in a boolean context. However, the issue doesn't provide a minimal reproducible example showing exactly how to trigger this bug, which would require some investigation to understand the specific usage pattern that causes the problem.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minute to 1 hour fix. The problem is clearly identified in the error traceback and the root cause is explained. Looking at the gold patch, the solution involves changing the comparison logic in `num_mock_patch_args` function from using `p.new in sentinels` to using identity comparison `p.new is mock_sentinel or p.new is ut_mock_sentinel`. This is a small, focused change that requires understanding the difference between equality (`in`) and identity (`is`) comparisons, and why identity comparison is needed to avoid triggering `__eq__` methods on objects like numpy arrays. An experienced engineer familiar with Python's comparison operators and the mock library would be able to identify and implement this solution relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix that tests a specific edge case with mock patching and array-like objects. The solution is clean and the test case provided in the patch clearly validates the fix.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-5787": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It provides concrete examples showing the difference in behavior between running pytest without xdist (which correctly displays the full chain of exceptions) and with xdist (-n auto) which only shows the last exception in the chain. The issue includes specific test code examples that demonstrate chained exceptions both with explicit \"from\" clauses and implicit exception chaining. The expected behavior is clear: when using xdist, the exception serialization should preserve and display the complete exception chain just like it does without xdist. The issue also provides version information for pytest (4.0.2) and pytest-xdist (1.25.0), giving important context about the environment where this occurs.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because: 1) It requires understanding how pytest serializes/deserializes exception information when distributing tests across workers with xdist, 2) The solution involves modifying the report serialization logic in _pytest/reports.py to handle ExceptionChainRepr objects, 3) Looking at the patch, it requires substantial changes across multiple functions (_report_to_json, _report_kwargs_from_json) with new logic for serializing/deserializing chained exceptions, 4) The implementation needs to handle the recursive structure of exception chains properly, 5) It requires adding new imports (ExceptionChainRepr) and understanding the existing codebase structure around exception reporting. While not the most complex issue, it's more than a simple bug fix and requires understanding distributed testing mechanics and exception representation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly described, the expected behavior is well-defined, and the test cases provided in the patch adequately verify the solution. The issue represents a legitimate bug in pytest's distributed testing functionality that affects the quality of error reporting, making it a good candidate for evaluating coding ability in debugging and fixing serialization issues.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5809": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly identifies the problem: the --pastebin feature in pytest uses lexer=\"python3\" when submitting output to bpaste.net, which causes HTTP 400 errors for some content. The issue provides a specific file reference (src/_pytest/pastebin.py#L68-73), explains the root cause, mentions it's related to issue #5764, and proposes a clear solution: change the lexer from \"python3\" to \"text\" because pytest console output is arbitrary text, not Python code. An experienced engineer would have all the information needed to implement this fix.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix that would take less than 15 minutes. The solution involves changing a single parameter value in the pastebin.py file from \"python3\" to \"text\". The issue clearly identifies the exact location of the problem and the specific change needed. The gold patch confirms this - it's literally a 4-line change removing the conditional logic for Python version and hardcoding \"text\" as the lexer. No research, complex logic, or understanding of intricate codebase details is required.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is straightforward, the solution is clear, and it's a good test of whether someone can make a simple but meaningful change to fix a reported bug. The test patch also shows this is easily verifiable.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-5840": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue describes a specific problem where pytest 5.1.2 introduced a regression related to case-insensitive file systems on Windows. The error message shows that a path \"c:\\azure\\kms\\componenttest\\python\\pisys\\conftest.py\" is having issues with module loading, specifically \"ModuleNotFoundError: No module named 'python'\". The issue mentions that paths are being \"converted to lower case\" which suggests the problem is related to how pytest handles path casing on Windows. While the issue description is brief and doesn't provide extensive reproduction steps, there's enough information to understand this is a Windows-specific path casing bug that was introduced in version 5.1.2. An experienced engineer familiar with pytest's codebase could reasonably identify that this relates to conftest loading and path resolution on case-insensitive file systems.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue because it requires understanding pytest's internal conftest loading mechanism and how it handles paths on case-insensitive file systems. The solution involves multiple files (config/__init__.py and pathlib.py) and requires knowledge of how Python path resolution works differently across operating systems. The engineer would need to: 1) Understand the unique_path function and its purpose, 2) Recognize that py.path.local and pathlib.Path handle case resolution differently, 3) Replace the custom unique_path logic with pathlib.Path.resolve(), and 4) Update the caching mechanism to use resolved paths as keys. While not extremely complex, this requires substantial understanding of the codebase and cross-platform file system behavior.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample demonstrates a real-world cross-platform compatibility problem that would be valuable for evaluating an engineer's ability to debug file system related issues and understand how different Python path libraries behave on various operating systems. The solution requires both understanding the existing code architecture and making thoughtful changes to improve cross-platform compatibility.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6197": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The core problem is clearly described: pytest 5.2.3 is trying to collect/import random __init__.py files under the current directory, which is causing failures when those files cannot be imported (like Windows-only packages on Linux systems). The error output shows a concrete example with \"foobar/__init__.py\" containing \"assert False\" that's being executed during collection. However, there are some gaps - the issue doesn't specify exactly what the expected behavior should be (should __init__.py files be completely ignored during collection? only certain ones? under what conditions?), and it doesn't provide details about what changed in version 5.2.3 that caused this regression. An experienced engineer would need to investigate the codebase and understand pytest's collection mechanism to determine the appropriate fix, but the core problem is clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix based on the complexity shown in the patches. The solution involves: 1) Understanding pytest's collection mechanism and how PyobjMixin and Module classes work, 2) Identifying why __init__.py files are being eagerly imported during collection, 3) Modifying the obj property in PyobjMixin to be lazy-loaded rather than eager, 4) Removing the __init__.py-specific logic in Module.__init__ that was setting _ALLOW_MARKERS=False, and 5) Removing the eager _mount_obj_if_needed() call from Package.collect(). The changes span multiple files and require understanding the interaction between collection, object mounting, and marker handling. While not extremely complex, it requires substantial investigation into pytest's internals and careful consideration of when objects should be loaded.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained regression bug with a clear reproduction case and specific version information. The issue provides enough context about the problem (pytest trying to import __init__.py files it shouldn't), and the test cases in the patch confirm the expected behavior. An engineer working on this would be able to reproduce the issue and validate their fix using the provided error case.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-6202": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue description is very well-specified. It clearly describes the problem: the string \".[\" is being incorrectly replaced with \"[\" in test report headlines, causing issues like \"test_boo[..[]\" becoming \"test_boo[.[]\". The author provides a concrete example of the problematic output and traces the issue through the codebase with specific file paths and line numbers. They identify the exact problematic line of code in src/_pytest/python.py at line 291: `return s.replace(\".[\", \"[\")` and propose the solution to replace it with `return s`. The author also mentions that this issue affects vscode-python test discovery. The problem is clearly defined with both the symptom (incorrect headline display) and the root cause (unnecessary string replacement in getmodpath method).",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a very simple fix that should take less than 15 minutes. The issue author has already done all the detective work - they've traced the problem to a specific line of code and identified the exact change needed. The solution is simply removing a string replacement operation (changing `return s.replace(\".[\", \"[\")` to `return s`). This is a one-line change that requires no complex logic or understanding of intricate system behavior. An experienced engineer would just need to verify the author's analysis, locate the file, make the single-line change, and run tests to confirm it works.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is straightforward, and the test patch appropriately verifies the fix by adding a test case that would fail before the fix and pass after it. The issue represents a good benchmark sample as it tests the ability to understand a specific bug report and implement a precise fix.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7205": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear. It provides a complete stack trace showing a BytesWarning occurring in src/_pytest/setuponly.py at line 69 where `tw.write(\"[{}]\".format(fixturedef.cached_param))` is called. The error message \"BytesWarning: str() on a bytes instance\" clearly indicates the problem: when `cached_param` contains bytes data, the implicit string conversion via `format()` triggers a warning. The issue author even suggests the solution: \"Shouldn't that be using `saferepr` or something rather than (implicitly) `str()`?\" This gives a clear direction for what needs to be fixed - replace the direct formatting with a safer representation method that can handle bytes objects properly.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified in the stack trace and error message, and the solution is straightforward: replace the implicit string conversion with saferepr. Looking at the gold patch, it involves: 1) Adding an import for saferepr from _pytest._io.saferepr, 2) Changing one line from `tw.write(\"[{}]\".format(fixturedef.cached_param))` to `tw.write(\"[{}]\".format(saferepr(fixturedef.cached_param, maxsize=42)))`. The main time would be spent understanding the codebase structure to locate the saferepr utility and deciding on appropriate parameters (like maxsize=42). This requires some familiarity with the pytest codebase but is not a complex algorithmic change.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue is clear, the solution is focused and testable, and it represents a realistic debugging scenario that software engineers encounter - handling edge cases with different data types. The test changes show proper validation that the fix works with bytes parameters while maintaining existing functionality.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7236": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The core problem is clearly described: when running pytest with --pdb flag, tearDown methods are being executed on skipped tests, which should not happen. The issue includes a concrete example showing the problematic behavior with error output. However, there are some missing details: (1) The test file content that reproduces the issue is referenced but not fully shown - we see the tearDown method with 'xxx' but don't see the complete test setup, (2) The skip mechanism being used isn't explicitly shown in the reproduction case, and (3) The exact expected behavior vs actual behavior could be stated more clearly. Despite these gaps, an experienced engineer familiar with pytest and unittest integration could reasonably infer what needs to be fixed: prevent tearDown execution for skipped tests when --pdb is used.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. While the problem is clear (tearDown shouldn't run for skipped tests with --pdb), the solution requires: (1) Understanding pytest's unittest integration architecture, (2) Locating where the --pdb logic intersects with test skipping logic, (3) Understanding how unittest skipping works (__unittest_skip__ attribute), (4) Finding the specific code path that handles tearDown execution during --pdb mode. Looking at the gold patch, the solution involves creating a helper function _is_skipped() and modifying the condition in runtest() method to check if a test is skipped before deciding to defer tearDown. This requires navigating through pytest's unittest.py module and understanding the interaction between pytest's pdb functionality and unittest's skip mechanism. The actual code changes are relatively small but finding the right place and understanding the logic flow takes significant investigation time.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is a legitimate bug with clear expected behavior, the reproduction case is provided, and the solution requires genuine debugging and code analysis skills that would be valuable to test in a coding benchmark.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7324": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides clear symptoms (Python interpreter crash with specific assertion failure) and includes the exact error message from Python/compile.c:3559 showing that the assertion fails when compiler_nameop encounters \"None\", \"True\", or \"False\". While the issue doesn't explicitly state what needs to be fixed in pytest's code, an experienced engineer can reasonably infer from the error that pytest is generating Python AST nodes with identifiers that conflict with Python's reserved keywords/constants. The reference to the related Python bug report provides additional context. There are some blanks to fill in about the specific implementation approach, but the core problem is identifiable.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This requires 1-4 hours because: 1) The engineer needs to understand how pytest's mark expression parsing works and how it generates Python AST nodes, 2) They must identify that the issue occurs in src/_pytest/mark/expression.py where identifiers are converted to ast.Name nodes, 3) The solution involves adding a prefix to identifiers to avoid conflicts with Python keywords, and 4) They need to modify both the AST generation and the matcher lookup logic. While the code changes are relatively small (adding IDENT_PREFIX constant and modifying two functions), understanding the mark expression system and debugging the AST generation issue requires substantial investigation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with clear symptoms and a focused solution area. The patch shows the fix is localized to one file with minimal changes, making it suitable for benchmarking. The test changes also demonstrate clear verification criteria.",
            "q2_5_confidence": 4
        }
    },
    {
        "pytest-dev__pytest-7490": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that in pytest 5.x, dynamically adding an xfail marker using `request.node.add_marker(mark)` would treat a failing test as xfailed (expected behavior), but in pytest 6.0.0rc0 it raises an AssertionError instead. The issue includes a concrete code example showing the exact problem: a test that dynamically adds an xfail marker but still fails with \"assert 0\" instead of being treated as xfailed. The expected behavior is clear - the test should be treated as xfailed when the marker is added dynamically, just like it worked in pytest 5.x. The version information and error output are provided, making it straightforward to understand what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it involves understanding pytest's internal marker evaluation system and the timing of when xfail markers are processed. Looking at the gold patch, the solution required modifying the `pytest_runtest_setup` and `pytest_runtest_call` functions in `src/_pytest/skipping.py` to re-evaluate xfail marks after test execution, since markers can be added dynamically during the test run. The engineer would need to: (1) understand how pytest's hook system works, (2) trace through the marker evaluation logic, (3) identify that the issue is timing-related (xfail evaluation happens before dynamic markers are added), and (4) implement a solution that re-evaluates markers after the test runs. This requires substantial understanding of pytest internals but isn't extremely complex once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly described with a concrete reproducible example, the expected behavior is well-defined based on how it worked in previous versions, and the solution involves standard software engineering practices around hook systems and marker evaluation. This is a good benchmark sample as it tests understanding of plugin architectures, timing issues, and backward compatibility concerns.",
            "q2_5_confidence": 5
        }
    },
    {
        "pytest-dev__pytest-7521": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes a behavioral change in pytest 6.0.0rc1 where capfd.readouterr() is converting carriage returns (\\r) to newlines (\\n). The issue provides two concrete test cases that demonstrate the problem: one from borgbackup testing and another custom test. Both show expected behavior (preserving \\r characters) versus actual behavior (converting \\r to \\n). The error messages are clear and show exactly what's happening - the assertion failures demonstrate that '\\r' characters are being converted to '\\n'. This gives an engineer a precise understanding of what needs to be fixed: the capture mechanism should preserve original line endings instead of normalizing them.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the provided patch, this is a relatively simple fix that takes 15 minutes to 1 hour. The solution involves adding a single parameter 'newline=\"\"' to the TextIOWrapper constructor in src/_pytest/capture.py. Once an engineer understands that the issue is related to Python's text mode file handling (which automatically converts line endings), the fix is straightforward. The main time would be spent: (1) understanding the capture mechanism in pytest's codebase, (2) identifying that TextIOWrapper with default newline handling is causing the conversion, and (3) knowing that newline=\"\" preserves original line endings. The actual code change is minimal - just one line addition.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of Python's text I/O behavior, debugging skills to trace the issue to the capture mechanism, and knowledge of TextIOWrapper parameters. The issue description provides clear expected vs actual behavior, making it suitable for automated testing.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10297": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly states that RidgeClassifierCV is missing the store_cv_values parameter that should exist according to the documentation. The problem is concrete: (1) Documentation mentions cv_values_ attribute depends on store_cv_values=True, (2) Attempting to use store_cv_values parameter results in \"TypeError: __init__() got an unexpected keyword argument 'store_cv_values'\", and (3) The expected behavior is that this parameter should work like it does in the related RidgeCV class. The issue provides clear version information and specific error messages, making it straightforward to understand what needs to be implemented.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves: (1) Adding store_cv_values parameter to RidgeClassifierCV.__init__() with default False, (2) Passing this parameter to the parent class constructor, and (3) Adding documentation for the parameter. Looking at the patch, this requires minimal code changes - just adding one parameter to the constructor signature and docstring, and passing it through to the parent class. The pattern already exists in RidgeCV, so it's mostly copying existing implementation patterns. An experienced engineer familiar with the codebase could implement this quickly once they understand the inheritance structure.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained, has a clear solution pattern to follow from the existing RidgeCV implementation, and the test patch shows exactly what functionality needs to be verified. This is a good benchmark sample as it tests understanding of class inheritance, parameter passing, and API consistency in scikit-learn.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-10908": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: when a CountVectorizer is initialized with a vocabulary parameter, calling get_feature_names() raises a NotFittedError even though it shouldn't need to. The issue explains that transform() works correctly in this scenario because it calls _validate_vocabulary() which sets vocabulary_, but get_feature_names() doesn't do this validation step. The expected behavior is clearly stated - get_feature_names() should work when vocabulary is provided at initialization, just like transform() does. The technical details are sufficient to understand both the current problematic behavior and the desired correct behavior.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a straightforward fix that should take 15 minutes to 1 hour. The issue clearly identifies that the problem is in the get_feature_names() method of CountVectorizer, and points to the solution pattern already used in the transform() method. Looking at the gold patch confirms this - it's just adding 3 lines of code to call _validate_vocabulary() if vocabulary_ hasn't been set yet. The fix requires understanding the existing code pattern and applying it consistently, but doesn't require complex logic or extensive refactoring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for a coding benchmark. The issue is clearly described, the solution is focused and well-scoped, and the fix demonstrates understanding of object lifecycle and consistency patterns in API design. The test patch shows comprehensive coverage including both the bug fix and related edge cases.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-12585": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the `clone` function fails when parameters are estimator types (classes) rather than instances. The issue provides a specific error traceback showing exactly what goes wrong - `TypeError: get_params() missing 1 required positional argument: 'self'` occurs because the code tries to call `get_params()` on a class rather than an instance. The author explains their use case (storing wrapped estimators as classes in sklearn-xarray), provides the exact location where the fix should be applied (base.py line 51), and even suggests the specific code change needed: `elif not hasattr(estimator, 'get_params') or isinstance(estimator, type):`. The problem, location, and solution approach are all clearly defined.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified and the solution is straightforward - adding `or isinstance(estimator, type)` to an existing conditional check. The fix involves understanding that when `estimator` is a type/class rather than an instance, calling `get_params()` will fail because it expects `self`. The solution is to detect this case and handle it appropriately by using the existing fallback path. While it requires understanding the clone function logic and thinking about the implications, it's a simple one-line change that addresses a specific edge case.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-documented bug with a clear reproduction case, specific error message, and targeted fix. The test case shows the expected behavior (that cloning should work with estimator types). The issue represents a legitimate edge case that needs handling.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13135": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: KBinsDiscretizer with strategy='kmeans' fails because the centers (and consequently bin_edges) are unsorted, which causes np.digitize to fail with a ValueError requiring monotonically increasing or decreasing bins. The issue includes a complete error traceback showing exactly where the failure occurs (line 255 in _discretization.py with np.digitize), the specific error message, and the context (using 5 bins with kmeans strategy). The version information is also provided. An experienced engineer would understand that the solution needs to ensure bin_edges are sorted before being passed to np.digitize.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is clearly identified: unsorted centers from k-means clustering cause unsorted bin_edges, which breaks np.digitize. The solution is straightforward - sort the centers after getting them from k-means. Looking at the gold patch, it's literally a 2-line addition: a comment and centers.sort(). The fix location is clear from the error traceback pointing to the transform method using np.digitize with bin_edges. An experienced engineer would quickly identify that the issue is in the fit method where centers are computed, and the fix is simply ensuring they're sorted.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear problem statement, obvious solution approach, and straightforward implementation. The test patch appropriately adds a test case for 5 bins to ensure the fix works and prevent regression.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13142": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is reasonably well-specified but has some gaps that require interpretation. The core problem is clearly stated: when n_init>1 in GaussianMixture, fit_predict(X) and predict(X) return different results, which they shouldn't. The issue provides concrete evidence with a traceback showing an AssertionError from assert_array_equal(c1,c2) with 88.6% mismatch between arrays. However, there are some details missing: the issue doesn't show the actual test code that reproduces the problem (only mentions \"test_gm.py\" line 17), and doesn't explain what the expected behavior should be beyond that the two methods should agree. An experienced engineer would reasonably interpret that fit_predict() and predict() should return identical cluster assignments when called on the same data, which is a standard expectation for ML library consistency.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires understanding the internal workings of the GaussianMixture class, specifically the interaction between n_init (multiple random initializations), fit_predict(), and predict() methods. The engineer would need to: 1) Reproduce the issue by creating test cases with n_init>1, 2) Trace through the fit_predict() method to understand why it differs from fit().predict(), 3) Understand that fit_predict() runs multiple initializations but may not be using the same final model state that predict() would use, 4) Identify that the final e-step needs to be performed after setting the best parameters rather than before. The solution involves moving a few lines of code but requires significant investigation to understand the root cause and ensure the fix doesn't break other functionality.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the expected behavior is clear from ML library standards, and the provided patches show both the fix and appropriate test cases. The issue is suitable for benchmarking coding ability as it tests understanding of ML algorithms, debugging skills, and code organization.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-13328": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear description of the problem (TypeError with boolean predictors), (2) Complete stack trace showing exactly where the error occurs, (3) Specific error message indicating the numpy boolean negative operator issue, (4) Version information for reproducibility, (5) The error points to a specific line in huber.py and the exact numpy operation causing the problem. An experienced engineer would understand this is a data type handling issue where boolean inputs need to be converted to float types before processing.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The error message clearly indicates the problem (inappropriate use of `-` operator with boolean arrays), and the stack trace points to the exact location. The solution requires understanding that boolean inputs need type conversion to float before mathematical operations. The fix involves adding a dtype parameter to check_X_y() to ensure proper type conversion. It's a small, targeted change that requires some understanding of the codebase and numpy data types, but not extensive research or major code restructuring.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is an ideal benchmark problem with a clear error, specific location, and straightforward solution that tests understanding of data type handling in scientific computing.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-13779": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description provides clear information about the problem: a voting estimator fails when weights are passed and an estimator is None, resulting in an AttributeError. The error message \"'NoneType' object has no attribute 'fit'\" clearly indicates the root cause. However, there are some gaps that require interpretation: (1) The specific context of \"voting estimator\" - while this likely refers to VotingClassifier/VotingRegressor in sklearn.ensemble, this isn't explicitly stated; (2) The exact scenario when \"an estimator is None\" occurs - though the test patch shows this happens via set_params(lr=None); (3) The location in the codebase isn't specified, though the gold patch shows it's in sklearn/ensemble/voting.py. Despite these gaps, an experienced developer familiar with scikit-learn could reasonably infer the requirements and implement a solution that checks for None estimators before calling fit methods.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that should take 15 minutes to 1 hour. The solution involves adding a simple None check before attempting to call methods on estimators. From the gold patch, the fix is just 2 lines of code (if step is None: continue) added to an existing loop. An experienced engineer would need some time to: (1) locate the relevant code in the voting estimator implementation, (2) understand the flow where sample_weight support is checked, and (3) add the appropriate None check. The logic is straightforward once the location is identified, but requires some exploration of the codebase to understand the voting estimator structure and where the error occurs.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined with a clear error message and reproducible scenario. The test patch provides good validation that the fix works correctly for both VotingClassifier and VotingRegressor. The issue represents a realistic bug that could occur in practice when users set estimators to None and pass sample weights.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14053": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some gaps but provides enough information for a sensible interpretation. It clearly states that `export_text` throws an `IndexError: list index out of range` when there is a single feature, and provides version information showing this is in scikit-learn 0.21.1. However, it lacks crucial details like: (1) A minimal reproducible example showing how to trigger the error, (2) The complete stack trace showing where exactly the IndexError occurs, (3) What the expected behavior should be instead of the error. Despite these missing details, an experienced engineer could reasonably infer that the problem occurs in the `export_text` function when handling decision trees with only one feature, and that the expected behavior would be successful text export without errors.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the provided patch, the solution is quite straightforward - it adds a simple conditional check to handle undefined tree features. The core issue is that when there's only one feature, some tree nodes may have undefined features (represented by _tree.TREE_UNDEFINED), and the original code was trying to access feature_names[i] where i could be this undefined value, causing an IndexError. The fix adds a conditional to check if i != _tree.TREE_UNDEFINED before accessing the feature_names array. While reproducing the issue might take some time due to the lack of a clear example in the issue description, once reproduced, the fix itself is a small, targeted change requiring only basic understanding of the tree export logic.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues that would prevent using this sample. The issue is well-suited for a coding benchmark as it represents a realistic bug that could occur in practice, has a clear pass/fail criteria (does export_text work with single-feature trees?), and requires understanding of both the scikit-learn tree structure and defensive programming practices. The test case provided in the patch gives a clear specification of the expected behavior.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14087": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The description clearly shows that LogisticRegressionCV with refit=False throws an IndexError with a specific stack trace pointing to line 2194 in logistic.py. The error message \"too many indices for array\" and the stack trace provide clear indication of where the problem occurs. However, the issue description doesn't explain the root cause or what the expected behavior should be when refit=False. An experienced engineer would need to understand that when refit=False, the model should not refit on the full dataset but should still properly handle coefficient extraction from cross-validation paths. The provided versions and minimal reproduction case (LogisticRegressionCV(refit=False).fit(X, y)) make it reproducible, though the actual X, y data is not provided.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) The engineer needs to understand the LogisticRegressionCV implementation and how cross-validation coefficient paths work, (2) Debug why the array indexing fails when refit=False, (3) The actual fix involves multiple changes: changing self.multi_class to multi_class variable, adding conditional logic for elasticnet penalty when handling l1_ratio_, and ensuring proper array dimensions. (4) The engineer needs to understand the difference between binary/multiclass scenarios and how ovr vs multinomial affects coefficient extraction. (5) Testing requires understanding various combinations of penalty types and multi_class settings. While the core changes are relatively small (about 10 lines), the debugging and understanding of the cross-validation logic requires significant investigation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is reproducible, the error is clear, and the fix involves understanding scikit-learn's internal cross-validation logic which is appropriate for evaluating coding ability. The solution requires both debugging skills and understanding of machine learning library internals.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-14629": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear what is required for a successful solution. The reporter clearly identifies: (1) The specific problem: AttributeError when using cross_val_predict(method='predict_proba') with MultiOutputClassifier, (2) The root cause: the code tries to access estimator.classes_ but MultiOutputClassifier doesn't have this attribute, (3) The location of the problematic code with a specific GitHub link to line 857-866 in _validation.py, (4) The suggested solution approach: use mo_clf.estimators_[i].classes_ instead, and (5) Provides the exact error message. An experienced engineer would have enough information to understand that MultiOutputClassifier needs a classes_ attribute that aggregates the classes_ from its individual estimators to be compatible with cross_val_predict.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution requires adding a classes_ attribute to MultiOutputClassifier that aggregates the classes_ from individual estimators. Looking at the gold patch, the fix involves overriding the fit method to set self.classes_ = [estimator.classes_ for estimator in self.estimators_] after calling super().fit(). This is a straightforward implementation that requires understanding the MultiOutputClassifier structure and how it wraps individual estimators, but doesn't require extensive research or complex logic. An experienced engineer familiar with scikit-learn patterns could implement this relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is clearly defined, the solution is straightforward, and the test patch shows that proper tests can be written to verify the fix. This is a good benchmark sample as it tests understanding of object-oriented design patterns, attribute management, and API compatibility in machine learning libraries.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14710": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: HistGradientBoostingClassifier fails when using string targets with early stopping enabled because the scorer receives integer-encoded y_true but original string y_pred values, causing a type mismatch. The issue includes a complete error traceback showing the exact failure point in _check_targets() where numpy.union1d() fails due to comparing strings and floats. The description explains the root cause - that y_true needs to be encoded back to original classes before scoring. A potential resolution is even provided showing exactly where the fix should go in gradient_boosting.py. The problem location, cause, and solution approach are all clearly documented.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-60 minute fix. The problem is clearly identified in the issue description with a complete stack trace pointing to the exact failure location. The root cause is well-explained: during early stopping, y_true values are integer-encoded but y_pred values are original string classes, causing a type mismatch in the scorer. The solution involves adding 2-4 lines of code in the _check_early_stopping_scorer method to decode the integer targets back to original string classes using self.classes_[]. The suggested fix in the issue description is almost exactly what's needed. An experienced engineer would need minimal time to understand the codebase context, verify the issue, and implement the straightforward solution.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug with a clear problem statement, obvious failure mode, and straightforward solution. The issue would make an excellent benchmark sample as it tests understanding of machine learning pipeline internals, target encoding/decoding, and debugging skills. The solution requires minimal code changes but demonstrates understanding of how classifiers handle string targets internally.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-14894": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clear what is required for a successful solution. The problem statement clearly identifies that there is a ZeroDivisionError occurring in the _sparse_fit function in sklearn/svm/base.py at line 302, specifically when support_vectors_ is empty and the code tries to divide dual_coef_indices.size by n_class. The error traceback provides the exact location of the issue, and the context makes it clear that this happens with sparse data when support_vectors_ attribute is empty. An experienced engineer would understand they need to add a check for the case where support_vectors_ is empty (n_SV = 0) to prevent the division by zero error. The solution approach is evident from the error description and traceback.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is straightforward - a division by zero error that occurs when support_vectors_ is empty. Looking at the gold patch, the solution is relatively simple: add a conditional check for when n_SV (number of support vectors) is 0, and handle that case by setting dual_coef_ to an empty sparse matrix. The fix involves adding just a few lines of code with an if-else block. An experienced engineer familiar with the codebase would quickly identify the problem location from the traceback, understand that they need to handle the empty support vectors case, and implement the appropriate conditional logic. The solution doesn't require extensive research or complex algorithmic changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is relatively straightforward, and the test case provided in the patch confirms the fix works correctly. This is a good example for a coding benchmark as it tests ability to debug from error messages, understand the codebase structure, and implement appropriate error handling.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25747": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified with some blanks to fill in. The user clearly describes: (1) They want to use pandas transform output with a custom transformer in FeatureUnion that aggregates data, (2) This combination produces a ValueError about length mismatch, (3) It works fine with default numpy output. The error traceback shows the exact problem occurs in _wrap_in_pandas_container when trying to set the index, with a length mismatch (4 vs 96 elements). However, the issue lacks a complete minimal reproducible example - we don't see the definition of MyTransformer() or the data variable, though from context and the patch we can infer MyTransformer aggregates data (reducing rows). An experienced engineer could reasonably interpret this as needing to fix the pandas output wrapping logic to handle cases where transformers change the number of rows.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is localized to the _wrap_in_pandas_container function in sklearn/utils/_set_output.py. The error traceback clearly points to the issue: when the input is already a DataFrame, the function tries to set a new index that has a different length than the transformed data. The solution is straightforward - simply avoid setting the index when the input data is already a DataFrame (since the transformer should handle its own indexing). This requires understanding the error, locating the specific function, and making a small logical change (removing 2 lines of code that force index assignment). The fix is conceptually simple and doesn't require deep architectural changes or extensive codebase knowledge.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined enough with a clear error traceback, the solution is localized to a specific function, and the provided test case demonstrates the exact scenario that needs to be handled. This is a good candidate for evaluating debugging and problem-solving skills.",
            "q2_5_confidence": 4
        }
    },
    {
        "scikit-learn__scikit-learn-25931": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes: 1) The exact bug - a warning \"X does not have valid feature names, but IsolationForest was fitted with feature names\" occurs when fitting IsolationForest with a pandas DataFrame and non-default contamination parameter, 2) The specific conditions that trigger it (contamination != \"auto\"), 3) Why the warning is unexpected (X does have valid feature names and it's raised during fit(), not predict()), 4) The root cause analysis pointing to line 337 in the codebase where predict is called on training data to determine offset_, and 5) A reproducible example showing the warning. The issue provides enough technical detail for an engineer to understand both the problem and its likely location in the code.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15min-1hour fix. The issue clearly identifies the root cause (line 337 calling predict on training data during fit), and the solution shown in the patch is straightforward: create a private _score_samples method that bypasses input validation (which strips feature names), and call that instead of the public score_samples method during fit. The fix involves refactoring one method into two parts and changing one line of code. An experienced engineer familiar with the codebase could identify this solution and implement it within an hour.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined, the solution is focused and testable, and the test case provided is clear and specific. This would make a good benchmark sample as it tests understanding of sklearn's input validation system and the ability to refactor code to avoid unnecessary validation while preserving functionality.",
            "q2_5_confidence": 5
        }
    },
    {
        "scikit-learn__scikit-learn-25973": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The reporter clearly states that SequentialFeatureSelector should accept an iterable of splits for the cv parameter (according to documentation), but currently fails when passed splits from a cross validator. They provide a concrete error traceback showing an IndexError in _aggregate_score_dicts. However, the issue description lacks a minimal reproducible example showing exactly how they're generating the splits variable, which would be helpful for understanding the specific use case. Despite this, an experienced engineer can infer from the context, error traceback, and the provided patches that the problem is related to the cv parameter not being properly processed through check_cv() before use, which is a common pattern in scikit-learn.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves importing check_cv and is_classifier, calling check_cv() to properly validate and convert the cv parameter, and passing the processed cv object through the method chain. The changes are minimal (about 10 lines of code changes) and follow established patterns in scikit-learn. An experienced engineer familiar with scikit-learn would recognize this as a standard cv parameter handling issue. The main time would be spent understanding the codebase structure and ensuring the fix follows the library's conventions, but the actual implementation is straightforward.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The sample is suitable for evaluation as it tests understanding of scikit-learn's cross-validation parameter handling patterns, proper error analysis from stack traces, and knowledge of the check_cv utility function. The test case provides good validation that the fix works correctly with generator-style cv objects.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-10323": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is moderately well-specified. The user describes a specific problem with the literalinclude directive in Sphinx where using `prepend` or `append` options combined with `dedent` results in incorrect indentation formatting and produces warnings. They provide a concrete example showing the problematic RST code and the resulting warning message. The user also suggests a potential solution direction - that `dedent` should only be applied to the literalinclude content and not to the `prepend` and `append` content. However, there are some gaps: the expected behavior isn't completely explicit, and the exact desired output format could be clearer. An experienced engineer would need to infer some details about the correct formatting behavior, but there's enough information to understand the core problem and work toward a reasonable solution.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. Looking at the gold patch, the solution involves reordering the filter functions in the `read` method of `LiteralIncludeReader` class. The fix is quite simple - moving the `dedent_filter` to be applied before the `prepend_filter` and `append_filter` rather than after them. This ensures that dedenting only affects the original file content and not the prepended/appended text. The change is minimal (just reordering 3 lines in a list) but requires understanding the filter pipeline and the order of operations. An experienced engineer would need some time to understand the codebase structure, locate the relevant code in `sphinx/directives/code.py`, and understand how the filters work, but the actual implementation is straightforward once the problem is understood.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good benchmark sample as it tests understanding of filter pipelines and the order of operations in text processing. The issue is clear enough to solve, the solution is non-trivial but achievable, and the test case would effectively validate whether the fix works correctly. The problem domain (documentation generation with Sphinx) is common enough that it represents a realistic coding scenario.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-10673": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some blanks to fill in. The problem is clearly described: when adding `genindex`, `modindex`, and `search` to a toctree directive in Sphinx, warnings are generated saying these documents don't exist. The desired solution is also clear - these directives should work without generating warnings. However, the issue doesn't explain what these special documents are (they're auto-generated index pages in Sphinx), or provide details about where in the codebase the fix should be implemented. An experienced engineer familiar with Sphinx would understand these are built-in generated documents that should be recognized by the toctree system, but someone less familiar might need to do research to understand the full context.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix for several reasons: 1) It requires understanding Sphinx's architecture, specifically how toctree processing works and how generated documents (genindex, modindex, search) are handled. 2) The solution involves modifying multiple files (sphinx/directives/other.py, sphinx/environment/adapters/toctree.py, sphinx/environment/collectors/toctree.py) to properly recognize generated docnames. 3) The engineer needs to understand where generated document names are stored (env.domains['std'].initial_data['labels']) and how to integrate this with existing document validation logic. 4) The fix requires careful consideration of different code paths where document existence is checked. While not extremely complex, it requires substantial understanding of the codebase and touches multiple interconnected components.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined within the Sphinx ecosystem, the solution approach is clear from the patch, and the test coverage demonstrates the fix works correctly. This is a good benchmark sample for evaluating understanding of documentation tool internals and cross-module integration.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7440": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The error message clearly indicates a \"duplicate term description of mysql\" in a glossary file, and the context suggests this is related to Sphinx documentation generation. The bug report mentions it's about \"duplicate term with a different case\" in the title, which combined with the error message suggests the issue is that Sphinx is treating terms with different cases (e.g., \"mysql\" and \"MySQL\") as duplicates when they shouldn't be. An experienced engineer could reasonably infer that the solution involves fixing how Sphinx handles case sensitivity for glossary terms, though the exact implementation details would need to be figured out.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a relatively straightforward fix that involves removing case normalization in two specific places in the Sphinx codebase. The changes are: (1) removing .lower() call when noting glossary terms, and (2) removing lowercase=True parameter from XRefRole for terms. This would take 15 minutes to 1 hour for an experienced engineer because while the fix itself is simple (just a couple of parameter/method changes), it requires understanding how Sphinx processes glossary terms and cross-references, and finding the right locations in the codebase where case normalization is happening. The engineer would need to trace through the code to understand the flow from glossary processing to term storage and referencing.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear technical problem and straightforward solution. The test changes show exactly what behavior is expected (preserving case in glossary terms), making it suitable for evaluation purposes. The issue doesn't require domain-specific knowledge beyond understanding how documentation tools handle glossaries.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-7462": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It provides: (1) A clear reproduction case showing the exact code that triggers the bug: `def foo() -> Tuple[()]:` with an empty tuple type annotation, (2) The exact error message: `IndexError: pop from empty list` occurring in `sphinx/domains/python.py`, line 112, in the `unparse` function, (3) Complete environment details including OS, Python version (3.8.0), and Sphinx version (3.0.1), (4) A link to the original mypy issue that documents this empty tuple notation syntax. The bug is clearly an edge case where the code assumes tuples always have elements and tries to pop from an empty list when processing empty tuple annotations.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is a classic edge case bug where the code assumes tuples always have elements. Looking at the gold patch, the solution involves adding a simple conditional check `if node.elts:` before the existing logic that processes tuple elements and adds commas, then provides an alternative path for empty tuples that returns `[addnodes.desc_sig_punctuation('', '('), addnodes.desc_sig_punctuation('', ')')]`. The same pattern is applied to both `sphinx/domains/python.py` and `sphinx/pycode/ast.py`. An experienced engineer would quickly identify this as a missing edge case handling and implement the fix relatively quickly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is an excellent sample for a coding benchmark. The issue is clear, the reproduction case is straightforward, and the solution requires understanding the codebase logic while implementing proper edge case handling. The fix demonstrates good software engineering practices (checking for empty collections before operations like pop()) and the test cases show exactly what behavior is expected.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-7985": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description has some blanks but provides a sensible interpretation of what's required. The title clearly states \"linkcheck could also check local (internal) links\" and the problem section confirms that linkcheck currently doesn't check local links but this would be useful. The error log shows concrete evidence - it displays \"(line 14) -local- doesntexist\" indicating that local links are detected but not properly validated. While the issue doesn't explicitly specify all implementation details (like how to handle relative paths, what constitutes a valid local file, etc.), the core requirement is clear: extend the linkcheck functionality to validate local/internal links instead of just marking them as unchecked. An experienced engineer could reasonably infer that local links should be checked for existence in the filesystem and report broken vs working status accordingly.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This falls into the 1-4 hours category because it requires: (1) Understanding the existing linkcheck.py codebase and how URI checking currently works, (2) Designing logic to distinguish between local file paths and unsupported URI schemes (the solution shows a regex approach), (3) Implementing file existence checking with proper path resolution relative to source directory, (4) Integrating with the existing ignore patterns system, (5) Testing the solution across different scenarios. The code changes span multiple logical areas and require understanding of file system operations, regex patterns, and the existing architecture. While not extremely complex, it's more substantial than a simple one-line fix and requires thoughtful design decisions about edge cases.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The problem is well-scoped within Sphinx's linkcheck functionality, has clear success criteria (local links should be validated), and the test patches demonstrate concrete expected behaviors. The solution is testable and the scope is reasonable for a coding benchmark.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-8269": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified and clearly describes the problem. It explains that when `linkcheck_anchors` is True, the linkcheck command incorrectly reports \"Anchor not found\" even when the actual issue is an HTTP error status code (e.g. 404, 500) from the server. The bug description is precise: the tool should report the HTTP error instead of masking it with an anchor-related error message. The environment details and configuration setting (`linkcheck_anchors`) are provided. An experienced engineer would understand that they need to modify the linkcheck functionality to properly handle and report HTTP status errors before checking for anchors.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "Looking at the gold patch, this is a small fix that adds a single line `response.raise_for_status()` in the linkcheck.py file. This is a straightforward solution that requires understanding how HTTP requests work in Python and knowing that `raise_for_status()` will raise an exception for bad status codes. An experienced engineer familiar with the requests library would likely identify this solution within 15-60 minutes after understanding the codebase structure and locating the relevant linkcheck code. The fix is conceptually simple - check for HTTP errors before proceeding to check anchors.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear problem statement and straightforward solution. The test patch shows comprehensive testing that validates the fix works correctly. The issue is suitable for benchmarking coding ability as it tests understanding of HTTP error handling, the requests library, and the ability to navigate a codebase to find the right place to make the change.",
            "q2_5_confidence": 5
        }
    },
    {
        "sphinx-doc__sphinx-8551": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some domain knowledge to fully understand. It clearly describes a problem where `:type:` and `:rtype:` fields in Sphinx documentation are generating false \"ambiguous class lookup warnings\" when they should resolve to classes in the current module context (mod.submod.A) rather than searching all modules. The expected behavior is specified (no warnings, resolve to mod.submod.A), and concrete warning messages are provided. However, the issue assumes familiarity with Sphinx's cross-reference system, Python documentation fields, and module resolution mechanisms. An experienced engineer with some time to understand the Sphinx codebase could reasonably interpret what needs to be fixed - the implicit xrefs created by these info fields need to respect module context like explicit xref roles do.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because it requires: (1) Understanding Sphinx's complex cross-reference resolution system and how info fields like :type: and :rtype: work differently from explicit xref roles, (2) Identifying that the issue is in the docfields.py module where info fields are processed, and that context information (py:module, py:class) needs to be passed through, (3) Modifying the make_xref call to include environment context and updating the PyXRefRole.make_xref method to preserve module/class context. The solution involves editing 2 files with targeted changes (~5 lines total), but requires significant research into Sphinx's architecture to understand where and how to pass the missing context information through the cross-reference creation pipeline.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within Sphinx's cross-reference system, the solution is clean and targeted, and the test adequately verifies the fix. While domain knowledge of Sphinx is helpful, an experienced engineer should be able to understand the codebase sufficiently to implement this fix within the expected timeframe.",
            "q2_5_confidence": 4
        }
    },
    {
        "sphinx-doc__sphinx-9711": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clearly described: the `needs_extensions` check in Sphinx compares version strings lexicographically rather than semantically, causing issues where version \"0.10.0\" is considered less than \"0.6.0\" when compared as strings. The bug report provides a concrete example with sphinx-gallery showing the error message. However, there are some gaps - the issue doesn't specify exactly which file contains the problematic code or the exact function name, though an experienced engineer could reasonably locate the `needs_extensions` functionality in the Sphinx codebase. The expected behavior is clear: semantic version comparison should be used instead of string comparison.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The problem is well-understood (string vs semantic version comparison), and the solution is straightforward - replace string comparison with proper version parsing using a library like packaging.version. Looking at the provided patch, it's a focused change to one function in one file, adding proper version comparison logic with fallback to string comparison for invalid versions. An experienced engineer would need some time to locate the right function and understand the codebase structure, but the actual fix is relatively simple and involves adding ~15 lines of code.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark sample. The issue represents a real-world bug with clear symptoms and a well-defined solution. The problem domain (version comparison) is common in software engineering, and the solution requires understanding both the specific issue and general best practices for version handling. The test coverage provided in the patch is appropriate and would effectively validate a correct solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-13372": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. The core problem is clear: there's an UnboundLocalError where 'reprec' is referenced before assignment in the evalf.py file. The stack trace provides specific location information (line 1308 in evalf function). The reporter also provides a helpful hint about the likely solution: adding an \"else: raise NotImplementedError\" clause to the elif statements that define reprec and imprec. However, the issue description lacks some context - it mentions that this error appeared \"after changing the order of Mul args\" in PR #13059, but doesn't provide the specific reproduction case. An experienced engineer would need to understand the codebase structure and potentially create test cases to reproduce the issue, but the error message and suggested fix provide enough direction for a meaningful solution attempt.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 minutes to 1 hour fix. The issue is a classic UnboundLocalError where variables (reprec and imprec) are referenced before being assigned in certain code paths. The stack trace clearly points to the problematic location (evalf.py line 1308), and the reporter provides a specific suggestion for the fix. Looking at the actual patch confirms this is a simple addition of two \"else: raise NotImplementedError\" clauses to handle cases where the elif conditions don't match. An experienced engineer familiar with Python would quickly recognize this error pattern and implement the fix. The main time would be spent understanding the surrounding code context and writing a test case to verify the fix works correctly.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a good example of a clear bug report with specific error information and a reasonable solution path. The test case provided in the patch (NS(Mul(Max(0, y), x, evaluate=False).evalf()) == 'x*Max(0, y)') helps validate that the fix works correctly. The issue is self-contained and doesn't require deep domain knowledge beyond basic Python error handling.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-13480": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly identifies: (1) A specific error - NameError: name 'cotm' is not defined, (2) The exact file and line number where it occurs (hyperbolic.py, line 590), (3) The function that triggers it (.subs on coth(log(tan(x)))), (4) Specific failing test cases (integral values 2, 3, 5, 6, 8, 9, 11, 12, 13, 15, 18). The error message and location provide enough information for an experienced engineer to identify that this is a simple variable name typo - 'cotm' should be 'cothm' based on the context and the patch shows this is exactly the case.",
            "q2_1_difficulty": 0,
            "q2_2_explanation": "This is a trivial fix taking less than 15 minutes. The issue is a simple typo where 'cotm' should be 'cothm' on line 590. An experienced engineer would: (1) Look at the error location, (2) See that 'cotm' is undefined but 'cothm' is defined a few lines above, (3) Recognize this is a variable name typo, (4) Make the one-character fix changing 'cotm' to 'cothm'. The patch confirms this is exactly a single character change. No research, complex logic, or multiple file changes are needed.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a straightforward bug fix that tests well - the error is easily reproducible with the given examples, and the fix is a simple typo correction. The test cases provided in the issue description give clear examples to verify the fix works correctly.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-13877": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some blanks to fill in. The core problem is clear: a TypeError \"Invalid NaN comparison\" occurs when computing the determinant of a matrix with symbolic entries using the Bareiss algorithm. The traceback provides a detailed call stack showing the error originates from comparing coefficients in sympy/core/exprtools.py. The issue author also provides a minimal reproducing example: `f = lambda n: det(Matrix([[i + a*j for i in range(n)] for j in range(n)]))` and notes that the Bareiss algorithm may not be suitable for non-integer matrices. However, some details are left to interpretation, such as exactly what constitutes a proper fix (should the algorithm be changed, should better pivot selection be used, etc.). An experienced engineer could reasonably interpret this as needing to fix the NaN comparison issue in the determinant computation.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The problem requires understanding the Bareiss algorithm implementation, the pivot selection mechanism, and how symbolic mathematics comparisons work in SymPy. The fix involves modifying the pivot selection in the bareiss function to use a more robust zero-checking function (_is_zero_after_expand_mul) that can handle symbolic expressions without triggering NaN comparisons. This requires understanding both the mathematical algorithm and SymPy's symbolic computation internals. The solution touches multiple aspects: adding a new helper function, importing expand_mul, and changing the pivot selection logic. While not extremely complex, it requires domain knowledge of both numerical linear algebra and symbolic computation.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a good benchmark issue. The problem is mathematically and algorithmically interesting, involves debugging a complex symbolic computation library, and requires understanding of both the error (NaN comparison) and the underlying algorithm (Bareiss determinant computation). The solution demonstrates knowledge of symbolic mathematics, zero-testing, and proper pivot selection. The test cases provided are clear and verify the fix works for the reported problem cases.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-16792": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some blanks to fill in. It clearly describes the problem: when using cython backend for autowrap, array arguments that don't appear in the wrapped expression cause incorrect C function signature generation (double x instead of double *x). The issue provides a concrete error message (TypeError: only size-1 arrays can be converted to Python scalars) and shows the incorrect C code generated. However, it lacks a complete minimal reproducible example - we don't see the original SymPy code that triggers this issue, only the resulting incorrect C code. An experienced engineer would need to infer how to reproduce the issue and understand the expected behavior from the description and error pattern.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. Looking at the patch, the solution involves understanding the codegen.py module's routine method, specifically how it handles argument sequences and array symbols. The engineer needs to: 1) Understand how the current code processes array arguments and why it fails when arrays don't appear in expressions, 2) Identify that the issue is in the argument processing logic where InputArgument objects need proper metadata for array types, 3) Implement the fix by adding logic to detect IndexedBase and MatrixSymbol types and provide appropriate dimensions metadata. The patch shows this requires understanding SymPy's symbol types, the dimensions helper function, and the metadata system. While not trivial, it's a focused change to one function rather than a complete rewrite.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within the codegen module, has a clear test case that verifies the fix, and the solution is reasonably straightforward once the developer understands how SymPy's code generation works. The test patch provides good validation that the fix works correctly.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17139": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue provides a clear stack trace showing exactly what fails when calling `simplify(cos(x)**I)`. The error message \"Invalid comparison of complex I\" and the traceback pointing to line 504 in fu.py (`if (rv.exp < 0) == True:`) makes it clear that the problem is attempting to compare a complex number (I) with 0. While the issue doesn't explicitly state what the expected behavior should be, an experienced engineer can reasonably infer that the function should handle complex exponents gracefully rather than crashing. The stack trace provides sufficient context about where the problem occurs in the codebase.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that requires 15 minutes to 1 hour. The stack trace clearly points to the problematic line in fu.py where a comparison `rv.exp < 0` fails for complex numbers. An experienced engineer would quickly identify that the issue is trying to compare a complex number with a real number, which is invalid in Python. The solution involves adding a simple guard clause to check if the exponent is real before performing the comparison. The fix is just 2 lines of code and doesn't require deep understanding of the trigonometric simplification algorithms - just basic knowledge of how Python handles complex number comparisons.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue that clearly demonstrates a specific bug with a reproducible example. The stack trace provides excellent debugging information, and the fix is localized to a single function. The issue would work well in a benchmark as it tests the engineer's ability to read stack traces, understand Python's type system (specifically complex vs real number comparisons), and implement appropriate guard clauses.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17318": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It provides a clear error traceback showing an IndexError in sqrtdenest function, specifically \"IndexError: tuple index out of range\" occurring in _split_gcd when accessing a[0]. The expected behavior is clearly stated: \"If an expression cannot be denested it should be returned unchanged.\" The traceback shows the exact call path through multiple functions (sqrtdenest -> _sqrtdenest0 -> _sqrtdenest1 -> _sqrt_match -> split_surds -> _split_gcd), making it clear where the error occurs and what needs to be fixed.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix. While the error location is clear from the traceback, solving it requires: (1) Understanding the mathematical context of square root denesting algorithms, (2) Tracing through the call stack to understand why split_surds is called with empty arguments, (3) Understanding the condition in _sqrt_match that determines when to call split_surds, and (4) Identifying that the fix requires checking both is_Rational and is_positive conditions. The actual code change is small but requires mathematical understanding and careful analysis of the algorithm flow.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No other major issues. The problem is well-defined with clear error reproduction, expected behavior, and the provided test cases help clarify the expected functionality. The issue requires domain knowledge but is suitable for evaluation purposes.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17630": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps that require inference. The core problem is clear: when multiplying BlockMatrix objects containing ZeroMatrix blocks multiple times, an AttributeError occurs because 'Zero' objects don't have a 'cols' attribute. The issue provides specific error tracebacks showing the problem occurs in the colblocksizes property when accessing self.blocks[0, i].cols. The user identifies that zeros in the result are 'Zero' objects instead of 'ZeroMatrix' objects. However, the issue lacks a complete reproducible example - we can see the error tracebacks but don't have the exact code that creates the BlockMatrix 'b'. An experienced engineer would need to infer the setup and create test cases, but the core requirements (fix the AttributeError and ensure proper ZeroMatrix handling) are clear enough to work with.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The problem requires understanding SymPy's matrix expression system, specifically how BlockMatrix multiplication works and the difference between Zero and ZeroMatrix objects. The developer needs to trace through the _blockmul method and colblocksizes property to understand why Zero objects are being created instead of ZeroMatrix objects. Looking at the patch, the solution involves modifying the _postprocessor function in matexpr.py to handle MatAdd cases properly, ensuring ZeroMatrix objects are preserved. This requires understanding the matrix expression framework and how different matrix types should be handled during operations. While not trivial, it's a focused fix that doesn't require massive code changes.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined enough for a coding benchmark, with clear error conditions and expected behavior. The test patch provides good validation criteria.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-17655": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is well-specified. It clearly demonstrates a problem where `point1 + sympy.sympify(2.0) * point2` raises a GeometryError, while the equivalent operation `point1 + 2.0 * point2` (implied to work from context) should give the same result. The issue includes a complete stack trace showing the error occurs in the Point.__add__ method when trying to handle a Mul object (2.0*Point2D). The expected behavior is clearly stated: both expressions should produce the same result. The problem is specifically about missing support for right multiplication (rmul) of Point objects, which is evident from the error message and the provided patch that adds __rmul__ method.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is a classic missing magic method problem in Python. An experienced engineer would quickly identify that the error occurs because Point supports left multiplication (point * number) via __mul__ but not right multiplication (number * point) which requires __rmul__. The solution is straightforward: implement __rmul__ to delegate to __mul__. The patch shows this is exactly a 4-line addition. Understanding the problem from the stack trace, identifying the missing magic method, and implementing the simple delegation would take under an hour for someone familiar with Python's data model.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained problem with a clear, minimal solution that demonstrates good understanding of Python's magic methods and operator overloading. The test cases appropriately verify both 2D and 3D point multiplication works in both directions.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-18211": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue description is somewhat brief but provides enough context to understand the problem. It states that `solveset` raises `NotImplementedError` but should return a `ConditionSet` instead. While the issue doesn't provide specific examples of failing cases, the gold patch and test files clarify that this relates to the `_eval_as_set` method in relational.py. The patch shows that when `solve_univariate_inequality` raises `NotImplementedError`, the code should catch this exception and return a `ConditionSet` instead of letting the exception propagate. This is a reasonable interpretation that an experienced engineer could arrive at, especially given the context that this is about handling unsolvable equations/inequalities more gracefully.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The solution involves adding a try-catch block around an existing function call and returning a ConditionSet when NotImplementedError is raised. The patch is small (only about 8 lines of actual code changes) and the logic is straightforward: catch the exception and provide a fallback return value. An experienced engineer would need some time to understand the codebase structure, locate the relevant method (_eval_as_set in relational.py), understand what ConditionSet is and how to construct it properly, but the actual implementation is quite simple once the approach is clear.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-defined in the context of exception handling and the solution is testable. The test cases show clear expected behavior that can be verified.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20428": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It clearly identifies that the problem is with polynomials that should evaluate as zero but don't behave properly due to unstripped leading zeros in their DMP representation. The issue provides concrete examples of methods that fail (terms_gcd(), primitive()) with specific error traces. However, the connection between the title mentioning \"clear_denoms()\" and the actual fix in the expression domain's __bool__ method requires some inference. The root cause is identified (unstripped leading 0), and the desired outcome is clear (DMP should be properly stripped), but an engineer would need to trace through the codebase to understand how the expression domain's boolean evaluation relates to this stripping issue.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This appears to be a 15 min - 1 hour fix. The actual code change is minimal (one line), but it requires understanding the relationship between polynomial boolean evaluation and the stripping behavior. The error traces provide good clues about where the problem manifests, and the comparison between DMP([EX(0)], EX, None) vs DMP([], EX, None) gives a clear hint about what needs to be fixed. An experienced engineer would need to trace through the polynomial representation code to understand how __bool__ method affects stripping, but the fix itself is straightforward once the root cause is identified.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "The issue is suitable for benchmarking. While there's a slight disconnect between the title mentioning clear_denoms() and the actual fix being in the expression domain's __bool__ method, the issue provides sufficient information for an engineer to trace the problem. The error examples and stack traces give good starting points for investigation, and the root cause explanation about unstripped leading zeros provides the key insight needed to arrive at the solution.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20438": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue provides a clear error traceback showing that a 'Complement' object lacks an 'equals' method, causing an AttributeError in the simplification process at line 308 in relational.py. While the title mentions \"is_subset gives wrong results\", the actual error is about a missing method during simplification. The traceback clearly shows the failing code path through simplify() -> _eval_simplify() -> equals(). However, there's some ambiguity because the title suggests incorrect subset behavior but the traceback shows a missing method error. An experienced engineer could reasonably interpret this as needing to handle cases where objects don't have an 'equals' method during simplification.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This issue requires 1-4 hours because: (1) The engineer needs to understand the SymPy codebase structure, particularly the relational module and set operations; (2) The solution involves multiple files (relational.py, comparison.py, issubset.py) and requires understanding the interaction between different set types and mathematical operations; (3) The fix requires adding type checks to prevent calling methods on objects that don't support them, and implementing proper subset logic for ProductSet/FiniteSet combinations; (4) The engineer needs to understand the broader context of how simplification works in SymPy and ensure the fix doesn't break other functionality. While not extremely complex, it requires solid understanding of the codebase architecture and careful implementation across multiple components.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues identified. The traceback provides sufficient technical detail for an experienced engineer to locate and understand the problem, even though there's some disconnect between the title and the actual error. The test cases help clarify the expected behavior for subset operations.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-20590": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified. It clearly identifies the problem: Symbol instances in version 1.7 have __dict__ when they shouldn't, and this changed from version 1.6.2. The reporter correctly hypothesizes that this is related to __slots__ not being defined in a parent class. However, there are some gaps - it doesn't specify exactly which parent class is the culprit or provide a minimal reproduction case beyond the simple `sympy.Symbol('s').__dict__` example. An experienced engineer would need to investigate the Symbol class hierarchy to identify where __slots__ should be added, but the core issue and expected behavior are clear enough to work with.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15 min - 1 hour fix. The issue is straightforward once you understand the problem: a mixin class (Printable) that's used in the Symbol inheritance hierarchy is missing __slots__ definition. The solution is simple - add `__slots__ = ()` to the Printable class. The main time would be spent: (1) reproducing the issue with Symbol('s').__dict__, (2) tracing through the Symbol class hierarchy to identify that Printable is the mixin causing the issue, and (3) understanding that empty __slots__ in mixins prevents __dict__ creation. The actual code change is trivial - just adding one line.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "This is a well-contained issue with a clear problem statement and a straightforward solution. The fix involves adding empty __slots__ to a mixin class, which is a standard Python pattern. The issue demonstrates good understanding of __slots__ behavior and the reporter's hypothesis about the root cause is correct.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-21379": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The reporter clearly describes the problem: a PolynomialError occurs when using subs() on expressions involving hyperbolic functions (cosh, tanh, sinh) with piecewise arguments. They provide specific conditions under which the error occurs and doesn't occur, which helps narrow down the problem. However, the issue lacks a complete minimal reproducible example - they mention the error message and conditions but don't show the exact code that triggers it. The gold patch and test cases help fill this gap by showing the actual problematic expressions. While an experienced engineer could work with this description, they would need to do some detective work to construct the exact failing case from the clues provided.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour difficulty issue. The problem involves understanding the interaction between symbolic math operations (gcd calculation), piecewise functions, and hyperbolic trigonometric functions in SymPy. The solution requires: 1) Reproducing the exact error condition from the partial description, 2) Understanding why gcd() fails with piecewise generators in this specific context, 3) Implementing a try-catch block to handle the PolynomialError gracefully. The actual code change is small (adding a try-except block), but figuring out where and how to apply it requires understanding SymPy's internal architecture, particularly the mod.py module and polynomial handling. The engineer would need to trace through the code to understand why gcd() is being called and why it fails with piecewise expressions.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. The problem is well-contained within SymPy's symbolic math domain, and the solution approach is reasonable. The test cases in the patch provide good coverage of the edge cases described in the issue. While the issue description could be more complete with a full reproducible example, the provided information combined with the error message gives sufficient direction for an experienced engineer to work with.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-22714": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but has some gaps. The title mentions that \"simpify gives `Imaginary coordinates are not permitted.` with `evaluate(False)`\" and the description states that \"`with evaluate(False)` crashes unexpectedly with `Point2D`\". The error traceback clearly shows the problem occurs in the Point class constructor when checking for imaginary coordinates. However, the issue doesn't provide the exact code that triggers this error or explain why this should work with `evaluate(False)`. From the context and the provided patches, it appears the problem is that when `evaluate(False)` is used, imaginary parts might not be fully evaluated, causing the check `im(a)` to return a non-zero symbolic expression rather than zero, leading to a false positive for imaginary coordinates. While an experienced engineer could reasonably infer this from the error trace and understand that the solution involves making the imaginary coordinate check more robust when evaluation is disabled, some details need to be filled in.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a relatively straightforward fix that should take 15 minutes to 1 hour. The error traceback clearly points to line 155 in sympy/geometry/point.py where the imaginary coordinate check occurs. The issue is that `im(a)` returns a symbolic expression when `evaluate(False)` is active, and the current check `im(a)` evaluates to truthy even when the imaginary part is actually zero. The solution shown in the patch is simple and logical: change `im(a)` to `im(a).is_zero is False` to properly handle the case where `im(a)` returns a symbolic zero. An experienced engineer familiar with SymPy's evaluation system would likely identify this pattern quickly, as it's a common issue when dealing with symbolic expressions that may or may not be evaluated. The fix requires changing only one line of code and understanding the difference between symbolic and evaluated expressions in SymPy.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No significant issues. The problem is well-contained, the solution is elegant and targeted, and the test case clearly demonstrates the expected behavior. This is a good example of a bug that occurs due to the interaction between SymPy's evaluation system and coordinate validation logic.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-23824": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: the kahane_simplify() function in sympy's physics.hep module incorrectly reverses the order of leading uncontracted gamma matrices. The issue provides a specific mathematical example showing both the expected behavior (\u03b3\u1d56 \u03b3\u209a \u03b3\u1d56 \u03b3\u1d48 should simplify to 4\u03b3\u1d56 \u03b3\u1d48) and the current incorrect behavior where the order gets flipped in certain cases. The issue includes a concrete test case that fails with an AssertionError, and even identifies the root cause of the bug: \"the leading matrices are removed at the beginning of the function and then inserted at the start of the product at the end of the function, and the insertion loop is just backward.\" The mathematical context is clear, the function name is specified (kahane_simplify()), and the expected vs actual behavior is well-documented. An experienced engineer would have sufficient information to understand what needs to be fixed.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a 15-minute to 1-hour fix. The issue author has already identified the root cause very precisely - the insertion loop is backward when reinserting leading matrices. Looking at the gold patch, the fix is indeed straightforward: replacing a loop that inserts elements in reverse order with a single line that concatenates the free positions correctly. The change is only 2 lines (removing the loop and replacing with list comprehension), and the logic is simple once you understand that the order needs to be preserved rather than reversed. An experienced engineer familiar with the codebase would need some time to understand the gamma matrix simplification algorithm, but with the clear problem description and root cause identified, implementing the fix would be relatively quick.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, good test coverage, and a straightforward solution. The mathematical domain knowledge required (gamma matrices) is explained sufficiently in the issue description, and the problem is isolated to a specific function with a clear behavioral expectation.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-23950": {
            "q1_1_is_well_specified": 0,
            "q1_2_explanation": "The issue is very well-specified. It clearly describes the problem: Contains.as_set() incorrectly returns a Contains object instead of a set, which causes failures because Contains objects lack the as_relational method. The issue provides a concrete error example with a full traceback showing how this manifests in the Piecewise function when calling c.as_set().as_relational(x). The expected behavior is implicit but clear from context - as_set() should return the actual set (the second argument of Contains) rather than the Contains object itself. The problem statement is unambiguous and provides sufficient information for an engineer to understand both what is wrong and what the correct behavior should be.",
            "q2_1_difficulty": 1,
            "q2_2_explanation": "This is a simple fix that should take 15 minutes to 1 hour. The problem is clearly identified in the issue description, and the solution is straightforward: change the as_set() method in Contains class to return self.args[1] (the set argument) instead of raising NotImplementedError. The actual code change is just one line, but it requires a few minutes to understand the codebase structure, locate the Contains class in sympy/sets/contains.py, understand what self.args[1] represents, and write appropriate tests. The fix is conceptually simple but requires some basic understanding of the SymPy codebase structure.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. This is a well-contained bug fix with clear requirements, a simple solution, and straightforward test cases. The issue provides good context and the fix doesn't appear to have complex side effects or require deep domain knowledge beyond basic understanding of the Contains class structure.",
            "q2_5_confidence": 5
        }
    },
    {
        "sympy__sympy-24066": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is mostly well-specified but requires some domain knowledge to fully understand. The error message clearly shows that SI._collect_factor_and_dimension() is incorrectly handling exponential functions - it should recognize that exp() of any dimensionless quantity must return a dimensionless result (Dimension(1)), but instead it's returning a dimension with units. However, an engineer would need to understand: (1) the physics/units system context, (2) that exponential functions must always be dimensionless in dimensional analysis, and (3) the structure of the _collect_factor_and_dimension method. The error message provides a concrete example and expected behavior, making the core problem identifiable, though the solution approach requires understanding the codebase structure.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding the dimensional analysis system and why exponential functions must be dimensionless, (2) The engineer needs to locate and understand the _collect_factor_and_dimension method logic, particularly the Function case handling, (3) The solution involves modifying the algorithm to check if dimensions are dimensionless using the dimension system's is_dimensionless method, (4) Writing appropriate tests requires understanding the units framework. While the actual code change is relatively small (a few lines), the conceptual understanding and proper implementation requires significant analysis of the existing system.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major additional issues. This is a well-contained bug in a specific mathematical/physics computation that has clear expected behavior and a concrete test case. The domain (dimensional analysis in physics) is specialized but the programming concepts are straightforward once the domain knowledge is understood.",
            "q2_5_confidence": 4
        }
    },
    {
        "sympy__sympy-24443": {
            "q1_1_is_well_specified": 1,
            "q1_2_explanation": "The issue is reasonably well-specified but requires some interpretation. It clearly identifies the problem: `_check_homomorphism` is broken on PermutationGroups, with a specific ValueError being raised. The issue points to the exact location in the code (line 336-337 in homomorphisms.py) and explains the root cause: when `r[i]` is an inverted generator, the `in gens` test fails. However, the issue description is somewhat technical and assumes familiarity with group theory concepts (homomorphisms, generators, PermutationGroups). An experienced engineer familiar with the SymPy codebase could understand what needs to be fixed, though they would need to investigate the specific implementation details to craft a solution.",
            "q2_1_difficulty": 2,
            "q2_2_explanation": "This is a 1-4 hour fix because: (1) It requires understanding complex mathematical concepts (group homomorphisms, generators, relators) and how they're implemented in SymPy; (2) The engineer needs to trace through the `_check_homomorphism` function and understand why the current `_image()` implementation fails for inverted generators; (3) Looking at the solution patch, it involves a substantial rewrite of the `_image()` function logic, simplifying the complex array-form processing and replacing it with a cleaner approach using symbol-to-generator mapping; (4) The fix touches core algorithmic logic rather than being a simple bug fix, requiring careful consideration of edge cases and mathematical correctness; (5) While not >100 lines, it's a significant conceptual change that requires domain expertise.",
            "q2_3_other_issues": 0,
            "q2_4_other_notes": "No major issues. The problem is well-contained within the combinatorics module, has a clear test case that demonstrates the fix works, and the solution approach (simplifying the _image function) is reasonable. The mathematical domain knowledge required is typical for SymPy contributions.",
            "q2_5_confidence": 4
        }
    }
]